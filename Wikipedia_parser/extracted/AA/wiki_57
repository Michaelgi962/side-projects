<doc id="4452" url="http://en.wikipedia.org/wiki?curid=4452" title="Book of Micah">
Book of Micah

The Book of Micah is a prophetic book in the TanakhOld Testament, and the sixth of the twelve minor prophets. It records the sayings of Micah, "Mikayahu", meaning "Who is like Yahweh?", an 8th-century B.C. prophet from the village of Moresheth in Judah. The book has three major divisions, chapters 1–2, 3–5 and 6–7, each introduced by the word "Hear," with a pattern of alternating announcements of doom and expressions of hope within each division. Micah reproaches unjust leaders, defends the rights of the poor against the rich and powerful, and preaches social justice; while looking forward to a world at peace centered on Zion under the leadership of a new Davidic monarch.
While the book is relatively short it includes lament (1.8–16; 7.8–10), theophany (1.3–4), hymnic prayer of petition and confidence (7.14–20), and the "covenant lawsuit" (6.1–8), a distinct genre in which Yahweh (God) sues Israel for breach of contract, that is, for violation of the Sinai covenant.
It has been noticed as remarkable that this book commences with the last words of another prophet with similar name, “Micaiah the son of Imlah” (1 Kings 22:28): “Hearken, O people, every one of you.” (the three Hebrew words are exactly the same as the beginning of Micah 1:2).
Content.
Structure.
At the broadest level Micah can be divided into three roughly equal parts:
Within this broad three-part structure are a series of alternating oracles of judgment and promises of restoration:
Setting and composition.
Setting.
Chapter 1:1 identifies the prophet as "Micah of Moresheth" (a town in southern Judah), and states that he lived during the reigns of Yehotam, Ahaz and Hezekiah, roughly 750–700 BC.
Micah's career corresponds to the period when, after a long period of peace, Israel, Judah, and the other nations of the region came under increasing pressure from the aggressive and rapidly expanding Assyrian empire. Between 734 and 727 Tiglath-Pileser III of Assyria conducted almost annual campaigns in Palestine, reducing Israel, Judah and the Philistine cities to vassalage, receiving tribute from Ammon, Moab and Edom, and absorbing Damascus (the kingdom of Aram) into the Assyrian empire. On Tiglath-Pileser's death Israel rebelled, resulting in an Assyrian counter-attack and the destruction of the capital, Samaria, in 721 after a three-year siege. Micah 1:2–7 draws on this event: Samaria, says the prophet, has been destroyed by God because of its crimes of idolatry, oppression of the poor, and misuse of power. The Assyrian attacks on Israel (the northern kingdom) led to an influx of refugees into Judah, which would have increased social stresses, while at the same time the authorities in Jerusalem had to invest huge amounts in tribute and defense.
When the Assyrians attacked Judah in 701 they did so via the Philistine coast and the Shephelah, the border region which included Micah's village of Moresheth, as well as Lachish, Judah's second largest city. This in turn forms the background to verses 1:8–16, in which Micah warns the towns of the coming disaster (Lachish is singled out for special mention, accused of the corrupt practices of both Samaria and Jerusalem). In verses 2:1–5 he denounces the appropriation of land and houses, which might simply be the greed of the wealthy and powerful, or possibly the result of the militarising of the area in preparation for the Assyrian attack.
Composition.
It is widely, though not universally, accepted by scholars that only chapters 1–3 contain material from the late 8th century prophet Micah. The latest material comes from the post-Exilic period after the Temple was rebuilt in 515 BC, so that the early 5th century BC seems to be the period when the book was completed. The first stage was the collection and arrangement of some spoken sayings of the historical Micah (the material in chapters 1–3), in which the prophet attacks those who build estates through oppression and depicts the Assyrian invasion of Judah as Yahweh's punishment on the kingdom's corrupt rulers, including a prophecy that the Temple will be destroyed. The prophecy was not fulfilled in Micah's time, but a hundred years later when Judah was facing a similar crisis with Babylon, and Micah's prophecies were reworked and expanded to reflect the new situation. Still later, after Jerusalem did fall to the Babylonians, the book was revised and expanded further to reflect the circumstances of the late exilic and post-exilic community.
Themes.
Micah addresses the future of Judah/Israel after the Babylonian exile. Like Isaiah, the book has a vision of the punishment of Israel and creation of a "remnant", followed by world peace centred on Zion under the leadership of a new Davidic monarch; the people should do justice, turn to Yahweh, and await the end of their punishment. However, whereas Isaiah sees Jacob/Israel joining "the nations" under Yahweh's rule, Micah looks forward to Israel ruling over the nations. Insofar as Micah appears to draw on and rework parts of Isaiah, it seems designed at least partly to provide a counterpoint to that book.

</doc>
<doc id="4453" url="http://en.wikipedia.org/wiki?curid=4453" title="Book of Nahum">
Book of Nahum

The Book of Nahum is the seventh book of the 12 minor prophets of the Hebrew Bible. It is attributed to the prophet Nahum, and was probably written in Jerusalem in the 7th century BC.
Background.
Nahum prophesied, according to some, in the beginning of the reign of Ahaz (740s BC). Others, however, think that his prophecies are to be referred to the latter half of the reign of Hezekiah (8th century BC). Probably the book was written in Jerusalem, where he witnessed the invasion of Sennacherib and the destruction of his host (2 Kings 19:35). And still others support the idea that the "book of vision" was written shortly before the fall of Nineveh at the hands of the Medes and Babylonians (612 BC). This theory is evidenced by the fact that the oracles must be dated after the Assyrian destruction of Thebes in 663 BC as this event is mentioned in Nahum 3:8.
Author.
Little is known about Nahum’s personal history. His name means "comforter," and he was from the town of Alqosh, (Nahum 1:1) which scholars have attempted to identify with several cities, including the modern `Alqush of Assyria and Capharnaum of northern Galilee. He was a very nationalistic Hebrew, and lived amongst the Elkoshites in peace. One account suggests that his writings are a prophecy written in about 615 BC, just before the downfall of Assyria, while another account suggests that he wrote this passage as liturgy just after its downfall in 612 BC.
Historical context.
The subject of Nahum's prophecy is the approaching complete and final destruction of Nineveh, the capital of the great and at that time flourishing Assyrian empire. Assur-bani-pal was at the height of his glory. Nineveh was a city of vast extent, and was then the center of the civilization and commerce of the world, a "bloody city all full of lies and robbery" (Nahum 3:1), for it had robbed and plundered all the neighboring nations. It was strongly fortified on every side, bidding defiance to every enemy. One popular verse is 3:5, "Behold, I am against thee, saith YHWH of hosts, and I will uncover thy skirts upon thy face; and I will show the nations thy nakedness, and the kingdoms thy shame." This is very symbolic showing that Nineveh was known for being a city full of prostitutes.
Jonah had already uttered his message of warning, and Nahum was followed by Zephaniah, who also predicted (Zephaniah 2:4–15) the destruction of the city. 
Nineveh was destroyed apparently by fire around 625 BC, and the Assyrian empire came to an end, an event which changed the face of Asia.
Archaeological digs have uncovered the splendor of Nineveh in its zenith under Sennacherib (705–681 BC), Esarhaddon (681–669 BC), and Ashurbanipal (669–633 BC). Massive walls were eight miles in circumference. It had a water aqueduct, palaces and a library with 20,000 clay tablets, including accounts of a creation in Enuma Elish and a flood in the Epic of Gilgamesh.
The Babylonian chronicle of the fall of Nineveh tells the story of the end of Nineveh. Naboplassar of Babylon joined forces with Cyaxares, king of the Medes, and laid siege for three months.
Assyria lasted a few more years after the loss of its fortress, but attempts by Egyptian Pharaoh Neco II to rally the Assyrians failed due to opposition from king Josiah of Judah, and it seemed to be all over by 609 BC.
Overview.
The book of Nahum consists of two parts:
Chapter one shows the majesty and might of God the LORD in goodness and severity. 
Chapters two and three describe the fall of Nineveh in 612 BCE. Nineveh is compared to Thebes, the Egyptian city that Assyria itself had destroyed in 663 BCE. Nahum describes the siege and frenzied activity of Nineveh’s troops as they try in vain to halt the invaders. Poetically, he becomes a participant in the battle, and with subtle irony, barks battle commands to the defenders. Nahum uses numerous similes and metaphors. Nineveh is ironically compared with a lion, in reference to the lion as an Assyrian symbol of power; Nineveh is the lion of strength that has a den full of dead prey but will become weak like the lion hiding in its den. It comes to conclusion with a taunt song and funeral dirge of the impending destruction of Nineveh and the "sleep" or death of the Assyrian people and demise of the once great Assyrian conqueror-rulers.
Themes.
The fall of Nineveh.
Nahum’s prophecy carries a particular warning to the Ninevites of coming events, although he is partly in favor of the destruction. One might even say that the book of Nahum is "a celebration of the fall of Assyria." And this is not just a warning or speaking positively of the destruction of Ninevah, it is also a positive encouragement and "message of comfort for Israel, Judah, and others who had experienced the "endless cruelty" (3:19) of the Assyrians." The prophet Jonah shows us where God shows concern for the people of Nineveh, while Nahum’s writing testifies to his belief in the righteousness/justice of God and how God dealt with those Assyrians in punishment according to "their cruelty" (Nahum 3:19). The Assyrians had been used as God's "rod of [...] anger, and the staff in their hand indignation." (Isaiah 10:5)
The nature of God.
From its opening, Nahum shows God to be slow to anger but that He will by no means clear the guilty, but will bring his vengeance and wrath to pass. God is presented as a God who will punish evil but will protect those who trust in Him. The opening passage (Nahum 1:2–3) states: "God is jealous, and the LORD revengeth; the LORD revengeth, and is furious; the LORD will take vengeance on his adversaries, and he reserveth wrath for his enemies. The LORD is slow to anger, and great in power, and will not at all acquit the wicked". God is strong and will use means, but a mighty God doesn't need anyone else to carry out vengeance and wrath for him.
Nahum 1:3 (NIV) "The LORD is slow to anger and Quick to love; the LORD will not leave the guilty unpunished."
Nahum 1:7 (NIV) "The LORD is good, a refuge in times of trouble. He cares for those who trust in him"
Importance.
God's judgement on Ninevah is "all because of the wanton lust of a harlot, alluring, the mistress of sorceries, who enslaved nations by her prostitution and peoples by her witchcraft." (Nahum 3:4 NIV). Sexual infidelity, according to the prophets, related to spiritual unfaithfulness. For example: "the land is guilty of the vilest adultery in departing from the LORD."(Hosea 1:2 NIV) The apostle John used a similar analogy in Revelation chapter 17.
Discourse.
The book was introduced in Calvin's Commentary as a complete and finished poem:
Nahum, taking words from Moses himself, have shown in a general way what sort of "Being God is". The Reformation theologian Calvin argued, Nahum painted God by which his nature must be seen, and "it is from that most memorable vision, when God appeared to Moses after the breaking of the tables." 
Nahum's writings could be taken as prophecy or as history. One account suggests that his writings are a prophecy written in about 615 BC, just before the downfall of Assyria, while another account suggests that he wrote this passage as liturgy just after its downfall in 612 BC. 
The book could be seen as an allusion to the history as described by Moses; for the minor Prophets, in promising God’s assistance to his people, must often remind how God in a miraculous manner brought up the Jews from Egypt.

</doc>
<doc id="4454" url="http://en.wikipedia.org/wiki?curid=4454" title="Book of Haggai">
Book of Haggai

The Book of Haggai is a book of the Hebrew Bible or Tanakh, and has its place as the antepenultimate of the Minor Prophets. It is a short book, consisting of only two chapters. The historical setting dates around 520 BCE before the Temple has been rebuilt. 520 BCE falls between the start of the Persian empire in 539 BCE and 520 BCE a period that saw major kings such as Zerubbabel help lead the Jews in their return to the land.
Authorship.
The Book of Haggai is named after its presumed author, the prophet Haggai. There is no biographical information given about the prophet in the Book of Haggai, so we know no personal information about him. Haggai's name is derived from the Hebrew verbal root "hgg", which means "to make a pilgrimage." W. Sibley Towner suggests that Haggai's name might come "from his single-minded effort to bring about the reconstruction of that destination of ancient Judean pilgrims, the Temple in Jerusalem." 
Date.
The "Book of Haggai" was written in 520 BCE some 18 years after Cyrus had conquered Babylon and issued a decree in 538 BCE allowing the captive Jews to return to Judea. Cyrus saw the restoration of the temple as necessary for the restoration of the religious practices and a sense of peoplehood after a long exile.
Synopsis.
Haggai's message is filled with an urgency for the people to proceed with the rebuilding of the second Jerusalem temple. Haggai attributes a recent drought to the people's refusal to rebuild the temple, which he sees as key to Jerusalem’s glory. The book ends with the prediction of the downfall of kingdoms, with one Zerubbabel, governor of Judah, as the Lord’s chosen leader. The language here is not as finely wrought as in some other books of the minor prophets, yet the intent seems straightforward.
The first chapter contains the first address (2–11) and its effects (12–15). 
The second chapter contains:
These discourses are referred to in Ezra 5:1 and 6:14. (Compare Haggai 2:7, 8 and 22)
Haggai reports that three weeks after his first prophecy, the rebuilding of the Temple began on September 7 521 BCE. "They came and began to work on the house of the LORD Almighty, their God, on the twenty-fourth day of the sixth month in the second year of King Darius.(Haggai 1:14–15) and the Book of Ezra indicates that it was finished on February 25 516 BCE "The Temple was completed on the third day of the month Adar, in the sixth year of the reign of King Darius." (Ezra 6:15)

</doc>
<doc id="4455" url="http://en.wikipedia.org/wiki?curid=4455" title="Book of Malachi">
Book of Malachi

Malachi (or Malachias, , Malʾaḫi, Mál'akhî) is the last book of the Neviim contained in the Tanakh, the last of the twelve minor prophets (canonically) and the final book of the Neviim. In the Christian ordering, the grouping of the Prophetic Books is the last section of the Old Testament, making Malachi the last book before the New Testament.
The book is commonly attributed to a prophet by the name of Malachi. Although the appellation Malachi has frequently been understood as a proper name, its Hebrew meaning is simply "My God's messenger" (or 'His messenger' in the Septuagint) and may not be the author's name at all. The sobriquet occurs in the superscription at 1:1 and in 3:1, although it is highly unlikely that the word refers to the same character in both of these references. Thus, there is substantial debate regarding the identity of the book's author. One of the Targums identifies Ezra (or Esdras) as the author of Malachi. St. Jerome suggests this may be because Ezra is seen as an intermediary between the prophets and the 'great synagogue'. There is, however, no historical evidence yet to support this claim.
Some scholars note affinities between Zechariah 9–14 and the book of Malachi. Zechariah 9, Zechariah 12, and Malachi 1 are all introduced as The word of Elohim. Many scholars argue that this collection originally consisted of three independent and anonymous prophecies, two of which were subsequently appended to the book of Zechariah (as what scholars refer to as Deutero-Zechariah) with the third becoming the book of Malachi. As a result, most scholars consider the book of Malachi to be the work of a single author who may or may not have been identified by the title Malachi. The present division of the oracles results in a total of twelve books of minor prophets—a number parallelling the sons of Jacob who became the heads of the twelve tribes of Israel. The Catholic Encyclopedia asserts that "We are no doubt in presence of an abbreviation of the name Mál'akhîyah, that is Messenger of Elohim.
Author.
Nothing is known of the biography of the author of the book of Malachi, although it has been suggested that he may have been Levitical (which is curious, considering that Ezra was a priest). The books of Zechariah and Haggai were written during the lifetime of Ezra (see 5:1); perhaps this may explain the similarities in style. Although the Ezra theory is disputed, it remains the dominant authorship theory.
According to the editors of the 1897 Easton's Bible Dictionary, some scholars believe the name "Malachi" is not a proper noun but rather an abbreviation of "messenger of YHWH". This reading could be based on Malachi 3:1, "Behold, I will send "my messenger"...", if "my messenger" is taken literally as the name "Malachi". Several scholars consider the book to be anonymous, with verse 1:1 being a later addition. However, other scholars, including the editors of the "Catholic Encyclopedia", argue that the grammatical evidence leads us to conclude that Malachi is in fact a name.
Another interpretation of the authorship comes from the Septuagint superscription, ὲν χειρὶ ἀγγήλου αὐτοῦ, which can be read as either "by the hand of his messenger" or as "by the hand of his angel". The "angel" reading found an echo among the ancient Leaders of the Church and ecclesiastical writers, and even gave rise to the "strangest fancies", especially among the disciples of Origen of Alexandria.
Period.
There are very few historical details in the book of Malachi. The greatest clue as to its dating may lie in the fact that the Persian-era term for governor (pehâ) is used in 1:8. This points to a post-exilic date of composition both because of the use of the Persian period term and because Judah had a king before the exile. Since, in the same verse, the temple has been rebuilt, the book must also be later than 515 BC. Malachi was apparently known to the author of Ecclesiasticus early in the 2nd century BC. Because of the development of themes in the book of Malachi, most scholars assign it to a position between Haggai and Zechariah, slightly before Nehemiah came to Jerusalem in 445 BC.
Aim.
The book of Malachi was written to correct the lax religious and social behaviour of the Israelites – particularly the priests – in post-exilic Jerusalem. Although the prophets urged the people of Judah and Israel to see their exile as punishment for failing to uphold their covenant with Elohim, it was not long after they had been restored to the land and to Temple worship that the people's commitment to their God began, once again, to wane. It was in this context that the prophet commonly referred to as Malachi delivered his prophecy.
In 1:2, Malachi has the people of Israel question God's love for them. This introduction to the book illustrates the severity of the situation which Malachi addresses. The graveness of the situation is also indicated by the dialectical style with which Malachi confronts his audience. Malachi proceeds to accuse his audience of failing to respect God as God deserves. One way in which this disrespect is made manifest is through the substandard sacrifices which Malachi claims are being offered by the priests. While Elohim demands animals that are "without blemish" (Leviticus 1:3, NRSV), the priests, who were "to determine whether the animal was acceptable" (Mason 143), were offering blind, lame and sick animals for sacrifice because they thought nobody would notice.
In 2:10, Malachi addresses the issue of divorce. On this topic, Malachi deals with divorce both as a social problem ("Why then are we faithless to one another ... ?" 2:10) and as a religious problem ("Judah ... has married the daughter of a foreign god" 2:11). In contrast to the book of Ezra (or not – see section below), Malachi urges each to remain steadfast to the wife of his youth.
Malachi also criticizes his audience for questioning God's justice. He reminds them that God is just, exhorting them to be faithful as they await that justice. Malachi quickly goes on to point out that the people have not been faithful. In fact, the people are not giving God all that God deserves. Just as the priests have been offering unacceptable sacrifices, so the people have been neglecting to offer their full tithe to Elohim. The result of these shortcomings is that the people come to believe that no good comes out of serving God.
Malachi assures the faithful among his audience that in the eschaton, the differences between those who served God faithfully and those who did not will become clear. The book concludes by calling upon the teachings of Moses and by promising that Elijah will return prior to the Day of Elohim.
Interpretations.
The book of Malachi is divided into three chapters in the Hebrew Bible and the Greek Septuagint and four chapters in the Latin Vulgate. The fourth chapter in the Vulgate consists of the remainder of the third chapter starting at verse 3:19.
Christianity.
The New Revised Standard Version of the Bible supplies headings for the book as follows:
The majority of scholars consider the book to be made up of six distinct oracles. According to this scheme, the book of Malachi consists of a series of disputes between Yahweh and the various groups within the Israelite community. In the course of the book's three or four chapters, Yahweh is vindicated while those who do not adhere to the law of Moses are condemned. Some scholars have suggested that the book, as a whole, is structured along the lines of a judicial trial, a suzerain treaty or a covenant—one of the major themes throughout the Hebrew Scriptures. Implicit in the prophet's condemnation of Israel's religious practices is a call to keep Yahweh's statutes.
The book of Malachi draws upon various themes found in other books of the Bible. Malachi appeals to the story of the rivalry between Jacob and Esau and of Yahweh's preference for Jacob contained in Book of Genesis 25–28. Malachi reminds his audience that, as descendants of Jacob (Israel), they have been and continue to be favoured by God as God's chosen people. In the second dispute, Malachi draws upon the Levitical Code (e.g. Leviticus 1:3) in condemning the priest for offering unacceptable sacrifices.
In the third dispute (concerning divorce), the author of the book of Malachi likely intends his argument to be understood on two levels. Malachi appears to be attacking either the practice of divorcing Jewish wives in favour of foreign ones (a practice which Ezra vehemently condemns) or, alternatively, Malachi could be condemning the practice of divorcing foreign wives in favour of Jewish wives (a practice which Ezra promoted). Malachi appears adamant that nationality is not a valid reason to terminate a marriage, "For I hate divorce, says the Lord . . ." (2:16).
In many places throughout the Hebrew Scriptures – particularly the book of Hosea – Israel is figured as Yahweh's wife or bride. Malachi's discussion of divorce may also be understood to conform to this metaphor. Malachi could very well be urging his audience not to break faith with Yahweh (the God of Israel) by adopting new gods or idols. It is quite likely that, since the people of Judah were questioning Yahweh's love and justice (1:2, 2:17), they might be tempted to adopt foreign gods. William LaSor suggests that, because the restoration to the land of Judah had not resulted in anything like the prophesied splendor of the messianic age which had been prophesied, the people were becoming quite disillusioned with their religion.
Indeed, the fourth dispute asserts that judgment is coming in the form of a messenger who "is like refiner's fire and like fullers' soap . . ." (3:2). Following this, the prophet provides another example of wrongdoing in the fifth dispute – that is, failing to offer full tithes. In this discussion, Malachi has Yahweh request the people to "Bring the full tithe . . . see if I will not open the windows of heaven for you and pour down on you an overflowing blessing" (3:10). This request offers the opportunity for the people to amend their ways. It also stresses that keeping the Lord's statutes will not only allow the people to avoid God's wrath, but will also lead to God's blessing. In the sixth dispute, the people of Israel illustrate the extent of their disillusionment. Malachi has them say "'It is vain to serve God . . . Now we count the arrogant happy; evildoers not only prosper, but when they put God to the test they escape'" (3:14–15). Once again, Malachi has Yahweh assure the people that the wicked will be punished and the faithful will be rewarded.
In the light of what Malachi understands to be an imminent judgment, he exhorts his audience to "Remember the teaching of my servant Moses, that statutes and ordinances that I commanded him at Horeb for all Israel" (4:4; 3:22, MT). Before the Day of the Lord, Malachi declares that Elijah (who "ascended in a whirlwind into heaven . . . [,]" 2 Kings 2:11) will return to earth in order that people might follow in God's ways.
Primarily because of its messianic promise, the book of Malachi is frequently referred to in the Christian New Testament. What follows is a brief comparison between the book of Malachi and the New Testament texts which refer to it (as suggested in Hill 84–88).
Although many Christians believe that the messianic prophecies of the book of Malachi have been fulfilled in the life, ministry, transfiguration, death and resurrection of Jesus of Nazareth, most Jews continue to await the coming of the prophet Elijah who will prepare the way for the Lord. The Latter-day Saints differ significantly in this regard.

</doc>
<doc id="4456" url="http://en.wikipedia.org/wiki?curid=4456" title="Book of Zechariah">
Book of Zechariah

The Book of Zechariah, attributed to the prophet Zechariah, is included in the Twelve Minor Prophets in the Hebrew Bible and is the penultimate book of the Old Testament of the Christian Bible.
Historical context.
Zechariah’s ministry took place during the reign of Darius the Great (), and was contemporary with Haggai in a post-exilic world after the fall of Jerusalem in 587/6 BC. Ezekiel and Jeremiah wrote prior to the fall of Jerusalem, while continuing to prophesy in the early exile period. Scholars believe Ezekiel, with his blending of ceremony and vision, heavily influenced the visionary works of Zechariah 1–8. Zechariah is specific about dating his writing (520–518 BC). 
During the Exile many Jews were taken to Babylon, where the prophets told them to make their homes (), suggesting they would spend a long period of time there. Eventually freedom did come to many Israelites, when Cyrus the Great overtook the Babylonians in 539 BC. In 538 BC, the famous Edict of Cyrus was released, and the first return took place under Sheshbazzar. After the death of Cyrus in 530 BC, Darius consolidated power and took office in 522 BC. His system divided the different colonies of the empire into easily manageable districts overseen by governors. Zerubbabel comes into the story, appointed by Darius as governor over the district of Yehud Medinata. 
Under the reign of Darius, Zechariah also emerged, centering around the rebuilding of the Temple. Unlike the Babylonians, the Persian Empire went to great lengths to keep “cordial relations” between vassal and lord. The rebuilding of the Temple was encouraged by the leaders of the empire in hopes that it would strengthen the authorities in local contexts. This policy was good politics on the part of the Persians, and the Jews viewed it as a blessing from God.
Prophet.
His name means "Yahweh has remembered." Not much is known about Zechariah’s life other than what may be inferred from the book. It has been speculated that his ancestor Iddo was the head of a priestly family who returned with Zerubbabel (), and that Zechariah may himself have been a priest as well as a prophet. This is supported by Zechariah's interest in the Temple and the priesthood, and from Iddo's preaching in the Books of Chronicles.
Authorship.
Some scholars accept the book as the writings of one individual. For example, George Livingstone Robinson's dissertation on chapters 9–14 concluded that those chapters had their origin in the period between 518 and 516 BC and stand in close relation to chapters 1–8, having most probably been composed by Zechariah himself. However, most modern scholars believe the book of Zechariah was written by at least two different people.
Zechariah 1–8, sometimes referred to as First Zechariah, was written in the 6th century BC. Zechariah 9–14, often called Second Zechariah, contains within the text no datable references to specific events or individuals but most scholars give the text a date in the fifth century BCE. Second Zechariah, in the opinion of some scholars, appears to make use of the books of Isaiah, Jeremiah, and Ezekiel, the Deuteronomistic History, and the themes from First Zechariah. This has led some to believe that the writer(s) or editor(s) of Second Zechariah may have been a disciple of the prophet Zechariah. There are some scholars who go even further and divide Second Zechariah into Second Zechariah (9–11) and Third Zechariah (12–14) since each begins with a heading oracle.
Composition.
The return from exile is the theological premise of the prophet's visions in chapters 1–6. Chapters 7–8 address the quality of life God wants his renewed people to enjoy, containing many encouraging promises to them. Chapters 9–14 comprise two "oracles" of the future. 
Chapters 1 to 6.
The book begins with a preface (), which recalls the nation's history, for the purpose of presenting a solemn warning to the present generation. Then follows a series of eight visions (), succeeding one another in one night, which may be regarded as a symbolical history of Israel, intended to furnish consolation to the returned exiles and stir up hope in their minds. The symbolic action, the crowning of Joshua (), describes how the kingdoms of the world become the kingdom of God's Messiah.
Chapters 7 and 8.
Chapters and , delivered two years later, are an answer to the question whether the days of mourning for the destruction of the city should be kept any longer, and an encouraging address to the people, assuring them of God's presence and blessing.
Chapters 9 to 14.
This section consists of two "oracles" or "burdens": 
Themes.
The purpose of this book is not strictly historical but theological and pastoral. The main emphasis is that God is at work and plans to live again with His people in Jerusalem. He will save them from their enemies and cleanse them from sin. 
Zechariah's concern for purity is apparent in the temple, priesthood and all areas of life as the prophecy gradually eliminates the influence of the governor in favour of the high priest, and the sanctuary becomes ever more clearly the centre of messianic fulfillment. The prominence of prophecy is quite apparent in Zechariah, but it is also true that Zechariah (along with Haggai) allows prophecy to yield to the priesthood; this is particularly apparent in comparing Zechariah to "Third Isaiah" (chapters 55–66 of the Book of Isaiah), whose author was active sometime after the first return from exile.
Most Christian commentators read the series of predictions in chapters 7 to 14 as Messianic prophecies, either directly or indirectly. These chapters helped the writers of the Gospels understand Jesus’ suffering, death and resurrection, which they quoted as they wrote of Jesus’ final days. Much of the Book of Revelation, which narrates the denouement of history, is also colored by images in Zechariah.
Apocalyptic literature.
Chapters 9–14 of the Book of Zechariah are an early example of apocalyptic literature. Although not as fully developed as the apocalyptic visions described in the Book of Daniel, the "oracles", as they are titled in Zechariah 9–14, contain apocalyptic elements. One theme these oracles contain is descriptions of the Day of the Lord, when "the Lord will go forth and fight against those nations as when he fights on a day of battle" (Zechariah 14:3). These chapters also contain "pessimism about the present, but optimism for the future based on the expectation of an ultimate divine victory and the subsequent transformation of the cosmos".
The final word in Zechariah proclaims that on the Day of the Lord "There will be no Canaanite in the house of the Lord of hosts on that day" (14:21), proclaiming the need for purity in the Temple, which would come when God judges at the end of time. The Revised Standard Version has this: "There will be no trader in the house of the Lord of hosts on that day." In the Masoretic Text it is: "and in that day there shall be no more a trafficker in the house of the Lord of hosts."

</doc>
<doc id="4457" url="http://en.wikipedia.org/wiki?curid=4457" title="Book of Zephaniah">
Book of Zephaniah

The superscription of the Book of Zephaniah attributes its authorship to “Zephaniah son of Cushi son of Gedaliah son of Amariah son of Hezekiah, in the days of King Josiah son of Amon of Judah” (1:1, NRSV). All that is known of Zephaniah comes from the text. The superscription of the book is lengthier than most and contains two features. The name Cushi, Zephaniah’s father, means ‘Ethiopian’. In a society where genealogy was considered extremely important because of God's covenant with Abraham and his descendants, the author may have felt compelled to establish his Hebrew lineage. 
The author of Zephaniah does not shrink from condemning the Cushites or Ethiopians. Chapter 2:12 contains a succinct but unequivocal message: “You also, O Ethiopians, / Shall be killed by my sword.” Some question whether the translation "You, also, O Ethiopians / Shall be killed by my sword" is a good translation, given that Ethiopia is a long way from Jerusalem.
As with many of the other prophets, there is no external evidence to directly associate composition of the book with a prophet by the name of Zephaniah. Some believe that much of the material does not date from the days of King Josiah (ca. 640–609 BC), but is actually post-monarchic. Three general possibilities are
Although it is possible that a post-monarchic author assumed the persona of a monarchic prophet to add credibility to his message, there is no evidence to support such a claim.
The prophetic book of the Bible attributed to Zephaniah occurs ninth among the twelve minor prophets, preceded by Habakkuk and followed by Haggai. Zephaniah (or Tzfanya, Sophonias, צפניה, Ẓəfanya, Ṣəp̄anyāh) means 'YHWH conceals', or 'YHWH protects'.
Date.
If the superscription of the book of Zephaniah is a reliable indicator of the time that the bulk of the book was composed, then Zephaniah was a contemporary of the prophet Jeremiah (or Jeremias). King Josiah ruled over Judah from approximately 640–609 BC. Some scholars believe that the picture of Jerusalem which Zephaniah gives indicates that he was active prior to the religious reforms of King Josiah which are described in 2 Kings 23. These reforms took place in 622 BC. Scholars also cite the reference to “the officials and the king’s sons . . .” in 1:8 as evidence that the kingdom was still ruled by a regent for Josiah. The portrait of foreign nations in chapter 2 also indicates the late seventh century.
Probably the first prophet following the prophecies of Isaiah and the violent reign of Manasseh. Both Zephaniah and Jeremiah urged King Josiah to enact religious reforms, which he eventually did.
Other scholars have presented evidence pointing to a post-monarchic date (as late as 200 BC) based on language and theme, although the book might still have been based on an earlier composition.
Purpose.
There are two possible reasons for the creation of the book of Zephaniah. Either way, the primary purpose of the book’s composition was to alter the behaviour (particularly religious behaviour) of the author’s contemporary Jerusalemites.
If the book of Zephaniah was largely composed during the monarchic period, the author of the book of Zephaniah attempts to accomplish this change in behaviour through the threat of future calamity for “those who have turned back from following the Lord, / who have not sought the Lord or inquired of him” (1:6). The author conceives of a date in the future – the ‘great day of the Lord’ – when the Lord will judge all the people of the earth. This coming judgment will affect all of the nations, including the author’s own nation of Judah where God is understood to reside. The threats made against Jerusalem, however, are much more specific than the oracles concerning foreign nations. This strengthened the belief that the Israelites, who understood themselves to be God’s chosen people, were even more culpable than other peoples for not living up to God’s statutes because they were to be a ‘light unto the nations’. The book concludes by extending a promise of deliverance to the remnant of Israel which remains. The fulfilment of this prophecy is commonly understood to have taken place when Judah was captured by the nation of Babylon and many of its inhabitants were exiled in an event known as the Babylonian captivity. 
If the book gained most of its present form in post-monarchic period, then the author likely intended to draw upon an understanding of the Babylonian captivity as a punishment from the Lord, urging his own contemporaries not to repeat the mistakes of the past. It is not known whether the religious syncretism, alluded to in chapter one (as in verse 5), was a significant issue in post-exilic Judah.
Themes.
The book of Zephaniah consists of three chapters in the Hebrew Masoretic Text. In English versions, the book is also divided into three chapters. The New Revised Standard Version of the Bible supplies headings for the book as follows:
It is important to note that there are a number of different sub-divisions in use for the text with no clear consensus.
Despite its relatively short length, the book of Zephaniah incorporates a number of common prophetic themes. Zephaniah includes one of the most vivid descriptions in the prophetic literature of God’s wrath. Yet, it is also unequivocal in its proclamation of a restoration for those who survive the ‘Great Day of the Lord'.
The book of Zephaniah incorporates a good deal of phrases and terminology which are found in other books of the Bible. This suggests that the author of Zephaniah was familiar with and drew upon earlier Israelite religious tradition and also that later biblical writers regarded the book of Zephaniah as an authoritative (or at least respectable) work in the prophetic corpus.
The book of Zephaniah draws upon several themes from the Book of Genesis and reverses them. The opening verses of the book of Zephaniah are reminiscent both of the creation and of Noah’s flood. Chapter 1:2–3 declare that “I will sweep away everything / from the face of the earth says the Lord. / I will sweep away humans and animals; I will sweep away the birds of the air / and the fish of the sea.” The order of the creatures to be destroyed in Zephaniah is the opposite of the order in which they are created in Genesis 1:20–27. It is also worth noting that in both Noah’s flood and Zephaniah’s Day of the Lord, a ‘remnant’ survives God’s wrath.
It is also not surprising that the book of Zephaniah bears marked similarities to the book of Deuteronomy and the Deuteronomistic history. Similarities might be expected to each of these works because the Deuteronomistic history covers an overlapping period of time and because the issues which are dealt with in the book of Zephaniah go straight to the heart of the covenant which is reaffirmed in the book of Deuteronomy before Israel enters into the Promised Land of Canaan. The first 3–4 of the Ten Commandments (or Ten Words, Decalogue) contained in Exodus 20:1–17 and Deuteronomy 5:1–22 directly concern Israel’s relationship with Yahweh. It is this integral component of the covenant between Yahweh and Israel which is threatened by the practices to which the author of the book of Zephaniah refers in 1:4–6. In this manner, Zephaniah invokes one of the most common themes, not only in prophetic literature, but in the whole of the Hebrew Scriptures.
Zephaniah also draws upon the emerging idea that Yahweh is quite different from the regional or tribal gods of the surrounding nations. Rather, Yahweh is beginning to be understood as the only God and the God who rules over all nations. It was an apparently unique belief in the ancient Middle East that a god could send a foreign nation to execute that god’s judgment (as the Israelites believed Yahweh did with Babylon). In the book of Zephaniah, all nations are portrayed as being subject to Yahweh’s divine judgment.
The book of Zephaniah also interacts with the prophetic tradition – both borrowing from and contributing to the corpus in terms of language and images.

</doc>
<doc id="4458" url="http://en.wikipedia.org/wiki?curid=4458" title="Book of Habakkuk">
Book of Habakkuk

The Book of Habakkuk is the eighth book of the 12 minor prophets of the Hebrew Bible. It is attributed to the prophet Habakkuk, and was probably composed in the late 7th century BC.
Of the three chapters in the book, the first two are a dialog between Yahweh and the prophet. The central message, that "the just shall live by his faith" (2:4), plays an important role in Christian thought. It is used in the Epistle to the , Epistle to the , and the Epistle to the as the starting point of the concept of faith. A copy of these chapters is included in the Habakkuk Commentary, found among the Dead Sea Scrolls. Chapter 3 may be an independent addition, now recognized as a liturgical piece, but was possibly written by the same author as chapters 1 and 2.
Background.
The prophet Habakkuk is generally believed to have written his book in the mid-to-late 7th century BC, not long before the Babylonians' siege and capture of Jerusalem.
Author.
Habakkuk identifies himself as a prophet in the opening verse. Due to the liturgical nature of the book of Habakkuk, there have been some scholars who think that the author may have been a temple prophet. Temple prophets are described in 1 Chronicles 25:1 as using lyres, harps and cymbals. Some feel that this is echoed in Habakkuk 3:19b, and that Habakkuk may have been a Levite and singer in the Temple.
There is no biographical information on the prophet Habakkuk; in fact less is known about him than any other writer of the Bible. The only canonical information that exists comes from the book that is named for him. His name comes either from the Hebrew word חבק ("khavak") meaning "embrace" or else from an Akkadian word "hambakuku" for a kind of plant.
Although his name does not appear in any other part of the Jewish Bible, Rabbinic tradition holds Habakkuk to be the Shunammite woman's son, who was restored to life by Elisha in 2 Kings 4:16. The prophet Habakkuk is also mentioned in the narrative of Bel and the Dragon, part of the deuterocanonical additions to Daniel in a late section of that book. In the superscription of the Old Greek version, Habakkuk is called the son of Joshua of the tribe of Levi. In this book Habakkuk is lifted by an angel to Babylon to provide Daniel with some food while he is in the lion's den.
Historical context.
It is unknown when Habakkuk lived and preached, but the reference to the rise and advance of the Chaldeans in 1:6–11 places him in the middle to last quarter of the 7th century BC. One possible period might be during the reign of Jehoiakim, from 609–598 BC. The reasoning for this date is that during his reign that the Babylonians were growing in power. The Babylonians marched against Jerusalem in 598. Jehoiakim died while the Babylonians were marching towards Jerusalem and Jehoiakim's eighteen-year-old son Jehoiachin assumed the throne. Upon the Babylonians' arrival, Jehoiachin and his advisors surrendered Jerusalem after a short time. With the transition of rulers and the young age and inexperience of Jehoiachin, they were not able to stand against Chaldean forces. There is a sense of an intimate knowledge of the Babylonian brutality in 1:12–17.
Overview.
The book of Habakkuk is a book of the Tanakh (the Old Testament) and stands eighth in a section known as the 12 Minor Prophets in the Masoretic and Greek texts. In the Masoretic listing, it follows Nahum and precedes Zephaniah, who are considered to be his contemporaries.
The book consists of three chapters and the book is neatly divided into three different genres:
Themes.
The major theme of Habakkuk is trying to grow from a faith of perplexity and doubt to the height of absolute trust in God. Habakkuk addresses his concerns over the fact that God will use the evil Babylonian empire to execute judgment on Judah for their sins.
Habakkuk is unique among the prophets in that he openly questions the wisdom of God. In the first part of the first chapter, the Prophet sees the injustice among his people and asks why God does not take action. ""1:2 Yahweh, how long will I cry, and you will not hear? I cry out to you “Violence!” and will you not save?" – World English Bible."
In the middle part of Chapter 1, God explains that he will send the Chaldeans to punish his people. "1:5 “Look among the nations, watch, and wonder marvelously; for I am working a work in your days, which you will not believe though it is told you. 1:6 For, behold, I raise up the Chaldeans, that bitter and hasty nation, that march through the breadth of the earth, to possess dwelling places that are not theirs. (World English Bible)"
One of the "Eighteen Emendations to the Hebrew Scriptures" appears at 1:12. (Actually there were more than eighteen.) According to the professional Jewish scribes, the Sopherim, the text of 1:12 was changed from "You do not die" to "We shall not die." The Sopherim considered it disrespectful to say to God, ""You" do not die."
In the final part of the first chapter, the prophet expresses shock at God's choice of instrument for judgment. "1:13 You who have purer eyes than to see evil, and who cannot look on perversity, why do you tolerate those who deal treacherously, and keep silent when the wicked swallows up the man who is more righteous than he, (World English Bible [http://www.ebible.org/web/Habakkuk.htm])"
In Chapter 2, he awaits God's response to his challenge. God explains that He will also judge the Chaldeans, and much more harshly. "2:8 Because you have plundered many nations, all the remnant of the peoples will plunder you, because of men’s blood, and for the violence done to the land, to the city and to all who dwell in it. 2:9 Woe to him who gets an evil gain for his house, (World English Bible [http://www.ebible.org/web/Habakkuk.htm])"
Finally, in Chapter 3, Habakkuk expresses his ultimate faith in God, even if he doesn't fully understand.
"3:17 For though the fig tree doesn’t flourish, nor fruit be in the vines; the labor of the olive fails, the fields yield no food; the flocks are cut off from the fold, and there is no herd in the stalls: 3:18 yet I will rejoice in Yahweh. I will be joyful in the God of my salvation! (World English Bible [http://www.ebible.org/web/Habakkuk.htm])"
Importance.
The book of Habakkuk is accepted as canonical by adherents of the Jewish and Christian faiths. A commentary on the first two chapters of the book was found among the Dead Sea Scrolls at Qumran. Passages from Habakkuk are quoted by authors of the New Testament, and its message has inspired modern Christian hymn writers.
Judaism.
The Book of Habakkuk is the eighth book of the Twelve Prophets of the Hebrew Bible, and this collection appears in all copies of texts of the Septuagint, the Ancient Greek translation of the Hebrew Bible completed by 132 BC. Likewise, the book of Sirach (or Ecclesiasticus), also written in the 2nd century BC, mentions "The Twelve Prophets".
A partial copy of Habakkuk itself is included in the Habakkuk Commentary, a "pesher" found among the original seven Dead Sea Scrolls discovered in 1947. The Commentary contains a copy of the first two chapters of Habakkuk, but not of the third chapter. The writer of the "pesher" draws a comparison between the Babylonian invasion of the original text and the Roman threat of the writer's own period. What is even more significant than the commentary in the "pesher" is the quoted text of Habakkuk itself. The divergences between the Hebrew text of the scroll and the standard Masoretic Text are startlingly minimal. The biggest differences are word order, small grammatical variations, addition or omission of conjunctions, and spelling variations, but these are small enough to not to damage the meaning of the text.
Some scholars suggest that Chapter 3 may be a later independent addition to the book, in part because it is not included among the Dead Sea Scrolls. However, this chapter does appear in all copies of the Septuagint, as well as in texts from as early as the 3rd century BC. This final chapter is a poetic praise of God, and has some similarities with texts found in the Book of Daniel. However, the fact that the third chapter is written in a different style, as a liturgical piece, does not necessarily mean that Habakkuk was not also its author. Its omission from the Dead Sea Scrolls is attributed to the inability of the Qumran sect to fit Habakkuk's theology with their own narrow viewpoint.
Christianity.
The second half of is quoted by some of the earliest Christian writers. Although this passage is only three words in the original Hebrew, it is quoted three times in the New Testament. Paul the Apostle quotes this verse twice in his epistles: in Romans 1:17 and again in Galatians 3:11. In doing so, Paul extends Habakkuk's original concept of righteous living at the present time into a future life. The same verse is quoted in Hebrews 10:37–38, where Habakkuk's vision is tied to Christ and used to comfort the church during a period of persecution. These three epistles are considered to be "the three great doctrinal books of the New Testament," and Habakkuk's statement concerning faith forms the backbone of each book.
Modern Christian hymns have been inspired by the words of the prophet Habakkuk. The Christian hymn "", written in 1900 by William J. Kirkpatrick, is based on verse 2:20. The fourth verse of William Cowper's hymn "Sometimes a Light Surprises", written in 1779, quotes .

</doc>
<doc id="4459" url="http://en.wikipedia.org/wiki?curid=4459" title="Backward compatibility">
Backward compatibility

In telecommunications and computing, a product or technology is backward or downward compatible if it can work with input generated by an "older" product or technology. If products designed for the new standard can receive, read, view or play older standards or formats, then the product is said to be backward-compatible; examples of such a standard include data formats and communication protocols. Modifications to a system that do not allow backward compatibility are sometimes called "breaking changes."
The reverse is forward compatibility, which implies that old devices allow (or are expected to allow) data formats generated by "new" (or future) devices, perhaps without supporting all new features. A standard supports forward compatibility if older product versions can receive, read, view, or play the new standard.
For example, the introduction of FM stereo transmission allowed backward compatibility since new FM radio receivers could receive monaural signals generated by old transmitters. It also allowed forward compatibility, since old monaural FM radio receivers could still receive a signal from a new transmitter.
In programming languages, backward compatibility refers to the ability of a compiler for version "N" of the language to accept programs or data that worked under version "N" - 1. By this definition, if previous versions ("N" - 1, "N" - 2, etc.) were also backward compatible, which is often the case, then, by induction, version "N" will also accept input that worked under any prior version after, and including, the latest one that was not backward compatible. However, in practice, features are often deprecated and support is dropped in a later release, which is yet thought of as backward compatible.
In other contexts, a product or a technology is said to be backward compatible when it is able to fully take the place of an older product, by inter-operating with products that were designed for the older product. 
A data format is also said to be backward compatible with its predecessor if every message or file that is valid under the old format is also valid, and retains its meaning, under the new.
Description.
Backward compatibility is a relationship between two components, rather than being an attribute of just one of them. More generally, a new component is said to be backward compatible if it provides all of the functionality of the old component.
"Backward" compatibility is the special case of compatibility in which the new component has a direct historical ancestral relationship with the old component. If this special relationship does not exist then it not usually spoken of as "backward" compatibility but is instead just "compatible"—a consistent interface allowing interoperability between components and products that were each developed separately.
In the case of a program that creates document files, a new version of that program ("v2") is said to be backward compatible with the old version of the program ("v1") when it can both read and write documents that work with v1. Everything that v1 could do must also be possible with v2, including saving documents that can be read by v1 (which is something that v1 could do).
If a newer software version cannot save files that can be read by the older version, it is not backward compatible with the older version, although it may provide an irreversible upgrade capability for the old files. This situation has often been used strategically by software vendors to force customers to purchase upgrades since, over time, the number of data files usable by an old version diminishes at a rate proportional to the number of other customers that have upgraded (assuming that all customers generate files at the same the average rate).
Levels of compatibility vary. In software, "binary compatibility" and "source compatibility" are distinguishable. Binary compatibility means that programs can work correctly with the new version of this library without requiring recompilation. Source compatibility requires recompilation but no changes to the source code.
Many platforms rely on emulation, the simulation of an older platform in software, to achieve backward compatibility.
Bugwards compatible.
If a newer system is attempting to achieve integration with an older system which has known flaws (or "bugs"), then the new system may be referred to as bug-compatible or "bugwards-compatible".

</doc>
<doc id="4460" url="http://en.wikipedia.org/wiki?curid=4460" title="Bacterial conjugation">
Bacterial conjugation

Bacterial conjugation is the transfer of genetic material (plasmid) between bacterial cells by direct cell-to-cell contact or by a bridge-like connection between two cells. Discovered in 1946 by Joshua Lederberg and Edward Tatum, conjugation is a mechanism of horizontal gene transfer as are transformation and transduction although these two other mechanisms do not involve cell-to-cell contact.
Bacterial conjugation is often regarded as the bacterial equivalent of sexual reproduction or mating since it involves the exchange of genetic material. During conjugation the "donor" cell provides a conjugative or mobilizable genetic element that is most often a plasmid or transposon. Most conjugative plasmids have systems ensuring that the "recipient" cell does not already contain a similar element.
The genetic information transferred is often beneficial to the recipient. Benefits may include antibiotic resistance, xenobiotic tolerance or the ability to use new metabolites. Such beneficial plasmids may be considered bacterial endosymbionts. Other elements, however, may be viewed as bacterial parasites and conjugation as a mechanism evolved by them to allow for their spread.
Mechanism.
[[File:Conjugation.svg|right|thumb|350px|Schematic drawing of bacterial conjugation. '"
Conjugation diagram
1- Donor cell produces pilus. 2- Pilus attaches to recipient cell and brings the two cells together. 3- The mobile plasmid is nicked and a single strand of DNA is then transferred to the recipient cell. 4- Both cells synthesize a complementary strand to produce a double stranded circular plasmid and also reproduce pili; both cells are now viable donors.]]
The prototypical conjugative plasmid is the F-plasmid, or F-factor. The F-plasmid is an episome (a plasmid that can integrate itself into the bacterial chromosome by homologous recombination) with a length of about 100 kb. It carries its own origin of replication, the "oriV", and an origin of transfer, or "oriT". There can only be one copy of the F-plasmid in a given bacterium, either free or integrated, and bacteria that possess a copy are called "F-positive" or "F-plus" (denoted F+). Cells that lack F plasmids are called "F-negative" or "F-minus" (F-) and as such can function as recipient cells.
Among other genetic information the F-plasmid carries a "tra" and "trb" locus, which together are about 33 kb long and consist of about 40 genes. The "tra" locus includes the "pilin" gene and regulatory genes, which together form pili on the cell surface. The locus also includes the genes for the proteins that attach themselves to the surface of F- bacteria and initiate conjugation. Though there is some debate on the exact mechanism of conjugation it seems that the pili are not the structures through which DNA exchange occurs. This has been shown in experiments where the pilus are allowed to make contact, but then are denatured with SDS and yet DNA transformation still proceeds. Several proteins coded for in the "tra" or "trb" locus seem to open a channel between the bacteria and it is thought that the traD enzyme, located at the base of the pilus, initiates membrane fusion.
When conjugation is initiated by a signal the relaxase enzyme creates a nick in one of the strands of the conjugative plasmid at the "oriT". Relaxase may work alone or in a complex of over a dozen proteins known collectively as a relaxosome. In the F-plasmid system the relaxase enzyme is called TraI and the relaxosome consists of TraI, TraY, TraM and the integrated host factor IHF. The nicked strand, or "T-strand", is then unwound from the unbroken strand and transferred to the recipient cell in a 5'-terminus to 3'-terminus direction. The remaining strand is replicated either independent of conjugative action (vegetative replication beginning at the "oriV") or in concert with conjugation (conjugative replication similar to the rolling circle replication of lambda phage). Conjugative replication may require a second nick before successful transfer can occur. A recent report claims to have inhibited conjugation with chemicals that mimic an intermediate step of this second nicking event.
If the F-plasmid that is transferred has previously been integrated into the donor’s genome (producing an Hfr strain ["High Frequency of Recombination"]) some of the donor’s chromosomal DNA may also be transferred with the plasmid DNA. The amount of chromosomal DNA that is transferred depends on how long the two conjugating bacteria remain in contact. In common laboratory strains of "E. coli" the transfer of the entire bacterial chromosome takes about 100 minutes. The transferred DNA can then be integrated into the recipient genome via homologous recombination.
A cell culture that contains in its population cells with non-integrated F-plasmids usually also contains a few cells that have accidentally integrated their plasmids. It is these cells that are responsible for the low-frequency chromosomal gene transfers that occur in such cultures. Some strains of bacteria with an integrated F-plasmid can be isolated and grown in pure culture. Because such strains transfer chromosomal genes very efficiently they are called Hfr (high frequency of recombination). The "E. coli" genome was originally mapped by interrupted mating experiments in which various Hfr cells in the process of conjugation were sheared from recipients after less than 100 minutes (initially using a Waring blender). The genes that were transferred were then investigated.
Since integration of the F-plasmid into the "E. coli" chromosome is a rare spontaneous occurrence, and since the numerous genes promoting DNA transfer are in the plasmid genome rather than in the bacterial genome, it has been argued that conjugative bacterial gene transfer is not an evolutionary adaptation of the bacterial host, nor is it likely ancestral to eukaryotic sex.
Inter-kingdom transfer.
Bacteria related to the nitrogen fixing "Rhizobia" are an interesting case of inter-kingdom conjugation. For example, the tumor-inducing (Ti) plasmid of "Agrobacterium" and the root-tumor inducing (Ri) plasmid of "A. rhizogenes" contain genes that are capable of transferring to plant cells. The expression of these genes effectively transforms the plant cells into opine-producing factories. Opines are used by the bacteria as sources of nitrogen and energy. Infected cells form crown gall or root tumors, respectively. The Ti and Ri plasmids are thus endosymbionts of the bacteria, which are in turn endosymbionts (or parasites) of the infected plant.
The Ti and Ri plasmids can also be transferred between bacteria using a system (the "tra", or transfer, operon) that is different and independent of the system used for inter-kingdom transfer (the "vir", or virulence, operon). Such transfers create virulent strains from previously avirulent strains.
Genetic engineering applications.
Conjugation is a convenient means for transferring genetic material to a variety of targets. In laboratories, successful transfers have been reported from bacteria to yeast, plants, mammalian cells and isolated mammalian mitochondria. Conjugation has advantages over other forms of genetic transfer including minimal disruption of the target's cellular envelope and the ability to transfer relatively large amounts of genetic material (see the above discussion of "E. coli" chromosome transfer). In plant engineering, "Agrobacterium"-like conjugation complements other standard vehicles such as tobacco mosaic virus (TMV). While TMV is capable of infecting many plant families these are primarily herbaceous dicots. "Agrobacterium"-like conjugation is also primarily used for dicots, but monocot recipients are not uncommon.

</doc>
<doc id="4461" url="http://en.wikipedia.org/wiki?curid=4461" title="Galjoen">
Galjoen

The galjoen, black bream, or blackfish, "Dichistius capensis", is a species of marine fish found only along the Atlantic coast of southern Africa from Angola to South Africa. It can be found around reefs at depths around. This species can reach in total length and a weight of . This species is important to local commercial fisheries and is also popular as a game fish.
It appears in particular to be partial to the white mussels residing in the sandy beaches and inlets of the rocky outcrops along the southern coast. It is the national fish of South Africa.

</doc>
<doc id="4462" url="http://en.wikipedia.org/wiki?curid=4462" title="Blue crane">
Blue crane

The Blue Crane ("Anthropoides paradiseus"), also known as the Stanley Crane and the Paradise Crane, is the national bird of South Africa.
Description.
The Blue Crane is a tall, ground-dwelling bird, but is fairly small by the standards of the crane family. It is tall, with a wingspan of and weighs . Among standard measurements, the wing chord measures , the exposed culmen measures and the tarsus measures . This crane is pale blue-gray in colour becoming darker on the upper head, neck and nape. From the crown to the lores, the plumage is distinctly lighter, sometimes whitish. The bill is ochre to greyish, with a pink tinge. The long wingtip feathers which trail to the ground. The primaries are black to slate grey, with dark coverts and blackish on the secondaries. Unlike most cranes, it has a relatively large head and a proportionately thin neck. Juveniles are similar but slightly lighter, with tawny coloration on the head, and no long wing plumes.
Habitat.
Blue Cranes are birds of the dry grassy uplands, usually the pastured grasses of hills, valleys, and plains with a few scattered trees. They prefer areas in the nesting season that have access to both upland and wetland areas, though they feed almost entirely in dry areas. They are altitudinal migrants, generally nesting in the lower grasslands of an elevation of around 1,300 to 2,000 m and moving down to lower altitudes for winter. Though historically found in areas of low human disturbance, the blue crane is currently thriving in the highly transformed agricultural areas of the Western Cape. This is the only portion of its range where the population is increasing, though they still face threats such as poisoning in the region.
Movements and behaviour.
Of the 15 species of crane, the Blue Crane has the most restricted distribution of all. Even species with lower population numbers now (such as Siberian or Whooping Cranes) are found over a considerable range in their migratory movements. The Blue Crane is migratory, primarily altitudinal, but details are little known.
The Blue Crane is partially social, less so during the breeding season. There is a strict hierarchy in groups, with the larger adult males being dominant. They overlap in range with 3 other crane species but interactions with these species and other "large wader" type birds are not known. They are relentlessly aggressive to various other animals during the nesting season, attacking non-predatory species such as cattle, tortoises, plovers and even sparrows. Humans are also attacked if they approach a nest too closely, with the aggressive male having torn clothes and drawn blood in such cases.
Feeding.
Blue Cranes feed from the ground and appear to rarely feed near wetland areas. Most of their diet is comprised by grasses and sedges, with many types fed on based on their proximity to the nests. They are also regularly insectivorous, feeding on numerous, sizeable insects such as grasshoppers. Small animals such as crabs, snails, frogs, small lizards and snakes may supplement the diet, with such protein-rich food often being broken down and fed to the young.
Breeding.
The breeding period is highly seasonal, with eggs being recorded between October and March. Pair-formation amongst groups often beings in October, beginning with both potential parents running in circles with each other. The male then engages in a "dance" flings various objects in the air and then jumps. Eventually, a female from the group and the male appear to "select" each other and both engage in the dance of throwing objects and jumping. After the dance, mating commences in around two weeks.
In a great majority of known nests, two eggs are laid (rarely 1 or 3). Both males are females will incubate, with the male often incubating at night and, during the day, defending the nest territory during the day while the female incubates. The incubation stage lasts around 30 days. The young are able to walk after two days and can swim well shortly thereafter. They are fed primarily by their mothers, who regurgitates food into the mouths. Depending on the growth rate of chicks, the fledging stage has been reported when the young are anywhere from 30 to 60 days old. The young continue to be tended to until the next breeding season, at which time they are chased off by their parents.
Decline.
While it remains common in parts of its historic range, and approx. 26 000 individuals remain, it began a sudden population decline from around 1980 and is now classified as vulnerable.
In the last two decades, the Blue Crane has largely disappeared from the Eastern Cape, Lesotho, and Swaziland. The population in the northern Free State, Limpopo, Gauteng, Mpumalanga and North West Province has declined by up to 90%. The majority of the remaining population is in eastern and southern South Africa, with a small and separate population in the Etosha Pan of northern Namibia. Occasionally, isolated breeding pairs are found in five neighbouring countries.
The primary causes of the sudden decline of the Blue Crane are human population growth, the conversion of grasslands into commercial tree plantations, and poisoning: deliberate (to protect crops) or accidental (baits intended for other species, and as a side-effect of crop dusting).
The South African government has stepped up legal protection for the Blue Crane. Other conservation measures are focusing on research, habitat management, education, and recruiting the help of private landowners.
Cultural references.
The Blue Crane is a bird very special to the amaXhosa, who call it indwe. When a man distinguished himself by deeds of valour, or any form of meritorious conduct, he was often decorated by a chief by being presented with the feathers of this bird. After a battle, the chief would organise a ceremony called ukundzabela – a ceremony for the heroes, at which feathers would be presented. Men so honoured – they wore the feathers sticking out of their hair – were known as men of ugaba (trouble) – the implication being that if trouble arose, these men would reinstate peace and order.
The Blue Crane is one of the species to which the "Agreement on the Conservation of African-Eurasian Migratory Waterbirds" (AEWA) applies.

</doc>
<doc id="4463" url="http://en.wikipedia.org/wiki?curid=4463" title="Babrak Karmal">
Babrak Karmal

Babrak Karmal (, born Sultan Hussein; 6 January 1929 – 1 or 3 December 1996) was an Afghan politician and statesman during the Cold War. Karmal was born in Kamari and educated at Kabul University. When the People's Democratic Party of Afghanistan (PDPA) was formed, Karmal became one of its leading members, having been introduced to Marxism by Mir Akbar Khyber during his imprisonment for activities deemed too radical by the government. He eventually became the leader of the Parcham faction. When the PDPA split in 1967, the Parcham-faction established a Parcham PDPA, while their ideological nemeses, the Khalqs, established a Khalqist PDPA. Under Karmal's leadership, the Parchamite PDPA participated in Mohammad Daoud Khan's rise to power, and his subsequent regime. While relations were good at the beginning, Daoud began a major purge of leftist influence in the mid-1970s. This in turn led to the reformation of the PDPA in 1977. The PDPA took power in the 1978 Saur Revolution.
Karmal was appointed Deputy Chairman of the Revolutionary Council, synonymous with vice head of state, in the communist government. The Parchamite faction found itself under significant pressure by the Khalqists soon after taking power. In June 1978, a PDPA Central Committee meeting voted in favor of giving the Khalqist faction exclusive control over PDPA policy. This decision was followed by a failed Parchamite coup, after which Hafizullah Amin, a Khalqist, initiated a purge against the Parchamites. Karmal survived this purge but was exiled to Prague. Karmal remained in exile until December 1979, when the Soviet Union intervened in Afghanistan (with the consent of the Afghan government) to stabilize the country.
Karmal was promoted to Chairman of the Revolutionary Council and Chairman of the Council of Ministers on December 27, 1979. He remained in office until 1981, when he was succeeded by Sultan Ali Keshtmand. Throughout his term, Karmal worked to establish a support base for the PDPA by introducing several reforms. Among these were the Fundamental Principles of the Democratic Republic of Afghanistan, introducing a general amnesty for those people imprisoned during Nur Mohammad Taraki's and Amin's rule. He also replaced the Khalqist flag with a more traditional one. These policies failed to increase the PDPA's legitimacy in the eyes of the Afghan people.
These policy failures, and the stalemate that ensued after the Soviet intervention, led the Soviet leadership to become highly critical of Karmal's leadership. Under Mikhail Gorbachev, the Soviet Union deposed Karmal and replaced him with Mohammad Najibullah. Following his loss of power, he was again exiled, this time to Moscow. He was allowed to return to Afghanistan in 1991 by the Najibullah government. Back in Afghanistan he became an associate of Abdul Rashid Dostum, and helped remove the Najibullah government from power Not long after, in 1996, Karmal died from liver cancer.
Early life and career.
Karmal was born Sultan Hussein on January 6, 1929, the second of five children. His father was Muhammad Hussein Hashem, a Major General in the Afghan Army and former governor of the province of Paktia. His family was one of the wealthier families in Kabul.
Karmal was born in Kamari, a village close to Kabul. He attended Nejat High School, a German-speaking school, graduated in 1948, and applied to enter the College of Law and Political Science of Kabul University. Karmal's application was turned down initially because of his Marxist student union activities. He studied at the College of Law and Political Science at Kabul University from 1951 to 1953. In 1953 Karmal was arrested for continuation of the same activities, but was released three years later in 1956 in an amnesty by Muhammad Daoud Khan. In 1957, Karmal found work as an English and German translator, before quitting and leaving for military training. Karmal eventually graduated from the College of Law and Political Science in 1960, and in 1961, he began working in the Compilation and Translation Department of the Ministry of Education. From 1961 to 1963 he worked in the Ministry of Planning.
Communist politics.
Imprisoned from 1953 to 1956, Karmal befriended fellow inmate Mir Akbar Khyber, who introduced Karmal to Marxism. Karmal changed his name from Sultan Hussein to Babrak Karmal, which means "Comrade of the Workers'" in Pashtun, to disassociate himself from his bourgeoisie background. When he was released from prison, he continued his activities in the student union, and began to promote Marxism. Karmal spent the rest of the 1950s and the early 1960s becoming involved with Marxist organizations, of which there were at least four in Afghanistan at the time; two of the four were established by Karmal. When the 1964 Afghan Provisional Constitution, which legalised the establishment of new political entities, was introduced several prominent Marxists agreed to establish a communist political party. The People's Democratic Party of Afghanistan (PDPA, the Communist Party) was established in January 1965 in Nur Muhammad Taraki's home. Factionalism within the PDPA quickly became a problem; the party split into the Khalq led by Taraki alongside Hafizullah Amin, and the Parcham led by Karmal.
During the 1965 parliamentary election Karmal was one of four PDPA members elected to the lower house of parliament; the three others were Anahita Ratebzad, Nur Ahmed Nur and Fezanul Haq Fezan. No Khalqists were elected; however, Amin was 50 votes short of being elected. The Parchamite victory may be explained by the simple fact that Karmal could contribute financially to the PDPA electoral campaign. Karmal became a leading figure within the student movement in the 1960s, electing Mohammad Hashim Maiwandwal as Prime Minister after a student demonstration (called for by Karmal) concluded with three deaths under the former leadership.
In 1967, the PDPA unofficially split into two formal parties, one Khalqist and one Parchamist. The dissolution of the PDPA was initiated by the closing down of the Khalqist newspaper, "Khalq". Karmal criticised the "Khalq" for being too communist, and believed that its leadership should have hidden its Marxist orientation instead of promoting it. According to the official version of events, the majority of the PDPA Central Committee rejected Karmal's criticism. The vote was a close one, and it is reported that Taraki expanded the Central Committee to win the vote; this plan resulted in eight of the new members becoming politically unaligned with and one switching to the Parchamite side. Karmal and half the PDPA Central Committee left the PDPA to establish a Parchamite-led PDPA. Officially the split was caused by ideological differences, but the party may have divided between the different leadership styles and plans of Taraki versus Karmal. Taraki wanted to model the party after Leninist norms while Karmal wanted to establish a democratic front. Other differences were socioeconomic. The majority of Khalqists came from rural areas; hence they were poorer, and were of Pashtun origin. The Parchamites were urban, richer, and spoke Dari more often than not. The Khalqists accused the Parchamites of having a connection with the monarchy, and because of it, referred to the Parchamite PDPA as the "Royal Communist Party". Both Karmal and Amin retained their seats in the lower house of parliament in the 1969 parliamentary election.
The Daoud era.
Mohammed Daoud Khan, in collaboration with the Parchamite PDPA and radical military officers, overthrew the monarchy and instituted the Afghan Republic in 1973. After Daoud's seizure of power, an American embassy cable stated that the new government had established a Soviet-style Central Committee, in which Karmal and Mir Akbar Khyber were given leading positions. Most ministries were given to Parchamites; Hassan Sharq became Deputy Prime Minister, Major Faiz Mohammad became Minister of Internal Affairs and Nematullah Pazhwak became Minister of Education. The Parchamites took control over the ministries of finance, agriculture, communications and border affairs. The new government quickly suppressed the opposition, and secured their power base. At first, the National Front government between Daoud and the Parchamites seemed to work. By 1975, Daoud had strengthened his position by enhancing the executive, legislative and judicial powers of the Presidency. To the dismay of the Parchamites, all parties other than the National Revolutionary Party (NRP, established by Daoud) were made illegal.
Shortly after the ban on opposition to the NRP, Daoud began a massive purge of Parchamites in government. Mohammad lost his position as interior minister, Abdul Qadir Dagarwal was demoted, and Karmal was put under government surveillance. To mitigate Daoud's suddenly anti-communist directives, the Soviet Union reestablished the PDPA; Taraki was elected its General Secretary and Karmal, Second Secretary. While the Saur Revolution (literally the April Revolution) was planned for August, the assassination of Khyber led to a chain of events which ended with the communists seizing power. Karmal, when taking power in 1979, accused Amin of ordering the assassination of Khyber.
Taraki–Amin rule.
Taraki was appointed Chairman of the Presidium of the Revolutionary Council and Chairman of the Council of Ministers, retaining his post as PDPA general secretary. Taraki initially formed a government which consisted of both Khalqists and Parchamites; Karmal became Deputy Chairman of the Revolutionary Council, while Amin became Minister of Foreign Affairs and Deputy Chairman of the Council of Ministers.Mohammad Aslam Watanjar became Deputy Chairman of the Council of Ministers. The two Parchamites Abdul Qadir Dagarwal and Mohammad Rafi, became Minister of Defence and Minister of Public Works, respectively. The appointment of Amin, Karmal and Watanjar led to splits within the Council of Ministers: the Khalqists answered to Amin; Karmel led the civilian Parchamites; and the military officers (who were Parchamites) were answerable to Watanjar (a Khalqist). The first conflict arose when the Khalqists wanted to give PDPA Central Committee membership to military officers who had participated in the Saur Revolution; Karmal opposed such a move but was overruled. A PDPA Politburo meeting voted in favour of giving Central Committee membership to the officers.
On June 27, three months after the Saur Revolution, Amin outmaneuvered the Parchamites at a Central Committee meeting, giving the Khalqists exclusive right over formulating and deciding policy. A purge against the Parchamites was initiated by Amin and supported by Taraki on July 1, 1979. Karmal, fearing for his safety, went into hiding in one of his Soviet friends' homes. Karmal tried to contact Alexander Puzanov, the Soviet ambassador to Afghanistan, to talk about the situation. Puzanov refused, and revealed Karmal's location to Amin. It should be noted that the Soviets probably saved Karmal's life by sending him to the Socialist Republic of Czechoslovakia. In exile, Karmal established a network with the remaining Parchamites in government. A coup to overthrow Amin was planned for September 4, 1979. Its leading members in Afghanistan were Qadir and the Army Chief of Staff General Shahpur Ahmedzai. The coup was planned for the Festival of Eid, in anticipation of relaxed military vigilance. The conspiracy failed when the Afghan ambassador to India told the Afghan leadership about the plan. Another purge was initiated, and Parchamite ambassadors were recalled. Few returned to Afghanistan; Karmal and Mohammad Najibullah stayed in their respective countries.
Amin was informed of the Soviet decision to intervene in Afghanistan and was initially supportive, but was assassinated. Under the command of the Soviets, Karmal ascended to power. On December 27, 1979 Radio Kabul broadcast Karmal's pre-recorded speech to the Afghan people, saying: "Today the torture machine of Amin has been smashed, his accomplices – the primitive executioners, usurpers and murderers of tens of thousand of our fellow countrymen – fathers, mothers, sisters, brothers, sons and daughters, children and old people ..." Karmal was not in Kabul when the speech was broadcast; he was in Bagram, protected by the KGB.
That evening Yuri Andropov, the Chairman of the KGB, congratulated Karmal on his rise to the Chairmanship of the Presidium of the Revolutionary Council, some time before Karmal received an official appointment. Karmal returned to Kabul on December 28. He travelled alongside a Soviet military column. For the next few days Karmal lived in a villa on the outskirts of Kabul under the protection of the KGB. On January 1, 1980 Leonid Brezhnev, the General Secretary of the Central Committee of the Communist Party of the Soviet Union, and Alexei Kosygin, the Soviet Chairman of the Council of Ministers, congratulated Karmal on his "election" as leader.
Presidency.
Domestic policies.
The "Fundamental Principles" and amnesty.
When he came to power, Karmal promised an end to executions, the establishment of democratic institutions and free elections, the creation of a constitution, and legalization of alternative political parties. Prisoners incarcerated under the two previous governments would be freed in a general amnesty. He promised the creation of a coalition government which would not espouse socialism. At the same time, he told the Afghan people that he had negotiated with the Soviet Union to give economic, military and political assistance. The mistrust most Afghans felt towards the government was a problem for Karmal. Many still remembered he had said he would protect private capital in 1978—a promise later proven to be a lie.
Karmal's three most important promises were the general amnesty of prisoners, the promulgation of the Fundamental Principles of the Democratic Republic of Afghanistan and the adoption of a new flag containing the traditional black, red and green (the flag of Taraki and Amin was red). His government granted concessions to religious leaders and the restoration of confiscated property. Some property, which was confiscated during earlier land reforms, was also partially restored. All these measures, with the exception of the general amnesty of prisoners, were introduced gradually. Of 2,700 prisoners, 2,600 were released from prison; 600 of these were Parchamites. The general amnesty was greatly publicized by the government. While the event was hailed with enthusiasm by some, many others greeted the event with disdain, since their loved ones or associates had died during earlier purges. Amin had planned to introduce a general amnesty on January 1, 1980, to coincide with the PDPA's sixteenth anniversary.
Work on the Fundamental Principles had started under Amin: it guaranteed democratic rights such as freedom of speech, the right to security and life, the right to peaceful association, the right to demonstrate and the right that "no one would be accused of crime but in accord with the provisions of law" and that the accused had the right to a fair trial. The Fundamental Principles envisaged a democratic state led by the PDPA, the only party then permitted by law. The Revolutionary Council, the organ of supreme power, would convene twice every year. The Revolutionary Council in turn elected a Presidium which would take decisions on behalf of the Revolutionary Council when it was not in session. The Presidium consisted mostly of PDPA Politburo members. The state would safeguard three kinds of property: state, cooperative and private property. The Fundamental Principles said that the state had the right to change the Afghan economy from an economy where man was exploited to an economy were man was free. Another clause stated that the state had the right to take "families, both parents and children, under its supervision." While it looked democratic at the outset, the Fundamental Principles was based on contradictions.
The Fundamental Principles led to the establishment of two important state organs: the Special Revolutionary Court, a specialized court for crimes against national security and territorial integrity, and the Institute for Legal and Scientific Research and Legislative Affairs, the supreme legislative organ of state, This body could amend and draft laws, and introduce regulations and decrees on behalf of the government. The introduction of more Soviet-style institutions led the Afghan people to distrust the communist government even more.
Separation of power: Khalq–Parcham.
With Karmal's ascension to power, Parchamites began to "settle old scores". Revolutionary Troikas were created to arrest, sentence and execute people. Amin's guard were the first victims of the terror which ensued. Those commanders who had stayed loyal to Amin were arrested, filling the prisons. The Soviets protested, and Karmal replied, "As long as you keep my hands bound and do not let me deal with the Khalq faction there will be no unity in the PDPA and the government cannot become strong ... They tortured and killed us. They still hate us! They are the enemies of the party ..." Amin's daughter, along with her baby, was imprisoned for twelve years, until Mohammad Najibullah, then leader of the PDPA, released her. When Karmal took power, leading posts in the Party and Government bureaucracy were taken over by Parchamites. The Khalq faction was removed from power, and only technocrats, opportunists and individuals which the Soviets trusted would be appointed to the higher echelons of government. Khalqists remained in control of the Ministry of Interior, but Parchamites were given control over KHAD and the secret police. The Parchamites and the Khalqists controlled an equal share of the military. Two out of Karmal's three Council of Ministers deputy chairmen were Khalqists. Khalqists controlled the Ministry of Communications and the interior ministry. Parchamites, on the other hand, controlled the Ministry of Foreign Affairs and the Ministry of Defence. In addition to the changes in government, the Parchamites held clear majority in the PDPA Central Committee. Only one Khalqi, Saleh Mohammad Zeary, was a member of the PDPA Secretariat during Karmal's rule.
Over March 14 and 15, 1982 the PDPA held a party "conference" at the Kabul Polytechnic Institute instead of a party "congress", since a party congress would have given the Khalq faction a majority and could have led to a Khalqist takeover of the PDPA. The rules of holding a party conference were different, and the Parchamites had a three-fifths majority. This infuriated several Khalqists; the threat of expulsion did not lessen their anger. The conference was not successful, but it was portrayed as such by the official media. The conference broke up after one and a half days of a 3-day long program, because of the inter-party struggle for power between the Khalqists and the Parchamites. A "program of action" was introduced, and party rules were given minor changes. As an explanation of the low party membership, the official media also made it seem hard to become a member of the party.
PDPA base.
When Karmal took power, he began expanding the support base of the PDPA. Karmal tried to persuade certain groups, which had been referred to class enemies of the revolution during Taraki and Amin's rule, to support the PDPA. Karmal appointed several non-communists to top positions. Between March and May 1980, 78 out of the 191 people appointed to government posts were not members of the PDPA. Karmal reintroduced the old Afghan custom of having an Islamic invocation every time the government issued a proclamation. In his first live speech to the Afghan people, Karmal called for the establishment of the National Fatherland Front (NFF); the NFF's founding congress was held in June 1981. Unfortunately for Karmal, his policies did not lead to a notable increase in support for his regime, and it did not help Karmal that most Afghans saw the Soviet intervention as an invasion.
By 1981, the government gave up on political solutions to the conflict. At the fifth PDPA Central Committee plenum in June, Karmal resigned from his Council of Ministers chairmanship and was replaced by Sultan Ali Keshtmand, while Nur Ahmad Nur was given a bigger role in the Revolutionary Council. This was seen as "base broadening". The previous weight given to non-PDPA members in top positions ceased to be an important matter in the media by June 1981. This was significant, considering that up to five members of the Revolutionary Council were non-PDPA members. By the end of 1981, the previous contenders, who had been heavily presented in the media, were all gone; two were given ambassadorships, two ceased to be active in politics, and one continued as an advisor to the government. The other three changed sides, and began to work for the opposition.
The national policy of reconciliation continued: in January 1984 the land reform introduced by Taraki and Amin was drastically modified, the limits of landholdings were increased to win the support of middle class peasants, the literacy programme was continued, and concessions to women were made. In 1985 the Loya Jirga was reconvened. The 1985 Loya Jirga was followed by a tribal jirga in September. In 1986 Abdul Rahim Hatef, a non-PDPA member, was elected to the NFF chairmanship. During the 1985–86 elections it was said that 60 percent of the elected officials were non-PDPA members. By the end of Karmal's rule, several non-PDPA members had high-level government positions.
Civil war and military.
In March 1979, the military budget was 6.4 million US$, which was 8.3 percent of the government budget, but only 2.2 of gross national product. After the Soviet intervention, the defence budget increased to 208 million US$ in 1980, and 325 million US$ by 1981. In 1982 it was reported that the government spent around 22 percent of total expenditure.
When the political solution failed (see "PDPA base" section), the Afghan government and the Soviet military decided to solve the conflict militarily. The change from a political to a military solution did not come suddenly. It began in January 1981, as Karmal doubled wages for military personnel, issued several promotions, and decorated one general and thirteen colonels. The draft age was lowered, the obligatory length of arms duty was extended and the age for reservists was increased to thirty-five years of age. In June 1981, Assadullah Sarwari lost his seat in the PDPA Politburo, replaced by Mohammad Aslam Watanjar, a former tank commander and Minister of Communications, Major General Mohammad Rafi was madeMinister of Defence and Mohammad Najibullah appointed KHAD Chairman.
These measures were introduced due to the collapse of the army during the Soviet intervention. Before the invasion the army could field 100,000 troops, after the invasion only 25,000. Desertions were pandemic, and the recruitment campaigns for young people often drove them to the opposition. To better organize the military, seven military zones were established, each with its own Defence Council. The Defence Councils were established at the national, provincial and district level to empower the local PDPA. It is estimated that the Afghan government spent as much as 40 percent of government revenue on defense.
Economy.
During the civil war and the ensuing Soviet war in Afghanistan, most of the country's infrastructure was destroyed. Normal patterns of economic activity were disrupted. The Gross national product (GNP) fell substantially during Karmal's rule because of the conflict; trade and transport was disrupted with loss of labor and capital. In 1981 the Afghan GDP stood at 154.3 billion Afghan afghanis, a drop from 159.7 billion in 1978. GNP per capita decreased from 7,370 in 1978 to 6,852 in 1981. The dominant form of economic activity was in the agricultural sector. Agriculture accounted for 63 percent of gross domestic product (GDP) in 1981; 56 percent of the labor force was working in agriculture in 1982. Industry accounted for 21 percent of GDP in 1982, and employed 10 percent of the labor force. All industrial enterprises were government-owned. The service sector, the smallest of the three, accounted for 10 percent of GDP in 1981, and employed an estimated one-third of the labour force. The balance of payments, which had grown in the pre-communist administration of Muhammad Daoud Khan, decreased, turning negative by 1982 at 70.3 million $US. The only economic activity which grew substantially during Karmal's rule was export and import.
Foreign policy.
Karmal observed in the spring of 1983 that without Soviet intervention, "It is unknown what the destiny of the Afghan Revolution would be ... We are realists and we clearly realize that in store for us yet lie trials and deprivations, losses and difficulties." Two weeks before this statement Sultan Ali Keshtmand, the Chairman of the Council of Ministers, lamented the fact that half the schools and three-quarters of communications had been destroyed since 1979. The Soviet Union rejected several Western-made peace plans, such as the Carrington Plan, since they did not take into consideration the PDPA government. Most Western peace plans had been made in collaboration with the Afghan opposition forces. At the 26th Congress of the Communist Party of the Soviet Union (CPSU) Leonid Brezhnev, the General Secretary of the CPSU Central Committee, stated;
The stance of the Pakistani government was clear, demanding complete Soviet withdrawal from Afghanistan and the establishment of a non-PDPA government. Karmal, summarizing his discussions with Iran and Pakistan, said "Iran and Pakistan have so far not opted for concrete and constructive positions." During Karmal's rule Afghan–Pakistani relations remained hostile; the Soviet intervention in Afghanistan was the catalyst for the hostile relationship. The increasing numbers of Afghan refugees in Pakistan challenged the PDPA's legitimacy to rule.
The Soviet Union threatened in 1985 that it would support the Baloch separatist movement in Pakistan if the Pakistani government continued to aid the mujahideen in Afghanistan. Karmal, problematically for the Soviets, did not want a Soviet withdrawal, and he hampered attempts to improve relations with Pakistan since the Pakistanti government had refused to recognise the PDPA government.
Fall from power and succession.
Mikhail Gorbachev, then General Secretary of the Central Committee of the Communist Party of the Soviet Union, said, "The main reason that there has been no national consolidation so far is that Comrade Karmal is hoping to continue sitting in Kabul with our help." Karmal's position became less secure when the Soviet leadership began blaming him for the failures in Afghanistan. Gorbachev, worried over the situation, told the Soviet Politburo "If we don't change approaches evacuate Afghanistan, we will be fighting there for another 20 or 30 years." It is not clear when the Soviet leadership began to campaign for Karmal's dismissal, but Andrei Gromyko discussed the possibility of Karmal's resignation with Javier Pérez de Cuéllar, the Secretary-General of the United Nations in 1982. While it was Gorbachev who would dismiss Karmal, there may have been a consensus within the Soviet leadership in 1983 that Karmal should resign. Gorbachev's own plan was to replace Karmal with Mohammad Najibullah, who had joined the PDPA at its creation. Najibullah was thought highly of by Yuri Andropov, Boris Ponomarev and Dmitriy Ustinov, and negotiations for his succession may have started in 1983. Najibullah was not the Soviet leadership's only choice for Karmal's succession; a GRU report noted that the majority of the PDPA leadership would support Assadullah Sarwari's ascension to leadership. According to the GRU, Sarwari was a better candidate as he could balance between the Pashtuns, Tajiks and Uzbeks; Najibullah was a Pashtun nationalist. Another viable candidate was Abdul Qadir Dagarwal, who had been a participant in the Saur Revolution.
Najibullah was appointed to the PDPA Secretariat in November 1985. During Karmal's March 1986 visit to the Soviet Union, the Soviets tried to persuade Karmal that he was too ill to govern, and that he should resign. This backfired, as a Soviet doctor attending to Karmal told him he was in good health. Karmal asked to return home to Kabul, and said that he understood and would listen to the Soviet recommendations. Before leaving, Karmal promised he would step down as PDPA General Secretary. The Soviets did not trust him and sent Vladimir Kryuchkov, the head of intelligence in the KGB, into Afghanistan. At a meeting in Kabul, Karmal confessed his undying love for the Soviet Union, comparing his ardor to his Muslim faith. Kryuchkov, concluding that he could not persuade Karmal to resign, left the meeting. After Kryuchkov left the room, the Afghan defence minister and the state security minister visited Karmal's office, telling him that he had to resign from one of his posts. Understanding that his Soviet support had been eliminated, Karmal resigned from the office of the General Secretary at the 18th PDPA Central Committee plenum. He was succeeded in his post by Najibullah.
Karmal still had support within the party, and used his base to curb Najibullah's powers. He began spreading rumors that he would be reappointed General Secretary. Najibullah's power base was in the KHAD, the Afghan equivalent to the KGB, and not the party. Considering the fact that the Soviet Union had supported Karmal for over six years, the Soviet leadership wanted to ease him out of power gradually. Yuli Vorontsov, the Soviet ambassador to Afghanistan, told Najibullah to begin undermining Karmal's power slowly. Najibullah complained to the Soviet leadership that Karmal used most of his spare time looking for errors and "speaking against the National Reconciliation ". At a meeting of the Soviet Politburo on November 13, 1986 it was decided that Najibullah should remove Karmal; this motion was supported by Gromyko, Vorontsov, Eduard Shevardnadze, Anatoly Dobrynin and Viktor Chebrikov. A PDPA meeting in November relieved Karmal of his Revolutionary Council chairmanship, and exiled him to Moscow where he was given a state-owned apartment and a dacha. Karmal was succeeded as Revolutionary Council chairman by Haji Mohammad Chamkani, who was not a member of the PDPA.
Later life and death.
For unknown reasons, Karmal was invited back to Kabul by Najibullah, and "for equally obscure reasons Karmal accepted." If Najibullah's plan was to strengthen his position within the Homeland Watan Party (the renamed PDPA) by appeasing the pro-Karmal Parchamites, he failed. Karmal's apartment became a center for opposition to Najibullah's government. When Najibullah was toppled in 1992, Karmal became the most powerful politician in Kabul through leadership of the Parcham. However, his negotiations with the rebels collapsed quickly, and on April 16, 1992 the rebels, led by Gulbuddin Hekmatyar, took Kabul. After the fall of Najibullah's government, Karmal was based in Hairatan. There, it is alleged, Karmal used most of his time either trying to establish a new party, or advising people to join the National Islamic Movement (NIM). Abdul Rashid Dostum, the leader of NIM, was a supporter of Karmal during his rule. It is unknown how much control Karmal had over Dostum, but there is little evidence that Karmal was in any commanding position. Karmal's influence over Dostum appeared indirect – some of his former associates supported Dostum. Those who spoke with Karmal during this period noted his lack of interest in politics. In early December 1996, Karmal died in Moscow's Central Clinical Hospital from liver cancer. The date of his death was reported by some sources as December 1 and by others as December 3. The Taliban summed up his rule as follows:

</doc>
<doc id="4468" url="http://en.wikipedia.org/wiki?curid=4468" title="Buddhist philosophy">
Buddhist philosophy

Buddhist philosophy is the elaboration and explanation of the delivered teachings of the Buddha as found in the Tripitaka and Agama. Its main concern is with explicating the dharmas constituting reality. A recurrent theme is the reification of concepts, and the subsequent return to the Buddhist middle way.
Early Buddhism avoided speculative thought on metaphysics, phenomenology, ethics, and epistemology, but was based instead on empirical evidence gained by the sense organs ("ayatana"). 
Nevertheless, Buddhist scholars have addressed ontological and metaphysical issues subsequently. Particular points of Buddhist philosophy have often been the subject of disputes between different schools of Buddhism. These elaborations and disputes gave rise to various schools in early Buddhism of Abhidhamma, and to the Mahayana traditions and schools of the prajnaparamita, Madhyamaka, buddha-nature and Yogacara.
Indian background.
The historical Buddha lived during a time of spiritual and philosophical revival in Northern India when the overly ritualistic practices of the vedas came under rational scrutiny. 
As well as the Buddha's own teachings, new ethical and spiritual philosophies such as those of Mahavira became established during this period when alternatives to the mainstream religion arose in an atmosphere of freethought and renewed vitality in spiritual endeavour. This general cultural movement is today known as the Sramanic tradition and the epoch of new thought as the axial era. 
These heterodox groups held widely divergent opinions but were united by a critical attitude towards the established religion whose explanations they found unsatisfactory and whose animal sacrifices increasingly distasteful and irrelevant. In Greece, China and India there was a return to fundamental questions and a new interest in the question of how humans should live.
Life and teachings of the Buddha.
Biography.
According to the traditional accounts, Gautama, the future Buddha, born into a Vedic Kshatriya family, was a prince who grew up in an environment of luxury and opulence. He became convinced that sense-pleasures and wealth did not provide the satisfaction that human beings longed for deep within. He abandoned worldly life to live as a mendicant. He studied under a number of teachers, developing his insight into the problem of suffering. 
After his awakening he regarded himself as a physician rather than a philosopher. Whereas philosophers merely had views about things, he taught the Noble Eightfold Path which liberates from suffering.
Philosophy.
The Buddha discouraged his followers from indulging in intellectual disputation for its own sake, which is fruitless, and distracting from true awakening. Nevertheless, the delivered sayings of the Buddha contain a philosophical component, in its teachings on the working of the mind, and its criticisms of the philosophies of his contemporaries.
According to the scriptures, during his lifetime the Buddha remained silent when asked several metaphysical questions. These regarded issues such as whether the universe is eternal or non-eternal (or whether it is finite or infinite), the unity or separation of the body and the self, the complete inexistence of a person after Nirvana and death, and others.
Emphasis on awakening.
One explanation for this silence is that such questions distract from activity that is practical to realizing enlightenment and bring about the danger of substituting the experience of liberation by conceptual understanding of the doctrine or by religious faith.
Attachments to the skandhas.
Another explanation is that both affirmative and negative positions regarding these questions are based on attachment to and misunderstanding of the aggregates and senses. That is, when one sees these things for what they are, the idea of forming positions on such metaphysical questions simply does not occur.
Emptiness.
Another closely related explanation is that reality is devoid of sensory mediation and conception, or empty, and therefore language itself is "a priori" inadequate without direct experience.
Thus, the Buddha's silence does not indicate misology or disdain for philosophy. Rather, it indicates that he viewed the answers to these questions as not understandable by the unenlightened. Dependent arising provides a framework for analysis of reality that is not based on metaphysical assumptions regarding existence or non-existence, but instead on imagining direct cognition of phenomena as they are presented to the mind. This informs and supports the Buddhist approach to liberation from adventitious distortion and engaging in the Noble Eightfold Path.
The Buddha of the earliest Buddhists texts describes Dharma (in the sense of "truth") as "beyond reasoning" or "transcending logic", in the sense that reasoning is a subjectively introduced aspect of the way unenlightened humans perceive things, and the conceptual framework which underpins their cognitive process, rather than a feature of things as they really are. Going "beyond reasoning" means in this context penetrating the nature of reasoning from the inside, and removing the causes for experiencing any future stress as a result of it, rather than functioning outside of the system as a whole.
Early Buddhism.
Basic teachings.
Certain basic teachings appear in many places throughout the early texts, so most scholars conclude that the Buddha must at least have taught these teachings:
According to these scholars, there was something they variously call "earliest Buddhism", "original Buddhism" or "pre-canonical Buddhism". 
Some scholars disagree, and have proposed other theories. According to some scholars, the philosophical outlook of earliest Buddhism was primarily negative, in the sense that it focused on what doctrines to "reject" more than on what doctrines to "accept". Only knowledge that is useful in achieving enlightenment is valued. According to this theory, the cycle of philosophical upheavals that in part drove the diversification of Buddhism into its many schools and sects only began once Buddhists began attempting to make explicit the implicit philosophy of the Buddha and the early suttas. 
Other scholars reject this theory. After the death of the Buddha, attempts were made to gather his teachings and transmit them in a commonly agreed form, first orally, then also in writing (the "Tripiṭaka").
Dukkha.
Dukkha, often translated as "suffering", is the inherent unsatisfactoriness of life. This unsatisfactoriness drives our yearning for a better way of life, yet keeps us imprisoned in wordly existence and rebirth.
Dependent origination.
The working of the rising and ceasing of suffering is explained by Pratitya-samutpada, "dependent origination". It states that events are not predetermined, nor are they random. It rejects notions of direct causation, which are necessarily undergirded by a substantialist metaphysics. Instead, it posits the arising of events under certain conditions which are inextricable, such that the processes in question at no time, are considered to be entities.
Dependent origination posits that certain specific events, concepts, or realities are always dependent on other specific things. Craving, for example, is always dependent on, and caused by, emotion. Emotion is always dependent on contact with our surroundings. This chain of causation purports to show that the cessation of decay, death, and sorrow is indirectly dependent on the cessation of craving.
This concept leaves no room for the existence of everlasting, absolute entities. The world must be thought of in procedural terms, not in terms of things or substances. Likewise,
Anatta.
The Buddha asserted the non inherently existent concept of the ego, in opposition to the Upanishadic concept of an unchanging ultimate self. The Buddha held that attachment to the appearance of a permanent self in this world of change is the cause of suffering, and the main obstacle to liberation. The apparent ego is merely the result of identification with the temporary aggregates, the components of the individual human being's body and consciousness at any given moment in time.
Ethics.
Eightfold Path.
Although there are many ethical tenets in Buddhism that differ depending on whether one is a monk or a layman, and depending on individual schools, the Buddhist system of ethics can be summed up in the eightfold path:
The purpose of living an ethical life is to escape the suffering inherent in samsara. Skillful actions condition the mind in a positive way and lead to future happiness, while the opposite is true for unskillful actions. Ethical discipline also provides the mental stability and freedom to embark upon mental cultivation via meditation.
The part of the Noble Eightfold path that covers morality/ethics is right speech, right action and right livelihood. The other parts cover concentration and wisdom, with wisdom being covered by right view and right intention and the remaining three belonging to concentration.
Precepts.
While the precepts for monks and nuns differ somewhat depending on which tradition one has ordained in (Tibetan, Thai Theravadan, etc.), the precepts for laymen and laywomen followers of the Buddha are the same. 
There are the five precepts that all followers of the Buddha must observe if they hope to be reborn as a human being. Eight precepts are practiced by anagarikas and lay-followers staying in temples. Ten precepts are followed by bhikkhus or other serious practitioners.
Textual authority.
Decisive in distinguishing Buddhism from what is commonly called Hinduism is the issue of epistemological justification. 
All schools of Indian logic recognize various sets of valid justifications for knowledge, or "pramāṇa". Buddhism recognizes a set that is smaller than the others. For some schools of Hinduism and Buddhism the received textual tradition is an epistemological category equal to perception and inference (although this is not necessarily true for some other schools).
Thus, in the Hindu schools, if a claim was made that could not be substantiated by appeal to the textual canon, it would be considered as ridiculous as a claim that the sky was green and, conversely, a claim which could not be substantiated via conventional means might still be justified through textual reference, differentiating this from the epistemology of modern science. It must also be remembered that most Hindu schools do believe that logical inference and perception of the sense organs is the most effective means testing of a claim. The Nyaya school of Hinduism is one which highly believes in application of logic and reason more than canonical evidence.
Early Buddhist schools.
The main early Buddhist philosophical schools are the "Abhidharma" schools, particularly Sarvāstivāda and Theravāda.
Sarvastivadin realism.
Early Buddhist philosophers and exegetes of the Sarvāstivādins created a pluralist metaphysical and phenomenological system, in which all experiences of people, things and events can be broken down into smaller and smaller perceptual or perceptual-ontological units called ""dharmas"". 
Other schools incorporated some parts of this theory and criticized others. The Sautrāntikas, another early school, and the Theravādins, now the only modern survivor of the early Buddhist schools, criticized the realist standpoint of the Sarvāstivādins.
Theravada.
Theravada promotes the concept of vibhajjavada (Pāli, literally "Teaching of Analysis") to non-Buddhists. This doctrine says that insight must come from the aspirant's experience, critical investigation, and reasoning instead of by blind faith. As the Buddha said according to the canonical scriptures:
Mahayana.
Mahayana often adopts a pragmatic concept of truth: doctrines are regarded as conditionally "true" in the sense of being spiritually beneficial. In modern Chinese Buddhism, all doctrinal traditions are regarded as equally valid.
Main Mahayana philosophical schools and traditions include the prajnaparamita, Madhyamaka, Tathagatagarbha, Yogācāra, Huayan, and Tiantai schools.
Indian Mahayana.
Prajnaparamita.
The Prajanaparamita-sutras emphasize the emptiness of the five skandhas. The Heart sutra, a text from the prajnaparamita-sutras, articulates this in the following saying in which the five skandhas are said to be "empty":
Madhyamaka.
The Mahāyānist Nāgārjuna, one of the most influential Buddhist thinkers, promoted classical Buddhist emphasis on phenomena and attacked Sarvāstivāda realism and Sautrāntika nominalism in his magnum opus, "The Fundamental Verses on the Middle Way" ("Mūlamadhyamakakārikā").
Nagarjuna asserted a direct connection between, even identity of, dependent origination, selflessness ("anatta"), and emptiness ("śūnyatā"). He pointed out that implicit in the early Buddhist concept of dependent origination is the lack of any substantial being ("anatta") underlying the participants in origination, so that they have no independent existence, a state identified as emptiness ("śūnyatā"), or emptiness of a nature or essence ("svabhāva").
Tathagatagarbha.
The "tathāgathagarbha sutras", in a departure from mainstream Buddhist language, insist that the potential for awakening is inherent to every sentient being. They marked a shift from a largely apophatic (negative) philosophical trend within Buddhism to a decidedly more cataphatic (positive) modus. 
Prior to the period of these scriptures, Mahāyāna metaphysics had been dominated by teachings on emptiness in the form of Madhyamaka philosophy. The language used by this approach is primarily negative, and the "tathāgatagarbha" genre of sutras can be seen as an attempt to state orthodox Buddhist teachings of dependent origination using positive language instead, to prevent people from being turned away from Buddhism by a false impression of nihilism. 
In these sutras the perfection of the wisdom of not-self is stated to be the true self; the ultimate goal of the path is then characterized using a range of positive language that had been used previously in Indian philosophy by essentialist philosophers, but which was now transmuted into a new Buddhist vocabulary to describe a being who has successfully completed the Buddhist path.
The word "self" ("atman") is used in a way idiosyncratic to these sutras; the "true self" is described as the perfection of the wisdom of not-self in the "Buddha-Nature Treatise", for example. Language that had previously been used by essentialist non-Buddhist philosophers was now adopted, with new definitions, by Buddhists to promote orthodox teachings.
The "tathāgatagarbha" does not, according to some scholars, represent a substantial self; rather, it is a positive language expression of emptiness and represents the potentiality to realize Buddhahood through Buddhist practices. In this interpretation, the intention of the teaching of "tathāgatagarbha" is soteriological rather than theoretical.
The "tathāgathagarbha", the Theravāda doctrine of "bhavaṅga", and the Yogācāra store consciousness were all identified at some point with the luminous mind of the Nikāyas.
In the Mahayana "Mahaparinirvana Sutra", the Buddha insists that while pondering upon Dharma is vital, one must then relinquish fixation on words and letters, as these are utterly divorced from liberation and the Buddha-nature.
Yogacara.
The Yogacara-school tries to explain the arising of suffering by explaining the workings of our mind. It takes the concepts of the five skandhas and the six consciousnesses, to explain how manas creates vijnapti, "concepts" to which we cling.
Chinese Buddhism.
Tiantai and the Lotus School.
The schools of Buddhism that had existed in China prior to the emergence of the Tiantai are generally believed to represent direct transplantations from India, with little modification to their basic doctrines and methods. However, Tiantai grew and flourished as a natively Chinese Buddhist school under the 4th patriarch, Zhiyi, who developed a hierarchy Buddhist sutras that asserted the Lotus Sutra as the supreme teaching, as well as a system of meditation and practices around it.
Huayan and Avatamsaka-sutra.
The Huayan developed the doctrine of "interpenetration" or "coalescence" (Wylie: "zung-'jug"; Sanskrit: "yuganaddha"), based on the "Avataṃsaka Sūtra", a Mahāyāna scripture. It holds that all phenomena (Sanskrit: "dharmas") are intimately connected (and mutually arising). Two images are used to convey this idea. The first is known as Indra's net. The net is set with jewels which have the extraordinary property that they reflect all of the other jewels. The second image is that of the world text. This image portrays the world as consisting of an enormous text which is as large as the universe itself. The words of the text are composed of the phenomena that make up the world. However, every atom of the world contains the whole text within it. It is the work of a Buddha to let out the text so that beings can be liberated from suffering. The doctrine of interpenetration influenced the Japanese monk Kūkai, who founded the Shingon school of Buddhism. Interpenetration and essence-function are mutually informing in the East Asian Buddhist traditions, especially the Korean Buddhist tradition.
In Tibetan Buddhism, it is iconographically represented by "yab-yum".
Tibetan Buddhism.
The Tibetan tantra entitled the "All-Creating King" ("Kunjed Gyalpo Tantra") also emphasizes how Buddhist realization lies beyond the range of discursive/verbal thought and is ultimately mysterious. Samantabhadra, states there: 
Also later, the famous Indian Buddhist practitioner and teacher, mahasiddha Tilopa discouraged any intellectual activity in his six words of advice.
Comparison with other philosophies.
Baruch Spinoza, though he argued for the existence of a permanent reality, asserts that all phenomenal existence is transitory. In his opinion sorrow is conquered "by finding an object of knowledge which is not transient, not ephemeral, but is immutable, permanent, everlasting." The Buddha taught that the only thing which is eternal is Nirvana. David Hume, after a relentless analysis of the mind, concluded that consciousness consists of fleeting mental states. Hume's Bundle theory is a very similar concept to the Buddhist "skandhas", though his skepticism about causation lead him to opposite conclusions in other areas. Arthur Schopenhauer's philosophy parallels Buddhism in his affirmation of asceticism and renunciation as a response to suffering and desire. 
Ludwig Wittgenstein's "language-game" closely parallel the warning that intellectual speculation or papañca is an impediment to understanding, as found in the Buddhist "Parable of the Poison Arrow". Friedrich Nietzsche, although himself dismissive of Buddhism as yet another nihilism, had a similar impermanent view of the self. Heidegger's ideas on being and nothingness have been held by some to be similar to Buddhism today.
An alternative approach to the comparison of Buddhist thought with Western philosophy is to use the concept of the Middle Way in Buddhism as a critical tool for the assessment of Western philosophies. In this way Western philosophies can be classified in Buddhist terms as eternalist or nihilist. In a Buddhist view all philosophies are considered non-essential views (ditthis) and not to be clung to.

</doc>
<doc id="4471" url="http://en.wikipedia.org/wiki?curid=4471" title="Billy Bob Thornton">
Billy Bob Thornton

Billy Bob Thornton (born August 4, 1955) is an American actor, screenwriter, director, producer, and musician.
Thornton made his first break with co-writing and starring in 1992 film "One False Move" and came to international attention after writing, directing, and starring in the highly acclaimed independent film "Sling Blade" (1996), for which he won an Academy Award for Best Adapted Screenplay and was nominated for an Academy Award for Best Actor. He appeared in several major film roles following "Sling Blade"s success which brought him a bigger international recognition and critical acclaim including "U Turn" (1997), "Primary Colors" (1998), "Armageddon" (1998), "A Simple Plan" (1998) which earned him his third Oscar nomination, "Monster's Ball" (2001), "Bandits" (2001), "The Man Who Wasn't There" (2001), "Bad Santa" (2003), "Intolerable Cruelty" (2003), "Love Actually" (2003), "Friday Night Lights" (2004), "The Alamo" (2004), "Eagle Eye" (2008) and "Faster" (2010).
Thornton is cited as an "anti-film star" who approaches his roles like a character actor and rarely accept roles in blockbusters. He has been vocal about his disrespect for celebrity culture, choosing to keep his life out of the public eye. However, the attention of the media has proven unavoidable in certain cases, his marriage to Angelina Jolie being a notable example. As an influential actor, Thornton is known for his diversity, wide range, and prolificacy, appearing in at least one film per year nearly every year since 1991. Thornton has written a variety of films, usually set in the Southern United States and mainly co-written with Tom Epperson, including "A Family Thing" (1996) and "The Gift" (2000). After "Sling Blade", he directed several other films, including "Daddy and Them" (2001), "All the Pretty Horses" (2001), and "Jayne Mansfield's Car" (2012).
Thornton has received President's Award from Academy of Science Fiction, Fantasy & Horror Films, Special Achievement Award from National Board of Review and a star on the Hollywood Walk of Fame. He was also nominated for an Emmy Award, four Golden Globes and three Screen Actors Guild Awards. In addition to film work, Thornton began a career as a singer-songwriter. He has released four solo albums and is the vocalist of a blues rock band "The Boxmasters".
Early life.
Thornton was born in Hot Springs, Arkansas, the son of Virginia Roberta (née Faulkner), a psychic, and William Raymond "Billy Ray" Thornton (November 1929 – August 1974), a high school history teacher and basketball coach who died when Thornton was 18. Thornton was one of three brothers with Jimmy Don (April 1958 – October 1988), who died of a heart attack at 30, and John David (born 1969), who resides in California. Jimmy Don wrote a number of songs, two of which ("Island Avenue" and "Emily") Thornton has recorded on his solo albums. During his childhood, Thornton lived in Alpine, Arkansas, and Malvern, Arkansas. He was raised a Methodist, in an extended family in a shack that had neither electricity nor plumbing. He graduated from high school in 1973. A good high school baseball player, he tried out for the Kansas City Royals, but was released after an injury. After a short period laying asphalt for the Arkansas State Transportation Department, he attended Henderson State University to pursue studies in psychology, but dropped out after two semesters.
In the mid-1980s, Thornton settled in Los Angeles to pursue his career as an actor, with future writing partner Tom Epperson. He initially had a difficult time succeeding as an actor, and worked in telemarketing, offshore wind farming, and fast food management between auditioning for acting jobs. He also played drums and sang with South African rock band Jack Hammer. While Thornton worked as a waiter for an industry event, he served film director and screenwriter Billy Wilder, who is famous for films such as "Double Indemnity" and "Sunset Boulevard". Thornton struck up a conversation with Wilder, who advised Thornton to consider a career as a screenwriter.
Career.
Film.
Thornton's first screen role was in 1980's "South of Reno", where he played a small role as a counter man in a restaurant. Thornton also made an appearance in a 1987 episode of Andy Griffith's popular show "Matlock" titled "The Photographer" as a pawn store clerk.
Another one of Thornton's early screen roles was as a cast member on the CBS sitcom "Hearts Afire" with John Ritter and Markie Post. His role as the villain in 1992's "One False Move", which he also co-wrote, brought him to the attention of critics. He also had small roles in the early 1990s films "Indecent Proposal", "On Deadly Ground", "Bound by Honor", "Grey Knight", and "Tombstone". Thornton put Wilder's advice to good use, and went on to write, direct and star in the independent film "Sling Blade", which was released in 1996. The film, an expansion of a short film titled "Some Folks Call It a Sling Blade", introduced the story of Karl Childers, a mentally handicapped man imprisoned for a gruesome and seemingly inexplicable murder. "Sling Blade" garnered international acclaim. Thornton's screenplay earned him an Academy Award for Best Adapted Screenplay, a Writers Guild of America Award, and an Edgar Award, while his performance received Oscar and Screen Actors Guild nominations for Best Actor.
In 1998, he portrayed the James-Carville-like Richard Jemmons in "Primary Colors". Thornton adapted the book "All the Pretty Horses" into a 2000 film with the same name, starring Matt Damon and Penélope Cruz. The negative experience (he was forced to cut more than an hour) led to his decision to never direct another film (a subsequent release, "Daddy and Them", had been filmed earlier). Also in 2000, an early script which he and Tom Epperson wrote together was made into "The Gift" which starred Cate Blanchett, Hilary Swank, Keanu Reeves, Katie Holmes, Greg Kinnear, and Giovanni Ribisi. In 2000, he also appeared in Travis Tritt's music video for the song "Modern Day Bonnie and Clyde".
Thornton's screen persona has been described by the press as that of a "tattooed, hirsute man's man". He appeared in several major film roles following "Sling Blade" 's success, including 1998's "Armageddon" with Ben Affleck and Bruce Willis, and "A Simple Plan." In 2001, he directed "Daddy and Them", while also securing starring roles in three Hollywood pictures, "Monster's Ball", "Bandits" and "The Man Who Wasn't There", for which he received many awards. He played a malicious mall Santa Claus in 2003's "Bad Santa", a black comedy that performed well at the box office and established Thornton as a leading comic actor, and in the same year, portrayed a womanizing President of the United States in the British romantic comedy "Love Actually". Thornton has stated that, following "Bad Santa"'s success, audiences "like to watch play that kind of guy," and "they [casting directors call up when they need an asshole. It's kinda that simple... you know how narrow the imagination in this business can be." In 2004 he played David Crockett in "The Alamo".
He appeared in the comic film "School for Scoundrels", which was released on September 29, 2006. In the film, he plays a self-help doctor; the role was written specifically for Thornton. More recent films include "The Astronaut Farmer", a drama released on February 23, 2007, and the comedy, "Mr. Woodcock", in which Thornton plays a sadistic gym teacher. In September 2008, Thornton starred in the big brother action movie "Eagle Eye" alongside Shia LaBeouf and Michelle Monaghan. Thornton has also expressed an interest in directing another film, possibly a period piece about cave explorer Floyd Collins, based on the book "Trapped! The Story of Floyd Collins" by Robert K. Murray and Roger Brucker. Thornton received a star on the Hollywood Walk of Fame on October 7, 2004.
In 2014, Thornton starred in the hit FX miniseries, Fargo, as Lorne Malvo. The series is based on the Coen brothers' film of the same name.
Music.
During the late 1990s, Thornton, who has had a lifelong love for music, began a hobby as a singer-songwriter. He released a roots rock album titled "Private Radio" in 2001, and three more albums, "The Edge of the World" (2003), "Hobo" (2005) and "Beautiful Door" (2007). Thornton's manager, David Spero, helped his "Edge of the World" album get off the ground with a summer tour. Thornton was the singer of a blues rock band named "Tres Hombres". Guitarist Billy Gibbons referred to the band as "The best little cover band in Texas", and Thornton bears a tattoo with the band's name on it. He performed the Warren Zevon song "The Wind" on the tribute album "". Thornton recorded a cover of the Johnny Cash classic "Ring of Fire" for the "Oxford American" magazine's Southern Music CD in 2001.
On April 8, 2009, Thornton stopped cooperating with an interview while promoting a Canadian tour of his musical group The Boxmasters on CBC Radio One program "Q". Thornton evaded questions and walked off the set early, after explaining he had insisted the interview was not to refer to his movie career.
Personal life.
Relationships and children.
Thornton has been married five times, with each marriage ending in divorce, and he has four children by three women. From 1978 to 1980, he was married to Melissa Lee Gatlin, with whom he had a daughter, Amanda. Thornton married actress Toni Lawrence in 1986; they separated the following year and divorced in 1988. From 1990 to 1992, he was married to actress Cynda Williams, whom he cast in his writing debut, "One False Move" (1992). In 1993, Thornton married "Playboy" model Pietra Dawn Cherniak, with whom he had two sons, Harry James and William; the marriage ended in 1997, with Cherniak accusing Thornton of spousal abuse.
Thornton was engaged to be married to actress Laura Dern, whom he dated from 1997 to 1999, but in 2000, he married actress Angelina Jolie, with whom he starred in "Pushing Tin" (1999) and who is 20 years his junior. The marriage became known for the couple's eccentric displays of affection, which reportedly included wearing vials of each other's blood around their necks; Thornton later clarified that the "vials" were, instead, two small lockets, each containing only a single drop of blood. Thornton and Jolie announced the adoption of a child from Cambodia in March 2002, but it was later revealed that Jolie had adopted the child as a single parent. They separated in June 2002 and divorced the following year.
Since 2003, Thornton has been in a relationship with makeup effects crew member Connie Angland, with whom he has a daughter, Bella. The family resides in Los Angeles, California. Thornton has stated that he likely will not marry again, specifying that he believes marriage "doesn't work" for him.
Health issues.
During his early years in Los Angeles, Thornton was admitted to a hospital and diagnosed with myocarditis, a heart condition brought on by malnutrition. He has since said that he is vegan and "extremely healthy."
Thornton has obsessive–compulsive disorder. Various idiosyncratic behaviors have been well documented in interviews with Thornton; among these is a phobia of antique furniture—a disorder shared by Dwight Yoakam's character Doyle Hargraves in the Thornton-penned "Sling Blade", and by Thornton's own character in the 2001 film "Bandits". Additionally, he has stated that he has a fear of certain types of silverware, a trait assumed by his character Hank Grotowski in 2001's "Monster's Ball", in which Grotowski insists on a plastic spoon for his daily bowl of chocolate ice cream. In a 2004 interview with "The Independent", Thornton explained: "It's just that I won't use real silver. You know, like the big, old, heavy-ass forks and knives, I can't do that. It's the same thing as the antique furniture. I just don't like old stuff. I'm creeped out by it, and I have no explanation why...I don't have a phobia about American antiques, it's mostly French—you know, like the big, old, gold-carved chairs with the velvet cushions. The Louis XIV type. That's what creeps me out. I can spot the imitation antiques a mile off. They have a different vibe. Not as much dust."
Other.
A baseball fan, Thornton's favorite team is the St. Louis Cardinals. He has said that his childhood dream was to play for the Cardinals. He narrated "The 2006 World Series Film", the year-end retrospective DVD chronicling the Cardinals' championship season. Thornton is also a professed fan of the Indianapolis Colts football team.
Thornton is the cousin of professional wrestling legends Terry Funk and Dory Funk, Jr.

</doc>
<doc id="4472" url="http://en.wikipedia.org/wiki?curid=4472" title="The Big O">
The Big O

The story takes place forty years after a mysterious occurrence causes the residents of Paradigm City to lose their memories. The series follows Roger Smith, Paradigm City's top Negotiator. He provides this much needed service with the help of a robot named R. Dorothy Wayneright and his butler Norman Burg. When the need arises, Roger calls upon Big O, a giant relic from the city's past.
The television series is designed as a tribute to Japanese and Western shows from the 1960s and 1970s. The series is done in the style of "film noir" and combines the feel of a detective show with the mecha genre of anime. The setpieces are reminiscent of tokusatsu productions of the 1950s and 1960s, particularly Toho's kaiju movies, and the score is an eclectic mix of styles and musical homages.
"The Big O" premiered October 13, 1999 on WOWOW satellite television. It finished its run on January 19, 2000. The English-language version premiered on Cartoon Network on April 2, 2001 and ended on April 18 2001. Originally a 26 episode series, low viewership in Japan cut it down to just the first 13. However, positive fan response internationally resulted in a second season consisting of the remaining 13 episodes and co-produced by Cartoon Network, Sunrise, and Bandai Visual. Season two premiered on Japan's SUN-TV on January 2, 2003, with the American premiere taking place seven months later. Following the 2012 closure of Bandai Entertainment, Sunrise announced at Otakon 2013, that Sentai Filmworks has rescued both seasons of "The Big O", along with a handful of other former BEI titles.
Plot.
Setting.
The story of "The Big O" sets in a fictional city-state known as . The city is located on a sea coast and surrounded by a vast desert wasteland, the partially domed city is wholly controlled by the monopolistic Paradigm Corporation. Paradigm is known as , as forty years prior to the story, an unknown occurrence known only as takes place which destroys the world outside the city and leaves the survivors without any memories. In the final episodes of the series, the city is implied to have been an elaborate fabrication produced by an unknown power.
The city is characterized by severe class inequity; the higher-income population resides inside the more pleasant geodesic domes, with the remainder left in tenements outside. Androids coexist with the human inhabitants of Paradigm City; while their numbers are fairly low, and they're something of a rarity, there are enough of them that denizens of the city are not shocked by them and don't consider it particularly unusual to encounter one.
Several episodes show inhabitants of Paradigm City practicing some shape or form of Christianity, as people congregate in meeting places with crucifixes prominently displayed. However, the practice appears to be based on custom rather than doctrine, which no one remembers. A cathedral is shown in ruins and forgotten, although some elderly people occasionally feel compelled to stand in front of it and sing scraps of hymns. It is revealed in episode 11 that almost no one remembers Christmas and its meaning save for Alex Rosewater.
A holiday is observed on December 25, but as a celebration to commemorate the founding of Paradigm City, known as "Heaven's Day." The inhabitants of the city still put up generic Christmas decorations like decorated Christmas trees and streamers, but they don't really know the underlying reason behind all of this. Alex Rosewater seems to be the series' only character with knowledge of pre-Event Christianity. Dastun at one point mentions that Rosewater had in his possession fragments of a "Book of Revelation", although neither Dastun nor Roger had previously heard of it. It is possible that Rosewater also has other fragments of The Bible, as Rosewater describes the real meaning of Heaven's Day as being "the day God's son was born."
Story.
Forty years prior to the events of the series, disaster struck. The world was turned into a vast desert wasteland and the survivors were left without memories. The story takes place in Paradigm City, a corporate police state run by the Paradigm Corporation. The town is recognized for its geodesic domes, giant structures that house the richer citizens and segregate the poor.
"The Big O" deals with the nature of memories. A memory is a record stored in the brain of an organism, but in Paradigm City memories can mean much more. embody the lost knowledge of its residents, and can take the form of records from before the Event, forgotten artifacts from the previous era, or manifest themselves as recollection, hallucinations, and recurring dreams.
The first half of the series is episodic. Each Act revolves around different citizens of Paradigm dealing with the resurgence of lost Memories and how they manage to go on living without knowledge of what did or did not happen. The final episodes introduce elements that come into play during season two like the existence of people outside of Paradigm City, the nature of the Cataclysm that destroyed the world and the "Power of God wielded by the hand of man."
The second season takes an arc-based approach. Instead of self-contained stories like in season one, season two features a continuous storyline. The second season makes Alex Rosewater, CEO of the Paradigm Corporation, a direct antagonist to The Negotiator and introduces The Union, agents of a foreign power working within Paradigm.
Ending.
The series ends with the awakening of another "Big" megadeus and the revelation that the world appears to be a simulated reality constructed in the form of a giant mechanical stage which was created by Angel. A climactic battle ensues between Big O and Big Fau, after which the city and everything surrounding it begins to be systematically erased by the new Big megadeus (an embodiment of Angel). The Big, referred to as "Big Venus" by Dorothy, walks towards Big O, erasing everything it passes, including Big Fau with Alex Rosewater. Roger delivers a final speech to Angel telling her to "let go of the past" whether it is real or not and focus only on the present and the future. In what appears to be a TV Director's control room, the real Angel, who had been observing the fight through camera as well as reminding of herself of her other encounters with Roger on all of the screens surrounding her, watches as Roger tells her to 'live as a human being'. On the control panel is "Metropolis" (a book which had been featured prominently since the thirteenth episode and said to have been published by Gordon Rosewater, containing the information of what happened forty years ago (which Gordon himself dismisses as 'a lie')) and on its cover is an illustration of angel wings and the author's name, 'Angel Rosewater'. Angel then sheds a tear and turns to see Roger, as he puts his hand on her shoulder reassuringly, and Dorothy who is standing next to him states Roger's profession, "Roger the Negotiator". At that moment, Big Venus finally reaches Big O and the two walk into each other, subsequently "resetting" the world. The screen is then blinded with a white flash and then cuts to the first few scenes of the first episode, even including the opening text. New versions of Dorothy and Angel are then seen as they watch Roger drive down the street and he begins to hesitantly speak the first lines of the series "My name is... Roger Smith. I perform a much needed job here... in the City of Amnesia." The ending title card "We have come to terms" appears, the credits roll and (like the final episode of the first season) there is no preview. This ending has been thoroughly debated, however, as many fans feel it was simply symbolism and that the true nature of the ending remains open to interpretation.
Characters.
Roger Smith is the series's protagonist. As a Negotiator, his job entails finding a resolution for the troubles of the City of Amnesia. He'd negotiate almost anything for anyone, but he is a professional and expects the parties involved to behave professionally. When memories betray the people and force them to reawaken monstrosities of the city's past, Roger's only option is to fight back with a monstrosity of his own, the black megadeus Big O. Roger is voiced by Mitsuru Miyamoto in Japanese and Steven Blum in English.
R. Dorothy Wayneright is Roger's assistant. Introduced in Act:01 as Dorothy Soldano, daughter of rich industrialist Miguel Soldano, she is later revealed to be an android constructed by him. Her actual "father" would be Timothy Wayneright, the man who commissioned her construction and father of the real and late Dorothy Wayneright. To show her gratitude, and as a form of payment for Roger's help, she decides to move in with him and help out Norman with the chores. Dorothy is voiced by Akiko Yajima in Japanese and Lia Sargent in English. The character's name is consistent with naming practices in the science fiction works of Isaac Asimov, the first initial "R" standing for "robot".
Norman Burg is Roger's butler. Forty years before the commencement of the story, Norman, like the rest of Paradigm, lost all memories from before that day, but he would not think twice before going once more unto the breach for his master. Resourceful and talented, he is also caretaker of the Big O. Norman's skills give him a purpose and a mission to accomplish for Roger. Norman is voiced by Motomu Kiyokawa in Japanese and Milton James and Alan Oppenheimer in English for seasons one and two respectively.
Dan Dastun is the middle-aged Chief of the Military Police, introduced in Act:01. He is Roger's former commander and the Negotiator's contact in the police force. Roger describes him as "a hard-nosed cop is completely devoted to the force" and with "more pride in the Military Police than anything else." Dan is voiced by Tesshou Genda in Japanese and Peter Lurie in English.
Angel is the beautiful woman Roger encounters throughout the series. Introduced in Act:03 as Casey Jenkins, investigator for Paradigm Power Management, then again in Act:04 as Patricia Lovejoy, secretary for the publisher of Paradigm Press. Angel's true identity is a mystery, her motives questionable and her allegiance to no one but herself. Angel is voiced by Emi Shinohara in Japanese and Wendee Lee in English.
Production and release.
Development of the retro-styled series began in 1996. Keiichi Sato came up with the concept of "The Big O": a giant city-smashing robot, piloted by a man in black, in a Gotham-like environment. He later met up with Kazuyoshi Katayama, who had just finished directing "Those Who Hunt Elves", and started work on the layouts and character designs. But when things "were about to really start moving," production on Katayama's "Sentimental Journey" began, putting plans on-hold. Meanwhile, Sato was heavily involved with his work on "City Hunter".
Sato admits it all started as "a gimmick for a toy" but the representatives at Bandai Hobby Division did not see the same potential. From there on, the dealings would be with Bandai Visual, but Sunrise still needed some safeguards and requested more robots be designed to increase prospective toy sales. In 1999, with the designs complete, Chiaki J. Konaka was brought on as head writer. Among other things, Konaka came up with the idea of "a town without memory" and his writing staff put together the outline for a 26-episodes series.
"The Big O" premiered on 13 October 1999 on WOWOW. When the production staff was informed the series would be shortened to 13 episodes, the writers decided to end it with a cliffhanger, hoping the next 13 episodes would be picked up. In April 2001, "The Big O" premiered on Cartoon Network's Toonami lineup. In Canada, the English dub of both seasons aired as part of Anime Current, an Anime Television block, on pay-TV digital channel G4techTV Canada from March 30, 2007 to May 4, 2007.
The series garnered positive fan response internationally that resulted in a second season co-produced by Cartoon Network, Sunrise, and Bandai Visual. Season two premiered on Japan's SUN-TV on January 2003, with the American premiere taking place seven months later as an Adult Swim exclusive. The second season would not be seen on Toonami until July 27, 2013, 10 years after it began airing on Adult Swim.
The second season was scripted by Chiaki Konaka with input from the American producers. Along with the 13 episodes of season two, Cartoon Network had an option for 26 additional episodes to be written by Konaka, but according to Jason DeMarco, executive producer for season two, the middling ratings and DVD sales in the United States and Japan made any further episodes impossible to be produced.
Music.
"The Big O" was scored by "Geidai" alumnus Toshihiko Sahashi. His composition is richly symphonic and classical, with a number of pieces delving into electronica and jazz. Chosen because of his "frightening amount of musical knowledge about TV dramas overseas," Sahashi integrates musical homages into the soundtrack. The background music draws from "film noir", spy films and sci-fi television series like "The Twilight Zone". The battle themes are reminiscent of Akira Ifukube's compositions for the "Godzilla" series.
The first opening theme is the Queen-influenced "Big-O!". Composed, arranged and performed by Rui Nagai, the song resembles the theme to the "Flash Gordon" film. The second opening theme is "Respect," composed by Sahashi. The track is an homage to the music of Gerry Anderson's "UFO", composed by Barry Gray. In 2007, Rui Nagai composed "Big-O! Show Must Go On," a 1960s hard rock piece, for Animax's reruns of the show. The closing theme is the slow love ballad "And Forever," written by Chie and composed by Ken Shima. The duet is performed by Robbie Danzie and Naoki Takao.
Along with Sahashi's original compositions, the soundtrack features Chopin's Prelude No. 15 and a jazz saxophone rendition of "Jingle Bells." The complete score was released in two volumes by Victor Entertainment.
Publications.
"The Big O" was conceived as a media franchise. To this effect, Sunrise requested a manga be produced along with the animated series. "The Big O" manga started serialization in Kodansha's "Magazine Z" on July 1999, three months before the anime premiere. Authored by Hitoshi Ariga, the manga uses Keiichi Sato's concept designs in an all-new story. The series ended on October 2001. The issues were later collected in six volumes. The English version of the manga is published by Viz Media.
In anticipation to the broadcast of the second season, a new manga series was published. , authored by Hitoshi Ariga. "Lost Memory" takes place between volumes five and six of the original manga. The issues were serialized in "Magazine Z" from November 2002 to September 2003 and were collected in two volumes.
, a novel by Yuki Taniguchi, was released 16 July 2003 by Tokuma Shoten.
Design.
"The Big O" is the brainchild of Keiichi Sato and Kazuyoshi Katayama, an homage to the shows they grew up with. The show references the works of the superhero shows produced by the Toei Company and "old school" super robots. The series is done in the style of "film noir" and pulp fiction and combines the feel of a detective show with the giant robot genre.
Style.
"Film noir" is a stylistic approach to genre films forged in Depression-era detective and gangster films and hard-boiled detective stories which were a staple of pulp fiction. "The Big O" shares much of its themes, diction, archetypes and visual iconography with "film noirs" of the 1940s like "The Big Sleep" (1946).
Low-key lighting schemes mark most "noirs". The series incorporates the use of long dark shadows in the tradition of "chiaroscuro" and tenebrism. "Film noir" is also known for its use of odd angles, such as Roger's low shot introduction in the first episode. "Noir" cinematographers favoured this angle because it made characters almost rise from the ground, giving them dramatic girth and symbolic overtones. Other disorientating devices like dutch angles, mirror reflection and distorting shots are employed throughout the series.
The characters of "The Big O" fit the "noir" and pulp fiction archetypes. Roger Smith is a protagonist in the mold of Chandler's Philip Marlowe or Hammett's Sam Spade. He is canny and cynical, a disillusioned cop-turned-negotiator whose job has more in common with detective-style work than negotiating. Big Ear is Roger's street informant and Dan Dastun is the friend on the police force. The recurring Beck is the imaginative thug compelled by delusions of grandeur while Angel fills the role of the "femme fatale". Minor characters include crooked cops, corrupt business men and deranged scientists.
"Noir" characters often wisecrack and speak in double entendres. The dialogue in the series is recognized for its witty, wry sense of humor. The characters come off as charming and exchange banter not often heard in anime series, as the dialogue has the tendency to be straightforward. The plot is moved along by Roger's voice-over narration, a device used in "film noir" to place the viewer in the mind of the protagonist so it can intimately experience the character's angst and partly identify with the narrator.
The urban landscape, Paradigm City, is the perfect "noir" milieu. The tall buildings and giant domes create a sense of claustrophobia and paranoia characteristic of the style. The rural landscape, Ailesberry Farm, contrasts Paradigm City. "Noir" protagonists often look for sanctuary in such settings but, as seen in Act:23, they just as likely end up becoming a killing ground. The series score is representative of its setting. While no classic "noir" possesses a jazz score, the music could be heard in nightclubs within the films. Roger's recurring theme, a lone saxophone accompaniment to the protagonist's narration, best exemplifies the "noir" stylings of the series.
Amnesia is a common plot device in "film noir". Because most of these stories focused on a character proving his innocence, authors up the ante by making him an amnesiac, unable to prove his innocence even to himself. "The Big O" goes further, by removing the memories of the whole population. The convoluted past is told through the use of flashbacks. In most "noirs", the past is tangible and menacing. The characters are often trying to escape some trauma or crime tied to the Event, and confronting it becomes their only chance at redemption.
Influences.
Before "The Big O", Sunrise Studios was a subcontractor for Warner Bros. Animation's "", one of the series' influences.
Roger Smith is a pastiche of the Bruce Wayne persona and the Batman. The character design resembles Wayne, complete with slicked-back hair and double-breasted business suit. Like Bruce, Roger prides himself in being a rich playboy to the extent that one of his household's rules is only women may be let into his mansion without his permission. Like Batman, Roger Smith carries a no-gun policy, albeit more flexible. Unlike the personal motives of the Batman, Roger enforces this rule for "it's all part of being a gentleman." Among Roger's gadgetry is the Griffon, a large, black hi-tech sedan comparable to the Batmobile, a grappling cable that shoots out his wristwatch and the giant robot that Angel calls "Roger's alter ego."
"The Big O"'s cast of supporting characters includes Norman, Roger's faithful mechanically-inclined butler who fills the role of Alfred Pennyworth; R. Dorothy Wayneright, who plays the role of the sidekick; and Dan Dastun, a good honest cop who, like Jim Gordon, is both a friend to the hero and greatly respected by his comrades.
The other major influence is Mitsuteru Yokoyama's "Giant Robo". Before working on "The Big O", Kazuyoshi Katayama and other animators worked with Yasuhiro Imagawa on "". The feature, a "retro chic" homage to Yokoyama's career, took seven years to produce and suffered low sales and high running costs. Frustrated by the experience, Katayama and his staff put all their efforts into making "good" with "The Big O".
Like Giant Robo, the megadeuses of "Big O" are metal behemoths. The designs are strange and "more macho than practical," sporting big stovepipe arms and exposed rivets. Unlike the giants of other mecha series, the megadeuses do not exhibit ninja-like speed nor grace. Instead, the robots are armed with "old school" weaponry such as missiles, piston powered punches, machine guns and laser cannons.
Katayama also cited "Super Robot Red Baron" and "Super Robot Mach Baron" among influences on the inspiration of "The Big O". Believing that because "Red Baron" had such a low budget and the big fights always happened outside of a city setting, he wanted "Big O" to be the show he felt "Red Baron" could be with a bigger budget. He also spoke of how he first came up with designs for the robots first as if they were making designs to appeal to toy companies, rather than how "Gundam" was created with a toy company wanting an anime to represent their new product. Big O's large pumping chamber arms, for example, he felt would be cool gimmicks in a toy.
Media.
"The Big O Visual: The official companion to the TV series" (ISBN 4-575-29579-5) was published by Futabasha in 2003. The book contains full-color artwork, character bios and concept art, mecha sketches, video/LD/DVD jacket illustrations, history on the making of The Big O, staff interviews, "Roger's Monologues" comic strip and the original script for the final episode of the series.
"Walking Together On The Yellow Brick Road" was released by Victor Entertainment on 21 September 2000. The drama CD was written by series head writer Chiaki J. Konaka and featured the series' voice cast.
The first season of Big O is featured in "Super Robot Wars D" for the Game Boy Advance. The series, including its second season is also featured in Super Robot Wars Z, released in 2008.
Bandai released a non-scale model kit of Big O in 2000. Though it was an easy snap-together kit, it required painting, as all of the parts (except the clear orange crown and canopy) were molded in dark gray. The kit included springs that enabled the slide-action Side Piles on the forearms to simulate Big O's Sudden Impact maneuver. Also included was an unpainted Roger Smith figure.
PVC figures of Big O and Big Duo (Schwarzwald's Megadeus) were sold by Bandai America. Each came with non-poseable figures of Roger, Dorothy and Angel. Mini-figure sets were sold in Japan and America during the run of the second season. The characters included Big O (standard and attack modes), Roger, Dorothy & Norman, Griffon (Roger's car), Dorothy-1 (Big O's first opponent), Schwarzwald and Big Duo.
In 2009, Bandai released a plastic/diecast figure of the Big O under their Soul of Chogokin line. The figure has the same features as the model kit, but with added detail and accessories. Its design was closely supervised by original designer Keiichi Sato.
In 2011, Max Factory released action figures of Roger and Dorothy through their Figma toyline. Like most Figmas, they are very detailed, articulated and come with accessories and interchangeable faces. In the same year, Max Factory also released a 12-inch, diecast figure of Big O under their Max Gokin line. The figure contained most of the accessories as the Soul of Chogokin figure but also included some others that could be bought separately from the SOC figure, such as the Mobydick (hip) Anchors and Roger Smith's car: the Griffon. Like the Soul of Chogokin figure, its design was also supervised by Keiichi Sato. As well, in that same year, Max Factory released soft vinyl figures of Big Duo and Big Fau. These figures are high in detail but limited in articulation, such as the arms and legs being the only things to move. To date, this is the only action figure of Big Fau.
Reception.
"The Big O" premiered on 13 October 1999. The show was not a hit in its native Japan, rather it was reduced from an outlined 26 episodes to 13 episodes. Western audiences were more receptive and the series achieved the success its creators were looking for. In an interview with AnimePlay, Keiichi Sato said "This is exactly as we had planned", referring to the success overseas.
Several words appear constantly in the English-language reviews; adjectives like "hip", "sleek," "stylish",
The first season's reception was positive. Anime on DVD recommends it as an essential series. Chris Beveridge of the aforementioned site gave an A- to Vols. 1 and 2, and a B+ to Vols. 3 and 4. Mike Toole of Anime Jump gave it 4.5 (out of a possible 5) stars, while the review at the Anime Academy gave it a grade of 83, listing the series' high points as being "unique", the characters "interesting," and the action "nice." Reviewers, and fans alike, agree the season's downfall was the ending, or its lack thereof. The dangling plot threads frustrated the viewers and prompted Cartoon Network's involvement in the production of further episodes.
The look and feel of the show received a big enhancement in the second season. This time around, the animation is "near OVA quality" and the artwork "far more lush and detailed." Also enhanced are the troubles of the first season. The giant robot battles still seem out of place to some, while others praise the "over-the-top-ness" of their execution.
For some reviewers, the second season "doesn't quite match the first" addressing to "something" missing in these episodes. Andy Patrizio of IGN points out changes in Roger Smith's character, who "lost some of his cool and his very funny side in the second season." Like a repeat of season one, this season's ending is considered its downfall. Chris Beveridge of Anime on DVD wonders if this was head writer "Konaka's attempt to throw his hat into the ring for creating one of the most confusing and oblique endings of any series." Patrizio states "the creators watched "The Truman Show" and "The Matrix" a few times too many."
References.
http://en.wikipedia.org/wiki/Chiaki_J._Konaka#References. Wikipedia, www.wikipedia.org.

</doc>
<doc id="4473" url="http://en.wikipedia.org/wiki?curid=4473" title="BIOS">
BIOS

In IBM PC compatible computers, the Basic Input/Output System (BIOS), also known as System BIOS, ROM BIOS or PC BIOS (), is a "de facto" standard defining a firmware interface. The name originated from the Basic Input/Output System used in the CP/M operating system in 1975. The BIOS software is built into the PC, and is the first software run by a PC when powered on.
The fundamental purposes of the BIOS are to initialize and test the system hardware components, and to load a bootloader or an operating system from a mass memory device. The BIOS additionally provides an abstraction layer for the hardware, i.e. a consistent way for application programs and operating systems to interact with the keyboard, display, and other input/output devices. Variations in the system hardware are hidden by the BIOS from programs that use BIOS services instead of directly accessing the hardware. Modern operating systems ignore the abstraction layer provided by the BIOS and access the hardware components directly.
The BIOS of the original IBM PC/XT had no interactive user interface. Error messages were displayed on the screen, or coded series of sounds were generated to signal errors. Options on the PC and XT were set by switches and jumpers on the main board and on peripheral cards. Modern Wintel-compatible computers provide a setup routine, accessed at system power-up by a particular key sequence. The user can configure hardware options using the keyboard and video display.
BIOS software is stored on a non-volatile ROM chip on the motherboard. It is specifically designed to work with each particular model of computer, interfacing with various devices that make up the complementary chipset of the system. In modern computer systems, the BIOS contents are stored on a flash memory chip so that the contents can be rewritten without removing the chip from the motherboard. This allows BIOS software to be easily upgraded to add new features or fix bugs, but can make the computer vulnerable to BIOS rootkits.
MS-DOS (PC DOS), which was the dominant PC operating system from the early 1980s until the mid 1990s, relied on BIOS services for disk, keyboard, and text display functions. MS Windows NT, Linux, and other protected mode operating systems in general do not use it after loading.
BIOS technology is in transitional process toward the Unified Extensible Firmware Interface (UEFI) since 2010.
Terminology.
The term BIOS (Basic Input/Output System) was invented by Gary Kildall and first appeared in the CP/M operating system in 1975, describing the machine-specific part of CP/M loaded during boot time that interfaces directly with the hardware. (A CP/M machine usually has only a simple boot loader in its ROM.)
Versions of MS-DOS or PC DOS contain a file called variously "IO.SYS", "IBMBIO.COM", "IBMBIO.SYS", or "DRBIOS.SYS"; this file is known as the "DOS BIOS" (aka "DOS I/O System") and contains the lower-level hardware-specific part of the operating system. Together with the underlying hardware-specific, but operating system-independent "System BIOS", which resides in ROM, it represents the analogous to the "CP/M BIOS".
In other types of computers, the terms "boot monitor", "boot loader", and "boot ROM" are used instead. Some Sun and PowerPC-based computers use Open Firmware for this purpose.
With the introduction of PS/2 machines, IBM divided the System BIOS into real-mode and protected mode portions. The real-mode portion was meant to provide backward-compatibility with existing operating systems such as DOS, and therefore was named "CBIOS" (for Compatibility BIOS), whereas the "ABIOS" (for Advanced BIOS) provided new interfaces specifically suited for multitasking operating systems such as OS/2.
There are a few alternatives to the functionality of the "Legacy BIOS" in the x86 world: Extensible Firmware Interface, Open Firmware (used on the OLPC XO-1), and coreboot.
The BIOS boot process.
When the x86 processor is reset, it loads its program counter with a fixed address near the top of the 1 megabyte real-mode address space. The address of the BIOS's memory is located such that it will be executed when the computer is first started up. A jump instruction then directs the processor to start executing code in the BIOS. If the system has just been powered up or the reset button was pressed ("cold boot"), the full power-on self-test (POST) is run. If Ctrl+Alt+Delete was initiated ("warm boot"), a special flag value is detected in Nonvolatile memory (NVRAM) and the BIOS does not run the POST. This saves the time otherwise used to detect and test all memory. The NVRAM is in the real-time clock (RTC).
The power-on self-test tests, identifies, and initializes system devices such as the CPU, RAM, interrupt and DMA controllers and other parts of the chipset, video display card, keyboard, hard disk drive, optical disc drive and other basic hardware. The BIOS then locates boot loader software held on a storage device designated as a 'boot device', such as a hard disk, a floppy disk, CD, or DVD, and loads and executes that software, giving it control of the PC. This process is known as "booting", or booting up, which is short for "bootstrapping".
Boot devices.
The BIOS selects candidate boot devices using information collected by POST and configuration information from EEPROM, CMOS RAM or, in the earliest PCs, DIP switches. Option ROMs may also influence or supplant the boot process defined by the motherboard BIOS ROM. The BIOS checks each device in order to see if it is bootable. For a disk drive or a device that logically emulates a disk drive, such as a USB Flash drive or perhaps a tape drive, to perform this check the BIOS attempts to load the first sector (boot sector) from the disk to memory address codice_1. If the sector cannot be read (due to a missing or blank disk, or due to a hardware failure), the BIOS considers the disk unbootable and proceeds to check the next device. Some BIOSes will also check for the boot sector signature 0x55 0xAA in the last two bytes of the (512 byte long) sector, before accepting a boot sector.
The BIOS proceeds to test each device sequentially until a bootable device is found, at which time the BIOS transfers control to the loaded sector with a jump instruction and certain register values to its first byte at address codice_1 (1 KiB below the 32 KiB mark); see MBR invocation and VBR invocation.
Another device such as a network adapter attempts booting by a procedure that is defined by its option ROM (or the equivalent integrated into the motherboard BIOS ROM).
The behavior if the BIOS doesn't find a bootable device has varied as personal computers developed. The original IBM PC and XT had Microsoft Cassette BASIC in ROM, and if no bootable device was found, ROM BASIC was started. Therefore, barring a hardware failure, an original IBM PC or XT would never fail to boot, either into BASIC or from disk. One model of PC was available with no disk drive; a cassette recorder could be attached via the cassette port on the rear, for loading and saving BASIC programs to tape. Since few programs used BASIC in ROM, clone PC makers left it out; a computer that failed to boot from a disk would display "No ROM BASIC" and halt. Later computers would display a message like "No bootable disk found". Modern BIOSes may display nothing or may automatically enter the BIOS configuration utility when the boot process fails.
Historically, the BIOS would try to boot from a floppy drive first and a hard disk second. CD or DVD booting is an extension of this. With the El Torito optical media boot standard, the optical drive actually emulates a 3.5" high-density floppy disk to the BIOS for boot purposes. Optical disks are a special case, because their lowest level of data organization is typically a fairly high-level file system (e.g. ISO 9660 for CD-ROM). To read the "first sector" of a CD-ROM or DVD-ROM is not a simply defined operation. The complexity of the medium makes it difficult to write a useful boot program in one sector. Therefore, optical media booting uses the El Torito standard, which specifies a way for an optical disk to contain an image of a high-density ("1.44MB") floppy disk and for the drive to provide access to this disk image in a simple manner that emulates floppy disk drive operations. Therefore, CD-ROM drives boot as emulated floppy disk drives; the bootable virtual floppy disk can then contain software that provides access to the optical medium in its native format.
A little-known feature of the original IBM BIOS versions is that before beginning the normal boot process they would attempt to load a program through the keyboard port. This was intended for factory test or diagnostic purposes. This was of limited utility outside of factory or repair facilities.
BIOS extensions.
In the IBM PC and AT, peripheral cards such as hard-drive controllers and video display adapters had their own BIOS extension option ROMs, which provided additional functionality. Code in these extensions runs before the operating system is loaded from mass storage. These ROMs can test and initialize hardware, add BIOS services, or replace BIOS services with their own versions of those services. For example, a SCSI controller usually has a BIOS extension ROM that adds support for hard drives connected through that controller. Some video cards have extension ROMs that replace the video services of the motherboard BIOS with their own video services. BIOS extension ROMs gain total control of the machine, so they may never return control to the BIOS that invoked them. An extension ROM could in principle contain an entire operating system or an application program, or it could implement an entirely different boot process such as booting from a network. Operation of an IBM-compatible computer system can be completely changed by removing or inserting an adapter card (or a ROM chip).
A computer system can contain several BIOS firmware chips. The motherboard BIOS typically contains code to access hardware components necessary for bootstrapping the system, such as the keyboard, display, and storage. In addition, plug-in adapter cards such as SCSI, RAID, network interface cards, and video boards often include their own BIOS (e.g. Video BIOS), complementing or replacing the system BIOS code for the given component. Even devices built into the motherboard can behave in this way; their option ROMs can be stored as separate code on the main BIOS flash chip, and upgraded either in tandem with, or separately from, the main BIOS.
An add-in card requires an option ROM if it needs to be used before the operating system can be loaded (usually this means it is required in the bootstrapping process), and is not supported by the main BIOS.
After completing the POST, the motherboard BIOS scans for extension ROMs in an area of the "upper memory area" space and runs each ROM found, in order. To discover memory-mapped ISA option ROMs during the boot process, BIOS implementations scan real-mode address space from codice_3 to codice_4 on 2 KiB boundaries, looking for a ROM "signature": 0x55 followed by 0xAA. In a valid expansion ROM, this signature is followed by a single byte indicating the number of 512-byte blocks it occupies in real memory. The next byte contains an offset describing the option ROM's entry point. A checksum of the specified number of 512-byte blocks is calculated, and if the ROM has a valid checksum the BIOS transfers control to the specified entry address. At this point, the expansion ROM code takes over, using BIOS services to register interrupt vectors for use by post-boot applications, to provide a user configuration interface, or to display diagnostic information.
There are many methods and utilities for examining the contents of various motherboard BIOS and expansion ROMs, such as Microsoft DEBUG or the Unix dd.
Boot environment.
The environment for the boot program is very simple: the CPU is in real mode and the general-purpose and segment registers are undefined. All BIOS services are available, and the memory below address codice_5 contains the interrupt vector table and the 256-byte BIOS data area, but the boot program must set up its own stack (or at least MS-DOS 6 acts like it must). All memory at and above address codice_5 can be used by the boot program; it may even overwrite itself. The BIOS initializes a reserved block of system RAM with various parameters initialized during the POST. The interrupt vectors corresponding to the BIOS interrupts have been set to point at the appropriate entry points in the BIOS.
Operating system services.
The BIOS ROM is customized to the particular manufacturer's hardware, allowing low-level services (such as reading a keystroke or writing a sector of data to diskette) to be provided in a standardized way to an operating system. For example, an IBM PC might have either a monochrome or a color display adapter (using different display memory addresses and hardware), but a single, standard, BIOS system call may be invoked to display a character at a specified position on the screen in text mode.
The BIOS provides a small library of basic input/output functions to operate peripherals (such as the keyboard, rudimentary text and graphics display functions and so forth). When using MS-DOS, BIOS services could be accessed by an application program (or by MS-DOS) by executing an INT 13H interrupt instruction to access disk sectors, or one of a number of other documented BIOS interrupt calls to access video display, keyboard, cassette, and other devices.
Operating systems and executive software, designed to supersede this basic firmware functionality, provide replacement software interfaces to applications. This began even in the 1980s under MS-DOS, when programmers observed that using the BIOS video services for graphics display was very slow. To increase the speed of screen output, many programs bypassed the BIOS and programmed the video display hardware directly. Since the AT-compatible BIOS ran in Intel real mode, operating systems on '286 and later processors required hardware device drivers compatible with protected mode operation to replace BIOS services. In modern personal computers the BIOS is used only during booting and initial loading of system software. Before the operating system's first graphical screen is displayed, input and output are typically handled through BIOS. A boot menu such as the textual menu of Windows that allows one to choose an operating system to boot or to boot into Safe Mode or to use the last known good configuration, is displayed and receives keyboard input through BIOS.
BIOS components.
In Intel systems, the BIOS may contain components such as the Memory Reference Code (MRC), which is responsible for handling memory timings and related hardware settings.
Setup utility.
Historically, the BIOS in the IBM PC and XT had no built-in user interface. The BIOS versions in earlier PCs (XT-class) were not software configurable; instead, users set the options via DIP switches on the motherboard. Later computers, including all IBM-compatibles with 80286 CPUs, had a battery-backed nonvolatile BIOS memory (CMOS RAM chip) that held BIOS settings. These settings, such as video-adapter type, memory size, and hard-disk parameters, could only be configured by running a configuration program from a disk, not built into the ROM. A special "reference diskette" was inserted in an IBM AT to configure settings such as memory size.
Early BIOS versions did not have passwords or boot-device selection options. The BIOS was hard-coded to boot from the first floppy drive, or, if that failed, the first hard disk. Access control in early AT-class machines was by a physical keylock switch (which was not hard to defeat if the computer case could be opened). Anyone who could switch on the computer could boot it.
Later, 386-class computers started integrating the BIOS setup utility in the ROM itself, alongside with the BIOS code; the computers usually booted into the BIOS setup utility if a certain key or key combination is pressed, otherwise the actual BIOS code was booted.
A modern BIOS setup utility has a menu-based user interface (UI) accessed by pressing a certain key on the keyboard when the PC starts. Usually the key is advertised for short time during the early startup, for example "Press F1 to enter CMOS setup". The actual key depends on specific hardware. In the BIOS setup utility, a user can:
Chips.
The original IBM PC BIOS (and cassette BASIC) was stored on mask-programmed read-only memory (ROM) chips in sockets on the motherboard. ROMs could be replaced, but not altered, by users. To allow for updates, many compatible computers used re-programmable memory devices such as EEPROM and later flash memory devices. According to Robert Braver, the president of the BIOS manufacturer Micro Firmware, Flash BIOS chips became common around 1995 because the electrically erasable PROM (EEPROM) chips are cheaper and easier to program than standard ultraviolet erasable PROM (EPROM) chips. Flash chips are programmed (and re-programmed) in-circuit, while EPROM chips need to be removed from the motherboard for re-programming. BIOS versions are upgraded to take advantage of newer versions of hardware and to correct bugs in previous revisions of BIOSes.
Beginning with the IBM AT, PCs supported a hardware clock settable through BIOS. It had a century bit which allowed for manually changing the century when the year 2000 happened. Most BIOS revisions created in 1995 and nearly all BIOS revisions in 1997 supported the year 2000 by setting the century bit automatically when the clock rolled past midnight, December 31, 1999.
The first flash chips were attached to the ISA bus. Starting in 1997, the BIOS flash moved to the LPC bus, a functional replacement for ISA, following a new standard implementation known as "firmware hub" (FWH). In 2006, the first systems supporting a Serial Peripheral Interface (SPI) appeared, and the BIOS flash memory moved again.
The size of the BIOS, and the capacities of the ROM, EEPROM and other media it may be stored on, has increased over time as new features have been added to the code; BIOS versions now exist with sizes up to 16 megabytes. Some modern motherboards are including even bigger NAND flash memory ICs on board which are capable of storing whole compact operating systems, such as some Linux distributions. For example, some ASUS motherboards included SplashTop Linux embedded into their NAND flash memory ICs.
Another type of firmware chip was found on the IBM PC and early compatibles. In the PC and AT, the keyboard interface was controlled by a microcontroller with its own programmable memory. On the IBM AT, this was a 40 pin socketed device. Some manufacturers used an EPROM version of this chip which resembled an EPROM. In the AT, this controller was also assigned the A20 gate function to manage memory above the 1 megabyte range; occasionally an upgrade of this "keyboard BIOS" was necessary to take advantage of software that could use upper memory.
Flashing the BIOS.
In modern PCs the BIOS is stored in rewritable memory, allowing the contents to be replaced or 'rewritten'. This rewriting of the contents is sometimes termed flashing. This can be done by a special program, usually provided by the system's manufacturer, or at POST, with a BIOS image in a hard drive or USB flash drive. A file containing such contents is sometimes termed 'a BIOS image'. A BIOS might be reflashed in order to upgrade to a newer version to fix bugs or provide improved performance or to support newer hardware, or a reflashing operation might be needed to fix a damaged BIOS.
Overclocking.
Some BIOS chips allow overclocking, an action in which the CPU is adjusted to a higher clock rate than its factory preset. Overclocking may, however, seriously compromise system reliability in insufficiently cooled computers and generally shorten component lifespan. Overclocking, incorrectly performed, may also cause components to overheat so quickly that they destroy themselves.
BIOS chip vulnerabilities.
EEPROM chips are advantageous because they could be easily updated by the user; hardware manufacturers frequently issue BIOS updates to upgrade their products, improve compatibility and remove bugs. However, this advantage had the risk that an improperly executed or aborted BIOS update could render the computer or device unusable. To avoid these situations, more recent BIOSes use a "boot block"; a portion of the BIOS which runs first and must be updated separately. This code verifies if the rest of the BIOS is intact (using hash checksums or other methods) before transferring control to it. If the boot block detects any corruption in the main BIOS, it will typically warn the user that a recovery process must be initiated by booting from removable media (floppy, CD or USB memory) so the user can try flashing the BIOS again. Some motherboards have a "backup" BIOS (sometimes referred to as DualBIOS boards) to recover from BIOS corruptions.
Virus attacks.
There are at least four known BIOS attack viruses, two of which were for demonstration purposes. The first one found in the wild was Mebromi, targeting Chinese users.
The first BIOS virus was CIH, whose name matches the initials of its creator, Chen Ing Hau. CIH was also called the "Chernobyl Virus", because its payload date was 1999-04-26, the 13th anniversary of the Chernobyl accident.
CIH appeared in mid-1998 and became active in April 1999. It was able to erase flash ROM BIOS content. Often, infected computers could no longer boot, and people had to remove the flash ROM IC from the motherboard and reprogram it. CIH targeted the then-widespread Intel i430TX motherboard chipset and took advantage of the fact that the Windows 9x operating systems, also widespread at the time, allowed direct hardware access to all programs.
Modern systems are not vulnerable to CIH because of a variety of chipsets being used which are incompatible with the Intel i430TX chipset, and also other flash ROM IC types. There is also extra protection from accidental BIOS rewrites in the form of boot blocks which are protected from accidental overwrite or dual and quad BIOS equipped systems which may, in the event of a crash, use a backup BIOS. Also, all modern operating systems such as FreeBSD, Linux, OS X, Windows NT-based Windows OS like Windows 2000, Windows XP and newer, do not allow user-mode programs to have direct hardware access. As a result, as of 2008, CIH has become essentially harmless, at worst causing annoyance by infecting executable files and from antivirus software. Other BIOS viruses remain possible, however; since most Windows home users without Windows Vista/7's UAC run all applications with administrative privileges, a modern CIH-like virus could in principle still gain access to hardware without first using an exploit. The operating system OpenBSD prevents all users from having this access and the grsecurity patch for the linux kernel also prevents this direct hardware access by default, the difference being an attacker requiring a much more difficult kernel level exploit or reboot of the machine.
The second BIOS virus was a technique presented by John Heasman, principal security consultant for UK-based Next-Generation Security Software. In 2006, at the Black Hat Security Conference, he showed how to elevate privileges and read physical memory, using malicious procedures that replaced normal ACPI functions stored in flash memory.
The third BIOS virus was a technique called "Persistent BIOS infection." It appeared in 2009 at the CanSecWest Security Conference in Vancouver, and at the SyScan Security Conference in Singapore. Researchers Anibal Sacco and Alfredo Ortega, from Core Security Technologies, demonstrated how to insert malicious code into the decompression routines in the BIOS, allowing for nearly full control of the PC at start-up, even before the operating system is booted.
The proof-of-concept does not exploit a flaw in the BIOS implementation, but only involves the normal BIOS flashing procedures. Thus, it requires physical access to the machine, or for the user to be root. Despite these requirements, Ortega underlined the profound implications of his and Sacco's discovery: “We can patch a driver to drop a fully working rootkit. We even have a little code that can remove or disable antivirus.”
Mebromi is a trojan which targets computers with AwardBIOS, Microsoft Windows, and antivirus software from two Chinese companies: Rising Antivirus and Jiangmin KV Antivirus.
In a December 2013 interview with CBS 60 Minutes, Deborah Plunkett, Information Assurance Director for the US National Security Agency claimed that NSA analysts had uncovered and thwarted a possible BIOS attack by a foreign nation state. The attack on the world's computers could have allegedly "literally taken down the US economy." The segment further cites anonymous cyber security experts briefed on the operation as alleging the plot was conceived in China. A later article in The Guardian cast doubt on the likelihood of such a threat, quoting Berkeley computer-science researcher Nicholas Weaver, Matt Blaze, a computer and information sciences professor at the University of Pennsylvania, and cybersecurity expert Robert David Graham in an analysis of the NSA's claims.
BIOS Boot Specification.
If the expansion ROM wishes to change the way the system boots (such as from a network device or a SCSI adapter for which the BIOS has no driver code), it can use the "BIOS Boot Specification" (BBS) API to register its ability to do so. Once the expansion ROMs have registered using the BBS APIs, the user can select among the available boot options from within the BIOS's user interface. This is why most BBS compliant PC BIOS implementations will not allow the user to enter the BIOS's user interface until the expansion ROMs have finished executing and registering themselves with the BBS API. The specification can be downloaded from the ACPICA website. The official title is BIOS Boot Specification (Version 1.01, 11 January 1996) and is available here:
Changing role of the BIOS.
BIOS services are not used by modern multitasking GUI operating systems after they initially load, so the importance of the primary part of BIOS is greatly reduced from what it was initially. The limitations of the AT-compatible BIOS were its 16-bit processor mode, 1 MByte addressable space and reliance on PC AT hardware.
Some operating systems, for example MS-DOS, rely on the BIOS to carry out most input/output tasks within the PC. Because the BIOS still runs in 16-bit real mode, calling BIOS services directly is inefficient for protected-mode operating systems. A number of larger, more powerful servers and workstations use a platform-independent Open Firmware (IEEE-1275) based on the Forth programming language; it is included with Sun's SPARC computers, IBM's RS/6000 line, and other PowerPC systems such as the CHRP motherboards. Later x86-based personal computer operating systems, like Windows NT, use their own, native drivers; this makes it much easier to extend support to new hardware.
Later BIOS took on more complex functions, by way of interfaces such as ACPI; these functions include power management, hot swapping, thermal management.
As of 2011, the BIOS is being replaced by the more complex Extensible Firmware Interface (EFI) in many new machines.
EFI is a specification which replaces the runtime interface of the legacy BIOS. Initially written for the Itanium architecture, EFI is now available for x86 and x86-64 platforms; the specification development is driven by The Unified EFI Forum, an industry Special Interest Group. EFI booting has been supported in only Microsoft Windows versions supporting GPT, the Linux kernel 2.6.1 and later, and Mac OS X on Intel-based Macs. However, the distinction between BIOS and EFI is rarely made in terminology by the average computer user, making BIOS a catch-all term for both systems.
SLIC.
Some BIOSes contain a "SLIC" (software licensing description table), a digital signature placed inside the BIOS by the manufacturer, for example Dell. (It is often casually called a BIOS tattoo or a tattooed BIOS.) This SLIC is inserted in the ACPI table and contains no active code.
Computer manufacturers that distribute OEM versions of Microsoft Windows and Microsoft application software can use the SLIC to authenticate licensing to the OEM Windows Installation disk and system recovery disc containing Windows software. Systems having a SLIC can be preactivated with an OEM product key, and they verify an XML formatted OEM certificate against the SLIC in the BIOS as a means of self-activating (see System Locked Preinstallation). If a user performs a fresh install of Windows, they will need to have possession of both the OEM key and the digital certificate for their SLIC in order to bypass activation; in practice this is extremely unlikely and hence the only real way this can be achieved is if the user performs a restore using a pre-customised image provided by the OEM. Cracks for non-genuine Windows distributions usually edit the SLIC or emulate it in order to bypass Windows activation.
Reprogrammable microcode.
Intel processors have reprogrammable microcode since the P6 microarchitecture. The BIOS may contain patches to the processor code to allow errors in the initial processor code to be fixed, updating the processor microcode each time the system is powered up. Otherwise, an expensive processor swap would be required. For example, the Pentium FDIV bug became an expensive fiasco for Intel that required a product recall because the original Pentium did not have patchable microcode.
The BIOS business.
IBM published the entire listings of the BIOS for its original PC, PC XT, PC AT, and other contemporary PC models, in an appendix of the Technical Reference manual for each machine type. The effect of the publication of the BIOS listings is that anyone can see exactly what a definitive BIOS does and how it does it. Phoenix Technology was the first company to write a fully compatible and completely legal BIOS through clean-room reverse engineering.
New standards grafted onto the BIOS are usually without complete public documentation or any BIOS listings. As a result, it is not as easy to learn the intimate details about the many non-IBM additions to BIOS as about the core BIOS services.
Most PC motherboard suppliers license a BIOS "core" and toolkit from a commercial third-party, known as an "independent BIOS vendor" or IBV. The motherboard manufacturer then customizes this BIOS to suit its own hardware. For this reason, updated BIOSes are normally obtained directly from the motherboard manufacturer. Major BIOS vendors include American Megatrends (AMI), Insyde Software, Phoenix Technologies and Byosoft. Former vendors include Award Software and Microid Research which were acquired by Phoenix Technologies in 1998; Phoenix later phased out the Award Brand name. General Software, which was also acquired by Phoenix in 2007, sold BIOS for Intel processor based embedded systems.
The open source community increased their effort to develop a replacement for proprietary BIOSes and their future incarnations with an open sourced counterpart through the coreboot and OpenBIOS/Open Firmware projects. AMD provided product specifications for some chipsets, and Google is sponsoring the project. Motherboard manufacturer Tyan offers coreboot next to the standard BIOS with their Opteron line of motherboards. MSI and Gigabyte Technology have followed suit with the MSI K9ND MS-9282 and MSI K9SD MS-9185 resp. the M57SLI-S4 models.

</doc>
<doc id="4474" url="http://en.wikipedia.org/wiki?curid=4474" title="Bose–Einstein condensate">
Bose–Einstein condensate

A Bose–Einstein condensate (BEC) is a state of matter of a dilute gas of bosons cooled to temperatures very close to absolute zero (that is, very near or ). Under such conditions, a large fraction of the bosons occupy the lowest quantum state, at which point quantum effects become apparent on a macroscopic scale. These effects are called macroscopic quantum phenomena.
Although later experiments have revealed complex interactions, this state of matter was first predicted, generally, in 1924–25 by Satyendra Nath Bose and Albert Einstein.
History.
Bose first sent a paper to Einstein on the quantum statistics of light quanta (now called photons). Einstein was impressed, translated the paper himself from English to German and submitted it for Bose to the "Zeitschrift für Physik", which published it. (The Einstein manuscript, once believed to be lost, was found in a library at Leiden University in 2005.). Einstein then extended Bose's ideas to material particles (or matter) in two other papers. The result of the efforts of Bose and Einstein is the concept of a Bose gas, governed by Bose–Einstein statistics, which describes the statistical distribution of identical particles with integer spin, now known as bosons. Bosonic particles, which include the photon as well as atoms such as helium-4 (4He), are allowed to share quantum states with each other. Einstein proposed that cooling bosonic atoms to a very low temperature would cause them to fall (or "condense") into the lowest accessible quantum state, resulting in a new form of matter.
In 1938 Fritz London proposed BEC as a mechanism for superfluidity in 4He and superconductivity.
In 1995 the first gaseous condensate was produced by Eric Cornell and Carl Wieman at the University of Colorado at Boulder NIST–JILA lab, using a gas of rubidium atoms cooled to 170 nanokelvin (nK) (). For their achievements Cornell, Wieman, and Wolfgang Ketterle at MIT received the 2001 Nobel Prize in Physics. In November 2010 the first photon BEC was observed. 
Concept.
This transition to BEC occurs below a critical temperature, which for a uniform three-dimensional gas consisting of non-interacting particles with no apparent internal degrees of freedom is given by:
where:
Einstein's non-interacting model.
Consider a collection of "N" noninteracting particles, which can each be in one of two quantum states, formula_2 and formula_3. If the two states are equal in energy, each different configuration is equally likely.
If we can tell which particle is which, there are formula_4 different configurations, since each particle can be in formula_2 or formula_3 independently. In almost all of the configurations, about half the particles are in formula_2 and the other half in formula_3. The balance is a statistical effect: the number of configurations is largest when the particles are divided equally.
If the particles are indistinguishable, however, there are only "N"+1 different configurations. If there are "K" particles in state formula_3, there are particles in state formula_2. Whether any particular particle is in state formula_2 or in state formula_3 cannot be determined, so each value of "K" determines a unique quantum state for the whole system. If all these states are equally likely, there is no statistical spreading out; it is just as likely for all the particles to sit in formula_2 as for the particles to be split half and half.
Suppose now that the energy of state formula_3 is slightly greater than the energy of state formula_2 by an amount "E". At temperature "T", a particle will have a lesser probability to be in state formula_3 by exp(−"E"/"kT"). In the distinguishable case, the particle distribution will be biased slightly towards state formula_2, and the distribution will be slightly different from half-and-half. But in the indistinguishable case, since there is no statistical pressure toward equal numbers, the most-likely outcome is that most of the particles will collapse into state formula_2.
In the distinguishable case, for large "N", the fraction in state formula_2 can be computed. It is the same as flipping a coin with probability proportional to "p" = exp(−"E"/"T") to land tails. The probability to land heads is , which is a smooth function of "p", and thus of the energy.
In the indistinguishable case, each value of "K" is a single state, which has its own separate Boltzmann probability. So the probability distribution is exponential:
For large "N", the normalization constant "C" is . The expected total number of particles not in the lowest energy state, in the limit that formula_21, is equal to formula_22. It does not grow when "N" is large; it just approaches a constant. This will be a negligible fraction of the total number of particles. So a collection of enough Bose particles in thermal equilibrium will mostly be in the ground state, with only a few in any excited state, no matter how small the energy difference.
Consider now a gas of particles, which can be in different momentum states labeled formula_23. If the number of particles is less than the number of thermally accessible states, for high temperatures and low densities, the particles will all be in different states. In this limit, the gas is classical. As the density increases or the temperature decreases, the number of accessible states per particle becomes smaller, and at some point, more particles will be forced into a single state than the maximum allowed for that state by statistical weighting. From this point on, any extra particle added will go into the ground state.
To calculate the transition temperature at any density, integrate, over all momentum states, the expression for maximum number of excited particles, :
When the integral is evaluated with the factors of "k""B" and restored by dimensional analysis, it gives the critical temperature formula of the preceding section. Therefore, this integral defines the critical temperature and particle number corresponding to the conditions of negligible chemical potential. In Bose–Einstein statistics distribution, μ is actually still nonzero for BEC'"s"; however, μ is less than the ground state energy. Except when specifically talking about the ground state, μ can consequently be approximated for most energy or momentum states as μ ≈ 0.
Interacting models.
Gross–Pitaevskii equation.
In some simplest cases, the state of condensed particles can be described with a nonlinear Schrödinger equation, also known as Gross-Pitaevskii or Ginzburg-Landau equation. The validity of this approach is actually limited to the case of ultracold temperatures, which fits well for the most alkali atoms experiments.
This approach originates from the assumption that the state of the BEC can be described by the unique wavefunction of the condensate formula_26. For a system of this nature, formula_27 is interpreted as the particle density, so the total number of atoms is formula_28
Provided essentially all atoms are in the condensate (that is, have condensed to the ground state), and treating the bosons using mean field theory, the energy (E) associated with the state formula_26 is:
Minimizing this energy with respect to infinitesimal variations in formula_26, and holding the number of atoms constant, yields the Gross–Pitaevski equation (GPE) (also a non-linear Schrödinger equation):
where:
In the case of zero external potential, the dispersion law of interacting Bose-Einstein-condensed particles is given by so-called Bogoliubov spectrum (for formula_33):
The Gross-Pitaevskii equation (GPE) provides a relatively good description of the behavior of atomic BEC's. However, GPE does not take into account the temperature dependence of dynamical variables, and is therefore valid only for formula_33.
It is not applicable for example for the condensates of excitons, magnons and photons, where the critical temperature is up to room one.
Weaknesses of Gross–Pitaevskii model.
The Gross–Pitaevskii model of BEC is a physical approximation valid for certain classes of BECs only. By construction, GPE uses the following simplifications: it assumes that interactions between condensate particles are of the contact two-body type and also it neglects anomalous contributions to self-energy. These assumptions are suitable mostly for the dilute three-dimensional condensates. If one relaxes any of these assumptions, the equation for the condensate wavefunction acquires the terms containing higher-order powers of the wavefunction. Moreover, for some physical systems the amount of such terms turns out to be infinite, therefore, the equation becomes essentially non-polynomial. The examples where this could happen are the Bose–Fermi composite condensates, effectively lower-dimensional condensates, and dense condensates and superfluid clusters and droplets.
Other models.
However, it is clear that in a general case the behaviour of Bose-Einstein condensate can be described by coupled evolution equations for condensate density, superfluid velocity and distribution function of elementary excitations. This problem was in 1977 by Peletminskii et al. in microscopical approach. The Peletminskii equations are valid for any finite temperatures below the critical point. Years after, in 1985, Kirkpatrick and Dorfman obtained similar equations using another microscopical approach. The Peletminskii equations also reproduce Khalatnikov hydrodynamical equations for superfluid as a limiting case.
Superfluidity of BEC and Landau criterion.
The phenomena of superfluidity of a Bose gas and superconductivity of a strongly-correlated Fermi gas (a gas of Cooper pairs) are tightly connected to Bose-Einstein condensation. Under corresponding conditions, below the temperature of phase transition, these phenomena were observed in helium-4 and different classes of superconductors. (In this sense, the superconductivity is often called simply as the superfluidity of Fermi gas).
In the simplest form, the origin of supefluidity can be seen from the weakly interacting bosons model.
Experimental observation.
Superfluid He-4.
In 1938, Pyotr Kapitsa, John Allen and Don Misener discovered that helium-4 became a new kind of fluid, now known as a superfluid, at temperatures less than 2.17 K (the lambda point). Superfluid helium has many unusual properties, including zero viscosity (the ability to flow without dissipating energy) and the existence of quantized vortices. It was quickly believed that the superfluidity was due to partial Bose–Einstein condensation of the liquid. In fact, many of the properties of superfluid helium also appear in the gaseous Bose–Einstein condensates created by Cornell, Wieman and Ketterle (see below). Superfluid helium-4 is a liquid rather than a gas, which means that the interactions between the atoms are relatively strong; the original theory of Bose–Einstein condensation must be heavily modified in order to describe it. Bose–Einstein condensation remains, however, fundamental to the superfluid properties of helium-4. Note that helium-3, consisting of fermions instead of bosons, also enters a superfluid phase at low temperature, which can be explained by the formation of bosonic Cooper pairs of two atoms each (see also fermionic condensate).
Atomic condensates.
The first "pure" Bose–Einstein condensate was created by Eric Cornell, Carl Wieman, and co-workers at JILA on 5 June 1995. They did this by cooling a dilute vapor consisting of approximately two thousand rubidium-87 atoms to below 170 nK using a combination of laser cooling (a technique that won its inventors Steven Chu, Claude Cohen-Tannoudji, and William D. Phillips the 1997 Nobel Prize in Physics) and magnetic evaporative cooling. About four months later, an independent effort led by Wolfgang Ketterle at MIT created a condensate made of sodium-23. Ketterle's condensate had about a hundred times more atoms, allowing him to obtain several important results such as the observation of quantum mechanical interference between two different condensates. Cornell, Wieman and Ketterle won the 2001 Nobel Prize in Physics for their achievements. A group led by Randall Hulet at Rice University announced the creation of a condensate of lithium atoms only one month following the JILA work. Lithium has attractive interactions which causes the condensate to be unstable and to collapse for all but a few atoms. Hulet and co-workers showed in a subsequent experiment that the condensate could be stabilized by the quantum pressure from trap confinement for up to about 1000 atoms.
The Bose–Einstein condensation also applies to quasiparticles in solids. A magnon in an antiferromagnet carries spin 1 and thus obeys Bose–Einstein statistics. The density of magnons is controlled by an external magnetic field, which plays the role of the magnon chemical potential. This technique provides access to a wide range of boson densities from the limit of a dilute Bose gas to that of a strongly interacting Bose liquid. A magnetic ordering observed at the point of condensation is the analog of superfluidity. In 1999 Bose condensation of magnons was demonstrated in the antiferromagnet Tl Cu Cl3. The condensation was observed at temperatures as large as 14 K. Such a high transition temperature (relative to that of atomic gases) is due to the greater density achievable with magnons and the smaller mass (roughly equal to the mass of an electron). In 2006, condensation of magnons in ferromagnets was even shown at room temperature, where the authors used pumping techniques.
Velocity-distribution data graph.
In the image accompanying this article, the velocity-distribution data indicates the formation of a Bose–Einstein condensate out of a gas of rubidium atoms. The false colors indicate the number of atoms at each velocity, with red being the fewest and white being the most. The areas appearing white and light blue are at the lowest velocities. The peak is not infinitely narrow because of the Heisenberg uncertainty principle: since the atoms are trapped in a particular region of space, their velocity distribution necessarily possesses a certain minimum width. This width is given by the curvature of the magnetic trapping potential in the given direction. More tightly confined directions have bigger widths in the ballistic velocity distribution. This anisotropy of the peak on the right is a purely quantum-mechanical effect and does not exist in the thermal distribution on the left. This graph served as the cover design for the 1999 textbook "Thermal Physics" by Ralph Baierlein.
Peculiarities of a condensate.
Vortices.
As in many other systems, vortices can exist in BECs. These can be created, for example, by 'stirring' the condensate with lasers, or rotating the confining trap. The vortex created will be a quantum vortex. These phenomena are allowed for by the non-linear formula_27 term in the GPE. As the vortices must have quantized angular momentum the wavefunction may have the form formula_37 where formula_38 and formula_39 are as in the cylindrical coordinate system, and formula_40 is the angular number. This is particularly likely for an axially symmetric (for instance, harmonic) confining potential, which is commonly used. The notion is easily generalized. To determine formula_41, the energy of formula_26 must be minimized, according to the constraint formula_37. This is usually done computationally, however in a uniform medium the analytic form
where:
demonstrates the correct behavior, and is a good approximation.
A singly charged vortex (formula_45) is in the ground state, with its energy formula_46 given by
where:
For multiply charged vortices (formula_49) the energy is approximated by
which is greater than that of formula_40 singly charged vortices, indicating that these multiply charged vortices are unstable to decay. Research has, however, indicated they are metastable states, so may have relatively long lifetimes.
Closely related to the creation of vortices in BECs is the generation of so-called dark solitons in one-dimensional BECs. These topological objects feature a phase gradient across their nodal plane, which stabilizes their shape even in propagation and interaction. Although solitons carry no charge and are thus prone to decay, relatively long-lived dark solitons have been produced and studied extensively.
Attractive interactions.
The experiments led by Randall Hulet at Rice University from 1995 through 2000 showed that lithium condensates with attractive interactions could stably exist, but only up to a certain critical atom number. Beyond this critical number, the attraction overwhelmed the zero-point energy of the harmonic confining potential, causing the condensate to collapse in a burst reminiscent of a supernova explosion where an explosion is preceded by an implosion. By quench cooling the gas of lithium atoms, they observed the condensate to first grow, and subsequently collapse when the critical number was exceeded.
Further experimentation on attractive condensates was performed in 2000 by the JILA team, consisting of Cornell, Wieman and coworkers. They originally used rubidium-87, an isotope whose atoms naturally repel each other, making a more stable condensate. Their instrumentation now had better control over the condensate so experimentation was made on naturally "attracting" atoms of another rubidium isotope, rubidium-85 (having negative atom–atom scattering length). Through a process called Feshbach resonance involving a sweep of the magnetic field causing spin flip collisions, they lowered the characteristic, discrete energies at which the rubidium atoms bond into molecules, making their Rb-85 atoms repulsive and creating a stable condensate. The reversible flip from attraction to repulsion stems from quantum interference among condensate atoms which behave as waves.
When the JILA team raised the magnetic field strength still further, the condensate suddenly reverted to attraction, imploded and shrank beyond detection, and then exploded, expelling off about two-thirds of its 10,000 or so atoms. About half of the atoms in the condensate seemed to have disappeared from the experiment altogether, not being seen either in the cold remnant or the expanding gas cloud. Carl Wieman explained that under current atomic theory this characteristic of Bose–Einstein condensate could not be explained because the energy state of an atom near absolute zero should not be enough to cause an implosion; however, subsequent mean field theories have been proposed to explain it. The atoms that seem to have disappeared almost certainly still exist in some form, just not in a form that could be accounted for in that experiment. Most likely they formed molecules consisting of two bonded rubidium atoms. The energy gained by making this transition imparts a velocity sufficient for them to leave the trap without being detected.
Current research.
Compared to more commonly encountered states of matter, Bose–Einstein condensates are extremely fragile. The slightest interaction with the outside world can be enough to warm them past the condensation threshold, eliminating their interesting properties and forming a normal gas. 
Nevertheless, they have proven useful in exploring a wide range of questions in fundamental physics, and the years since the initial discoveries by the JILA and MIT groups have seen an explosion in experimental and theoretical activity. Examples include experiments that have demonstrated interference between condensates due to wave–particle duality, the study of superfluidity and quantized vortices, the creation of bright matter wave solitons from Bose condensates confined to one dimension, and the slowing of light pulses to very low speeds using electromagnetically induced transparency. Vortices in Bose–Einstein condensates are also currently the subject of analogue gravity research, studying the possibility of modeling black holes and their related phenomena in such environments in the lab. Experimenters have also realized "optical lattices", where the interference pattern from overlapping lasers provides a periodic potential for the condensate. These have been used to explore the transition between a superfluid and a Mott insulator, and may be useful in studying Bose–Einstein condensation in fewer than three dimensions, for example the Tonks–Girardeau gas.
Bose–Einstein condensates composed of a wide range of isotopes have been produced.
Related experiments in cooling fermions rather than bosons to extremely low temperatures have created degenerate gases, where the atoms do not congregate in a single state due to the Pauli exclusion principle. To exhibit Bose–Einstein condensation, the fermions must "pair up" to form compound particles (e.g. molecules or Cooper pairs) that are bosons. The first molecular Bose–Einstein condensates were created in November 2003 by the groups of Rudolf Grimm at the University of Innsbruck, Deborah S. Jin at the University of Colorado at Boulder and Wolfgang Ketterle at MIT. Jin quickly went on to create the first fermionic condensate composed of Cooper pairs.
In 1999, Danish physicist Lene Hau led a team from Harvard University which succeeded in slowing a beam of light to about 17 meters per second. She was able to achieve this by using a superfluid. Hau and her associates at Harvard University have since successfully made a group of condensate atoms recoil from a "light pulse" such that they recorded the light's phase and amplitude, which was recovered by a second nearby condensate, by what they term "slow-light-mediated atomic matter-wave amplification" using Bose–Einstein condensates: details of the experiment are discussed in an article in the journal "Nature", 8 February 2007.
Researchers in the new field of atomtronics use the properties of Bose–Einstein condensates when manipulating groups of identical cold atoms using lasers. Further, Bose–Einstein condensates have been proposed by Emmanuel David Tannenbaum to be used in anti-stealth technology.
Isotopes.
The effect has mainly been observed on alkaline atoms which have nuclear properties particularly suitable for working with traps. As of 2012, using ultra-low temperatures of or below, Bose–Einstein condensates had been obtained for a multitude of isotopes, mainly of alkaline, alkaline earth,
and lanthanoid atoms (7Li, 23Na, 39K, 41K, 85Rb, 87Rb, 133Cs, 52Cr, 40Ca, 84Sr, 86Sr, 88Sr, 174Yb, 164Dy, and 168Er ). Condensation research was finally successful even with hydrogen with the aid of special methods. In contrast, the superfluid state of the bosonic 4He at temperatures below is not a good example of Bose–Einstein condensation, because the interaction between the 4He bosons is too strong. Only 8% of the atoms are in the single-particle ground state near zero temperature, rather than the 100% expected of a true Bose–Einstein condensate.
The spin-statistics theorem of Wolfgang Pauli states that half-integer spins (in units of formula_52) lead to fermionic behavior, e.g., the Pauli exclusion principle forbidding that more than two electrons possess the same energy, whereas integer spins lead to bosonic behavior, e.g., condensation of identical bosonic particles in a common ground state.
The bosonic, rather than fermionic, behavior of some of these alkaline gases appears odd at first sight, because their nuclei have half-integer total spin. The bosonic behavior arises from a subtle interplay of electronic and nuclear spins: at ultra-low temperatures and corresponding excitation energies, the half-integer total spin of the electronic shell and the half-integer total spin of the nucleus of the atom are coupled by a very weak hyperfine interaction. The total spin of the atom, arising from this coupling, is an integer value leading to the bosonic ultra-low temperature behavior of the atom. The chemistry of the systems at room temperature is determined by the electronic properties, which is essentially fermionic, since at room temperature, thermal excitations have typical energies much higher than the hyperfine values.

</doc>
<doc id="4475" url="http://en.wikipedia.org/wiki?curid=4475" title="B (programming language)">
B (programming language)

B is a programming language developed at Bell Labs circa 1969. It is the work of Ken Thompson with Dennis Ritchie. B first appeared circa 1969.
B was derived from BCPL, and its name may be a contraction of BCPL. It is possible that its name may be based on Bon, an earlier but unrelated, and rather different, programming language that Thompson designed for use on Multics.
B was designed for recursive, non-numeric, machine independent applications, such as system and language software.
History.
Initially Ken Thompson and later Dennis Ritchie developed B basing it mainly on the BCPL language Thompson used in the Multics project. B was essentially the BCPL system stripped of any component Thompson felt he could do without in order to make it fit within the memory capacity of the minicomputers of the time. The BCPL to B transition also included changes made to suit Thompson's preferences (mostly along the lines of reducing the number of non-whitespace characters in a typical program). Much of the typical algol-like syntax of BCPL was rather heavily changed in this process, such as the := and = operators which were replaced with = for assignment and == for equality test. (The & and | of BCPL was later changed to && and || in the transition to what is now known as C.) 
Thomson invented arithmetic assignment operators for B, using x=+y to add y to x (nowadays the operator is spelled +=). B also introduced the increment and decrement operators (++ and --); Their prefix or postfix position determines whether the value is taken prior or post alteration of the operand. These innovations were not in the earliest versions of B. Some guess that they were created for the auto-increment and auto-decrement address modes of the DEC PDP-11. This is historically impossible as there was no PDP-11 at the time that B was developed.
B is typeless, or more precisely has one data type: the computer word. Most operators (e.g., +, -, *, /) treated this as an integer, but others treated it as a memory address to be dereferenced. In many other ways it looked a lot like an early version of C. There are a few library functions, including some that vaguely resemble functions from the standard I/O library in C.
Early implementations were for the DEC PDP-7 and PDP-11 minicomputers using early Unix, and Honeywell 36-bit mainframes running the operating system GCOS. The earliest PDP-7 implementations compiled to threaded code, and then Ritchie wrote a compiler using TMG which produced machine code. In 1970 a PDP-11 was acquired and threaded code was used for the port. An early version of yacc was produced with this PDP-11 configuration. Ritchie took over maintenance during this period.
The typeless nature of B made sense on the Honeywell, PDP-7 and many older computers, but was a problem on the PDP-11 because it was difficult to elegantly access the character data type that the PDP-11 and most modern computers fully support. Starting in 1971 Ritchie made changes to the language while converting its compiler to produce machine code, most notably adding data typing for variables. During 1971 and 1972 B evolved into "New B" (NB) and then C.
B continues to see use (as of 2014) on GCOS mainframes, and on certain embedded systems for a variety of reasons, including limited hardware in the small systems; extensive libraries, tools, licensing cost issues; and simply being good enough for the job on others. The highly influential AberMUD was originally written in B.
B is almost extinct, having been superseded by the C language.
Examples.
The following example is from the "Users' Reference to B" by Ken Thompson:

</doc>
<doc id="4476" url="http://en.wikipedia.org/wiki?curid=4476" title="Beer–Lambert law">
Beer–Lambert law

In physical optics and transport theory, the Beer–Lambert law, also known as Beer's law, the Lambert–Beer law, or the Beer–Lambert–Bouguer law (named after August Beer, Johann Heinrich Lambert, and Pierre Bouguer) relates the attenuation of light to the properties of the material through which one substance like light, neutrons or host rarefied gases is traveling. It is strongly related to BGK model.
Equations.
The law states that there is a logarithmic dependence between the transmission (or transmissivity or transmittance), "T", of light through a substance and the product of the attenuation coefficient of the substance, "Σ", and the distance the light travels through the material (i.e., the path length), "ℓ". The attenuation coefficient can, in turn, be written as a product of either an absorptivity of the attenuator, "ε", and the concentration "c" of attenuating species in the material, or a total (absorption and scattering) cross section, "σ", and the (number) density "N' of attenuators.
In some chemistry applications for liquids these relations are usually written with the notation:
whereas in biology and physics, they are normally written as:
where formula_3 and formula_4 are the intensity (power per unit area) of the incident radiation and the transmitted radiation, respectively; σ is attenuation cross section and N is the concentration (number per unit volume) of attenuating medium.
The base 10 and base "e" conventions must not be confused because they have different numerical values for the attenuation coefficient: formula_5. However, it is easy to convert one to the other, using
The transmissivity (ability to transmit) is expressed in terms of an absorbance which is defined as
whereas it can be expressed in decibels as:
This implies that the absorbance becomes linear with the concentration (or number density of attenuators) according to
and
for the two cases, respectively.
Thus, if the path length and the attenuation coefficient (or the total cross section) are known and the absorbance is measured, the concentration of the substance (or the number density of attenuators) can be deduced.
Although several of the expressions above often are used as Beer–Lambert law, the name should strictly speaking only be associated with the latter two. The reason is that historically, the Lambert law states that attenuation is proportional to the light path length, whereas the Beer law states that attenuation is proportional to the concentration of attenuating species in the material.
If the concentration is expressed as a mole fraction i.e., a dimensionless fraction, the attenuation coefficient ("ε") takes the same dimension as the attenuation coefficient, i.e., reciprocal length (e.g., m−1). However, if the concentration is expressed in moles per unit volume, the attenuation coefficient ("ε") is used in L·mol−1·cm−1, or sometimes in converted SI units of m2·mol−1.
The attenuation coefficient "Σ"' is one of many ways to describe the atenuation of electromagnetic waves. For the others, and their interrelationships, see the article: Mathematical descriptions of opacity. For example, "Σ"' can be expressed in terms of the imaginary part of the refractive index, "κ", and the wavelength of the radiation(in free space), "λ"0, according to
In molecular attenuation spectrometry, the attenuation cross section "σ" is expressed in terms of a linestrength, "S", and an (area-normalized) lineshape function, "Φ". The frequency scale in molecular spectroscopy is often in cm−1, where the lineshape function is expressed in units of 1/cm−1. Since "N" is given as a number density in units of 1/cm3, the linestrength is often given in units of cm2cm−1/molecule. A typical linestrength in one of the vibrational overtone bands of smaller molecules, e.g., around 1.5 μm in CO or CO2, is around 10−23 cm2cm−1, although it can be larger for species with strong transitions, e.g., C2H2. The linestrengths of various transitions can be found in large databases, e.g., HITRAN. The lineshape function often takes a value around a few 1/cm−1, up to around 10/cm−1 under low pressure conditions, when the transition is Doppler broadened, and below this under atmospheric pressure conditions, when the transition is collision broadened. It has also become commonplace to express the linestrength in units of cm−2/atm since then the concentration is given in terms of a pressure in units of atm. A typical linestrength is then often in the order of 10−3 cm−2/atm. Under these conditions, the detectability of a given technique is often quoted in terms of ppm•m.
The fact that there are two commensurate definitions of attenuation (in base 10 or e) implies that the absorbance and the attenuation coefficient for the cases with gases, "A"' and "Σ"', are ln 10 (approximately 2.3) times as large as the corresponding values for liquids, i.e., "A" and "Σ", respectively. Therefore, care must be taken when interpreting data that the correct form of the law is used.
The law tends to break down at very high concentrations, especially if the material is highly scattering. If the radiation is especially intense, nonlinear optical processes can also cause variances. The main reason, however, is the following. At high concentrations, the molecules are closer to each other and begin to interact with each other. This interaction will change several properties of the molecule, and thus will change the attenuation. If the attenuation is different at higher concentrations than at lower ones, then the plot of the attenuation coefficient will not be linear, as is suggested by the equation, so you can only use it when all the concentrations you are working with are low enough that the absorbtivity is the same for all of them.
Derivation.
Classically, the Beer–Lambert law was first devised independently where Lambert's law stated that absorbance is directly proportional to the thickness of the sample, and Beer's law stated that absorbance is proportional to the concentration of the sample. The modern derivation of the Beer–Lambert law combines the two laws and correlate the absorbance to both, the concentration as well as the thickness (path length) of the sample.
In concept, the derivation of the Beer–Lambert law is straightforward. Divide the attenuating sample into thin slices that are perpendicular to the beam of light. The light that emerges from a slice is slightly less intense than the light that entered because some of the photons have run into molecules in the sample and did not make it to the other side. For most cases where measurements of attenuation are needed, a vast majority of the light entering the slice leaves without being attenuated. Because the physical description of the problem is in terms of differences—intensity before and after light passes through the slice—we can easily write an ordinary differential equation model for attenuation. The difference in intensity due to the slice of attenuating material formula_12 is reduced; leaving the slice, it is a fraction formula_13 of the light entering the slice formula_4. The thickness of the slice is formula_15, which scales the amount of attenuation (thin slice does not attenuates much light but a thick slice attenuates a lot). In symbols, formula_16, or formula_17. This conceptual overview uses formula_13 to describe how much light is attenuated. All we can say about the value of this constant is that it will be different for each material. Also, its values should be constrained between −1 and 0. The following paragraphs cover the meaning of this constant and the whole derivation in much greater detail.
Assume that particles may be described as having an attenuation cross section (i.e., area), "σ", perpendicular to the path of light through a solution, such that a photon of light is attenuated if it strikes the particle, and is transmitted if it does not.
Define "z" as an axis parallel to the direction that photons of light are moving, and "A" and "dz" as the area and thickness (along the "z" axis) of a 3-dimensional slab of space through which light is passing.
We assume that "dz" is sufficiently small that one particle in the slab cannot obscure another particle in the slab when viewed along the "z" direction. The concentration of particles in the slab is represented by "N".
It follows that the fraction of photons attenuated (absorbed and scattered away) when passing through this slab is equal to the total opaque area of the particles in the slab, "σAN dz", divided by the area of the slab "A", which yields "σN dz". Expressing the number of photons attenuated by the slab as "dI""z", and the total number of photons incident on the slab as "I""z", the number of photons attenuated by the slab is given by
Note that because there are fewer photons which pass through the slab than are incident on it, "dI""z" is actually negative (It is proportional in magnitude to the number of photons attenuated).
The solution to this simple differential equation is obtained by integrating both sides to obtain "I""z" as a function of "z"
The difference of intensity for a slab of real thickness ℓ is "I"0 at "z" = 0, and "I"l at "z" = "ℓ". Using the previous equation, the difference in intensity can be written as,
rearranging and exponentiating yields,
This implies that
and
The quantity Σ is called the total macroscopic cross section or attenuation coefficient, depending on the topic (for example in respectively the first term is used transport theory and the second one in shielding and radiation protection).
The derivation assumes that every attenuating particle behaves independently with respect to the light and is not affected by other particles. While it is commonly thought that error is introduced when particles are lying along the same optical path such that some particles are in the "shadow" of others, this is actually a key part of the derivation and why integration is used.
When the path taken is long enough to make the medium attenuation coefficient not uniform, the original equation must be modified as follows:
where z is the distance along the path through the medium, all other symbols are as defined above. This is taken into account in each formula_26 in the atmospheric equation above.
Deviations from Beer–Lambert law.
Under certain conditions Beer–Lambert law fails to maintain a linear relationship between attenuation and concentration of analyte. These deviations are classified into three categories:
Prerequisites.
There are at least six conditions that need to be fulfilled in order for Beer’s law to be valid. These are:
If any of these conditions are not fulfilled, there will be deviations from Beer’s law.
Chemical analysis.
Beer's law can be applied to the analysis of a mixture by spectrophotometry, without the need for extensive pre-processing of the sample. An example is the determination of bilirubin in blood plasma samples. The spectrum of pure bilirubin is known, so the molar attenuation coefficient is known. Measurements are made at one wavelength that is nearly unique for bilirubin and at a second wavelength in order to correct for possible interferences.The concentration is given by "c" = "A"corrected / "ε".
For a more complicated example, consider a mixture in solution containing two components at concentrations "c"1 and "c"2. The absorbance at any wavelength, λ is, for unit path length, given by
Therefore, measurements at two wavelengths yields two equations in two unknowns and will suffice to determine the concentrations "c"1 and "c"2 as long as the molar absorbances of the two components, "ε"1 and "ε"2 are known at both wavelengths. This two system equation can be solved using Cramer's rule. In practice it is better to use linear least squares to determine the two concentrations from measurements made at more than two wavelengths. Mixtures containing more than two components can be analyzed in the same way, using a minimum of "n" wavelengths for a mixture containing "n" components.
The law is used widely in infra-red spectroscopy and near-infrared spectroscopy for analysis of polymer degradation and oxidation (also in biological tissue). The carbonyl group attenuation at about 6 micrometres can be detected quite easily, and degree of oxidation of the polymer calculated.
Beer–Lambert law in the atmosphere.
This law is also applied to describe the attenuation of solar or stellar radiation as it travels through the atmosphere. In this case, there is scattering of radiation as well as absorption. The Beer–Lambert law for the atmosphere is usually written
where each formula_26 is the optical depth whose subscript identifies the source of the absorption or scattering it describes:
formula_41 is the "optical mass" or "airmass factor", a term approximately equal (for small and moderate values of formula_42) to formula_43, where formula_42 is the observed object's zenith angle (the angle measured from
the direction perpendicular to the Earth's surface at the observation site).
This equation can be used to retrieve formula_45, the aerosol optical thickness,
which is necessary for the correction of satellite images and also important in accounting for the role of
aerosols in climate.
History.
The law was discovered by Pierre Bouguer before 1729. It is often attributed to Johann Heinrich Lambert, who cited Bouguer's "Essai d'Optique sur la Gradation de la Lumiere" (Claude Jombert, Paris, 1729) — and even quoted from it — in his "Photometria" in 1760. Much later, August Beer extended the exponential attenuation law in 1852 to include the concentration of solutions in the attenuation coefficient.

</doc>
<doc id="4477" url="http://en.wikipedia.org/wiki?curid=4477" title="The Beach Boys">
The Beach Boys

The Beach Boys are an American rock band, formed in Hawthorne, California in 1961. The group's original lineup consisted of brothers Brian, Dennis and Carl Wilson, their cousin Mike Love and friend Al Jardine. Initially managed by the Wilsons' father Murry, the Beach Boys signed with Capitol Records in 1962. The band's early music gained popularity across the United States for its close vocal harmonies and lyrics reflecting a Southern California youth culture of surfing, cars and romance. During the early to mid-1960s, Brian Wilson's creative ambition and songwriting ability would dominate the group's musical direction. The primarily Wilson-composed "Pet Sounds" album and "Good Vibrations" single (both released in 1966) featured a complex, intricate and multi-layered sound that represented a departure from the simple surf rock of the Beach Boys' early years.
Starting in 1967, Wilson gradually ceded control to the rest of the band, reducing his input due to mental health and substance abuse issues. Though the more democratic incarnation of the Beach Boys recorded a string of albums in various musical styles that garnered international critical success, the group struggled to reclaim their commercial momentum in America, despite the period when they were the primary competitors to the Beatles. Since the 1980s, much-publicized legal wrangling over royalties, songwriting credits and use of the band's name transpired. Dennis Wilson drowned in 1983 and Carl died of lung cancer in 1998. After Carl's death, many different live configurations of the band fronted by Love and Johnston continued to tour into the 2000s while other members pursued solo projects. For the band's 50th anniversary, they briefly reunited as the Beach Boys for a new studio album, world tour, and career-spanning retrospective box set.
The Beach Boys have often been called "America's Band", and AllMusic stated that their "unerring ability…made them America's first, best rock band." The group had over eighty songs chart worldwide, thirty-six of them United States Top 40 hits (the most by an American rock band), four reaching number-one on the "Billboard Hot 100" chart. The Beach Boys have sold in excess of 100 million records worldwide, making them one of the world's best-selling bands of all time and are listed at number 12 on "Rolling Stone" magazine's 2004 list of the "100 Greatest Artists of All Time".</ref> The core quintet of the three Wilsons, Love and Jardine were inducted into the Rock and Roll Hall of Fame in 1988.
1958–66: Brian Wilson era.
Formation.
At age 16, Brian Wilson shared a bedroom with his brothers, Dennis and Carl, in their family home in Hawthorne. He watched his father, Murry Wilson, play piano and listened intently to the harmonies of vocal groups such as the Four Freshmen. One night he taught his brothers a song called "Ivory Tower" and how to sing the background harmonies. For his 16th birthday, Brian was given a reel-to-reel tape recorder. He learned how to overdub, using his vocals and those of Carl and their mother. Brian would play piano with Carl and David Marks (an eleven-year-old longtime neighbor) playing guitars they got as Christmas presents.
Soon Brian was avidly listening to Johnny Otis on his KFOX radio show, a favorite station of Carl's. Inspired by the simple structure and vocals of the rhythm and blues songs he heard, he changed his piano-playing style and started writing songs. His enthusiasm interfered with his music studies at school. Family gatherings brought the Wilsons in contact with cousin Mike Love. Brian taught Love's sister Maureen and a friend harmonies. Later, Brian, Mike Love and two friends performed at Hawthorne High School. Brian also knew Al Jardine, a high school classmate who had already played guitar in a folk group called the Islanders. Brian suggested to Jardine that they team up with his cousin and brother Carl. It was at these sessions, held in Brian's bedroom, that "the Beach Boys sound" began to form. Brian says: "Everyone contributed something. Carl kept us hip to the latest tunes, Al taught us his repertoire of folk songs, and Dennis, though he didn't play anything, added a combustible spark just by his presence." Love encouraged Brian to write songs and gave the fledgling band its name: "The Pendletones", a portmanteau of "Pendleton", a style of woolen shirt popular at the time and "tone", the musical term. In their earliest performances, the band wore heavy wool jacket-like shirts which were favored by surfers in the South Bay. Although surfing motifs were very prominent in their early songs, Dennis was the only avid surfer in the group. He suggested that his brothers compose some songs celebrating his hobby and the lifestyle which had developed around it in Southern California.
Jardine and a singer friend, Gary Winfrey, went to Brian's to see if he could help out with a version of a folk song they wanted to record—"Sloop John B". In Brian's absence, the two spoke with Murry, a music industry veteran of modest success. Murry arranged for the Pendletones to meet publisher Hite Morgan. The group performed a slower ballad, "Their Hearts Were Full of Spring", but failed to impress the Morgans. After an awkward pause, Dennis mentioned they had an original song, "Surfin'". With help from Love, Brian finished the song and the group rented guitars, drums, amplifiers and microphones. They practiced for three days while the Wilsons' parents were on a short vacation.
In October, the Pendletones recorded twelve takes of "Surfin'" in the Morgans' cramped offices, David Marks was not present at the session as he was at school. A small number of singles were pressed. When the boys eagerly unpacked the first box of singles, on the Candix Records label, they were shocked to see their band name changed to "Beach Boys". Murry Wilson, now intimately involved with the band's fortunes, called the Morgans. Apparently a young promotion worker, Russ Regan, made the change to more obviously tie the group in with other surf bands of the time. Released in December 1961, "Surfin'" was soon aired on KFWB and KRLA, two of Los Angeles' most influential teen radio stations. It was a hit on the West Coast, going to number three in Southern California, and peaked at number 75 on the national pop charts. By the final weeks of 1961 "Surfin'" had sold more than 40,000 copies. Murry Wilson told the boys he did not like "Surfin'". By now the de facto manager of the Beach Boys, he landed the group's first paying gig on New Year's Eve, 1961, at the Ritchie Valens Memorial Dance in Long Beach, headlined by Ike & Tina Turner. Brian recalls how he wondered what they were doing there: "five clean-cut, unworldly white boys from a conservative white suburb, in an auditorium full of black kids". Brian describes the night as an "education"—he knew afterwards that success was all about "R&B, rock and roll, and money".
Early successes with surf and hot rod-themed rock.
Although Murry effectively seized managerial control of the band, Brian acknowledged that he "deserves credit for getting us off the ground... he hounded us mercilessly... also worked hard himself". In the first half of February 1962, Jardine left the band and was replaced by Marks. The band recorded two more originals on April 19 at Western Studios, Los Angeles; "Lonely Sea" and "409", also re-recording "Surfin' Safari". On June 4, the band released their second single "Surfin' Safari" backed with "409". The release prompted national coverage in the June 9 issue of "Billboard" where the magazine praised Love's lead vocal and deemed the song to have strong hit potential. After being turned down by Dot and Liberty, the Beach Boys eventually signed a seven-year contract with Capitol Records on July 16 based on the strength of the June demo session. By November, their first album was ready—"Surfin' Safari" which reached 32 on the US Billboard charts. Their song output continued along the same commercial line, focusing on California youth lifestyle.
In January 1963, three months after the release of their debut album, the band began recording their sophomore effort, "Surfin' U.S.A.", placing a greater emphasis on surf rock instrumentals and tighter production. It has been hypothesized that the shift to a sound more typical of the surf rock genre was in response to the Californian surfer locals who were dismissive of the band's debut as it strayed from the sound of other surf acts. After the moderate success of "Surfin' Safari", "Surfin' U.S.A.", released on March 25, 1963 met a more enthusiastic reception, reaching number two on the Billboard charts and propelling the band into a nationwide spotlight. Five days prior to the release of "Surfin' U.S.A." Brian produced "Surf City", a song he had written for Jan and Dean. "Surf City" hit number one on the "Billboard" charts in July 1963, a development that pleased Brian but angered Murry, who felt his son had "given away" what should have been the Beach Boys' first chart-topper.
Sometime around late 1963, Brian Wilson heard the song "Be My Baby" by the Ronettes for the first time, which "revamped" Wilson's creative interests and songwriting. "Be My Baby" was later claimed by critics to be the epitome of Phil Spector's Wall of Sound production technique, a recording method that would fascinate Wilson for the next several decades. Wilson later in life stated: "I was unable to really think as a producer until I really got familiar with Phil Spector's work." Apart from Murry, Spector and the close vocal harmonies of Brian's favorite groups, early inspiration came from Chuck Berry. "Surfin' U.S.A." is a variation of Berry's "Sweet Little Sixteen". Under pressure from Berry's publisher, Wilson's father and manager, Murry Wilson, had given the copyright, including Brian Wilson's lyrics, to Arc Music.
At the beginning of a tour of the Mid-West in April 1963, Jardine rejoined the Beach Boys at Brian's request. As he began playing live gigs again, Brian left the road to focus on writing and recording. Around this time, Brian began utilizing members of the Wrecking Crew, session musicians also used by Spector. The session musicians were never an outright replacement for members of the band, but were used to augment arrangements or save recording time. The result of this arrangement produced the albums "Surfer Girl", released on September 16, 1963 and "Little Deuce Coupe", released less than a month later on October 7, 1963. This sextet incarnation of the Beach Boys didn't extend beyond these two albums, as Marks officially left the band in early October due to conflict with manager Murry, pulling Brian back into touring.
Following a successful Australasian tour in January and February 1964, the band returned home to face the "British invasion" through the Beatles appearances on the Ed Sullivan Show. Reportedly, Brian wanted more time to complete their next album, yet their record label insisted they finish recording swiftly to avoid being forgotten in the throes of the impending "invasion". Satisfying these demands, the band hastily finished the sessions on February 20, 1964 and titled the album "Shut Down Volume 2". Critics have found evaluating the album's worth difficult through the years. Though songs like "The Warmth of the Sun" and "Don't Worry Baby" are widely acclaimed and seen as impressive milestones in the artistic growth of the band, others have not lasted.
In April 1964, during recording of the single "I Get Around", Murry was relieved of his duties as manager. Brian reflected, "We love the family thing – y'know: three brothers, a cousin and a friend is a really beautiful way to have a group – but the extra generation can become a hang-up". When the single was released in May of that month, it would climb to number one, their first single to do so. Two months later, the album that the song later appeared on, "All Summer Long", reached number four on the Billboard 200 charts. The album was a swan-song to the surf and car music the Beach Boys built their commercial standing upon. Later albums took a different stylistic and lyrical path.
The group's early songs made them major pop stars in the United States, the United Kingdom, Australia and other countries. They had sixteen hit singles between 1962 and 1965. The Beach Boys were one of the few American bands formed prior to the 1964 British Invasion to continue their success. Their early hits also helped raise the profile of the state of California and associated the band with surfing, hot-rod racing, and the pursuit of happiness by carefree teens.
"Today!" and "Summer Days".
By the end of 1964, the stress of road travel, composing, producing and maintaining a high level of creativity became too much for Brian Wilson. On December 23, while on a flight, he suffered an anxiety attack and left the tour. In January, 1965, he announced his withdrawal from touring to concentrate entirely on songwriting and record production. For the rest of 1964 and into 1965, Glen Campbell served as Wilson's temporary replacement in concert, until his own career success pulled him from the group in April 1965. Bruce Johnston was asked to locate a replacement for Campbell; having failed to find one, Johnston himself subsequently became a full-time member of the band on May 19, 1965, first replacing Wilson on the road and later contributing in the studio, beginning with the vocal sessions for "California Girls" on June 4, 1965.
During the recording sessions for "The Beach Boys Today!", Love told Melody Maker that he and the band wanted to look beyond surf rock, to avoid living in the past or resting on their laurels. The resulting LP had largely guitar-oriented pop songs such as "Dance, Dance, Dance" and "Good to My Baby" on side A with B-side ballads such as "Please Let Me Wonder" and "She Knows Me Too Well".
In June, 1965, the band released "Summer Days (And Summer Nights!!)". The album included a reworked arrangement of "Help Me, Rhonda" which had become the band's second number one single in the spring of 1965, displacing the Beatles' "Ticket to Ride". "Let Him Run Wild" tapped into the youthful angst that would later pervade their music. In November 1965, the group followed up their US number three charting "California Girls" from "Summer Days (And Summer Nights!!)" with another top-twenty single, "The Little Girl I Once Knew". It was considered the band's most experimental statement thus far, using silence as a pre-chorus, clashing keyboards, moody brass and vocal tics. Perhaps too extreme an arrangement to go much higher than its number 20 peak, it was the band's second single not to reach the top ten since their 1962 breakthrough. In December they scored an unexpected number two hit (number three in the UK) with "Barbara Ann", which Capitol released as a single with no band input. A cover of a 1961 song by the Regents, it became one of the Beach Boys' most recognized hits.
"Pet Sounds", "Good Vibrations", and "Smile".
In 1966, the Beach Boys formally established their use of unconventional instruments and elaborate layers of vocal harmonies on their groundbreaking record "Pet Sounds". An early album in the emerging psychedelic rock style, "Pet Sounds" has been championed and emulated for its experimental and revolutionary baroque instrumentation. In the same year, they released "Good Vibrations", one of their best known and most celebrated songs. The song made use of a Tannerin (an easier-to-manipulate version of a Theremin) which helped them claim a new hippie audience.
"Pet Sounds" displayed Wilson's growing mastery of studio recording. His increasingly sophisticated songs and complex arrangements peaked with this work. Influenced by psychedelic drugs, Brian turned inward and probed his deep-seated self-doubts and emotional longings. The piece did not address the problems in the world around them, unlike other psychedelic rock groups. The album's meticulously layered harmonies and inventive instrumentation (performed by Los Angeles session musicians known as the Wrecking Crew) set a new standard for pop and rock music. It remains one of the most evocative releases of the decade, with distinctive lushness, melancholy and nostalgia. The tracks "Wouldn't It Be Nice" and "God Only Knows" showcased Wilson's growing mastery as a composer, arranger, and producer as did "Caroline, No", which was issued as a Brian Wilson solo single, the only time he was credited as a solo artist during the early Capitol years. The album also included two instrumental tracks, "Let's Go Away for Awhile" and the title track. Because of his withdrawal from touring, Wilson was able to complete almost all the backing tracks for the album while the Beach Boys were on tour. They returned to find a substantially complete album, requiring only their vocals and a small amount of instrumental work to finish it.
Despite the critical praise it received, "Pet Sounds" was indifferently promoted by Capitol and failed to become the major hit Wilson had hoped it would be. Its failure to gain wider recognition in the US hurt him deeply. "Pet Sounds" reached number ten in the US and number two in the UK, an accomplishment which helped the Beach Boys become the strongest selling album act in the UK for the final quarter of 1966; dethroning the three-year reign of native bands such as the Beatles.
Seeking to expand on "Pet Sounds"' advances, Wilson began an even more ambitious project, originally dubbed "Dumb Angel"; in due course, the project became "Smile". Its first fruit was "Good Vibrations", which Brian described as a "pocket symphony". The song became the Beach Boys' biggest hit to date and a US and UK number one single in 1966; many critics consider it to be one of the best rock singles of all time. It was one of the most complex pop productions ever undertaken, and was reputed to have been the most expensive American single ever recorded at that time. Costing a reported $50,000, more than most albums, sessions for the song stretched over several months in at least four major studios. According to Wilson, the electro-theremin work itself cost $15,000. In contrast to his work on "Pet Sounds", Wilson adopted a modular approach to "Good Vibrations": he broke the song into sections and taped multiple versions of each at different studios to take advantage of the different sound and ambience of each facility. He then assembled his favorite sections into a master backing track and added vocals, the sessions being the most demanding of the group's career.
While putting the finishing touches to "Pet Sounds," Brian Wilson met musician and songwriter Van Dyke Parks. In mid-1966, Brian and Parks began an intense collaboration that resulted in a suite of challenging new songs for "Smile". Using the same techniques as on "Good Vibrations", recording began in August 1966 and carried on into early 1967. Although the structure of the album and the exact running order of the songs have been subjects of speculation, it is known that Wilson and Parks intended "Smile" to be a continuous suite of songs that were linked both thematically and musically, with the main songs being linked together by small vocal pieces and instrumental segments that elaborated upon the musical themes of the major songs.
Many factors combined to put intense pressure on Brian Wilson as "Smile" neared completion: his own mental instability, the pressure to create against fierce internal opposition to his new music, the relatively unenthusiastic response to "Pet Sounds" in the United States, Carl Wilson's draft resistance, and a major dispute with Capitol Records. Further, Wilson's reliance on both prescription drugs and amphetamines exacerbated his underlying mental health problems. "Smile" was shelved in May 1967, and would go on to become the most famous unreleased album in the history of popular music. Comparable to Brian Jones and Syd Barrett, Brian Wilson's use of psychedelic drugs—especially LSD—led to a nervous breakdown in the late-1960s. As his legend grew, the "Smile" period came to be seen as the pivotal episode in his decline and he became tagged as one of the most notorious celebrity drug casualties of the rock era.
1967–75: The Beach Boys as a democratic unit.
"Smiley Smile" and "Wild Honey".
Some of the "Smile" tracks were salvaged and re-recorded in scaled-down versions at Brian's new home studio. Along with the single version of "Good Vibrations", these tracks were released on "Smiley Smile", an album which elicited positive critical and commercial response abroad, but was the first real commercial failure for the group in the United States. By this time the Beach Boys' management (Nick Grillo and David Anderle) had created the band's own record label, Brother. One of the first labels to be owned by a rock group, Brother Records was intended for releases of Beach Boys side projects, and as an invitation to new talent. The initial output of the label, however, was limited to "Smiley Smile" and two resulting singles from the album; the failure of "Gettin' Hungry" caused the band to shelve Brother until 1970. Despite the cancellation of "Smile", several tracks—including "Our Prayer", "Cabin Essence" and "Surf's Up"—continued to trickle out in later albums often as filler songs to offset Brian's unwillingness to contribute. The band was still expecting to complete and release "Smile" as late as 1973 before it became clear that only Brian could comprehend the endless fragments that had been recorded. "Smiley Smile" was followed up three months later with "Wild Honey", featuring songs written by Wilson and Love, including the hit "Darlin'" and a rendition of Stevie Wonder's "I Was Made to Love Her". The album fared better than its predecessor, reaching number 24 in the US.
Compounding the group's recent setbacks, their public image took a cataclysmic hit following their withdrawal from the 1967 Monterey Pop Festival for the reason that they had no new material to play while their forthcoming single and album lay in limbo. Their cancellation was seen as "a damning admission that they were washed up unable to compete with the 'new music'". This notion was exacerbated by "Rolling Stone" writer Jann Wenner, whom within contemporary publications criticized Brian Wilson for his oft-repeated "genius" label which he called a "promotional shuck" and an attempt to compare with the Beatles. However, Wenner later responded to their "Wild Honey" album with more optimism, remarking two months later that "[in any case it's good to see that the Beach Boys are getting their heads straight once again".
"Friends" and "20/20".
After meeting Maharishi Mahesh Yogi at a UNICEF Variety Gala in Paris, France on December 15, 1967, Love, along with other high-profile celebrities such as Donovan and the Beatles traveled to Rishikesh in India during February and March 1968. The following Beach Boys album "Friends" (1968) had songs influenced by the Transcendental Meditation taught by the Maharishi. The album reached number 13 in the UK and 126 in the US, the title track placing at number 25 in the UK and number 47 in the US, the band's lowest singles peak since 1962. In support of the "Friends" album, Love had arranged for the Beach Boys to tour with the Maharishi in the US, which has been called "one of the more bizarre entertainments of the era". Starting on May 3, 1968, the tour lasted five shows and was cancelled when the Maharishi had to withdraw to fulfill film contracts. Due to disappointing audience numbers and the Maharishi's withdrawal, twenty-four tour dates were subsequently cancelled at a cost estimated at US$250,000 (approximately US$ today) for the band. This tour was followed by the release of "Do It Again", a single critics described as an update of the Beach Boys' surf rock past in a late-1960's style. The single went to the top of the Australian and UK single charts in 1968 and was moderately successful in the US, peaking at number 20.
In spring 1968, Dennis began a tenuous relationship with musician Charles Manson which persisted for several months afterward. Dennis bought him studio time at Brian's home studio and recorded one song: "Cease to Exist" rewritten as "Never Learn Not To Love". It was released as a Beach Boys single. Growing fearful, Dennis gradually distanced himself from Manson, whose family had taken over his home. Manson was eventually convicted for murder conspiracy; Dennis remained too afraid to testify.
For a short time in mid-1968, Brian Wilson sought psychological treatment in hospital. During his absence, other members began writing and producing material themselves. To complete their contract with Capitol, they produced one more album. "20/20" (1969) was one of the group's most stylistically diverse albums, including hard rock songs such as "All I Want to Do", the waltz-based "Time to Get Alone" and a remake of the Ronettes' "I Can Hear Music". The diversity of genres have been described as an indicator that the group was trying to establish an updated identity. The album performed strongly in the UK, reaching number three on the charts. In the US, the album reached a modest 68.
On April 12, 1969, the band revisited their 1967 lawsuit against Capitol Records after they alleged an audit undertaken revealed the band were owed over US$2,000,000 (US$ today) for unpaid royalties and production duties. The band's contract with Capitol Records expired on June 30, 1969, after which Capitol Records deleted the Beach Boys' catalog from print, effectively cutting off their royalty flow. In November 1969, Murry Wilson sold Sea of Tunes, the Beach Boys' catalogue, to Irving Almo Music, a decision which according to Marilyn Wilson "devastated Brian". In late 1969, the Beach Boys reactivated their Brother label and signed with Reprise. Around this time, the band commenced recording for a new album. At the time the Beach Boys tenure ended with Capitol in 1969, they had sold 65 million records worldwide, closing the decade as the most commercially successful American group in popular music.
"Sunflower", "Surf's Up", "Carl and the Passions", and "Holland".
In 1970, armed with the new Reprise contract, the band appeared rejuvenated, releasing the album "Sunflower" to critical acclaim. The album features a strong group presence with significant writing contributions from all band members. Brian Wilson was quite active during this period, writing or co-writing seven of the twelve songs on "Sunflower" and performing at half of the band's domestic concerts in 1970. "Sunflower" reached number 29 in the UK and number 151 in the US, the band's lowest domestic chart showing to that point. A version of "Cottonfields" arranged by Al Jardine appeared on European releases of "Sunflower" and as a single, reached number one in Australia, Norway, South Africa and Sweden and the top-five in six other countries, including the UK.
After "Sunflower", the band hired Jack Rieley as their manager. Under Rieley's management, the group's music began emphasizing political and social awareness. During this time, Carl Wilson gradually assumed leadership of the band and Rieley contributed lyrics. On August 30, 1971 the band released "Surf's Up", named after the Brian Wilson/Van Dyke Parks composition "Surf's Up". The album was moderately successful, reaching the US top 30, a marked improvement over their recent releases. While the record charted, the Beach Boys added to their renewed fame by performing a near-sellout set at Carnegie Hall, followed by an appearance with the Grateful Dead at Fillmore East on April 27, 1971. The live shows during this era included reworked arrangements of many of the band's previous songs.
Johnston ended his first stint with the band shortly after "Surf's Ups release, reportedly because of friction with Rieley. At Carl's suggestion, the addition of Ricky Fataar and Blondie Chaplin in February 1972 led to a dramatic restructuring in the band's sound. The album "Carl and the Passions – "So Tough"" was an uncharacteristic mix that included several songs written by Fataar and Chaplin. For their next project the band, their families, assorted associates and technicians moved to the Netherlands for the summer of 1972. They rented a farmhouse to convert into a makeshift studio where recording sessions for the new project would take place. By the end of their sessions, the band felt they had produced one of their strongest efforts yet. Reprise, however, felt that the album required a strong single. This resulted in the song "Sail On, Sailor", a collaboration between Brian Wilson, Tandyn Almer, Ray Kennedy, Jack Rieley and Van Dyke Parks featuring a soulful lead vocal by Chaplin. Reprise subsequently approved and the resulting album, "Holland", was released early in 1973, peaking at number 37. Brian's musical children story, "Mount Vernon and Fairway (A Fairy Tale)", narrated by Rieley and strongly directly influenced by Randy Newman's "Sail Away" album, was included as a bonus EP. Despite indifference from Reprise, the band's concert audience started to grow.
"The Beach Boys in Concert", a double album documenting the 1972 and 1973 US tours, was another top-30 album and became the band's first gold record under Reprise. During this period the band established itself as one of America's most popular live acts. Chaplin and Fataar helped organize the concerts to obtain a high quality live performance, playing material off "Surf's Up", "Carl and the Passions" and "Holland" and adding songs from their older catalog. This concert arrangement lifted them back into American public prominence. In late 1973, the soundtrack to American Graffiti, "41 Original Hits from the Soundtrack of American Graffiti", was released to mass commercial and critical success. The soundtrack included early Beach Boy songs "Surfin' Safari" and "All Summer Long" and was a catalyst in creating a wave of nostalgia that reintroduced the Beach Boys into contemporary American consciousness. In 1974, Capitol Records issued "Endless Summer", the band's first major pre-"Pet Sounds" greatest hits package. The record sleeve's sunny, colorful graphics caught the mood of the nation and surged to the top of the "Billboard" album charts. It was the group's first multi-million selling record since "Good Vibrations", and remained on the album chart for three years. The following year, Capitol released a second compilation, "Spirit of America", which also sold well. With these compilations, the Beach Boys became one of the most popular acts in rock, propelling themselves from opening for Crosby, Stills, Nash and Young to headliners selling out basketball arenas in a matter of weeks. "Rolling Stone" named the Beach Boys the "Band of the Year" for 1974, solely on the basis of their juggernaut touring schedule and material written over a decade earlier.
Rieley, who remained in the Netherlands after "Holland"s release, was relieved of his managerial duties in late 1973. Chaplin also left in late 1973 after an argument with Steve Love, the band's business manager (and Mike's brother). Fataar remained until 1974, when he was offered a chance to join a new group led by future Eagles member Joe Walsh. Chaplin's replacement, James William Guercio, started offering the group career advice that resulted in his becoming their new manager. Under Guercio, the Beach Boys staged a highly successful 1975 joint concert tour with Chicago, with each group performing some of the other's songs, including their previous year's collaboration on Chicago's hit "Wishing You Were Here". Beach Boys vocals were also heard on Elton John's 1974 hit "Don't Let the Sun Go Down on Me". Nostalgia had settled into the Beach Boys' hype; the group had not officially released any new material since 1973's "Holland". While their concerts continuously sold out, the stage act slowly changed from a contemporary presentation followed by oldies encores to an entire show made up of mostly pre-1967 music.
1976–77: Second Brian Wilson era.
"15 Big Ones" (1976) marked Brian's return as a major force in the group. The album included new songs by Brian, as well as cover versions of oldies such as "Rock and Roll Music" (#5), "Blueberry Hill", and "In the Still of the Night". Brian and Love's "It's O.K." was in the vein of their early sixties style and was a moderate hit. The album was publicized by an August 1976 NBC-TV special, simply titled "The Beach Boys". The special, produced by "Saturday Night Live" (SNL) creator Lorne Michaels, featured appearances by "SNL" cast members John Belushi and Dan Aykroyd.
For the remainder of 1976 to early 1977, Brian Wilson spent his time making sporadic public appearances and producing the band's next album "Love You" (1977), a quirky collection of 14 songs mostly written, arranged and produced by Brian. Brian revealed to biographer Peter Ames Carlin that "Love You" is one of his favorite Beach Boys releases, telling him "That's when it all happened for me. That's where my heart lies." "Love You" peaked at number 28 in the UK and number 53 in the US and developed a cult following; regarded as one of the band's best albums by fans and critics alike.
After "Love You" was released, Brian began to record and assemble "Adult/Child" an effort largely consisting of songs written by Wilson from 1976 and 1977 with select big band arrangements by Dick Reynolds. Though publicized as the Beach Boys' next release, "Adult/Child" reportedly caused tension within the group and was ultimately shelved. Following this period, his concert appearances with the band gradually diminished and their performances were occasionally erratic. Despite the much-hyped "Brian's Back" campaign in the mid to late 1970s, most critics at this point would comment on how he could become the latest celebrity drug casualty.
The internal wrangling came to a head after a show at Central Park on September 1, 1977, when the band effectively split into two camps; Dennis and Carl Wilson on one side, Mike Love and Al Jardine on the other with Brian remaining neutral. Following a confrontation on an airport tarmac, the band broke up for two and a half weeks, until a band meeting on September 17, at Brian's house. In light of a potential new Caribou Records the parties negotiated a settlement resulting in Love gaining control of Brian's vote in the group, allowing Love and Jardine to outvote Carl and Dennis Wilson on any matter.
1978–present: fluctuating control.
Infighting and the Wilsons' retreat.
The Beach Boys' last album for Reprise, "M.I.U. Album" (1978), was recorded at Maharishi International University in Iowa at the suggestion of Love. Dennis and Carl made limited contributions; the album was mostly produced by Jardine and Ron Altbach, with Brian appearing as "Executive Producer". "M.I.U." was largely a contractual obligation to finish out their association with Reprise, who likewise did not promote the result. The record cemented the divisions in the group. Love and Jardine focused on rock and roll-oriented material while Carl and Dennis chose the progressive focus they had established with the albums "Carl and the Passions" and "Holland". Dennis withdrew from the group to focus on his second solo album and follow-up to "Pacific Ocean Blue" entitled "Bambu". However alcoholism and marital problems overcame all three Wilson brothers and "Bambu" was shelved. Carl appeared intoxicated during concerts (notably at appearances on their disastrous 1978 Australia tour) and Brian gradually slid back into addiction and an unhealthy lifestyle.
After departing Reprise, the Beach Boys signed with CBS Records. They received a substantial advance and were paid $1 million per album even as CBS deemed their preliminary review of the band's first product, "L.A. (Light Album)" as unsatisfactory. Faced with the realization that Brian was unable to contribute, the band recruited Johnston as producer. The result paid off, as "Good Timin'" became a top 40 single. The album featured outstanding performances by both Dennis (cuts intended "Bambu") and Carl ("Full Sail"). The group enjoyed moderate success with a disco reworking of the "Wild Honey" song "Here Comes the Night" which was followed by their highest charting UK single in nine years: Jardine's "Lady Lynda" peaked at #6 in the UK Singles Chart. 1980 saw the release of "Keepin' the Summer Alive", with Johnston once again producing. Carl Wilson was the only Wilson to influence the finished product. Brian managed to contribute several ideas, as seen in the "Going Platinum" television special documenting the album's release, but was otherwise "persona non grata". Dennis' ongoing personal problems kept him out of the special and album, though his drumming is heard on the cover version of Chuck Berry's "School Days".
From 1980 through 1982, the Beach Boys and The Grass Roots performed Independence Day concerts at the National Mall in Washington, D.C., attracting large crowds.Phil McCombs and Richard Harrington, "Watt Sets Off Uproar with Music Ban", "The Washington Post", Washington, D.C., April 7, 1983, pp. A1, A17.</ref> However, in April 1983, James G. Watt, President Ronald Reagan's Secretary of the Interior, banned Independence Day concerts on the Mall by such groups. Watt said that "rock bands" that had performed on the Mall on Independence Day in 1981 and 1982 had encouraged drug use and alcoholism and had attracted "the wrong element", who would mug attendees. During the ensuing uproar, which included over 40,000 complaints to the Department of the Interior, the Beach Boys stated that the Soviet Union, which had invited them to perform in Leningrad in 1978, "obviously ... did not feel that the group attracted the wrong element". Vice President George H. W. Bush said of the Beach Boys, "They're my friends and I like their music". Watt later apologized to the band after learning that President Reagan and First Lady Nancy Reagan were fans. White House staff presented Watt with a plaster foot with a hole in it, showing that he had "shot himself in the foot". The band returned to D.C. for Independence Day in 1984 and performed to a crowd of 750,000 people.
In 1981, Carl quit the group due to unhappiness with the band's nostalgia format and lackluster live performances, subsequently pursuing a solo career. He returned in May 1982 — after approximately 14 months of being away — on the condition that the group reconsider their rehearsal and touring policies, along with refraining from "Las Vegas-type engagements".
Dennis Wilson's personal problems continued to escalate, and on December 28, 1983, he drowned in Marina del Rey while diving from a friend's boat trying to recover items he had previously thrown overboard in fits of rage. Despite his death, the Beach Boys continued as a successful touring act.
Soundtrack appearances, "Kokomo" and nostalgia.
On July 4, 1985, the Beach Boys played to an afternoon crowd of one million in Philadelphia and the same evening they performed for over 750,000 people on the Mall in Washington (the day's historic achievement was recorded in the Guinness Book of World Records). They also appeared nine days later at the Live Aid concert. That year, they released the eponymous album "The Beach Boys" and enjoyed a resurgence of interest later in the 1980s, assisted by tributes such as David Lee Roth's hit version of "California Girls". In 1987, they played with the rap group The Fat Boys, performing the song "Wipe Out" and filming a music video.
By 1988, Brian Wilson had officially left the Beach Boys and released his first solo album, which received critical acclaim. During this period the band unexpectedly claimed their first US number one hit single in 22 years with "Kokomo", which had appeared in the movie "Cocktail". Written by John Phillips, Scott McKenzie, Mike Love and Terry Melcher, the song became the band's largest selling single of all time. The video for the song received heavy airplay on the music video channel VH1, and prominently featured actor John Stamos on conga drums. Inducted into the Rock and Roll Hall of Fame earlier in the year, the group became the second artist after Aretha Franklin to hit number one in the US after their induction. They released the album "Still Cruisin"', which went gold in the US and gave them their best chart showing since 1976. In 1990, the band gathered several studio musicians and recorded the Melcher-produced title track of the comedy "Problem Child". Stamos again appeared on the video, and later appeared singing lead vocals on "Forever" (written by Dennis Wilson for the "Sunflower" album) on their 1992 album "Summer in Paradise". Having no new contributions from Brian Wilson due to interference from caretaker Eugene Landy, "Summer in Paradise" was poorly regarded by both critics and fans, was a commercial disaster and would become their last album of original material for two decades. Members of the band appeared on several television shows such as "Full House", "Home Improvement", and "Baywatch" in the late 1980s and 1990s.
In 1989, Wilson filed a lawsuit to reclaim the rights to his songs and the group's publishing company, Sea of Tunes, which he had supposedly signed away to his father Murry in 1969. He successfully argued that he had not been mentally fit to make an informed decision and that his father had potentially forged his signature. While Wilson failed to regain his copyrights, he was awarded $25 million for unpaid royalties. Soon after Wilson won his case, Love discovered that Murry Wilson had not properly credited him as co-writer on dozens of Beach Boys songs. With Love and Brian Wilson unable to determine exactly what Love was properly owed, Love sued Wilson in 1992, winning $13 million in 1994 for lost royalties. In interviews, Love revealed that on some songs he wrote most of the lyrics, on others only a line or two. Even though Love sued Wilson, both parties said in interviews that there was no malice between them; they simply couldn't come up with an agreeable settlement by themselves.
In 1993, the band appeared in Michael Feeney Callan's film "The Beach Boys Today", which included in-depth interviews with all members except Brian. Carl confided to Callan that Brian would record again with the band at some point in the near future. A few Beach Boys sessions devoted to new Brian Wilson compositions occurred during the mid-1990s, but they remain largely unreleased, and the album was quickly aborted due to tenuous relations. In February 1996, the Beach Boys guested with Status Quo on a re-recording of "Fun, Fun, Fun", which became a British Top-30 hit. In June, the group worked with comedian Jeff Foxworthy on the recording "Howdy From Maui", and eventually released "Stars and Stripes Vol. 1" in August 1996. The album consisted of country renditions of several Beach Boys hits, performed by popular country artists such as Toby Keith and Willie Nelson. Brian Wilson, who was in a better mental state at the time, acted as co-producer.
In early 1997, Carl Wilson was diagnosed with lung cancer after years of heavy smoking. Despite his terminal condition, Carl continued to perform with the band on its 1997 summer tour while undergoing chemotherapy. During performances, he sat on a stool and reportedly needed oxygen after every song. Carl was able to stand, however, when he played on "God Only Knows". By 1998 the cancer had spread to his brain. Carl died on February 6, 1998, two months after the death of the Wilsons' mother, Audree.
Splintering of the Beach Boys' name.
Following Carl's death, the remaining members splintered. Love, Johnston and former guitarist Marks continued to tour without Jardine, initially as "America's Band", but following several cancelled bookings under that name, they sought authorization through Brother Records Inc. (BRI) to tour as "The Beach Boys" and secured the necessary license. In turn Jardine began to tour regularly with his band dubbed "Beach Boys: Family & Friends" until he ran into legal issues for using the name without license. BRI, through its longtime attorney, Ed McPherson, sued Jardine in Federal Court. Jardine, in turn, counter-claimed against BRI for wrongful termination. BRI ultimately prevailed after several years. Love was allowed to continue to tour as The Beach Boys, while Jardine was prohibited from touring using any form of the name. Released from Landy's control, Brian Wilson sought different treatments for his illnesses that aided him in his solo career. He toured regularly with his backing band consisting of members of Wondermints and other LA/Chicago musicians. Marks also maintained a solo career. Their tours remained reliable draws, with Wilson and Jardine both remaining legal members of the Beach Boys organization.
In September 2004, Brian Wilson issued a free CD through the "Mail On Sunday" that included Beach Boys songs he'd rerecorded, five of which he'd co-authored with Love. The 10 track compilation had 2.6 million copies distributed and prompted Love to file a lawsuit claiming the promotion hurt the sales of the original recordings. Love's suit was dismissed in 2007 when a judge determined that there were no triable issues.
On June 13, 2006, the five surviving Beach Boys (Wilson, Love, Jardine, Johnston and Marks) appeared together for the celebration of the 40th anniversary of "Pet Sounds" and the double-platinum certification of their greatest hits compilation, "", in a ceremony atop the Capitol Records building in Hollywood. Plaques were awarded for their efforts, with Brian accepting on behalf of Dennis and Carl.
"The Smile Sessions", 50th anniversary tour and "That's Why God Made the Radio".
On October 31, 2011, the Beach Boys released surviving 1960s recordings from "Smile" in the form of "The Smile Sessions". The album—even in its incomplete form—garnered universal critical acclaim and experienced popular success, charting in both the Billboard US and UK Top 30. The artwork and packaging featured the original Frank Holmes illustrations and included the photo/illustration booklet insert that was intended for the 1967 release. The format of the recordings utilized Wilson's 2004 "Brian Wilson Presents Smile" solo effort as a template. The band was rewarded with glowing reviews, including inclusion in Rolling Stone's Top 500 album list at number 381. In a 500 set limited edition, the "The Smile Sessions" came in a box set with a lit-up shop front window. Each of these box sets came with Brian Wilson's signature on the box. "The Smile Sessions" deluxe album package went on to win Best Historical Album at the 2013 Grammy Awards. Brian Wilson personally accepted the award stating "I guess Van Dyke and I were on to something after all."
In February 2011, the Beach Boys released "Don't Fight the Sea", a charity single to aid the victims of the 2011 Japan earthquake. The single, released on Jardine's 2011 album "A Postcard From California" featured Jardine, Wilson, Love and Johnston, with prerecorded vocals by Carl Wilson. Rumors then circulated regarding a potential 50th anniversary band reunion.
On December 16, 2011, it was announced that Wilson, Love, Jardine, Johnston and Marks would reunite for a new album and 50th anniversary tour in 2012 to include a performance at the New Orleans Jazz Festival in April 2012. On February 12, 2012, the Beach Boys performed at the 2012 Grammy Awards, in what was billed as a "special performance" by organizers. It marked the group's first live performance to include Brian since 1996. This anniversary lineup performed "Good Vibrations" with Adam Levine and Mark Foster, after Maroon 5 opened the set with "Surfer Girl" and Foster the People played "Wouldn't It Be Nice" during the ceremony. Johnston said, "I never hoped for reunion, because I never thought any of us wanted to do it. We have probably, you know, the presidential honeymoon of six months but then we have to show something to keep it going. We have to make sure we have a great flowing song list but also make sure we don't sound like a greatest-hits band. We have a lot to balance." The Beach Boys appeared at the April 10, 2012, season opener for the Los Angeles Dodgers and performed "Surfer Girl" along with "The Star-Spangled Banner". They also performed at the Bonnaroo Music and Arts Festival in Manchester, Tennessee on June 14, 2012.
Johnston compared the sound of the new album to one of the band's least-successful albums but fan favorite "Sunflower", while Jardine said the album is "very lush, very "PetSound"-ing".</ref> The first single from the album, the title track, made its national radio debut April 25, 2012, on ESPN's "Mike and Mike in the Morning"</ref> and was released on iTunes and other digital platforms on April 26. "That's Why God Made the Radio" debuted at number three on US charts, the group's highest charting album since 1974's compilation "Endless Summer" and its highest charting studio album since 1965's "Summer Days (And Summer Nights!!)". It became the band's first top ten studio album since 1976's "15 Big Ones". The album made its debut in the UK at number 15, its highest studio album debut since 1971's "Surf's Up". The album also made US chart history by expanding the group's span of Billboard 200 top ten albums across 49 years and one week, passing the Beatles with 47 years of top ten albums.
Later in 2012, the group released the "Fifty Big Ones" and "Greatest Hits" compilations along with reissues of 12 of their albums. The next year, the group released "Live – The 50th Anniversary Tour" a 41 song, 2-CD set documenting their "50th Anniversary Tour". While there were no definite plans, Brian stated that he would like to make another Beach Boys album following the world tour. "This time I would like to do some rock n' roll," Wilson says. "I would like it to be a bit harder and faster." Love and Johnston continued to tour, while Wilson expressed in more touring and recording with the band.</ref> In July 2012 Love said that Wilson and producer Joe Thomas had over 80 hours of music recorded, much of it culled from material they were working on around the time of Wilson's 1998 "Your Imagination" album that "were always songs he had earmarked for the Beach Boys." 
Resuming solo careers.
On October 5, 2012, Love announced in a self-written press release to the LA Times that the band would return to its pre-50th Reunion Tour lineup with him and Johnston touring as The Beach Boys without Wilson, Jardine and Marks:
Reflecting upon the band's recent reunion in 2013, Love stated: "I had a wonderful experience being in the studio together. Brian has lost none of his ability to structure those melodies and chord progressions, and when we heard us singing together coming back over the speakers it sounded like 1965 again. Touring was more for the fans.…It was a great experience, it had a term to it, and now everyone's going on with their ways of doing things." In a July 8, 2013 interview, he continued by discussing the potential of another reunion, saying "I don’t know how that sacking controversy started.…The anniversary tour was originally 50 dates, and got extended to 73. At that point, Brian said: ‘No more dates for us, please.’ So once we finished those 73 shows we went back to the line-up of the band before he rejoined.…I’d very much like to get in a room, just him and I, to write more songs.…We didn’t write together on last year’s album, and I’d like to do that more than anything." On August 27, 2013 the group released "Made in California", a six disc collection featuring more than seven and a half hours of music, including more than 60 previously unreleased tracks. "Made in California" also concluded the Beach Boys' 50th anniversary campaign. That same year, former members of the Beach Boys touring band, Bobby Figueroa, Billy Hinsche, Ed Carter, Matt Jardine (son of Al Jardine), and Philip Bardowell (sometimes with Randell Kirsch and others) united to form California Surf, Incorporated, performing Beach Boy songs.
Jardine, Marks, Johnston and Love appeared together at the 2014 Ella Awards Ceremony, where Love was honored for his work as a singer. Marks sang "409" in honor of Love while Jardine performed "Help Me Rhonda". They closed the show by performing "Fun, Fun, Fun". Wilson's long time band associate Jeff Foskett also appeared, but not Wilson. On May 15, 2014 the touring Beach Boys (Love and Johnston) announced a tour celebrating "50 Years of 'Fun Fun Fun, named for their 1964 single. The tour will also feature the addition of Foskett replacing Mike's son Christian.
Legacy.
Regarded by some critics as one of the greatest American rock groups and an important catalyst in the evolution of popular music, the Beach Boys are one of the most critically acclaimed and commercially successful bands of all time. Influenced by barbershop music and rhythm and blues, they began playing 1950s style rock and roll married to a five part harmony. The band later went on to incorporate many different genres, from baroque pop to psychedelia and synthpop.
The Beach Boys' sales estimates range from 100 to 350 million records worldwide, and have influenced artists spanning many genres and decades. Artists influenced by the Beach Boys include: the Beatles, George Martin, The Velvet Underground, Pink Floyd, Cream, the Who, Elton John, ABBA, Bruce Springsteen, the Ramones, The Stone Roses, Sonic Youth, Beck, R.E.M., Weezer, Neutral Milk Hotel, Radiohead, Of Montreal, The Olivia Tremor Control, The Flaming Lips, My Bloody Valentine, Daft Punk, Air, Kraftwerk, Yellow Magic Orchestra, Belle and Sebastian, The Beta Band, Alex Chilton, Yo La Tengo, Saint Etienne, Pixies, Mr. Bungle, MGMT, Marvin B. Naylor, and Animal Collective.Video: Rivers Cuomo Covers the Beach Boys</ref>
"The Beach Boys Today!" (1965), "Wild Honey" (1967), "Sunflower" (1970), "Surf's Up" (1971), "Holland" (1973), and "The Smile Sessions" (2011) are featured in several "Greatest Albums of All Time" lists. The group's 1966 releases, "Pet Sounds" and "Good Vibrations", frequently rank among the top of critics' lists of the greatest albums and singles of all time. "Pet Sounds" is on the greatest-albums lists for "Time", "Rolling Stone", "New Musical Express", "Mojo", and "The Times". The record had a profound influence on many of the Beach Boys' contemporaries; McCartney named it one of his favorite albums of all time (with "God Only Knows" as his all-time favorite song). McCartney said that it was the inspiration behind the Beatles' album, "Sgt. Pepper's Lonely Hearts Club Band". Echoing this sentiment, Beatles producer George Martin said, "Pepper was an attempt to equal "Pet Sounds"."
In 1966 and 1967, reader polls conducted by the UK magazine "NME" crowned the Beach Boys as the world's number one vocal group, ahead of the Beatles and the Rolling Stones. In 1974, the Beach Boys were awarded "Band of the Year" by Rolling Stone. On December 30, 1980, the Beach Boys were awarded a star on the Hollywood Walk of Fame, located at 1500 Vine Street. The group was inducted into the Rock and Roll Hall of Fame in 1988. Ten years later they were selected for the Vocal Group Hall of Fame. In 2001, the group received a Grammy Lifetime Achievement Award. In 2004, "Rolling Stone" ranked the Beach Boys number 12 on its list of the 100 Greatest Artists of All Time. Brian Wilson was inducted into the UK Rock and Roll Hall of Fame in November 2006.
The Wilsons' California house, where the Wilson brothers grew up and the group began, was demolished in 1986 to make way for Interstate 105, the Century Freeway. A Beach Boys Historic Landmark (California Landmark No. 1041 at 3701 West 119th Street), dedicated on May 20, 2005, marks the location.
Selected filmography.
The Beach Boys also appear in the beach party films "The Girls on the Beach" in which they perform three songs "The Girls on the Beach", "Lonely Sea", and "Little Honda" and "The Monkey's Uncle" in which they perform "The Monkey's Uncle" with Annette Funicello.
The life of the Beach Boys is the subject of two TV movies: ' and '.
The Beach Boys appeared in an episode of "Full House" entitled "Beach Boy Bingo", which aired on November 18, 1988.
The Beach Boys also appeared in Season 6, Episode 4 of "Baywatch" (1995).

</doc>
<doc id="4479" url="http://en.wikipedia.org/wiki?curid=4479" title="BCE (disambiguation)">
BCE (disambiguation)

BCE may stand for: 

</doc>
<doc id="4480" url="http://en.wikipedia.org/wiki?curid=4480" title="BC">
BC

BC may refer to:

</doc>
<doc id="4481" url="http://en.wikipedia.org/wiki?curid=4481" title="Beatrix Potter">
Beatrix Potter

Beatrix Potter (born Helen Beatrix Potter; 28 July 186622 December 1943) was an English author, illustrator, natural scientist and conservationist best known for her imaginative children's books, featuring animals such as those in "The Tale of Peter Rabbit", which celebrated the British landscape and country life.
Born into a wealthy Unitarian family, Potter, along with her younger brother Walter Bertram (1872–1918), grew up with few friends outside her large extended family. Her parents were artistic, interested in nature and enjoyed the countryside. As children, Beatrix and Bertram had numerous small animals as pets which they observed closely and drew endlessly. Summer holidays were spent away from London, in Scotland and in the English Lake District where Beatrix developed a love of the natural world which was the subject of her painting from an early age.
She was educated by private governesses until she was 18. Her study of languages, literature, science and history was broad and she was an eager student. Her artistic talents were recognized early. She enjoyed private art lessons, and developed her own style, favouring watercolour. Along with her drawings of her animals, real and imagined, she illustrated insects, fossils, archaeological artefacts, and fungi. 
In the 1890s her mycological illustrations and research into the reproduction of fungus spores generated interest from the scientific establishment. Following some success illustrating cards and booklets, Potter wrote and illustrated "The Tale of Peter Rabbit", publishing it first privately in 1901, and a year later as a small, three-colour illustrated book with Frederick Warne & Co. She became unofficially engaged to her editor Norman Warne in 1905 despite the disapproval of her parents, but he died suddenly a month later of leukemia.
With the proceeds from the books and a legacy from an aunt, Potter bought Hill Top Farm in Near Sawrey, a tiny village, then in Lancashire, the English Lake District near Windermere, in 1905. Over the following decades, she purchased additional farms to preserve the unique hill country landscape. In 1913, at the age of 47, she married William Heelis, a respected local solicitor from Hawkshead. 
Potter was also a prize-winning breeder of Herdwick sheep and a prosperous farmer keenly interested in land preservation. She continued to write, illustrate and design spin-off merchandise based on her children's books for Warne until the duties of land management and her diminishing eyesight made it difficult to continue. 
Potter published over 23 books: the best known are those written between 1902 and 1922. She died of pneumonia and heart disease on 22 December 1943 at her home in Near Sawrey (Lancashire) at age 77, leaving almost all her property to the National Trust. She is credited with preserving much of the land that now comprises the Lake District National Park.
Potter's books continue to sell throughout the world, in many languages. Her stories have been retold in song, film, ballet and animation. "The World of Peter Rabbit and Friends", a TV series based on her stories, has been released on VHS by Pickwick Video and later Carlton Video.
Biography.
Early life.
Potter's paternal grandfather, Edmund Potter, from Glossop in Derbyshire, owned what was then the largest calico printing works in England, and later served as a Member of Parliament. Beatrix's father, Rupert William Potter (1832–1914), was educated at Manchester College by the Unitarian philosopher Dr. James Martineau.
Potter's family on both sides were from the Manchester area. They were English Unitarians, a dissenting Protestant sect who rejected the doctrine of the Trinity.
Beatrix was educated by three able governesses, the last of whom was Annie Moore ("née" Carter), just three years older than Beatrix, who tutored Beatrix in German as well as acting as lady's companion. She and Beatrix remained friends throughout their lives and Annie's eight children were the recipients of many of Potter's delightful picture letters. It was Annie who later suggested that these letters might make good children’s books.
In their school room Beatrix and Bertram kept a variety of small pets, mice, rabbits, a hedgehog and some bats, along with collections of butterflies and other insects which they drew and studied. Beatrix was devoted to the care of her small animals, often taking them with her on long holidays. In most of the first fifteen years of her life, Beatrix spent summer holidays at Dalguise, an estate on the River Tay in Perthshire, Scotland. There she sketched and explored an area that nourished her imagination and her observation. Beatrix and her brother were allowed great freedom in the country and both children became adept students of natural history. In 1887, when Dalguise was no longer available, the Potters took their first summer holiday in the Lake District, at Wray Castle near Windermere. Here Beatrix met Hardwicke Rawnsley, vicar of Wray and later the founding secretary of the National Trust, whose interest in the countryside and country life inspired the same in Beatrix and who was to have a lasting impact on her life.
About the age of 14 Beatrix began to keep a diary. It was written in a code of her own devising which was a simple letter for letter substitution. Her "Journal" was important to the development of her creativity, serving as both sketchbook and literary experiment: in tiny handwriting she reported on society, recorded her impressions of art and artists, recounted stories and observed life around her. The "Journal", decoded and transcribed by Leslie Linder in 1958, does not provide an intimate record of her personal life, but it is an invaluable source for understanding a vibrant part of British society in the late 19th century. It describes Potter's maturing artistic and intellectual interests, her often amusing insights on the places she visited, and her unusual ability to observe nature and to describe it. Started in 1881, her journal ends in 1897 when her artistic and intellectual energies were absorbed in scientific study and in efforts to publish her drawings. Precocious but reserved and often bored, she was searching for more independent activities and wished to earn some money of her own whilst dutifully taking care of her parents, dealing with her especially demanding mother, and managing their various households.
Scientific illustrations and work in mycology.
Beatrix Potter's parents did not discourage higher education. As was common in the Victorian era, women of her class were privately educated and rarely went to university.
Beatrix Potter was interested in every branch of natural science save astronomy. Botany was a passion for most Victorians and nature study was a popular enthusiasm. Potter was eclectic in her tastes: collecting fossils, studying archeological artefacts from London excavations, and interested in entomology. In all these areas she drew and painted her specimens with increasing skill. By the 1890s her scientific interests centred on mycology. First drawn to fungi because of their colours and evanescence in nature and her delight in painting them, her interest deepened after meeting Charles McIntosh, a revered naturalist and mycologist, during a summer holiday in Dunkeld in Perthshire in 1892. He helped improve the accuracy of her illustrations, taught her taxonomy, and supplied her with live specimens to paint during the winter. Curious as to how fungi reproduced, Potter began microscopic drawings of fungus spores (the agarics) and in 1895 developed a theory of their germination. Through the connections of her uncle Sir Henry Enfield Roscoe, a chemist and vice-chancellor of the University of London, she consulted with botanists at Kew Gardens, convincing George Massee of her ability to germinate spores and her theory of hybridisation. She did not believe in the theory of symbiosis proposed by Simon Schwendener, the German mycologist, as previously thought; rather she proposed a more independent process of reproduction.
Rebuffed by William Thiselton-Dyer, the Director at Kew, because of her gender and her amateur status, Beatrix wrote up her conclusions and submitted a paper, "On the Germination of the Spores of the Agaricineae", to the Linnean Society in 1897. It was introduced by Massee because, as a female, Potter could not attend proceedings or read her paper. She subsequently withdrew it, realising that some of her samples were contaminated, but continued her microscopic studies for several more years. Her paper has only recently been rediscovered, along with the rich, artistic illustrations and drawings that accompanied it. Her work is only now being properly evaluated. Potter later gave her other mycological and scientific drawings to the Armitt Museum and Library in Ambleside, where mycologists still refer to them to identify fungi. There is also a collection of her fungus paintings at the Perth Museum and Art Gallery in Perth, Scotland donated by Charles McIntosh. In 1967 the mycologist W.P.K. Findlay included many of Potter's beautifully accurate fungus drawings in his "Wayside & Woodland Fungi", thereby fulfilling her desire to one day have her fungus drawings published in a book. In 1997 the Linnean Society issued a posthumous apology to Potter for the sexism displayed in its handling of her research.
Artistic and literary career.
Potter’s artistic and literary interests were deeply influenced by fairies, fairy tales and fantasy. She was a student of the classic fairy tales of Western Europe. As well as stories from the Old Testament, John Bunyan's "The Pilgrim's Progress" and Harriet Beecher Stowe's "Uncle Tom's Cabin", she grew up with "Aesop's Fables", the fairy tales of the Brothers Grimm and Hans Christian Andersen, Charles Kingsley's "The Water Babies", the folk tales and mythology of Scotland, the German Romantics, Shakespeare, and the romances of Sir Walter Scott. As a young child, before the age of eight, Edward Lear's "Book of Nonsense", including the much loved "The Owl and the Pussycat", and Lewis Carroll's "Alice in Wonderland" had made their impression, although she later said of "Alice" that she was more interested in Tenniel's illustrations than what they were about. The "Brer Rabbit" stories of Joel Chandler Harris had been family favourites, and she later studied his "Uncle Remus" stories and illustrated them. She studied book illustration from a young age and developed her own tastes, but the work of the picture book triumvirate Walter Crane, Kate Greenaway and Randolph Caldecott, the last an illustrator whose work was later collected by her father, was a great influence. When she started to illustrate, she chose first the traditional rhymes and stories, "Cinderella", "Sleeping Beauty", "Ali Baba and the Forty Thieves", "Puss-in-boots", and "Red Riding Hood". But most often her illustrations were fantasies featuring her own pets: mice, rabbits, kittens, and guinea pigs.
In her teenage years Potter was a regular visitor to the art galleries of London, particularly enjoying the summer and winter exhibitions at the Royal Academy in London. Her "Journal" reveals her growing sophistication as a critic as well as the influence of her father's friend, the artist Sir John Everett Millais, who recognised Beatrix's talent of observation. Although Potter was aware of art and artistic trends, her drawing and her prose style were uniquely her own.
As a way to earn money in the 1890s, Beatrix and her brother began to print Christmas cards of their own design, as well as cards for special occasions. Mice and rabbits were the most frequent subject of her fantasy paintings. In 1890 the firm of Hildesheimer and Faulkner bought several of her drawings of her rabbit Benjamin Bunny to illustrate verses by Frederic Weatherly titled "A Happy Pair". In 1893 the same printer bought several more drawings for Weatherly's "Our Dear Relations", another book of rhymes, and the following year Potter sold a series of frog illustrations and verses for "Changing Pictures", a popular annual offered by the art publisher Ernest Nister. Potter was pleased by this success and determined to publish her own illustrated stories.
Whenever Potter went on holiday to the Lake District or Scotland, she sent letters to young friends, illustrating them with quick sketches. Many of these letters were written to the children of her former governess Annie Carter Moore, particularly to her eldest son Noel who was often ill. In September 1893 Potter was on holiday at Eastwood in Dunkeld, Perthshire. She had run out of things to say to Noel and so she told him a story about "four little rabbits whose names were Flopsy, Mopsy, Cottontail and Peter". It became one of the most famous children's letters ever written and the basis of Potter's future career as a writer-artist-storyteller.
In 1900, Potter revised her tale about the four little rabbits, and fashioned a dummy book of it - it has been suggested, in imitation of Helen Bannerman's 1899 bestseller "The Story of Little Black Sambo". Unable to find a buyer for the work, she published it for family and friends at her own expense in December 1901. It was drawn in black and white with a coloured frontispiece. Family friend Canon Hardwicke Rawnsley had great faith in Potter's tale, recast it in didactic verse, and made the rounds of the London publishing houses. Frederick Warne & Co had previously rejected the tale but, eager to compete in the booming small format children's book market, reconsidered and accepted the "bunny book" (as the firm called it) following the recommendation of their prominent children's book artist L. Leslie Brooke. The firm declined Rawnsley's verse in favour of Potter's original prose, and Potter agreed to colour her pen and ink illustrations, choosing the then new Hentschel three-colour process to reproduce her watercolours.
On 2 October 1902 "The Tale of Peter Rabbit" was published, and was an immediate success. It was followed the next year by "The Tale of Squirrel Nutkin" and "The Tailor of Gloucester", which had also first been written as picture letters to the Moore children. Working with Norman Warne as her editor, Potter published two or three little books each year: 23 books in all. The last book in this format was "Cecily Parsley's Nursery Rhymes" in 1922, a collection of favourite rhymes. Although "The Tale of Little Pig Robinson" was not published until 1930, it had been written much earlier. Potter continued creating her little books until after the First World War, when her energies were increasingly directed toward her farming, sheep-breeding and land conservation.
The immense popularity of Potter's books was based on the lively quality of her illustrations, the non-didactic nature of her stories, the depiction of the rural countryside, and the imaginative qualities she lent to her animal characters.
Potter was also a canny businesswoman. As early as 1903 she made and patented a Peter Rabbit doll. It was followed by other "spin-off" merchandise over the years, including painting books, board games, wall-paper, figurines, baby blankets and china tea-sets. All were licensed by Frederick Warne & Co and earned Potter an independent income, as well as immense profits for her publisher.
In 1905, Potter and Norman Warne became unofficially engaged. Potter's parents objected to the match because Warne was "in trade" and thus not socially suitable. The engagement lasted only one month until Warne died of leukemia at age 37. That same year Potter used some of her income and a small inheritance from an aunt to buy Hill Top Farm in Near Sawrey in the English Lake District. Potter and Warne may have hoped that Hill Top Farm would be their holiday home, but after Warne's death Potter went ahead with its purchase as she had always wanted to own that farm, and live in "that charming village".
Country life.
The tenant farmer John Cannon and his family agreed to stay on to manage the farm for her while she made physical improvements and learned the techniques of fell farming and of raising livestock, including pigs, cows and chickens; the following year she added sheep. Realising she needed to protect her boundaries, she sought advice from W.H. Heelis & Son, a local firm of solicitors with offices in nearby Hawkshead. With William Heelis acting for her she bought contiguous pasture, and in 1909 the Castle Farm across the road from Hill Top Farm. She visited Hill Top at every opportunity, and her books written during this period (such as "The Tale of Ginger and Pickles", about the local shop in Near Sawrey and "The Tale of Mrs. Tittlemouse", a wood mouse) reflect her increasing participation in village life and her delight in country living.
Owning and managing these working farms required routine collaboration with the widely respected William Heelis. By the summer of 1912 Heelis had proposed marriage and Beatrix had accepted; although she did not immediately tell her parents, who once again disapproved because Heelis was only a country solicitor. Potter and Heelis were married on 15 October 1913 in London at St Mary Abbots in Kensington. The couple moved immediately to Near Sawrey, residing at Castle Cottage, the renovated farm house on Castle Farm. Hill Top remained a working farm but was now remodelled to allow for the tenant family and Potter's private studio and work shop. At last her own woman, Potter settled into the partnerships that shaped the rest of her life: her country solicitor husband and his large family, her farms, the Sawrey community and the predictable rounds of country life. "The Tale of Jemima Puddle-Duck" and "The Tale of Tom Kitten" are representative of Hill Top Farm and of her farming life, and reflect her happiness with her country life.
After Rupert Potter died in 1914, Potter, now a wealthy woman, found Lindeth Howe, a large house in nearby Windermere where her mother lived until her death in 1931 at the age of 93. Potter continued to write stories for Frederick Warne & Co and fully participated in country life. She established a Nursing Trust for local villages, and served on various committees and councils responsible for footpaths and other rural issues.
Sheep farming.
Beatrix Potter Heelis became keenly interested in the breeding and raising of Herdwick sheep, the indigenous fell sheep, soon after acquiring Hill Top Farm. In 1923 she bought a former deer park and vast sheep farm in the Troutbeck Valley called Troutbeck Park Farm, restoring its land with thousands of Herdwick sheep. This established her as one of the major Herdwick sheep farmers in the area. She was admired by her shepherds and farm managers for her willingness to experiment with the latest biological remedies for the common diseases of sheep, and for her employment of the best shepherds, sheep breeders, and farm managers.
By the late 1920s Potter and her Hill Top farm manager Tom Storey had made a name for their prize-winning Herdwick flock, for which she won many prizes at the local agricultural shows, where she was also often asked to serve as a judge. In 1942 she was named President-elect of The Herdwick Sheepbreeders’ Association, the first time a woman had ever been elected to that office, but died before taking office.
Lake District conservation.
Potter had been a disciple of the land conservation and preservation ideals of her long-time friend and mentor, Canon Hardwicke Rawnsley, the first secretary and founding member of the National Trust for Places of Historic Interest or Natural Beauty. She supported the efforts of the National Trust to preserve not just the places of extraordinary beauty but also those heads of valleys and low grazing lands that would be irreparably ruined by development. She was also an authority on the traditional Lakeland crafts, period furniture and stonework. She restored and preserved the farms that she bought or managed, making sure that each farm house had in it a piece of antique Lakeland furniture. Potter was interested in preserving not only the Herdwick sheep, but also the way of life of fell farming. In 1930 the Heelises became partners with the National Trust in buying and managing the fell farms included in the large Monk Coniston Estate. The estate was composed of many farms spread over a wide area of north-western Lancashire, including the famously beautiful Tarn Hows. Potter was the "de facto" estate manager for the Trust for seven years until the National Trust could afford to buy most of the property back from her. Her stewardship of these farms earned her wide regard, but she was not without her critics. She was notable in observing the problems of afforestation, preserving the intake grazing lands, and husbanding the quarries and timber on these farms. All her farms were stocked with Herdwick sheep and frequently with Galloway cattle.
Later life.
Potter continued to write stories and to draw, although mostly for her own pleasure. Her books in the late 1920s included the semi-autobiographical "The Fairy Caravan", a fanciful tale set in her beloved Troutbeck fells. It was published only in the US during Potter's lifetime, and not until 1952 in the UK. "Sister Anne", Potter's version of the story of Bluebeard, was written especially for her American readers, but illustrated by Katharine Sturges. A final folktale, "Wag by Wall", was published posthumously by "The Horn Book" in 1944. Potter was a generous patron of the Girl Guides, whose troops she allowed to make their summer encampments on her lands and whose company she enjoyed as an older woman.
Potter and William Heelis enjoyed a happy marriage of thirty years, continuing their farming and preservation efforts throughout the hard days of the Second World War. Although they were childless, Potter played an important role in William’s large family, particularly enjoying her relationship with several nieces whom she helped educate and giving comfort and aid to her husband’s brothers and sisters.
Potter died of complications from pneumonia and heart disease on 22 December 1943 at Castle Cottage and her remains were cremated at Carleton Crematorium. She left nearly all her property to the National Trust, including over of land, sixteen farms, cottages and herds of cattle and Herdwick sheep. Hers was the largest gift at that time to the National Trust and it enabled the preservation of the lands now included in the Lake District National Park and the continuation of fell farming. The central office of the National Trust in Swindon was named "Heelis" in 2005 in her memory. William Heelis continued his stewardship of their properties and of her literary and artistic work for the eighteen months he survived her. When he died in August 1945 he left the remainder to the National Trust.
Legacies.
Potter left almost all the original illustrations for her books to the National Trust. The copyright to her stories and merchandise was then given to her publisher Frederick Warne & Co, now a division of the Penguin Group. On January 1, 2014, the copyright expired in the UK and other countries with a 70-years-after-death limit. Hill Top Farm was opened to the public by the National Trust in 1946; her artwork was displayed there until 1985 when it was moved to William Heelis’s former law offices in Hawkshead, also owned by the National Trust as the Beatrix Potter Gallery.
Potter gave her folios of mycological drawings to the Armitt Library and Museum in Ambleside before her death. "The Tale of Peter Rabbit" is owned by Frederick Warne and Company, "The Tailor of Gloucester" by the Tate Gallery and "The Tale of the Flopsy Bunnies" by the British Museum.
The largest public collection of her letters and drawings is the Leslie Linder Bequest and Leslie Linder Collection at the Victoria and Albert Museum in London. In the United States, the largest public collections are those in the Special Collections of the Free Library of Philadelphia, and the Lloyd Cotsen Children’s Library at Princeton University.
Themes.
There are many interpretations of Potter’s literary work, the sources of her art, and her life and times. These include critical evaluations of her corpus of children's literature, and Modernist interpretations of Humphrey Carpenter and Katherine Chandler. Judy Taylor, "That Naughty Rabbit: Beatrix Potter and Peter Rabbit" (rev. 2002) tells the story of the first publication and many editions.
Potter’s country life and her farming has also been widely discussed in the work of Susan Denyer and by other authors in the publications of The National Trust.
Potter's work as a scientific illustrator and her work in mycology is highlighted in several chapters in Linda Lear, "Beatrix Potter: A Life in Nature", 2007; "Beatrix Potter: The Extraordinary Life of a Victorian Genius". 2008, UK.
Adaptations and fictionalisations.
In 1971 a ballet film was released, "The Tales of Beatrix Potter", directed by Reginald Mills. Set to music by John Lanchbery with choreography by Frederick Ashton and performed in character costume by members of the Royal Ballet and the Royal Opera House orchestra. The ballet of the same name has been performed by other dance companies around the world.
In 1982, the BBC produced "The Tale of Beatrix Potter". This dramatisation of her life was written by John Hawkesworth and directed by Bill Hayes. It starred Holly Aird and Penelope Wilton as the young and adult Beatrix respectively.
In 2006 Chris Noonan directed "Miss Potter", a biopic of Potter’s life focusing on her early career and romance with her editor Norman Warne. Renée Zellweger and Ewan McGregor play the lead roles.
Potter is also featured in a series of light mysteries called "The Cottage Tales of Beatrix Potter" by Susan Wittig Albert. The eight books in the series starting with the "Tale of Hill Top Farm" (2004) deal with her life in the Lake District and the village of Near Sawrey between 1905 and 1913.
Publications.
The 23 Tales
Other books

</doc>
<doc id="4482" url="http://en.wikipedia.org/wiki?curid=4482" title="Liberal Party (UK)">
Liberal Party (UK)

The Liberal Party was a liberal political movement that formed one of the two major political parties in the United Kingdom during the 19th and early 20th centuries. Its influence then waned, but not before it had moved toward social liberalism and introduced important elements of Britain's welfare state.
The party arose from an alliance of Whigs and free-trade Peelites and Radicals during the 1850s. By the end of the nineteenth century, it had formed four governments under William Gladstoneone of the party's most significant leadersalthough they were punctuated by heavy election defeats. Despite becoming divided over the issue of Irish Home Rule, the party returned to power in 1906 with a landslide victory and, between then and the onset of World War I, Liberal governments oversaw the welfare reforms that created a basic British welfare state. During this time, the party's other two most significant leaders came to the fore: Herbert Asquith, Prime Minister between 1908 and 1916; and David Lloyd George, who followed Asquith as Prime Minister for the rest of World War I and thereafter until 1922.
1922 marked the end of the coalition the party had formed with the Conservative ("Tory") Party during the war and the last time the party was, in government, anything more than a junior coalition partner. By the end of the 1920s, the Labour Party had replaced it as the Tories' primary rival and the party went into a decline that, by the 1950s, saw it winning no more than six seats at general elections. Apart from a few notable by-election victories, the party's fortunes did not improve significantly until it formed the SDP–Liberal Alliance with the newly formed Social Democratic Party (SDP) in 1981. At the next general election, in 1983, the Alliance received over a quarter of the overall vote, but only secured 23 of the 650 seats contested. After the 1987 general election saw this share fall below 23%, the Liberal and SDP parties formally merged in 1988 to form the Liberal Democrats. A small splinter Liberal Party was formed in 1989 by former party members opposed to the merger.
Two of the most prominent intellectuals associated with the Liberal Party were the economist John Maynard Keynes and social planner William Beveridge.
Ideology.
During the 19th century, the Liberal Party was broadly in favour of what would today be called classical liberalism: supporting "laissez-faire" economic policies such as free trade and minimal government interference in the economy (this doctrine was usually termed 'Gladstonian Liberalism' after the Victorian era Liberal Prime Minister William Ewart Gladstone). The Liberal Party favoured social reform, personal liberty, reducing the powers of the Crown and the Church of England (many of them were Nonconformists) and an extension of the electoral franchise. Sir William Harcourt, a prominent Liberal politician in the Victorian era, said this about liberalism in 1872:
If there be any party which is more pledged than another to resist a policy of restrictive legislation, having for its object social coercion, that party is the Liberal party. (Cheers.) But liberty does not consist in making others do what you think right, (Hear, hear.) The difference between a free Government and a Government which is not free is principally this—that a Government which is not free interferes with everything it can, and a free Government interferes with nothing except what it must. A despotic Government tries to make everybody do what it wishes; a Liberal Government tries, as far as the safety of society will permit, to allow everybody to do as he wishes. It has been the tradition of the Liberal party consistently to maintain the doctrine of individual liberty. It is because they have done so that England is the place where people can do more what they please than in any other country in the world...It is this practice of allowing one set of people to dictate to another set of people what they shall do, what they shall think, what they shall drink, when they shall go to bed, what they shall buy, and where they shall buy it, what wages they shall get and how they shall spend them, against which the Liberal party have always protested.
The political terms of "modern", "progressive" or "new" Liberalism began to appear in the mid to late 1880s and became increasingly common to denote the tendency in the Liberal Party to favour an increased role for the state as more important than the classical liberal stress on self-help and freedom of choice.
By the early 20th century the Liberals stance began to shift towards "New Liberalism", what would today be called social liberalism: a belief in personal liberty with a support for government intervention to provide minimum levels of welfare. This shift was best exemplified by the Liberal government of Herbert Henry Asquith and his Chancellor David Lloyd George, whose Liberal reforms in the early 1900s created a basic welfare state.
David Lloyd George adopted a programme at the 1929 general election entitled "We Can Conquer Unemployment!", although by this stage the Liberals had declined to third-party status. The Liberals now (as expressed in the Liberal Yellow Book) regarded opposition to state intervention as being a characteristic of right-wing extremists.
After nearly becoming extinct in the 1940s and 50s, the Liberal Party revived its fortunes somewhat under the leadership of Jo Grimond in the 1960s, by positioning itself as a radical centrist non-socialist alternative to the Conservative and Labour Party governments of the time.
Origins.
The Liberal Party grew out of the Whigs, who had their origins in an aristocratic faction in the reign of Charles II, and the early 19th century Radicals. The Whigs were in favour of reducing the power of the Crown and increasing the power of Parliament. Although their motives in this were originally to gain more power for themselves, the more idealistic Whigs gradually came to support an expansion of democracy for its own sake. The great figures of reformist Whiggery were Charles James Fox (died 1806) and his disciple and successor Earl Grey. After decades in opposition, the Whigs returned to power under Grey in 1830 and carried the First Reform Act in 1832.
The Reform Act was the climax of Whiggism, but it also brought about the Whigs' demise. The admission of the middle classes to the franchise and to the House of Commons led eventually to the development of a systematic middle class liberalism and the end of Whiggery, although for many years reforming aristocrats held senior positions in the party. In the years after Grey's retirement, the party was led first by Lord Melbourne, a fairly traditional Whig, and then by Lord John Russell, the son of a Duke but a crusading radical, and by Lord Palmerston, a renegade Irish Tory and essentially a conservative, although capable of radical gestures.
As early as 1839 Russell had adopted the name of "Liberals", but in reality his party was a loose coalition of Whigs in the House of Lords and Radicals in the Commons. The leading Radicals were John Bright and Richard Cobden, who represented the manufacturing towns which had gained representation under the Reform Act. They favoured social reform, personal liberty, reducing the powers of the Crown and the Church of England (many of them were Nonconformists), avoidance of war and foreign alliances (which were bad for business), and above all free trade. For a century, free trade remained the one cause which could unite all Liberals.
In 1841 the Liberals lost office to the Conservatives under Sir Robert Peel, but their period in opposition was short, because the Conservatives split over the repeal of the Corn Laws, a free trade issue, and a faction known as the Peelites (but not Peel himself, who died soon after) defected to the Liberal side. This allowed ministries led by Russell, Palmerston, and the Peelite Lord Aberdeen to hold office for most of the 1850s and 1860s. A leading Peelite was William Ewart Gladstone, who was a reforming Chancellor of the Exchequer in most of these governments. The formal foundation of the Liberal Party is traditionally traced to 1859 and the formation of Palmerston's second government.
The Whig-Radical amalgam could not become a true modern political party, however, while it was dominated by aristocrats, and it was not until the departure of the "Two Terrible Old Men", Russell and Palmerston, that Gladstone could become the first leader of the modern Liberal Party. This was brought about by Palmerston's death in 1865 and Russell's retirement in 1868. After a brief Conservative government (during which the Second Reform Act was passed by agreement between the parties) Gladstone won a huge victory at the 1868 election and formed the first Liberal government. The establishment of the party as a national membership organisation came with the foundation of the National Liberal Federation in 1877.
The Gladstonian era.
For the next thirty years Gladstone and Liberalism were synonymous. William Ewart Gladstone served as prime minister four times (1868–74, 1880–85, 1886, and 1892–94). His financial policies, based on the notion of balanced budgets, low taxes, and laissez-faire, were suited to a developing capitalist society, but they could not respond effectively as economic and social conditions changed. Called the "Grand Old Man" later in life, Gladstone was always a dynamic popular orator who appealed strongly to the working class and to the lower middle class. Deeply religious, Gladstone brought a new moral tone to politics, with his evangelical sensibility and his opposition to aristocracy. His moralism often angered his upper-class opponents (including Queen Victoria), and his heavy-handed control split the Liberal Party.
In foreign policy, Gladstone was in general against foreign entanglements, but he did not resist the realities of imperialism. For example, he approved of the occupation of Egypt by British forces in 1882. His goal was to create a European order based on co-operation rather than conflict and on mutual trust instead of rivalry and suspicion; the rule of law was to supplant the reign of force and self-interest. This Gladstonian concept of a harmonious Concert of Europe was opposed to and ultimately defeated by a Bismarckian system of manipulated alliances and antagonisms.
As prime minister 1868 to 1874, Gladstone headed a Liberal Party which was a coalition of Peelites like himself, Whigs, and Radicals; he was now a spokesman for "peace, economy and reform." One major achievement was the Elementary Education Act of 1870, which provided England with an adequate system of elementary schools for the first time and made attendance compulsory. He also secured the abolition of the purchase of commissions in the army and of religious tests for admission to Oxford and Cambridge; the introduction of the secret ballot in elections; the legalization of trade unions; and the reorganization of the judiciary in the Judicature Act.
Regarding Ireland, the major Liberal achievements were land reform, where he ended centuries of landlord oppression, and the disestablishment of the (Anglican) Church of Ireland through the Irish Church Act 1869.
In the 1874 general election Gladstone was defeated by the Conservatives under Disraeli during a sharp economic recession. He formally resigned as Liberal leader and was succeeded by the Marquess of Hartington, but he soon changed his mind and returned to active politics. He strongly disagreed with Disraeli's pro-Ottoman foreign policy and in 1880 he conducted the first outdoor mass-election campaign in Britain, known as the Midlothian campaign. The Liberals won a large majority in the 1880 election. Hartington ceded his place and Gladstone resumed office.
Among the consequences of the Third Reform Act (1884–85) was the giving of the vote to the Catholic peasants in Ireland, and the consequent creation of an Irish Parliamentary Party led by Charles Stewart Parnell. In the 1885 general election this party won the balance of power in the House of Commons, and demanded Irish Home Rule as the price of support for a continued Gladstone ministry. Gladstone personally supported Home Rule, but a strong Liberal Unionist faction led by Joseph Chamberlain, along with the last of the Whigs, Hartington, opposed it. The Irish Home Rule bill gave all owners of Irish land a chance to sell to the state at a price equal to 20 years' purchase of the rents and allowing tenants to purchase the land. Irish nationalist reaction was mixed, Unionist opinion was hostile, and the election addresses during the 1886 election revealed English radicals to be against the bill also. Among the Liberal rank and file, several Gladstonian candidates disowned the bill, reflecting fears at the constituency level that the interests of the working people were being sacrificed to finance a rescue operation for the landed elite.
The result was a catastrophic split in the Liberal Party, and heavy defeat in the 1886 election at the hands of Lord Salisbury. There was a final weak Gladstone ministry in 1892, but it also was dependent on Irish support and failed to get Irish Home Rule through the House of Lords.
Gladstone finally retired in 1894, and his ineffectual successor, Lord Rosebery, led the party to another heavy defeat in the 1895 general election.
A major long-term consequence of the Third Reform Act was the rise of Lib-Lab candidates, in the absence of any committed Labour Party. The Act split all county constituencies (which were represented by multiple MPs) into single-member constituencies, roughly corresponding to population patterns. In areas with working class majorities, in particular coal-mining areas, Lib-Lab candidates were popular, and they received sponsorship and endorsement from trade unions. In the first election after the Act was passed (1885), thirteen were elected, up from two in 1874. The Third Reform Act also facilitated the demise of the Whig old guard: in two-member constituencies, it was common to pair a Whig and a radical under the Liberal banner. After the Third Reform Act, fewer Whigs were selected as candidates.
The Liberal zenith.
The Liberals languished in opposition for a decade, while the coalition of Salisbury and Chamberlain held power. The 1890s were marred by infighting between the three principal successors to Gladstone, party leader William Harcourt, former Prime Minister Lord Rosebery, and Gladstone's personal secretary, John Morley. This intrigue finally led Harcourt and Morley to resign their positions in 1898 as they continued to be at loggerheads with Rosebery over Irish home rule and issues relating to imperialism. Replacing Harcourt as party leader was Sir Henry Campbell-Bannerman. Harcourt's resignation briefly muted the turmoil in the party, but the beginning of the Second Boer War soon nearly broke the party apart, with Rosebery and a circle of supporters including important future Liberal leaders H.H. Asquith, Edward Grey, and Richard Burdon Haldane forming a clique dubbed the "Liberal Imperialists" that supported the government in the prosecution of the war. On the other side, more radical members of the party formed a Pro-Boer faction that denounced the conflict and called for an immediate end to hostilities. Quickly rising to prominence among the Pro-Boers was David Lloyd George, a relatively new MP and a master of rhetoric, who took advantage of having a national stage to speak out on a controversial issue to make his name in the party. Harcourt and Morley also sided with this group, though with slightly different aims. Campbell-Bannerman tried to keep these forces together at the head of a moderate Liberal rump, but in 1901 he delivered a speech on the government's "methods of barbarism" in South Africa that pulled him further to the left and nearly tore the party in two. The party was saved after Salisbury's retirement in 1902 when his successor, Arthur Balfour, pushed a series of unpopular initiatives such as a new education bill and Joseph Chamberlain called for a new system of protectionist tariffs. Campbell-Bannerman was able to rally the party around the traditional liberal platform of free trade and land reform and led them to the greatest election victory in their history. This would prove the last time the Liberals won a majority in their own right.
Although he presided over a large majority, Sir Henry Campbell-Bannerman was overshadowed by his ministers, most notably Herbert Henry Asquith at the Exchequer, Edward Grey at the Foreign Office, Richard Burdon Haldane at the War Office and David Lloyd George at the Board of Trade. An ill Campbell-Bannerman retired in 1908 and died later that year. He was succeeded by Asquith, who stepped up the government's radicalism. Lloyd George succeeded Asquith at the Exchequer, and was in turn succeeded at the Board of Trade by Winston Churchill, a recent defector from the Conservatives.
The Liberals pushed through much legislation, including the regulation of working hours, National Insurance and welfare. It was at this time that a political battle over the so-called People's Budget resulted in the passage of an act ending the power of the House of Lords to block legislation. The cost was high, however, as the government was required by the king to call two general elections in 1910 to validate its position and ended up frittering away most of its large majority, being left once again dependent on the Irish Nationalists.
As a result Asquith was forced to introduce a new third Home Rule bill in 1912. Since the House of Lords no longer had the power to block the bill, the Unionist's Ulster Volunteers led by Sir Edward Carson, launched a campaign of opposition that included the threat of armed resistance in Ulster and the threat of mutiny by army officers in Ireland in 1914 ("see Curragh Incident"). In their resistance to Home Rule the Ulster Protestants had the full support of the Conservatives, whose leader, Andrew Bonar Law, was of Ulster-Scots descent. The country seemed to be on the brink of civil war when the First World War broke out in August 1914. Historian George Dangerfield has argued that the multiplicity of crises in 1910 to 1914, before the war broke out, so weakened the Liberal coalition that it marked the "Strange Death of Liberal England." However, most historians date the collapse to the crisis of the First World War.
Liberal decline.
The war struck at the heart of everything British Liberals believed in. The party divided over the distinctly illiberal policies that were introduced under its auspices, including conscription and the Defence of the Realm Act. Several Cabinet ministers resigned, and Asquith, the master of domestic politics, proved a poor war leader. Lloyd George and Churchill, however, were zealous supporters of the war, and gradually forced the old peace-oriented Liberals out. The poor British performance in the early months of the war forced Asquith to invite the Conservatives into a coalition (on 17 May 1915). This marked the end of the last all-Liberal government. This coalition fell apart at the end of 1916, when the Conservatives withdrew their support from Asquith and gave it instead to Lloyd George, who became Prime Minister at the head of a coalition government largely made up of Conservatives. Asquith and his followers moved to the opposition benches in Parliament and the Liberal Party was split once again.
Wilson argues that Lloyd George abandoned many liberal principles in his single-minded crusade to win the war at all costs. That brought him and like-minded Liberals into a coalition with the Conservatives, largely on the ground long occupied by Conservatives: they were not oriented toward world peace or liberal treatment of Germany, nor discomfited by aggressive and authoritarian measures of state power. More deadly to the future of the Party, says Wilson, was its repudiation by ideological Liberals, who decided sadly that it no longer represented their principles. Finally the presence of the vigorous new Labour Party on the left gave a new home to voters disenchanted with the Liberal Party.
In the 1918 general election Lloyd George, "the Man Who Won the War", led his coalition into another "khaki election", and won a sweeping victory over the Asquithian Liberals and the newly emerging Labour Party. Lloyd George and the Conservative leader Andrew Bonar Law wrote a joint letter of support to candidates to indicate they were considered the official Coalition candidates – this "coupon", as it became known, was issued against many sitting Liberal MPs, often to devastating effect, though not against Asquith himself. Asquith and most of his colleagues lost their seats. Lloyd George still claimed to be leading a Liberal government, but he was increasingly under the influence of the rejuvenated Conservative party. In 1922 the Conservative backbenchers rebelled against the continuation of the coalition, citing in particular the Chanak Crisis over Turkey and Lloyd George's corrupt sale of honours, amongst other grievances, and Lloyd George was forced to resign. The Conservatives came back to power under Bonar Law and then Stanley Baldwin.
At the 1922 and 1923 elections the Liberals won barely a third of the vote and only a quarter of the seats in the House of Commons, as many radical voters abandoned the divided Liberals and went over to Labour. In 1922 Labour became the official opposition. A reunion of the two warring factions took place in 1923 when the new Conservative Prime Minister Stanley Baldwin committed his party to protective tariffs, causing the Liberals to reunite in support of free trade. The party gained ground in the 1923 general election but ominously made most of its gains from Conservatives whilst losing ground to Labour – a sign of the party's direction for many years to come. The party remained the third largest in the House of Commons, but the Conservatives had lost their majority. There was much speculation and fear about the prospect of a Labour government, and comparatively little about a Liberal government, even though it could have plausibly presented an experienced team of ministers compared to Labour's almost complete lack of experience, as well as offering a middle ground that could obtain support from both Conservatives and Labour in crucial Commons divisions. But instead of trying to force the opportunity to form a Liberal government, Asquith decided instead to allow Labour the chance of office, in the belief that they would prove incompetent and this would set the stage for a revival of Liberal fortunes at Labour's expense. It was a fatal error.
Labour was determined to destroy the Liberals and become the sole party of the left. Ramsay MacDonald was forced into a snap election in 1924, and although his government was defeated, he achieved his objective of virtually wiping the Liberals out as many more radical voters now moved to Labour, whilst moderate middle-class Liberal voters concerned about socialism moved to the Conservatives. The Liberals were reduced to a mere forty seats in Parliament, only seven of which had been won against candidates from both parties; and none of these formed a coherent area of Liberal survival. The party seemed finished, and during this period some Liberals, such as Churchill, went over to the Conservatives, while others went over to Labour. (Several Labour ministers of later generations, such as Michael Foot and Tony Benn, were the sons of Liberal MPs.)
Asquith died in 1928 and the enigmatic figure of Lloyd George returned to the leadership and began a drive to produce coherent policies on many key issues of the day. In the 1929 general election he made a final bid to return the Liberals to the political mainstream, with an ambitious programme of state stimulation of the economy called "We Can Conquer Unemployment!", largely written for him by the Liberal economist John Maynard Keynes. The Liberals gained ground, but once again it was at the Conservatives' expense whilst also losing seats to Labour. Indeed the urban areas of the country suffering heavily from unemployment, which might have been expected to respond the most to the radical economic policies of the Liberals, instead gave the party its worst results. By contrast most of the party's seats were won either due to the absence of a candidate from one of the other parties or in rural areas on the "Celtic fringe", where local evidence suggests that economic ideas were at best peripheral to the electorate's concerns. The Liberals now found themselves with 59 members, holding the balance of power in a Parliament where Labour was the largest party but lacked an overall majority. Lloyd George offered a degree of support to the Labour government in the hope of winning concessions, including a degree of electoral reform to introduce the alternative vote, but this support was to prove bitterly divisive as the Liberals increasingly divided between those seeking to gain what Liberal goals they could achieve, those who preferred a Conservative government to a Labour one and vice-versa.
The last majority Liberal Government in Britain was elected in 1906.
The years preceding the First World War were marked by worker strikes and civil unrest and saw many violent confrontations between civilians and the police and armed forces. Other issues of the period included women's suffrage and the Irish Home Rule movement.
After the carnage of 1914–1918, the democratic reforms of the Representation of the People Act 1918 instantly tripled the number of people entitled to vote in Britain from seven to twenty-one million. The Labour Party benefited most from this huge change in the electorate, forming its first minority government in 1924.
The splits over the National Government.
In 1931 MacDonald's government fell apart under the Great Depression, and the Liberals agreed to join his National Government, dominated by the Conservatives. Lloyd George himself was ill and did not actually join. Soon, however, the Liberals faced another divisive crisis when a National Government was proposed to fight the 1931 general election with a mandate for tariffs. From the outside, Lloyd George called for the party to abandon the government completely in defence of free trade, but only a few MPs and candidates followed. Another group under Sir John Simon then emerged, who were prepared to continue their support for the government and take the Liberal places in the Cabinet if there were resignations. The third group under Sir Herbert Samuel pressed for the parties in government to fight the election on separate platforms. In doing so the bulk of Liberals remained supporting the government, but two distinct Liberal groups had emerged within this bulk – the Liberal Nationals (officially the "National Liberals" after 1947) led by Simon, also known as "Simonites", and the "Samuelites" or "official Liberals", led by Samuel who remained as the official party. Both groups secured about 34 MPs but proceeded to diverge even further after the election, with the Liberal Nationals remaining supporters of the government throughout its life. There were to be a succession of discussions about them rejoining the Liberals, but these usually foundered on the issues of free trade and continued support for the National Government. The one significant reunification came in 1946 when the Liberal and Liberal National party organisations in London merged.
The official Liberals found themselves a tiny minority within a government committed to protectionism. Slowly they found this issue to be one they could not support. In early 1932 it was agreed to suspend the principle of collective responsibility to allow the Liberals to oppose the introduction of tariffs. Later in 1932 the Liberals resigned their ministerial posts over the introduction of the Ottawa Agreement on Imperial Preference. However, they remained sitting on the government benches supporting it in Parliament, though in the country local Liberal activists bitterly opposed the government. Finally in late 1933 the Liberals crossed the floor of the House of Commons and went into complete opposition. By this point their number of MPs was severely depleted. In the 1935 general election, just 17 Liberal MPs were elected, along with Lloyd George and three followers as "independent Liberals". Immediately after the election the two groups reunited, though Lloyd George declined to play much of a formal role in his old party. Over the next ten years there would be further defections as MPs deserted to either the Liberal Nationals or Labour. Yet there were a few recruits, such as Clement Davies, who had deserted to the National Liberals in 1931 but now returned to the party during the Second World War and who would lead it after the war.
Near extinction.
Samuel had lost his seat in the 1935 election and the leadership of the party fell to Sir Archibald Sinclair. With many traditional domestic Liberal policies now regarded as irrelevant, he focused the party on opposition to both the rise of Fascism in Europe and the appeasement foreign policy of the British government, arguing that intervention was needed, in contrast to the Labour calls for pacifism. Despite the party's weaknesses, Sinclair gained a high profile as he sought to recall the Midlothian Campaign and once more revitalise the Liberals as the party of a strong foreign policy.
In 1940 they joined Churchill's wartime coalition government, with Sinclair serving as Secretary of State for Air, the last British Liberal to hold Cabinet rank office for seventy years. However, it was a sign of the party's lack of importance that they were not included in the War Cabinet. At the 1945 general election, Sinclair and many of his colleagues lost their seats to both Conservatives and Labour, and the party returned just 12 MPs to Westminster. But this was just the beginning of the decline. In 1950, the general election saw the Liberals return just nine MPs. Another general election was called in 1951, and the Liberals were left with just six MPs in parliament; all but one of them were aided by the fact that the Conservatives refrained from fielding candidates in those constituencies.
In 1957 this total fell to five when one of the Liberal MPs died and the subsequent by-election was lost to the Labour Party, which selected the former Liberal Deputy Leader Lady Megan Lloyd George as its own candidate. The Liberal Party seemed close to extinction. During this low period, it was often joked that Liberal MPs could hold meetings in the back of one taxi.
Liberal revival.
Through the 1950s and into the 1960s the Liberals survived only because a handful of constituencies in rural Scotland and Wales clung to their Liberal traditions, whilst in two English towns, Bolton and Huddersfield, local Liberals and Conservatives agreed to each contest only one of the town's two seats. Jo Grimond, for example, who became Liberal leader in 1956, was MP for the remote Orkney and Shetland islands. Under his leadership a Liberal revival began, marked by the Orpington by-election of March 1962 which was won by Eric Lubbock. There, the Liberals won a seat in the London suburbs for the first time since 1935.
The Liberals became the first of the major British political parties to advocate British membership of the European Economic Community. Grimond also sought an intellectual revival of the party, seeking to position it as a non-socialist radical alternative to the Conservative government of the day. In particular he canvassed the support of the young post-war university students and recent graduates, appealing to younger voters in a way that many of his recent predecessors had not, and asserting a new strand of Liberalism for the post-war world.
The new middle-class suburban generation began to find the Liberals' policies attractive again. Under Grimond (who retired in 1967) and his successor, Jeremy Thorpe, the Liberals regained the status of a serious third force in British politics, polling up to 20% of the vote but unable to break the duopoly of Labour and Conservative and win more than fourteen seats in the Commons. An additional problem was competition in the Liberal heartlands in Scotland and Wales from the Scottish National Party and Plaid Cymru who both grew as electoral forces from the 1960s onwards. Although Emlyn Hooson held on to the seat of Montgomeryshire, upon Clement Davies death in 1962, the party lost five Welsh seats between 1950 and 1966. In September 1966 the Welsh Liberal Party formed their own state party, moving the Liberal Party into a fully federal structure.
In local elections Liverpool remained a Liberal stronghold, with the party taking the plurality of seats on the elections to the new Liverpool Metropolitan Borough Council in 1973. In the February 1974 general election the Conservative government of Edward Heath won a plurality of votes cast, but the Labour Party gained a plurality of seats due to the Ulster Unionist MPs refusing to support the Conservatives after the Northern Ireland Sunningdale Agreement. The Liberals now held the balance of power in the Commons. Conservatives offered Thorpe the Home Office if he would join a coalition government with Heath. Thorpe was personally in favour of it, but the party insisted on a clear government commitment to introducing proportional representation and a change of Prime Minister. The former was unacceptable to Heath's Cabinet and the latter to Heath personally, so the talks collapsed. Instead a minority Labour government was formed under Harold Wilson but with no formal support from Thorpe. In the October 1974 general election the Liberals slipped back slightly and the Labour government won a wafer-thin majority.
Thorpe was subsequently forced to resign after allegations about his private life. The party's new leader, David Steel, negotiated the Lib-Lab pact with Wilson's successor as Prime Minister, James Callaghan. According to this pact, the Liberals would support the government in crucial votes in exchange for some influence over policy. The agreement lasted from 1977 to 1978, but proved mostly fruitless, for two reasons: the Liberals' key demand of proportional representation was rejected by most Labour MPs, whilst the contacts between Liberal spokespersons and Labour ministers often proved detrimental, such as between finance spokesperson John Pardoe and Chancellor of the Exchequer Denis Healey, who were mutually antagonistic.
Alliance with Social Democrats.
When the Labour government fell in 1979, the Conservatives under Margaret Thatcher won a victory which served to push the Liberals back into the margins.
In 1981, defectors from the moderate wing of the Labour Party, led by former Cabinet ministers Roy Jenkins, David Owen and Shirley Williams, founded the Social Democratic Party. The new party and the Liberals quickly formed an alliance, which for a while polled as high as 50% in the opinion polls and appeared capable of winning the next general election. However, they were later overtaken in the polls by the Conservatives and at the 1983 general election the Conservatives triumphed by a landslide, with Labour once again forming the opposition, while the SDP-Liberal Alliance came close to Labour in terms of votes (a share of more than 25%) although it only had 23 MPs compared to Labour's 209.
The 1987 election saw the Alliance's share of the votes drop slightly and it now had 22 MPs, and in the election's aftermath Liberal leader David Steel proposed a merged of the two parties. Most SDP members voted in favour of the merger, but SDP leader David Owen objected and continued to lead a "rump" SDP, with the merger of the two parties being completed in March 1988 to form the Social and Liberal Democrats, becoming the Liberal Democrats in October 1989.
Merger with Social Democrats.
In 1988 the Liberals and Social Democrats merged to create what came to be called the Liberal Democrats. Over two-thirds of the members, and all the serving MPs, of the Liberal Party joined this party, led first jointly by Steel and the SDP leader Robert Maclennan, and later by Paddy Ashdown (1988–99), Charles Kennedy (1999–2006), Sir Menzies Campbell (2006–07) and Nick Clegg (incumbent).
Though the merger process was traumatic and the new party suffered a few years of extremely poor poll results, it gradually found much greater electoral success than the Liberal Party had achieved in the post-war era. In the 2005 general election, 62 Liberal Democrat MPs were elected to the House of Commons, a far cry from the days when the Liberals had just 5 MPs and Liberalism as a political force had seemed moribund.
As with the Liberal Party for most of the 20th century, the Liberal Democrats face constant questioning about which of the other two parties they are closer to, in particular about which they would support in the event of a hung parliament. The party is keen to maintain its independent identity, however, and argues that the need for a modern Liberal force in British politics has never been greater.
In the 2010 General Election, the Conservative Party won more seats than any other, but not enough to form a majority government. After several days of negotiation, the Liberal Democrats agreed to join the Conservatives as part of a coalition government.
The post-1988 Liberal Party.
A group of Liberal opponents of the merger with the Social Democrats, including Michael Meadowcroft (formerly Liberal MP for Leeds West) and Paul Wiggin (who served on Peterborough City Council as a Liberal), continued under the old name of "the Liberal Party". This was legally a new organisation (the headquarters, records, assets and debts of the old party were inherited by the Liberal Democrats), but its constitution asserts it to be the same Liberal party. The party retains influence in some local councils. Meadowcroft himself eventually joined the Liberal Democrats in 2007.
National Liberal Party.
In 1931 a number of Liberal MPs split from the original Liberal Party in protest at the leadership's adherence to free trade. The party name was appropriated in 2006 by the far-right political think tank the Third Way.

</doc>
<doc id="4484" url="http://en.wikipedia.org/wiki?curid=4484" title="Bank of England">
Bank of England

The Bank of England, formally the Governor and Company of the Bank of England, is the central bank of the United Kingdom and the model on which most modern central banks have been based. Established in 1694, it is the second oldest central bank in the world, after the Sveriges Riksbank, and the world's 8th oldest bank. It was established to act as the English Government's banker, and is still the banker for the Government of the United Kingdom. The Bank was privately owned by stockholders from its foundation in 1694 until nationalised in 1946.
In 1998, it became an independent public organisation, wholly owned by the Treasury Solicitor on behalf of the government, with independence in setting monetary policy.
The Bank is one of eight banks authorised to issue banknotes in the United Kingdom, but has a monopoly on the issue of banknotes in England and Wales and regulates the issue of banknotes by commercial banks in Scotland and Northern Ireland.
The Bank's Monetary Policy Committee has devolved responsibility for managing monetary policy. The Treasury has reserve powers to give orders to the committee "if they are required in the public interest and by extreme economic circumstances" but such orders must be endorsed by Parliament within 28 days. The Bank's Financial Policy Committee held its first meeting in June 2011 as a macro prudential regulator to oversee regulation of the UK's financial sector.
The Bank's headquarters have been in London's main financial district, the City of London, on Threadneedle Street, since 1734. It is sometimes known by the metonym "The Old Lady of Threadneedle Street" or "The Old Lady", a name taken from the legend of Sarah Whitehead, whose ghost is said to haunt the Bank's garden. The busy road junction outside is known as Bank junction.
Mark Carney assumed the post of The Governor of the Bank of England on 1 July 2013. He succeeded Mervyn King, who took over on 30 June 2003. Carney, a Canadian, will serve an initial five-year term rather than the typical eight, and will seek UK citizenship. He is the first non-British citizen to hold the post. As of January 2014, the Bank also has three Deputy Governors.
History.
England's crushing defeat by France, the dominant naval power, in naval engagements culminating in the 1690 Battle of Beachy Head, became the catalyst for England's rebuilding itself as a global power. England had no choice but to build a powerful navy. No public funds were available, and the credit of William III's government was so low in London that it was impossible for it to borrow the £1,200,000 (at 8 per cent) that the government wanted.
In order to induce subscription to the loan, the subscribers were to be incorporated by the name of the Governor and Company of the Bank of England. The Bank was given exclusive possession of the government's balances, and was the only limited-liability corporation allowed to issue bank notes. The lenders would give the government cash (bullion) and issue notes against the government bonds, which can be lent again. The £1.2m was raised in 12 days; half of this was used to rebuild the navy.
As a side effect, the huge industrial effort needed, from establishing iron-works to make more nails to agriculture feeding the quadrupled strength of the navy, started to transform the economy. This helped the new Kingdom of Great Britain – England and Scotland were formally united in 1707 – to become powerful. The power of the navy made Britain the dominant world power in the late eighteenth and early nineteenth centuries.
The establishment of the bank was devised by Charles Montagu, 1st Earl of Halifax, in 1694, to the plan which had been proposed by William Paterson three years before, but not acted upon. He proposed a loan of £1.2m to the government; in return the subscribers would be incorporated as The Governor and Company of the Bank of England with long-term banking privileges including the issue of notes. The Royal Charter was granted on 27 July through the passage of the Tonnage Act 1694. Public finances were in so dire a condition at the time that the terms of the loan were that it was to be serviced at a rate of 8% per annum, and there was also a service charge of £4,000 per annum for the management of the loan. The first governor was Sir John Houblon, who is depicted in the £50 note issued in 1994. The charter was renewed in 1742, 1764, and 1781.
The Bank's original home was in Walbrook in the City of London, where during reconstruction in 1954 archaeologists found the remains of a Roman temple of Mithras (Mithras was – rather fittingly – worshipped as being the God of Contracts); the Mithraeum ruins are perhaps the most famous of all twentieth-century Roman discoveries in the City of London and can be viewed by the public.
The Bank moved to its current location on Threadneedle Street, and thereafter slowly acquired neighbouring land to create the edifice seen today. Sir Herbert Baker's rebuilding of the Bank, demolishing most of Sir John Soane's earlier building, was described by architectural historian Nikolaus Pevsner as "the greatest architectural crime, in the City of London, of the twentieth century".
When the idea and reality of the National Debt came about during the 18th century this was also managed by the Bank. By the charter renewal in 1781 it was also the bankers' bank – keeping enough gold to pay its notes on demand until 26 February 1797 when war had so diminished gold reserves that the government prohibited the Bank from paying out in gold. This prohibition lasted until 1821.
The Bank also had a narrow escape of a different nature. In 1780, rioters in London tried to storm the building. Thereafter, every night until 1973, a detachment of soldiers patrolled the perimeter to ensure the safety of the nation's gold.
19th century.
The 1844 Bank Charter Act tied the issue of notes to the gold reserves and gave the Bank sole rights with regard to the issue of banknotes. Private banks that had previously had that right retained it, provided that their headquarters were outside London and that they deposited security against the notes that they issued. A few English banks continued to issue their own notes until the last of them was taken over in the 1930s. Scottish and Northern Irish private banks still have that right.
The bank acted as lender of last resort for the first time in the panic of 1866.
20th century.
Britain remained on the gold standard until 1931 when the gold and foreign exchange reserves were transferred to the treasury, but their management was still handled by the Bank. 
During the governorship of Montagu Norman, from 1920 to 1944, the Bank made deliberate efforts to move away from commercial banking and become a central bank. In 1946, shortly after the end of Norman's tenure, the bank was nationalised by the Labour government.
After 1945 the Bank pursued the multiple goals of Keynesian economics, especially "easy money" and low interest rates to support aggregate demand. It tried to keep a fixed exchange rate, and attempted to deal with inflation and sterling weakness by credit and exchange controls.
In 1977, the Bank set up a wholly owned subsidiary called Bank of England Nominees Limited (BOEN), a private limited company, with two of its hundred £1 shares issued. According to its Memorandum & Articles of Association, its objectives are:- “To act as Nominee or agent or attorney either solely or jointly with others, for any person or persons, partnership, company, corporation, government, state, organisation, sovereign, province, authority, or public body, or any group or association of them...” Bank of England Nominees Limited was granted an exemption by Edmund Dell, Secretary of State for Trade, from the disclosure requirements under Section 27(9) of the Companies Act 1976, because, “it was considered undesirable that the disclosure requirements should apply to certain categories of shareholders.” The Bank of England is also protected by its Royal Charter status, and the Official Secrets Act. BOEN is a vehicle for governments and heads of state to invest in UK companies (subject to approval from the Secretary of State), providing they undertake "not to influence the affairs of the company". BOEN is no longer exempt from company law disclosure requirements. Although a dormant company, dormancy does not preclude a company actively operating as a nominee shareholder. BOEN has two shareholders: the Bank of England, and the Secretary of the Bank of England.
In 1981 the reserve requirement for banks to hold a minimum fixed proportion of their deposits as reserves at the Bank of England was abolished - see reserve requirement#United Kingdom for more details.
On 6 May 1997, following the 1997 general election which brought a Labour government to power for the first time since 1979, it was announced by the Chancellor of the Exchequer, Gordon Brown, that the Bank would be granted operational independence over monetary policy. Under the terms of the Bank of England Act 1998 (which came into force on 1 June 1998), the Bank's Monetary Policy Committee was given sole responsibility for setting interest rates to meet the Government's Retail Prices Index (RPI) inflation target of 2.5%. The target has changed to 2% since the Consumer Price Index (CPI) replaced the Retail Prices Index as the treasury's inflation index. If inflation overshoots or undershoots the target by more than 1%, the Governor has to write a letter to the Chancellor of the Exchequer explaining why, and how he will remedy the situation.
The handing over of monetary policy to the Bank had been a key plank of the Liberal Democrats' economic policy since the 1992 general election. Conservative MP Nicholas Budgen had also proposed this as a private member's bill in 1996, but the bill failed as it had the support of neither the government nor the opposition.
Functions of the Bank.
The Bank performs all the functions of a central bank. The most important of these is supposed to be maintaining price stability and supporting the economic policies of the Government, thus promoting economic growth. There are two main areas which are tackled by the Bank to ensure it carries out these functions efficiently:
Financial stability.
The Bank works together with other institutions to secure both monetary and financial stability, including:
The 1997 Memorandum of Understanding describes the terms under which the Bank, the Treasury and the FSA work toward the common aim of increased financial stability. In 2010 the incoming Chancellor announced his intention to merge the FSA back into the Bank. As of 2012, the current director for financial stability is Andy Haldane.
The Bank acts as the government's banker, and it maintains the government's Consolidated Fund account. It also manages the country's foreign exchange and gold reserves. The Bank also acts as the bankers' bank, especially in its capacity as a lender of last resort.
The Bank has a monopoly on the issue of banknotes in England and Wales. Scottish and Northern Irish banks retain the right to issue their own banknotes, but they must be backed one to one with deposits in the Bank, excepting a few million pounds representing the value of notes they had in circulation in 1845. The Bank decided to sell its bank note printing operations to De La Rue in December 2002, under the advice of Close Brothers Corporate Finance Ltd.
Since 1998, the Monetary Policy Committee (MPC) has had the responsibility for setting the official interest rate. However, with the decision to grant the Bank operational independence, responsibility for government debt management was transferred to the new UK Debt Management Office in 1998, which also took over government cash management in 2000. Computershare took over as the registrar for UK Government bonds (gilt-edged securities or "gilts") from the Bank at the end of 2004.
The Bank used to be responsible for the regulation and supervision of the banking and insurance industries, although this responsibility was transferred to the Financial Services Authority in June 1998. After the financial crises in 2008 new banking legislation transferred the responsibility for regulation and supervision of the banking and insurance industries back to the Bank.
In 2011 the interim Financial Policy Committee (FPC) was created as a mirror committee to the MPC to spearhead the Bank's new mandate on financial stability. The FPC is responsible for macro prudential regulation of all UK banks and insurance companies.
In order to help maintain economic stability, the Bank attempts to broaden understanding of its role, both through regular speeches and publications by senior Bank figures, a semiannual Financial Stability Report, and through a wider education strategy aimed at the general public. It maintains a free museum and runs the Target Two Point Zero competition for A-level students.
Asset purchase facility.
The Bank has operated, since January 2009, an Asset Purchase Facility (APF) to buy "high-quality assets financed by the issue of Treasury bills and the DMO's cash management operations" and thereby improve liquidity in the credit markets. It has, since March 2009, also provided the mechanism by which the Bank's policy of quantitative easing (QE) is achieved, under the auspices of the MPC. Along with the managing the £200 billion of QE funds, the APF continues to operate its corporate facilities. Both are undertaken by a subsidiary company of the Bank of England, the Bank of England Asset Purchase Facility Fund Limited (BEAPFF).
Banknote issues.
The Bank has issued banknotes since 1694. Notes were originally hand-written; although they were partially printed from 1725 onwards, cashiers still had to sign each note and make them payable to someone. Notes were fully printed from 1855. Until 1928 all notes were "White Notes", printed in black and with a blank reverse. In the 18th and 19th centuries White Notes were issued in £1 and £2 denominations. During the 20th century White Notes were issued in denominations between £5 and £1000.
Until the mid-nineteenth century, commercial banks were able to issue their own banknotes, and notes issued by provincial banking companies were commonly in circulation. The Bank Charter Act 1844 began the process of restricting note issue to the Bank; new banks were prohibited from issuing their own banknotes and existing note-issuing banks were not permitted to expand their issue. As provincial banking companies merged to form larger banks, they lost their right to issue notes, and the English private banknote eventually disappeared, leaving the Bank with a monopoly of note issue in England and Wales. The last private bank to issue its own banknotes in England and Wales was Fox, Fowler and Company in 1921. However, the limitations of the 1844 Act only affected banks in England and Wales, and today three commercial banks in Scotland and four in Northern Ireland continue to issue their own banknotes, regulated by the Bank.
At the start of the First World War, the Currency and Bank Notes Act 1914 was passed, which granted temporary powers to HM Treasury for issuing banknotes to the value of £1 and 10/- (ten shillings). Treasury notes had full legal tender status and were not convertible for gold through the Bank, replacing the gold coin in circulation to prevent a run on sterling and to enable raw material purchases for armament production. These notes featured an image of King George V (Bank of England notes did not begin to display an image of the monarch until 1960). The wording on each note was:
Treasury notes were issued until 1928, when the Currency and Bank Notes Act 1928 returned note-issuing powers to the banks. The Bank of England issued notes for ten shillings and one pound for the first time on 22 November 1928.
During the Second World War the German Operation Bernhard attempted to counterfeit denominations between £5 and £50 producing 500,000 notes each month in 1943. The original plan was to parachute the money on the UK in an attempt to destabilise the British economy, but it was found more useful to use the notes to pay German agents operating throughout Europe – although most fell into Allied hands at the end of the war, forgeries frequently appeared for years afterwards, which led banknote denominations above £5 to be removed from circulation.
In 2006, over £53 million in banknotes belonging to the bank was stolen from a depot in Tonbridge, Kent.
Modern banknotes are printed by contract with De La Rue Currency in Loughton, Essex.
The vault.
The Bank is custodian to the official gold reserves of the United Kingdom and many other countries. The vault, beneath the City of London, covers a floor space greater than that of the second-tallest building in the City, Tower 42, and needs keys that are three feet long to open. The Bank is the 15th-largest custodian of gold reserves, holding around 4600 tonnes. These gold deposits were estimated in February 2012 to have a current market value of £156,000,000,000.

</doc>
<doc id="4485" url="http://en.wikipedia.org/wiki?curid=4485" title="Bakelite">
Bakelite

Bakelite ( ), or polyoxybenzylmethylenglycolanhydride, is an early plastic. It is a thermosetting phenol formaldehyde resin, formed from an elimination reaction of phenol with formaldehyde. It was developed by Belgian-born chemist Leo Baekeland in New York in 1907.
One of the first plastics made from synthetic components, Bakelite was used for its electrical nonconductivity and heat-resistant properties in electrical insulators, radio and telephone casings, and such diverse products as kitchenware, jewelry, pipe stems, firearms, and children's toys. Bakelite was designated a National Historic Chemical Landmark in 1993 by the American Chemical Society in recognition of its significance as the world's first synthetic plastic. The "retro" appeal of old Bakelite products has made them collectible.
History.
Dr. Baekeland had originally set out to find a replacement for shellac, made from the excretion of lac bugs. Chemists had begun to recognize that many natural resins and fibres were polymers, and Baekeland investigated the reactions of phenol and formaldehyde. He first produced a soluble phenol-formaldehyde shellac called "Novolak" that never became a market success, then turned to developing a binder for asbestos which, at that time, was molded with rubber. By controlling the pressure and temperature applied to phenol and formaldehyde, he produced a hard moldable material and patented in 1907 known as Bakelite. It was the first synthetic thermosetting plastic ever made. It was often referred to as "the material of 1000 uses", a phrase originated by Baekeland himself.
He announced the invention at a meeting of the American Chemical Society on February 5, 1909.
The Bakelite Corporation was formed in 1922 (after patent litigation favorable to Baekeland) from a merger of three companies: the General Bakelite Company, which Baekeland founded in 1910, the Condensite Company, founded by J.W. Aylesworth, and the Redmanol Chemical Products Company, founded by L.V. Redman. A factory was built near Bound Brook, New Jersey, in 1929.
Bakelite Limited, a merger of three British phenol formaldehyde resin suppliers (Damard Lacquer Company Limited of Birmingham, Mouldensite Limited of Darley Dale and Redmanol Chemical Products Company of London) was formed in 1926. A new factory opened in Tyseley, Birmingham, England around 1928. It was demolished in 1998.
In 1939 the companies were acquired by Union Carbide and Carbon Corporation. Union Carbide's phenolic resin business including the Bakelite and Bakelit registered trademarks are owned by Momentive Specialty Chemicals.
Properties.
Phenolics are more rarely used in general consumer products today, due to the cost and complexity of production and their brittle nature. Nevertheless they are still used in some applications where their specific properties are required, such as small precision-shaped components, molded disc brake cylinders, saucepan handles, electrical plugs and switches and parts for electrical irons. Today, Bakelite is manufactured and produced in sheet, rod and tube form for hundreds of industrial applications in the electronics, power generation and aerospace industries, and under a variety of commercial brand names. 
Phenolic sheet is a hard, dense material made by applying heat and pressure to layers of paper or glass cloth impregnated with synthetic resin. These layers of laminations are usually of cellulose paper, cotton fabrics, synthetic yarn fabrics, glass fabrics or unwoven fabrics. When heat and pressure are applied to the layers, a chemical reaction (polymerization) transforms the layers into a high-pressure thermosetting industrial laminated plastic. When rubbed or burnt, Bakelite has a distinctive, acrid, sickly-sweet fishy odor.
Bakelite phenolic sheet is produced in dozens of commercial grades and with various additives to meet diverse mechanical, electrical and thermal requirements. Some common types include:
Note that phenolic resin products are apt to swell slightly if they are used in areas which are perpetually damp. Varnishing the product helps to prevent this.
Synthesis.
Bakelite was a combination of phenol, formaldehyde, and wood flour. The mixture is put under pressure, and after curing, a hard plastic material forms.
Applications and usage.
In its industrial applications, Bakelite was particularly suitable for the emerging electrical and automobile industries because of its extraordinarily high resistance - not only to electricity, but to heat and chemical action. It was soon used for all nonconducting parts of radios and other electrical devices, such as bases and sockets for light bulbs and vacuum tubes, supports for electrical components, automobile distributor caps and other insulators.
Bakelite is used today for wire insulation, brake pads and related automotive components, and industrial electrical-related applications.
In the early 20th century, it was found in myriad applications including saxophone mouthpieces, whistles, cameras, solid-body electric guitars, telephone housings and handsets, early machine guns, pistol grips, and appliance casings. In the pure form it was made into such articles as pipe stems, buttons, etc.
The thermosetting phenolic resin was at one point considered for the manufacture of coins, due to a shortage of traditional material; in 1943, Bakelite and other non-metal materials were tested for usage for the one cent coin in the US before the Mint settled on zinc-coated steel.
After the Second World War, factories were retrofitted to produce Bakelite using a more efficient extrusion process which increased production and enabled the uses of Bakelite to extend into other genres: jewelry boxes, desk sets, clocks, radios, game pieces like chessmen, poker chips, billiard balls and Mah Jong sets. Kitchenware such as canisters and tableware were also made of Bakelite through the 1950s. Beads, bangles and earrings were produced by the Catalin Company which introduced 15 new colors in 1927. The creation of marbled Bakelite was also attributed to the Catalin Company. Translucent Bakelite jewelry, poker chips and other gaming items such as chess sets were also introduced in the 1940s under the Prystal Corporation name; however, its basic chemical composition remained the same.
The primary commercial uses for Bakelite today remain in the area of inexpensive board and tabletop games produced in China, India and Hong Kong. Items such as billiard balls, dominoes and pieces for games like chess, checkers, and backgammon are constructed of Bakelite for its look, durability, fine polish, weight, and sound. Common dice are sometimes made of Bakelite for weight and sound, but the majority are made of a thermoplastic polymer such as acrylonitrile butadiene styrene (ABS).
Bakelite is used to make the presentation boxes of Breitling watches and sometimes as a substitute for metal firearm magazines. Bakelite is also used in the mounting of metal samples in metallography.
Phenolic resins have been commonly used in ablative heat shields. Soviet heatshields for ICBM warheads and spacecraft reentry consisted of asbestos textolite, impregnated with Bakelite.
Patents.
The United States Patent and Trademark Office granted Baekeland a patent for a "Method of making insoluble products of phenol and formaldehyde" on December 7, 1909. Producing hard, compact, insoluble and infusable condensation products of phenols and formaldehyde marked the beginning of the modern plastics industry.

</doc>
<doc id="4487" url="http://en.wikipedia.org/wiki?curid=4487" title="Bean">
Bean

Bean is a common name for large plant seeds used for human food or animal feed of several genera of the family Fabaceae (alternately Leguminosae).
Terminology.
The term "bean" originally referred to the seed of the broad or fava bean, but was later expanded to include members of the New World genus "Phaseolus", such as the common bean and the runner bean, and the related genus "Vigna". The term is now applied generally to many other related plants such as Old World soybeans, peas, chickpeas (garbanzos), vetches, and lupins.
"Bean" is sometimes used as a synonym of "pulse", an edible legume, though the term "pulses" is normally reserved for leguminous crops harvested for their dry grain. The term "bean" usually excludes crops used mainly for oil extraction (such as soybeans and peanuts), as well as those used exclusively for sowing purposes (such as clover and alfalfa). Leguminous crops harvested green for food, such as snap peas, snow peas, and so on, are not considered beans, and are classified as vegetable crops. According to United Nation's Food and Agriculture Organization the term "bean" should include only species of "Phaseolus"; however, a strict consensus definition has proven difficult because in the past, several species such as "Vigna" "angularis" (azuki bean), "mungo" (black gram), "radiata" (green gram), "aconitifolia" (moth bean)) were classified as "Phaseolus" and later reclassified. The use of the term "bean" to refer to species other than "Phaseolus" thus remains. In some countries, the term "bean" can mean a host of different species.
In English usage, the word "bean" is also sometimes used to refer to the seeds or pods of plants that are not in the family leguminosae, but which bear a superficial resemblance to true beans—for example coffee beans, castor beans and cocoa beans (which resemble bean seeds), and vanilla beans, which superficially resemble bean pods.
Cultivation.
Unlike the closely related pea, beans are a summer crop that needs warm temperatures to grow. Maturity is typically 55–60 days from planting to harvest. As the bean pods mature, they turn yellow and dry up, and the beans inside change from green to their mature colour. As a vine, bean plants need external support, which may be provided in the form of special "bean cages" or poles. Native Americans customarily grew them along with corn and squash (the so-called Three Sisters), with the tall cornstalks acting as support for the beans.
In more recent times, the so-called "bush bean" has been developed which does not require support and has all its pods develop simultaneously (as opposed to pole beans which develop gradually). This makes the bush bean more practical for commercial production.
History.
Beans are one of the longest-cultivated plants. Broad beans, also called fava beans, in their wild state the size of a small fingernail, were gathered in Afghanistan and the Himalayan foothills. In a form improved from naturally occurring types, they were grown in Thailand since the early seventh millennium BCE, predating ceramics. They were deposited with the dead in ancient Egypt. Not until the second millennium BCE did cultivated, large-seeded broad beans appear in the Aegean, Iberia and transalpine Europe. In the "Iliad" (late-8th century) is a passing mention of beans and chickpeas cast on the threshing floor.
Beans were an important source of protein throughout Old and New World history, and still are today.
The oldest-known domesticated beans in the Americas were found in Guitarrero Cave, an archaeological site in Peru, and dated to around the second millennium BCE.
Most of the kinds commonly eaten fresh or dried, those of the genus "Phaseolus", come originally from the Americas, being first seen by a European when Christopher Columbus, during his exploration of what may have been the Bahamas, found them growing in fields. Five kinds of "Phaseolus" beans were domesticated by pre-Columbian peoples: common beans ("Phaseolus vulgaris") grown from Chile to the northern part of what is now the United States, and lima and sieva beans ("Phaseolus lunatus"), as well as the less widely distributed teparies ("Phaseolus acutifolius"), scarlet runner beans ("Phaseolus coccineus") and polyanthus beans ("Phaseolus polyanthus") One especially famous use of beans by pre-Columbian people as far north as the Atlantic seaboard is the "Three Sisters" method of companion plant cultivation:
Dry beans come from both Old World varieties of broad beans (fava beans) and New World varieties (kidney, black, cranberry, pinto, navy/haricot).
Beans are a heliotropic plant, meaning that the leaves tilt throughout the day to face the sun. At night, they go into a folded "sleep" position.
Types.
Currently, the world genebanks hold about 40,000 bean varieties, although only a fraction are mass-produced for regular consumption.
Some bean types include:
Toxins.
Some kinds of raw beans, especially red and kidney beans, contain a harmful toxin (lectin phytohaemagglutinin) that must be removed by cooking. A recommended method is to boil the beans for at least ten minutes; undercooked beans may be more toxic than raw beans.
Cooking beans in a slow cooker, because of the lower temperatures often used, may not destroy toxins even though the beans do not smell or taste 'bad' (though this should not be a problem if the food reaches boiling temperature and stays there for some time).
Fermentation is used in some parts of Africa to improve the nutritional value of beans by removing toxins. Inexpensive fermentation improves the nutritional impact of flour from dry beans and improves digestibility, according to research co-authored by Emire Shimelis, from the Food Engineering Program at Addis Ababa University. Beans are a major source of dietary protein in Kenya, Malawi, Tanzania, Uganda and Zambia.
Nutrition.
Beans have significant amounts of fiber and soluble fiber, with one cup of cooked beans providing between nine and 13 grams of fiber. Soluble fiber can help lower blood cholesterol. Beans are also high in protein, complex carbohydrates, folate, and iron.
Flatulence.
Many edible beans, including broad beans and soybeans, contain oligosaccharides (particularly raffinose and stachyose), a type of sugar molecule also found in cabbage. An anti-oligosaccharide enzyme is necessary to properly digest these sugar molecules. As a normal human digestive tract does not contain any anti-oligosaccharide enzymes, consumed oligosaccharides are typically digested by bacteria in the large intestine. This digestion process produces flatulence-causing gases as a byproduct. This aspect of bean digestion is the basis for the children's rhyme "Beans, Beans, the Musical Fruit".
Some species of mold produce alpha-galactosidase, an anti-oligosaccharide enzyme, which humans can take to facilitate digestion of oligosaccharides in the small intestine. This enzyme, currently sold in the United States under the brand-names Beano and Gas-X Prevention, can be added to food or consumed separately. In many cuisines beans are cooked along with natural carminatives such as anise seeds, coriander seeds and cumin .
One effective strategy is to soak beans in alkaline (baking soda) water overnight before rinsing thoroughly . Sometimes vinegar is added, but only after the beans are cooked as vinegar interferes with the beans' softening.
Fermented beans will usually not produce most of the intestinal problems that unfermented beans will, since yeast can consume the offending sugars.
Production.
The world leader in production of dry bean is India, followed by Brazil and Myanmar. In Africa, the most important producer is Tanzania.

</doc>
<doc id="4489" url="http://en.wikipedia.org/wiki?curid=4489" title="Breast">
Breast

The breast is the upper ventral region of the torso of a primate, in left and right sides, containing the mammary gland which in a female can secrete milk used to feed infants.
Both men and women develop breasts from the same embryological tissues. However, at puberty, female sex hormones, mainly estrogen, promote breast development which does not occur in men due to the higher amount of testosterone. As a result, women's breasts become far more prominent than those of men.
During pregnancy, the breast is responsive to a complex interplay of hormones that cause tissue development and enlargement in order to produce milk. Three such hormones are estrogen, progesterone and prolactin, which cause glandular tissue in the breast and the uterus to change during the menstrual cycle.
Each breast contains 15–20 lobes. The subcutaneous adipose tissue covering the lobes gives the breast its size and shape. Each lobe is composed of many lobules, at the end of which are sacs where milk is produced in response to hormonal signals.
Etymology.
The English word "breast" derives from the Old English word "brēost" (breast, bosom) from Proto-Germanic "breustam" (breast), from the Proto-Indo-European base bhreus– (to swell, to sprout). The "breast" spelling conforms to the Scottish and North English dialectal pronunciations.
Structure.
In women, the breasts overlay the pectoralis major muscles and usually extend from the level of the second rib to the level of the sixth rib in the front of the human rib cage; thus, the breasts cover much of the chest area and the chest walls. At the front of the chest, the breast tissue can extend from the clavicle (collarbone) to the middle of the sternum (breastbone). At the sides of the chest, the breast tissue can extend into the axilla (armpit), and can reach as far to the back as the latissimus dorsi muscle, extending from the lower back to the humerus bone (the longest bone of the upper arm). As a mammary gland, the breast is an composed of layers of different types of tissue, among which predominate two types, adipose tissue and glandular tissue, which effects the lactation functions of the breasts.
Morphologically, the breast is a cone with the base at the chest wall, and the apex at the nipple, the center of the NAC (nipple-areola complex). The superﬁcial tissue layer (superficial fascia) is separated from the skin by 0.5–2.5 cm of subcutaneous fat (adipose tissue). The suspensory Cooper's ligaments are fibrous-tissue prolongations that radiate from the superficial fascia to the skin envelope. The adult breast contains 14–18 irregular lactiferous lobes that converge to the nipple, to ducts 2.0–4.5 mm in diameter; the milk ducts (lactiferous ducts) are immediately surrounded with dense connective tissue that functions as a support framework. The glandular tissue of the breast is biochemically supported with estrogen; thus, when a woman reaches menopause (cessation of menstruation) and her body estrogen levels decrease, the milk gland tissue then atrophies, withers, and disappears, resulting in a breast composed of adipose tissue, superﬁcial fascia, suspensory ligaments, and the skin envelope.
The dimensions and weight of the breast vary among women, ranging from approximately 500 to 1,000 grams (1.1 to 2.2 pounds) each; thus, a small-to-medium-sized breast weighs 500 grams (1.1 pounds) or less; and a large breast weighs approximately 750 to 1,000 grams (1.7 to 2.2 pounds.) The tissue composition ratios of the breast likewise vary among women; some breasts have greater proportions of glandular tissue than of adipose or connective tissues, and vice versa; therefore the fat-to-connective-tissue ratio determines the density (firmness) of the breast. In the course of a woman's life, her breasts will change size, shape, and weight, because of the hormonal bodily changes occurred in thelarche (pubertal breast development), menstruation (fertility), pregnancy (reproduction), the breast-feeding of an infant child, and the climacterium (the end of fertility).
Glandular structure.
The breast is an apocrine gland that produces milk to feed an infant child; for which the nipple of the breast is centred in (surrounded by) an areola (nipple-areola complex, NAC), the skin color of which varies from pink to dark brown, and has many sebaceous glands. The basic units of the breast are the terminal duct lobular units (TDLUs), which produce the fatty breast milk. They give the breast its offspring-feeding functions as a mammary gland. They are distributed throughout the body of the breast; approximately two-thirds of the lactiferous tissue is within 30 mm of the base of the nipple. The terminal lactiferous ducts drain the milk from TDLUs into 4–18 lactiferous ducts, which drain to the nipple; the milk-glands-to-fat ratio is 2:1 in a lactating woman, and 1:1 in a non-lactating woman. In addition to the milk glands, the breast also is composed of connective tissues (collagen, elastin), white fat, and the suspensory Cooper's ligaments. Sensation in the breast is provided by the peripheral nervous system innervation, by means of the front (anterior) and side (lateral) cutaneous branches of the fourth-, the fifth-, and the sixth intercostal nerves, while the T-4 nerve (Thoracic spinal nerve 4), which innervates the dermatomic area, supplies sensation to the nipple-areola complex.
Lymphatic drainage.
Approximately 75% of the lymph from the breast travels to the axillary lymph nodes on the same side of the body, whilst 25% of the lymph travels to the parasternal nodes (beside the sternum bone). A small amount of remaining lymph travels to the other breast, and to the abdominal lymph nodes. The axillary lymph nodes include the pectoral (chest), subscapular (under the scapula), and humeral (humerus-bone area) lymph-node groups, which drain to the central axillary lymph nodes and to the apical axillary lymph nodes. The lymphatic drainage of the breasts is especially relevant to oncology, because breast cancer is a cancer common to the mammary gland, and cancer cells can metastasize (break away) from a tumour and be dispersed to other parts of the woman's body by means of the lymphatic system.
Shape and support.
The morphologic variations in the size, shape, volume, tissue density, pectoral locale, and spacing of the breasts determine their natural shape, appearance, and configuration upon the chest of a woman; yet such features do not indicate its mammary-gland composition (fat-to-milk-gland ratio), nor the potential for nursing an infant child. The size and the shape of the breasts are influenced by normal-life hormonal changes (thelarche, menstruation, pregnancy, menopause) and medical conditions (e.g. virginal breast hypertrophy). The shape of the breasts is naturally determined by the support of the suspensory Cooper's ligaments, the underlying muscle and bone structures of the chest, and the skin envelope. The suspensory ligaments sustain the breast from the clavicle (collarbone) and the clavico-pectoral fascia (collarbone and chest), by traversing and encompassing the fat and milk-gland tissues, the breast is positioned, affixed to, and supported upon the chest wall, while its shape is established and maintained by the skin envelope.
The base of each breast is attached to the chest by the deep fascia over the pectoralis major muscles. The space between the breast and the pectoralis major muscle is called retromammary space and gives mobility to the breast. Some breasts are mounted high upon the chest wall, are of rounded shape, and project almost horizontally from the chest, which features are common to girls and women in the early stages of thelarchic development, the sprouting of the breasts. In the high-breast configuration, the dome-shaped and the cone-shaped breast is affixed to the chest at the base, and the weight is evenly distributed over the base area. In the low-breast configuration, a proportion of the breast weight is supported by the chest, against which rests the lower surface of the breast, thus is formed the inframammary fold (IMF). Because the base is deeply affixed to the chest, the weight of the breast is distributed over a greater area, and so reduces the weight-bearing strain upon the chest, shoulder, and back muscles that bear the weight of the bust.
The chest (thoracic cavity) progressively slopes outwards from the thoracic inlet (atop the breastbone) and above to the lowest ribs that support the breasts. The inframammary fold, where the lower portion of the breast meets the chest, is an anatomic feature created by the adherence of the breast skin and the underlying connective tissues of the chest; the IMF is the lower-most extent of the anatomic breast. In the course of thelarche, some girls develop breasts the lower skin-envelope of which touches the chest below the IMF, and some girls do not; both breast anatomies are statistically normal morphologic variations of the size and shape of women's breasts.
Size.
Breast size varies with race and ethnic origin. A study released in 2013 suggests the existence of a single genetic mutation responsible for multiple characteristics of East Asians, including thicker hair, more sweat glands and smaller breasts on women.
Development.
The basic morphological structure of the human breast is the same in both male and female children. For a girl in puberty, during thelarche (the breast-development stage), the female sex hormones (principally estrogens) promote the sprouting, growth, and development of the breasts, in the course of which, as mammary glands, they grow in size and volume, and usually rest on her chest; these development stages of secondary sex characteristics (breasts, pubic hair, etc.) are illustrated in the five-stage Tanner Scale. During thelarche, the developing breasts sometimes are of unequal size, and usually the left breast is slightly larger; said condition of asymmetry is transitory and statistically normal to female physical and sexual development. Moreover, breast development sometimes is abnormal, manifested either as overdevelopment (e.g. virginal breast hypertrophy) or as underdevelopment (e.g. tuberous breast deformity) in girls and women; and manifested in boys and men as gynecomastia (woman's breasts), the consequence of a biochemical imbalance between the normal levels of the estrogen and testosterone hormones of the male body.
Asymmetry.
Approximately two years after the onset of puberty (a girl's first menstrual cycle), the hormone estrogen stimulates the development and growth of the glandular, fat, and suspensory tissues that compose the breast. This continues for approximately four years until establishing the final shape of the breast (size, volume, density) when she is a woman of approximately 21 years of age.
About 90% of women's breasts are asymmetrical to some degree, either in size, volume, or relative position upon the chest. Asymmetry can be manifested in the size of the breast, the position of the nipple-areola complex (NAC), the angle of the breast, and the position of the inframammary fold, where the breast meets the chest.
For about 5% to 10% women, their breasts are severely different, with the left breast being larger in 62% of cases. This is due to the breast proximity to the heart and a greater number of arteries and veins, along with a protective layer of fat surrounding the heart located beneath it. Up to 25% of women experience notable breast asymmetry of at least one cup size difference.
If a woman is uncomfortable with her breasts' asymmetry, she can minimize the difference with a corrective bra or use gel bra inserts. Alternatively, she can seek a surgical solution. Options include a minimally invasive procedure known as platelet injection fat transfer, which transfers fat cells from a woman's thighs to her smaller breast. More invasive procedures include corrective mammoplasty, such as mastopexy, breast reduction plasty, or breast augmentation, depending on the nature of the asymmetry. Most surgeons will only perform an augmentation procedure to treat asymmetry if the woman's breasts differ by at least one cup size.
Hormonal change.
Because the breasts are principally composed of adipose tissue, which surrounds the milk glands, their sizes and volumes fluctuate according to the hormonal changes particular to thelarche (sprouting of breasts), menstruation (egg production), pregnancy (reproduction), lactation (feeding of offspring), and menopause (end of menstruation). For example, during the menstrual cycle, the breasts are enlarged by premenstrual water retention; during pregnancy the breasts become enlarged and denser (firmer) because of the prolactin-caused organ hypertrophy, which begins the production of breast milk, increases the size of the nipples, and darkens the skin color of the nipple-areola complex; these changes continue during the lactation and the breastfeeding periods. Afterwards, the breasts generally revert to their pre-pregnancy size, shape, and volume, yet might present stretch marks and breast ptosis. At menopause, the breasts can decrease in size when the levels of circulating estrogen decline, followed by the withering of the adipose tissue and the milk glands. Additional to such natural biochemical stimuli, the breasts can become enlarged consequent to an adverse side effect of combined oral contraceptive pills; and the size of the breasts can also increase and decrease in response to the body weight fluctuations of the woman. Moreover, the physical changes occurred to the breasts often are recorded in the stretch marks of the skin envelope; they can serve as historical indicators of the increments and the decrements of the size and the volume of a woman's breasts throughout the course of her life.
Breast ptosis.
Ptosis is a normal consequence of aging where the breast tissue sags lower on the chest and the nipple points downward. The rate at which a woman develops ptosis depends on many factors including genetics, smoking, body mass index, number of pregnancies, the size of breasts before pregnancy, and age.
Plastic surgeons categorize ptosis by evaluating the position of the nipple relative to the inframammary crease (where the underside of the breast meets the chest wall). This is determined by measuring from the center of the nipple to the sternal notch (at the top of the breast bone) to gauge how far the nipple has fallen. The standard anthropometric measurement for young women is . This measurement is used to assess both breast ptosis and breast symmetry. The surgeon will assess the breast's angle of projection. The apex of the breast, which includes the nipple, can have a flat angle of projection (180 degrees) or acute angle of projection (greater than 180 degrees). The apex rarely has an angle greater than 60 degrees. The angle of the breast apex is partly determined by the tautness of the suspensory Cooper's ligaments. For example, when a woman lies on her back, the angle of the breast apex becomes a flat, obtuse angle (less than 180 degrees) while the base-to-length ratio of the breast ranges from 0.5 to 1.0.
Functions and health.
Lactation.
The primary function of the breasts – as mammary glands – is the feeding and the nourishing of an infant child with breast milk during the maternal lactation period. The round shape of the breast helps to limit the loss of maternal body heat, because milk production depends upon a higher-temperature environment for the proper, milk-production function of the mammary gland tissues, the lactiferous ducts. Regarding the shape of the breast, the study "The Evolution of the Human Beast" (2001) proposed that the rounded shape of a woman's breast evolved to prevent the sucking infant offspring from suffocating while feeding at the teat; that is, because of the human infant's small jaw, which did not project from the face to reach the nipple, he or she might block the nostrils against the mother's breast if it were of a flatter form (cf. chimpanzee); theoretically, as the human jaw receded into the face, the woman's body compensated with round breasts.
In a woman, the condition of lactation unrelated to pregnancy can occur as galactorrhea (spontaneous milk flow), and because of the adverse effects of drugs (e.g. antipsychotic medications), of extreme physical stress, and of endocrine disorders. In a newborn infant, the capability of lactation is consequence of the mother's circulating hormones (prolactin, oxytocin, etc.) in his or her blood stream, which were introduced by the shared circulatory system of the placenta. In men, the mammary glands are also present in the body, but normally remain undeveloped because of the hormone testosterone, however, when male lactation occurs, it is considered a pathological symptom of a disorder of the pituitary gland.
Reproduction.
In considering the human animal, zoologists proposed that the human female is the only primate that possesses permanent, full-form breasts when not pregnant. Other mammalian females develop full breasts only when pregnant.
Clinical significance.
The breast is susceptible to numerous benign and malignant conditions. The most frequent benign conditions are puerperal mastitis, fibrocystic breast changes and mastalgia. Breast cancer is one of the leading causes of death among women.
Society and culture.
Anthropomorphic geography.
There are many mountains named after the breast because they resemble it in appearance and so are objects of religious and ancestral veneration as a fertility symbol and of well-being. In Asia, there was "Breast Mountain," which had a cave where the Buddhist monk Bodhidharma (Da Mo) spent much time in meditation. Other such breast mountains are Mount Elgon on the Uganda-Kenya border, Beinn Chìochan and the Maiden Paps in Scotland, the "Bundok ng Susong Dalaga" (Maiden's breast mountains) in Talim Island, Philippines, the twin hills known as the Paps of Anu ("Dá Chích Anann" or the breasts of Anu), near Killarney in Ireland, the 2,086 m high "Tetica de Bacares" or "La Tetica" in the Sierra de Los Filabres, Spain, and Khao Nom Sao in Thailand, Cerro Las Tetas in Puerto Rico and the Breasts of Aphrodite in Mykonos, among many others. In the United States, the Teton Range is named after the French word for "breast."
Art history.
In European pre-historic societies, sculptures of female figures with pronounced or highly exaggerated breasts were common. A typical example is the so-called Venus of Willendorf, one of many Paleolithic Venus figurines with ample hips and bosom. Artifacts such as bowls, rock carvings and sacred statues with breasts have been recorded from 15,000 BC up to late antiquity all across Europe, North Africa and the Middle East. Many female deities representing love and fertility were associated with breasts and breast milk. Figures of the Phoenician goddess Astarte were represented as pillars studded with breasts. Isis, an Egyptian goddess who represented, among many other things, ideal motherhood, was often portrayed as suckling pharaohs, thereby confirming their divine status as rulers. Even certain male deities representing regeneration and fertility were occasionally depicted with breast-like appendices, such as the river god Hapy who was considered to be responsible for the annual overflowing of the Nile. Female breasts were also prominent in the Minoan civilization in the form of the famous Snake Goddess statuettes. In Ancient Greece there were several cults worshipping the "Kourotrophos", the suckling mother, represented by goddesses such as Gaia, Hera and Artemis. The worship of deities symbolized by the female breast in Greece became less common during the first millennium. The popular adoration of female goddesses decreased significantly during the rise of the Greek city states, a legacy which was passed on to the later Roman Empire.
During the middle of the first millennium BC, Greek culture experienced a gradual change in the perception of female breasts. Women in art were covered in clothing from the neck down, including female goddesses like Athena, the patron of Athens who represented heroic endeavor. There were exceptions: Aphrodite, the goddess of love, was more frequently portrayed fully nude, though in postures that were intended to portray shyness or modesty, a portrayal that has been compared to modern pin ups by historian Marilyn Yalom. Although nude men were depicted standing upright, most depictions of female nudity in Greek art occurred "usually with drapery near at hand and with a forward-bending, self-protecting posture". A popular legend at the time was of the Amazons, a tribe of fierce female warriors who socialized with men only for procreation and even removed one breast to become better warriors (the idea being that the right breast would interfere with the operation of a bow and arrow). The legend was a popular motif in art during Greek and Roman antiquity and served as an antithetical cautionary tale.
Body image.
Many women regard their breasts, which are female secondary sex characteristics, as important to their sexual attractiveness, as a sign of femininity that is important to their sense of self. So, when a woman considers her breasts deficient in some respect, she might choose to undergo a plastic surgery procedure to enhance them, either to have them augmented or to have them reduced, or to have them reconstructed if she suffered a deformative disease, such as breast cancer. After mastectomy, the reconstruction of the breast or breasts is done with breast implants or autologous tissue transfer, using fat and tissues from the abdomen, which is performed with a TRAM flap or with a back (latissiumus muscle flap). Breast reduction surgery is a procedure that involves removing excess breast tissue, fat, and skin, and the repositioning of the nipple-areola complex.
Cosmetic improvement procedures include breast lift (mastopexy), breast augmentation with implants, and combination procedures; the two types of available breast implants are models filled with silicone gel, and models filled with saline solution. These types of breast surgery can also repair inverted nipples by releasing milk duct tissues that have become tethered. Furthermore, in the case of the obese woman, a breast lift (mastopexy) procedure, with or without a breast volume reduction, can be part of an upper-body lift and contouring for the woman who has undergone massive body weight loss.
Surgery of the breast presents the health risk of interfering with the ability to breast-feed an infant child, and might include consequences such as altered sensation in the nipple-areola complex, interference with mammography (breast x-rays images) when there are breast implants present in the breasts. Regarding breast-feeding capability after breast reduction surgery, studies reported that women who underwent breast reduction can retain the ability to nurse an infant child, when compared to women in a control group who underwent breast surgery using a modern pedicle surgical technique. Plastic surgery organizations generally discourage elective cosmetic breast augmentation surgery for teen-aged girls, because, at that age, the volume of the breast tissues (milk glands and fat) can continue to grow as the girl matures to womanhood. Breast reduction surgery for teen-aged girls, however, is a matter handled according to the particulars of the case of hypoplasia. (see: breast hypertrophy)
Clothing.
Because breasts are mostly fatty tissue, their shape can within limits be molded by clothing, such as foundation garments. Bras are commonly worn by about 90% of Western women, and are often worn for support. The social norm in most Western cultures is to cover breasts in public, though the extent of coverage varies depending on the social context. Some religions ascribe a special status to the female breast, either in formal teachings or through symbolism. Islam forbids women from exposing their breasts in public.
Many cultures associate breasts with sexuality and tend to regard bare breasts as immodest or indecent. In some cultures, like the Himba in northern Namibia, bare-breasted women are normal, while a thigh is highly sexualised and not exposed in public. In a few Western countries female toplessness at a beach is acceptable, although it may not be acceptable in the town center. In some areas, exposing a woman's breasts applies only to the exposure of nipples.
In the United States, women who breast-feed in public can receive negative attention. There have been instances where women have been asked to leave public venues. In New York, the topfreedom equality movement helped to bring a case, "People v. Santorelli" (1992), to the New York State Court of Appeals. They ruled that New York's indecent exposure laws did not apply to a bare-breasted woman. Other (gender equality) efforts succeeded in most of Canada in the 1990s. Bare-breasted women are legal and culturally acceptable at public beaches in Australia and much of Europe.
Sexual characteristic.
In some cultures, breasts play a role in human sexual activity. Breasts and especially the nipples are among the various human erogenous zones. They are sensitive to the touch as they have many nerve endings; and it is common to press or massage them with hands or orally before or during sexual activity. Some women can achieve an orgasm from such activities. Research has suggested that the sensations are genital orgasms caused by nipple stimulation, and may also be directly linked to "the genital area of the brain". Sensation from the nipples travels to the same part of the brain as sensations from the vagina, clitoris and cervix. Nipple stimulation may trigger uterine contractions, which then produce a sensation in the genital area of the brain. In the ancient Indian work the "Kama Sutra", light scratching of the breasts with nails and biting with teeth are considered erotic. During sexual arousal, breast size increases, venous patterns across the breasts become more visible, and nipples harden. Compared to other primates, human breasts are proportionately large throughout adult females' lives. Some writers have suggested that they may have evolved as a visual signal of sexual maturity and fertility.
Many people regard the female human body, of which breasts are an important aspect, to be aesthetically pleasing, as well as erotic. Research conducted at the Victoria University of Wellington showed that breasts are often the first thing men look at, and for a longer time than other body parts. The writers of the study had initially speculated that the reason for this is due to endocrinology with larger breasts indicating higher levels of estrogen and a sign of greater fertility, but the researchers said that "Men may be looking more often at the breasts because they are simply aesthetically pleasing, regardless of the size."
Many people regard bare female breasts to be erotic, and they can elicit heightened sexual desires in men in many cultures. Some people show a sexual interest in female breasts distinct from that of the person, which may be regarded as a breast fetish. While U.S. culture prefers breasts that are youthful and upright, some cultures venerate women with drooping breasts, indicating mothering and the wisdom of experience.
Symbolism.
In Christian iconography, some works of art depict women with their breasts in their hands or on a platter, signifying that they died as a martyr by having their breasts severed; one example of this is Saint Agatha of Sicily.

</doc>
<doc id="4492" url="http://en.wikipedia.org/wiki?curid=4492" title="Baghdad">
Baghdad

Baghdad ( "", Iraqi pronunciation: ) is the capital of the Republic of Iraq, as well as the coterminous Baghdad Province. The population of Baghdad, as of 2011, is approximately 7,216,040, making it the largest city in Iraq, the second largest city in the Arab world (after Cairo, Egypt), and the second largest city in Western Asia (after Tehran, Iran). According to the government, which is preparing for a census, the population of the country has reached 35 million, with 9 million in the capital.
Located along the Tigris River, the city was founded in the 8th century and became the capital of the Abbasid Caliphate. Within a short time of its inception, Baghdad evolved into a significant cultural, commercial, and intellectual center for the Islamic world. This, in addition to housing several key academic institutions (e.g. House of Wisdom), garnered the city a worldwide reputation as the "Center of Learning". Throughout the High Middle Ages, Baghdad was considered to be the largest city in the world with an estimated population of 1,200,000 people. According to some archeologists it was the first city to reach a population over one million inhabitants. The city was largely destroyed at the hands of the Mongol Empire in 1258, resulting in a decline that would linger through many centuries due to frequent plagues and multiple successive empires. With the recognition of Iraq as an independent state (formerly the British Mandate of Mesopotamia) in 1938, Baghdad gradually regained some of its former prominence as a significant center of Arab culture.
In contemporary times, the city has often faced severe infrastructural damage, most recently due to the 2003 invasion of Iraq, and the subsequent Iraq War that lasted until December 2011. In recent years, the city has been frequently subjected to insurgency attacks. Though the nation continues to work toward rebuilding and reconciliation, as of 2012, Baghdad continues to be listed as one of the least hospitable places in the world to live, and was ranked by Mercer as the worst of 221 major cities as measured by quality-of-life.
City name.
The name Baghdad is pre-Islamic and its origins are under some dispute. The site where the city of Baghdad came to stand has been populated for millennia and by the 8th century AD several Aramaic Christian villages had developed there, one of which was called "Baghdad", the name which would come to be used for the Abbasid metropolis.
The name has been used as "Baghdadu" on Assyrian cuneiform and Babylonian records going back to at least 2000 BC. An inscription by Nebuchadnezzar (600 BC) describes how he rebuilt the old Babylonian town of "Bagh-dadu". There used to be another Babylonian settlement called Baghdad, in upper Mesopotamia, near the ancient city of Edessa. The name has not been attested outside of Mesopotamia.
Even though the name has been attested in pre-Persian times, a Persian origin has been accepted by most scholars. It has been proposed that the name is a Middle Persian compound of "Bag" "god" and "dād" "given", translating to "God-given" or "God's gift", from which comes Modern Persian "". This in turn can be traced to Old Persian. Another proposal is the Persian compound "bāğ" "garden" and "dād" "fair", translating to "The fair garden". However, a Persian explanation remains somewhat problematic, given that the name was used long before the Persians arrived in Mesopotamia.
When the Abbasid caliph, al-Mansur, founded a completely new city for his capital, he chose the name Madinat al-Salaam or "City of Peace". This was the official name on coins, weights, and other official usage, although the common people continued to use the old name. By the 11th century, "Baghdad" became almost the exclusive name for the world-renowned metropolis.
History.
Foundation.
After the fall of the Umayyads, the first Muslim dynasty, the victorious Abbasid rulers wanted their own capital. Choosing a site north of the Sassanid capital of Ctesiphon (and just north of where ancient Babylon and Seleucia once stood), on 30 July 762, the caliph Al-Mansur commissioned the construction of the city and it was built under the supervision of the Barmakids. Mansur believed that Baghdad was the perfect city to be the capital of the Islamic empire under the Abbasids. Mansur loved the site so much he is quoted saying, "This is indeed the city that I am to found, where I am to live, and where my descendants will reign afterward".
The city's growth was helped by its location, which gave it control over strategic and trading routes, along the Tigris. A reason why Baghdad provided an excellent location was the abundance of water and the dry climate. Water exists on both north and south ends of the city gates, allowing all households to have a plentiful supply, which was very uncommon during this time.
Baghdad eclipsed Ctesiphon, the capital of the Persian Empire, which was located some to the southeast. Today, all that remains of Ctesiphon is the shrine town of Salman Pak, just to the south of Greater Baghdad. Ctesiphon itself had replaced and absorbed Seleucia, the first capital of the Seleucid Empire. Seleucia had earlier replaced the city of Babylon.
In its early years, the city was known as a deliberate reminder of an expression in the Qur'an, when it refers to Paradise. It took four years to build (764-768). Mansur assembled engineers, surveyors, and art constructionists from around the world to come together and draw up plans for the city. Over 100,000 construction workers came to survey the plans; many were distributed salaries to start the building of the city. July was chosen as the starting time because two Astrologers, Naubakht Ahvazi and Mashallah, believed that the city should be built under the sign of the lion, Leo. Leo is associated with fire and symbolises productivity, pride, and expansion.
The bricks used to make the city were on all four sides. Abū Ḥanīfa was the counter of the bricks and he developed a canal, which brought water to the work site for the use of both human consumption and the manufacturing of the bricks. Marble was also used to make buildings throughout the city, and marble steps led down to the river's edge.
The basic framework of the city consists of two large semicircles about in diameter. The city was designed as a circle about in diameter, leading it to be known as the "Round City". The original design shows as single ring of residential and commercial structures along the inside of the city walls, but the final construction added another ring inside the first. Within the city there were many parks, gardens, villas, and promenades. In the center of the city lay the mosque, as well as headquarters for guards. The purpose or use of the remaining space in the center is unknown. The circular design of the city was a direct reflection of the traditional Persian Sasanian urban design. The Sasanian city of Gur in Fars, built 500 years before Baghdad, is nearly identical in its general circular design, radiating avenues, and the government buildings and temples at the centre of the city. This style of urban planning contrasted with Ancient Greek and Roman urban planning, in which cities are designed as squares or rectangles with streets intersecting each other at right angles.
The surrounding walls.
The four surrounding walls of Baghdad were named Kufa, Basra, Khurasan, and Damascus; named because their gates pointed in the directions of these destinations. The distance between these gates was a little less than . Each gate had double doors that were made of iron; the doors were so heavy it took several men to open and close them. The wall itself was about 44 m thick at the base and about 12 m thick at the top. Also, the wall was 30 m high, which included merlons, a solid part of an embattled parapet usually pierced by embrasures. This wall was surrounded by another wall with a thickness of 50 m. The second wall had towers and rounded merlons, which surrounded the towers. This outer wall was protected by solid glacis, which is made out of bricks and quicklime. Beyond the outer wall was a water-filled moat.
"Golden Gate Palace".
In the middle of Baghdad, in the central square was the Golden Gate Palace. The Palace was the residence of the caliph and his family. In the central part of the building was a green dome that was 39 m high. Surrounding the palace was an esplanade, a waterside building, in which only the caliph could come riding on horseback. In addition, the palace was near other mansions and officer's residences. Near the Gate of Syria a building served as the home for the guards. It was made of brick and marble. The palace governor lived in the latter part of the building and the commander of the guards in the front. In 813, after the death of caliph Al-Amin the palace was no longer used as the home for the caliph and his family.
The roundness points to the fact that it was based on Arabic script. The two designers who were hired by Al-Mansur to plan the city's design were Naubakht, a Zoroastrian who also determined that the date of the foundation of the city would be astrologically auspicious, and Mashallah, a Jew from Khorasan, Iran.
The Abbasids and the round city.
The Abbasid Caliphate was based on their being the descendants of the uncle of Muhammad and being part of the Quraysh tribe. They used Shi'a resentment, Khorasanian movement, and appeals to the ambitions and traditions of the newly conquered Persian aristocracy to overthrow the Umayyads.
The Abbasids sought to combine the hegemony of the Arab tribes with the imperial, court, ceremonial, and administrative structures of the Persians. The Abbasids considered themselves the inherittures and the need of Mansur to place the capital in a place that was representative of Arab-Islamic identity by building the House of Wisdom, where ancient texts were translated from their original language, such as Greek, to Arabic. Mansur is credited with the "Translation Movement" for this. Further, Baghdad is also near the ancient Sassanid imperial seat of Ctesiphon on the Tigris River.
A centre of learning (8th to 13th centuries).
Within a generation of its founding, Baghdad became a hub of learning and commerce. The House of Wisdom was an establishment dedicated to the translation of Greek, Middle Persian and Syriac works. Scholars headed to Baghdad from all over the Abbasid Caliphate, facilitating the introduction of Persian, Greek and Indian science into the Arabic and Islamic world at that time. Baghdad was likely the largest city in the world from shortly after its foundation until the 930s, when it was tied by Córdoba.
Several estimates suggest that the city contained over a million inhabitants at its peak. Many of the "One Thousand and One Nights" tales are set in Baghdad during this period.
The end of the Abbasids in Baghdad.
By the 10th century, the city's population was between 1.2 million and 2 million. Baghdad's early meteoric growth eventually slowed due to troubles within the Caliphate, including relocations of the capital to Samarra (during 808–819 and 836–892), the loss of the western and easternmost provinces, and periods of political domination by the Iranian Buwayhids (945–1055) and Seljuk Turks (1055–1135).
The Seljuks were a clan of the Oghuz Turks from the Central Asia that converted to the Sunni branch of Islam. In 1040, they destroyed the Ghaznavids, taking over their land and in 1055, Tughril Beg, the leader of the Seljuks, took over Baghdad. The Seljuks expelled the Buyid dynasty of Shiites that ruled for some time and took over power and control of Baghdad. They ruled as Sultans in the name of the Abbasid caliphs (they saw themselves as being part of the Abbasid regime). Tughril Beg saw himself as the protector of the Abbasid Caliphs.
In 1058, Baghdad was captured by the Fatimids under the Turkish general Abu'l-Ḥārith Arslān al-Basasiri, an adherent of the Ismailis along with the 'Uqaylid Quraysh. Not long before the arrival of the Saljuqs in Baghdad, al-Basasiri petitioned to the Fatimid Imam-Caliph al-Mustansir to support him in conquering Baghdad on the Ismaili Imam's behalf. It has recently come to light that the famed Fatimid "da'i", al-Mu'ayyad al-Shirazi, had a direct role in supporting al-Basasiri and helped the general to succeed in taking Mawṣil, Wāsit and Kufa. Soon after, by December 1058, a Shi'i "adhān" (call to prayer) was implemented in Baghdad and a "khutbah" (sermon) was delivered in the name of the Fatimid Imam-Caliph. Despite his Shi'i inclinations, Al-Basasiri received support from Sunnis and Shi'is alike, for whom opposition to the Saljuq power was a common factor.
On 10 February 1258, Baghdad was captured by the Mongols led by Hulegu, a grandson of Chingiz Khan (Genghis Khan), during the siege of Baghdad. Many quarters were ruined by fire, siege, or looting. The Mongols massacred most of the city's inhabitants, including the caliph Al-Musta'sim, and destroyed large sections of the city. The canals and dykes forming the city's irrigation system were also destroyed. The sack of Baghdad put an end to the Abbasid Caliphate, a blow from which the Islamic civilization never fully recovered.
At this point, Baghdad was ruled by the Il-Khanids, the Mongol rulers of Iran. In 1401, Baghdad was again sacked, by the Central Asian Turkic conqueror Timur ("Tamerlane"). When his forces took Baghdad, he spared almost no one, and ordered that each of his soldiers bring back two severed human heads. It became a provincial capital controlled by the Mongol Jalayirid (1400–1411), Turkic Kara Koyunlu (1411–1469), Turkic Ak Koyunlu (1469–1508), and the Iranian Safavid (1508–1534) dynasties.
Ottoman era (16th to 19th centuries).
In 1534, Baghdad was captured by the Ottoman Turks. Under the Ottomans, Baghdad fell into a period of decline, partially as a result of the enmity between its rulers and Iranian Safavids, which did not accept the Sunni control of the city. Between 1623 and 1638, it returned to Iranian rule before falling back into Ottoman hands.
Baghdad has suffered severely from visitations of the plague and cholera, and sometimes two-thirds of its population has been wiped out.
For a time, Baghdad had been the largest city in the Middle East. The city saw relative revival in the latter part of the 18th century under a Mamluk government. Direct Ottoman rule was reimposed by Ali Ridha Pasha in 1831. From 1851 to 1852 and from 1861 to 1867, Baghdad was governed, under the Ottoman Empire by Mehmed Namık Pasha. The Nuttall Encyclopedia reports the 1907 population of Baghdad as 185,000. Baghdad was also home to a substantial Jewish community, which comprised over a quarter of the city's population.
20th and 21st centuries.
Baghdad and southern Iraq remained under Ottoman rule until 1917, when captured by the British during World War I. From 1920, Baghdad became the capital of the British Mandate of Mesopotamia and, after 1932, Baghdad was the capital of the Kingdom of Iraq. Iraq was given formal independence in 1932 and increased autonomy in 1946. The city's population grew from an estimated 145,000 in 1900 to 580,000 in 1950.
On 1 April 1941, members of the "Golden Square" and Rashid Ali staged a coup in Baghdad. Rashid Ali installed a pro-German and pro-Italian government to replace the pro-British government of Regent Abdul Ilah. On 31 May, after the resulting Anglo-Iraqi War and after Rashid Ali and his government had fled, the Mayor of Baghdad surrendered to British and Commonwealth forces.
On 14 July 1958, members of the Iraqi Army, under Abd al-Karim Qasim, staged a coup to topple the Kingdom of Iraq. King Faisal II, former Prime Minister Nuri as-Said, former Regent Prince 'Abd al-Ilah, members of the royal family, and others were brutally killed during the coup. Many of the victim's bodies were then dragged through the streets of Baghdad.
During the 1970s, Baghdad experienced a period of prosperity and growth because of a sharp increase in the price of petroleum, Iraq's main export. New infrastructure including modern sewerage, water, and highway facilities were built during this period. The masterplans of the city (1967, 1973) were delivered by the Polish planning office Miastoprojekt-Kraków, mediated by Polservice. However, the Iran–Iraq War of the 1980s was a difficult time for the city, as money was diverted by Saddam Hussein to the army and thousands of residents were killed. Iran launched a number of missile attacks against Baghdad in retaliation for Saddam Hussein's continuous bombardments of Tehran's residential districts.
In 1991 and 2003, the Gulf War and the 2003 invasion of Iraq caused significant damage to Baghdad's transportation, power, and sanitary infrastructure as the US-led coalition forces launched massive aerial assaults in the city in the two wars.
Main sights.
Points of interest include the National Museum of Iraq whose priceless collection of artifacts was looted during the 2003 invasion, and the iconic Hands of Victory arches. Multiple Iraqi parties are in discussions as to whether the arches should remain as historical monuments or be dismantled. Thousands of ancient manuscripts in the National Library were destroyed under Saddam's command.
Baghdad Zoo.
The Baghdad Zoo was the largest zoo in the Middle East. Within eight days following the 2003 invasion, however, only 35 of the 650 animals in the facility survived. This was a result of theft of some animals for human food, and starvation of caged animals that had no food. South African Lawrence Anthony and some of the zoo keepers cared for the animals and fed the carnivores with donkeys they had bought locally. Eventually, L. Paul Bremer, Director of the Coalition Provisional Authority in Iraq from May 11, 2003 to June 28, 2004 ordered protection of the zoo and U.S. engineers helped to reopen the facility.
Kadhimiya mosque.
The Al-Kādhimiya Mosque is a shrine that is located in the Kādhimayn suburb of Baghdad, Iraq. It contains the tombs of the seventh Twelver Shīa Imām Musa al-Kadhim and the ninth Twelver Shīa Imām Muhammad at-Taqī al-Jawād. Many Shias travel to the mosque from far away places to commemorate.
Geography.
The city is located on a vast plain bisected by the River Tigris. The Tigris splits Baghdad in half, with the eastern half being called 'Risafa' and the Western half known as 'Karkh'. The land on which the city is built is almost entirely flat and low-lying, being of alluvial origin due to the periodic large floods which have occurred on the river.
Climate.
Baghdad has a subtropical arid climate (Köppen climate classification "BWh") and is, in terms of maximum temperatures, one of the hottest cities in the world. In the summer from June to August, the average maximum temperature is as high as accompanied by blazing sunshine: rainfall has in fact been recorded on fewer than half a dozen occasions at this time of year and has never exceeded . Temperatures exceeding in the shade are by no means unheard of, and even at night temperatures in summer are seldom below . Because the humidity is very low (usually under 10%) due to Baghdad's distance from the marshy Persian Gulf, dust storms from the deserts to the west are a normal occurrence during the summer.
Winters boast mild days and variable nights. From December to February, Baghdad has maximum temperatures averaging , though highs above are not unheard of. Morning temperatures can be chilly: the average January low is but lows below freezing only occur a couple of times per year.
Annual rainfall, almost entirely confined to the period from November to March, averages around , but has been as high as and as low as . On January 11, 2008, light snow fell across Baghdad for the first time in memory.
Administrative divisions.
The city of Baghdad has 89 official neighbourhoods within 9 districts. These official subdivisions of the city served as administrative centres for the delivery of municipal services but until 2003 had no political function. Beginning in April 2003, the U.S. controlled Coalition Provisional Authority (CPA) began the process of creating new functions for these. The process initially focused on the election of neighbourhood councils in the official neighbourhoods, elected by neighbourhood caucuses.
The CPA convened a series of meetings in each neighbourhood to explain local government, to describe the caucus election process and to encourage participants to spread the word and bring friends, relatives and neighbours to subsequent meetings. Each neighbourhood process ultimately ended with a final meeting where candidates for the new neighbourhood councils identified themselves and asked their neighbours to vote for them.
Once all 88 (later increased to 89) neighbourhood councils were in place, each neighbourhood council elected representatives from among their members to serve on one of the city's nine district councils. The number of neighbourhood representatives on a district council is based upon the neighbourhood's population. The next step was to have each of the nine district councils elect representatives from their membership to serve on the 37 member Baghdad City Council. This three tier system of local government connected the people of Baghdad to the central government through their representatives from the neighbourhood, through the district, and up to the city council.
The same process was used to provide representative councils for the other communities in Baghdad Province outside of the city itself. There, local councils were elected from 20 neighbourhoods (Nahia) and these councils elected representatives from their members to serve on six district councils (Qada). As within the city, the district councils then elected representatives from among their members to serve on the 35 member Baghdad Regional Council.
The first step in the establishment of the system of local government for Baghdad Province was the election of the Baghdad Provincial Council. As before, the representatives to the Provincial Council were elected by their peers from the lower councils in numbers proportional to the population of the districts they represent. The 41 member Provincial Council took office in February, 2004 and served until national elections held in January 2005, when a new Provincial Council was elected.
This system of 127 separate councils may seem overly cumbersome; however, Baghdad Province is home to approximately seven million people. At the lowest level, the neighbourhood councils, each council represents an average of 75,000 people.
The nine District Advisory Councils (DAC) are as follows:
The nine districts are subdivided into 89 smaller neighborhoods which may make up sectors of any of the districts above. The following is a "selection" (rather than a complete list) of these neighborhoods:
Economy.
Iraqi Airways, the national airline of Iraq, has its headquarters on the grounds of Baghdad International Airport in Baghdad. Al-Naser Airlines has its head office in Karrada, Baghdad.
Reconstruction efforts.
Most Iraqi reconstruction efforts have been devoted to the restoration and repair of badly damaged urban infrastructure. More visible efforts at reconstruction through private development, like architect and urban designer Hisham N. Ashkouri's Baghdad Renaissance Plan and the Sindbad Hotel Complex and Conference Center have also been made.
The Baghdad Eye, a tall Ferris wheel, was proposed for Baghdad in August 2008. At that time, three possible locations had been identified, but no estimates of cost or completion date were given. In October 2008, it was reported that Al-Zawraa Park was expected to be the site, and a wheel was installed there in March 2011.
Iraq's Tourism Board also is seeking investors to develop a "romantic" island on the River Tigris in Baghdad that was once a popular honeymoon spot for newlywed Iraqis. The project would include a six-star hotel, spa, an 18-hole golf course and a country club. In addition, the go-ahead has been given to build numerous architecturally unique skyscrapers along the Tigris that would develop the city's financial centre in Kadhehemiah. This project not only addresses the urgent need for new residential units in Baghdad but also acts as a real symbol of progress in the war torn city, as Baghdad has not seen projects of this scale for decades.
Housing.
In 2012, the Central Bank of Iraq signed a deal with Zaha Hadid Architects to build a tower which will be used as the bank's new headquarters.
Education.
The Mustansiriya Madrasah was established in 1227 by the Abbasid Caliph al-Mustansir. The name was changed to Al-Mustansiriya University in 1963. The University of Baghdad is the largest university in Iraq and the second largest in the Arab world.
Culture.
Baghdad has always played a significant role in the broader Arab cultural sphere, contributing several significant writers, musicians and visual artists. Famous Arab poets and singers such as Nizar Qabbani, Umm Kulthum, Fairuz, Salah Al-Hamdani, Ilham al-Madfai and others have performed for the city.
The dialect of Arabic spoken in Baghdad today differs from that of other large urban centres in Iraq, having features more characteristic of nomadic Arabic dialects (Verseegh, "The Arabic Language"). It is possible that this was caused by the repopulating of the city with rural residents after the multiple sacks of the late Middle Ages.
Institutions.
Some of the important cultural institutions in the city include:
The live theatre scene received a boost during the 1990s, when UN sanctions limited the import of foreign films. As many as 30 movie theatres were reported to have been converted to live stages, producing a wide range of comedies and dramatic productions.
Institutions offering cultural education in Baghdad include The Music and Ballet School of Baghdad and the Institute of Fine Arts Baghdad. Baghdad is also home to a number of museums which housed artifacts and relics of ancient civilization; many of these were stolen, and the museums looted, during the widespread chaos immediately after United States forces entered the city.
During the 2003 occupation of Iraq, AFN Iraq ("Freedom Radio") broadcast news and entertainment within Baghdad, among other locations. There is also a private radio station called "Dijlah" (named after the Arabic word for the Tigris River) that was created in 2004 as Iraq's first independent talk radio station. Radio Dijlah offices, in the Jamia neighborhood of Baghdad, have been attacked on several occasions.
Sport.
Baghdad is home to some of the most successful football (soccer) teams in Iraq, the biggest being current Iraqi League champions Al-Shorta (Police), Al-Quwa Al-Jawiya (Airforce club), Al-Zawra'a, and Talaba (Students). The largest stadium in Baghdad is Al-Shaab Stadium, which was opened in 1966. Another, but much larger stadium, is still in the opening stages of construction.
The city has also had a strong tradition of horse racing ever since World War I, known to Baghdadis simply as 'Races'. There are reports of pressures by the Islamists to stop this tradition due to the associated gambling.
Further reading.
Books:

</doc>
<doc id="4493" url="http://en.wikipedia.org/wiki?curid=4493" title="Outline of biology">
Outline of biology

The following outline is provided as an overview of and topical guide to biology:
Biology – study of living organisms. It is concerned with the characteristics, classification, and behaviors of organisms, how species come into existence, and the interactions they have with each other and with the environment. Biology encompasses a broad spectrum of academic fields that are often viewed as independent disciplines. However, together they address phenomena related to living organisms (biological phenomena) over a wide range of scales, from biophysics to ecology. All concepts in biology are subject to the same laws that other branches of science obey, such as the laws of thermodynamics and conservation of energy.

</doc>
<doc id="4495" url="http://en.wikipedia.org/wiki?curid=4495" title="British thermal unit">
British thermal unit

The British thermal unit (BTU or Btu) is a traditional unit of energy equal to about 1055 joules. It is the amount of energy needed to cool or heat one pound of water by one degree Fahrenheit. In science, the joule, the SI unit of energy, has largely replaced the BTU.
The BTU is most often used as a measure of power (as BTU/h) in the power, steam generation, heating, and air conditioning industries, and also as a measure of agricultural energy production (BTU/kg). It is still used in metric English-speaking countries (such as Canada), and remains the standard unit of classification for air conditioning units manufactured and sold in many non-English-speaking metric countries. In North America, the heat value (energy content) of fuels is expressed in BTUs.
Definitions.
A BTU is the amount of heat required to raise the temperature of of liquid water by at a constant pressure of one atmosphere. As with the calorie, several definitions of the BTU exist, because the temperature response of water to heat energy is non-linear. This means that the change in temperature of a water mass caused by adding a certain amount of heat to it will be a function of the water's initial temperature. Definitions of the BTU based on different water temperatures can therefore vary by up to 0.5%.
A BTU can be approximated as the heat produced by burning a single wooden match or as the amount of energy it takes to lift a one-pound weight .
The unit MBtu or mBtu was defined as one thousand BTU, presumably from the Roman numeral system where "M" or "m" stands for one thousand (1,000). This notation is easily confused with the SI mega- (M) prefix, which denotes multiplication by a factor of one million (×106), or with the SI milli- (m) prefix, which denotes division by a factor of one thousand (×10−3). To avoid confusion, many companies and engineers use the notation MMBtu or mmBtu to represent one million BTU (although, confusingly, MM in Roman numerals would traditionally represent 2,000) and in many contexts this form of notation is deprecated and discouraged in favour of the more modern SI prefixes. Alternatively, the term therm may be used to represent 100,000 (or 105) BTU, and quad for 1015 BTU. Some companies also use BtuE6 in order to reduce confusion between 103 BTU and 106 BTU.
Conversions.
One BTU is approximately:
As a unit of power.
When used as a unit of power for heating and cooling systems, BTU "per hour" (BTU/h) is the correct unit, though this is often abbreviated to just "BTU"..
Associated units.
The BTU should not be confused with the Board of Trade Unit (B.O.T.U.), which is a much larger quantity of energy ().
The BTU is often used to express the conversion-efficiency of heat into electrical energy in power plants. Figures are quoted in terms of the quantity of heat in BTU required to generate 1 kW·h of electrical energy. A typical coal-fired power plant works at 10,500 BTU/kW·h, an efficiency of 32–33%.

</doc>
<doc id="4497" url="http://en.wikipedia.org/wiki?curid=4497" title="Bugatti">
Bugatti

Automobiles Ettore Bugatti was a French car manufacturer of high-performance automobiles, founded in 1909 in the then German city of Molsheim, Alsace by Italian-born Ettore Bugatti. Bugatti cars were known for their design beauty (Ettore Bugatti was from a family of artists and considered himself to be both an artist and constructor) and for their many race victories. Famous Bugattis include the Type 35 Grand Prix cars, the Type 41 "Royale", the Type 57 "Atlantic" and the Type 55 sports car.
The death of Ettore Bugatti in 1947 proved to be the end for the marque, and the death of his son Jean Bugatti in 1939 ensured there was not a successor to lead the factory. No more than about 8000 cars were made. The company struggled financially, and released one last model in the 1950s, before eventually being purchased for its airplane parts business in the 1960s. In the 1990s, an Italian entrepreneur revived it as a builder of limited production exclusive sports cars. Today, the name is owned by German automobile manufacturing group Volkswagen.
They are now creating a Bugatti called the 16C Galibier coming out in 2015.
Under Ettore Bugatti.
Founder Ettore Bugatti was born in Milan, Italy, and the automobile company that bears his name was founded in 1909 in Molsheim located in the Alsace region which was part of the German Empire from 1871 to 1919. The company was known both for the level of detail of its engineering in its automobiles, and for the artistic way in which the designs were executed, given the artistic nature of Ettore's family (his father, Carlo Bugatti (1856–1940), was an important Art Nouveau furniture and jewelry designer).
World War I and its aftermath.
During the war Ettore Bugatti was sent away, initially to Milan and later to Paris, but as soon as hostilities had been concluded he returned to his factory at Molsheim. Less than four months after the Versailles Treaty formalised the transfer of Alsace from Germany to France, Bugatti was able to obtain, at the last minute, a stand at the 15th Paris motor show in October 1919. He exhibited three light cars, all of them closely based on their pre-war equivalents, and each fitted with the same overhead camshaft 4-cylinder 1,368cc engine with four valves per cylinder. Smallest of the three was a "Type 13" with a racing body (constructed by Bugatti themselves) and using a chassis with a wheelbase. The others were a "Type 22" and a "Type 23" with wheelbases of respectively.
Racing successes.
The company also enjoyed great success in early Grand Prix motor racing: in 1929 a privately entered Bugatti won the first ever Monaco Grand Prix. Racing success culminated with driver Jean-Pierre Wimille winning the 24 hours of Le Mans twice (in 1937 with Robert Benoist and 1939 with Pierre Veyron).
Bugatti cars were extremely successful in racing. The little Bugatti Type 10 swept the top four positions at its first race. The 1924 Bugatti Type 35 is probably the most successful racing car of all time, with over 2,000 wins. Bugattis swept to victory in the Targa Florio for five years straight from 1925 through 1929. Louis Chiron held the most podiums in Bugatti cars, and the modern marque revival Bugatti Automobiles S.A.S. named the 1999 Bugatti 18/3 Chiron concept car in his honour. But it was the final racing success at Le Mans that is most remembered—Jean-Pierre Wimille and Pierre Veyron won the 1939 race with just one car and meagre resources.
Airplane racing.
In the 1930s, Ettore Bugatti got involved in the creation of a racer airplane, hoping to beat the Germans in the Deutsch de la Meurthe prize. This would be the Bugatti 100P, which never flew. It was designed by Belgian engineer Louis de Monge who had already applied Bugatti Brescia engines in his "Type 7.5" lifting body.
Railcar.
Ettore Bugatti also designed a successful motorised railcar, the "Autorail" (Autorail Bugatti).
Family tragedy.
The death of Ettore Bugatti's son, Jean Bugatti, on 11 August 1939 marked a turning point in the company's fortunes. Jean died while testing a Type 57 tank-bodied race car near the Molsheim factory.
After the next war.
World War II left the Molsheim factory in ruins and the company lost control of the property. During the war, Bugatti planned a new factory at Levallois, a northwestern suburbs of Paris. After the war, Bugatti designed and planned to build a series of new cars, including the Type 73 road car and Type 73C single seat racing car, but in all Bugatti built only five Type 73 cars.
A 375 cc supercharged car was stopped when Ettore Bugatti died on 21 August 1947. Following Ettore Bugatti's death, the business declined further and made its last appearance as a business in its own right at a Paris Motor Show in October 1952.
After a long decline, the original incarnation of Bugatti ceased operations in 1952.
Design.
Bugattis noticeably focused on design. Engine blocks were hand scraped to ensure that the surfaces were so flat that gaskets were not required for sealing, many of the exposed surfaces of the engine compartment featured Guilloché (engine turned) finishes on them, and safety wires had been threaded through almost every fastener in intricately laced patterns. Rather than bolt the springs to the axles as most manufacturers did, Bugatti's axles were forged such that the spring passed though a carefully sized opening in the axle, a much more elegant solution requiring fewer parts. He famously described his arch competitor Bentley's cars as "the world's fastest lorries" for focusing on durability. According to Bugatti, "weight was the enemy".
Gallery.
Notable finds in the modern era.
Relatives of Dr. Harold Carr found a rare 1937 Bugatti Type 57S Atalante when cataloguing the doctor's belongings after his death in 2009. Dr. Carr's Type 57S is notable because it was originally owned by British race car driver Earl Howe. Because much of the car's original equipment is intact, it can be restored without relying on replacement parts.
On 10 July 2009, a 1925 Bugatti Brescia Type 22 which had lain at the bottom of Lake Maggiore on the border of Switzerland and Italy for 75 years was recovered from the lake. The Mullin Museum in Oxnard, California bought it at auction for $351,343 at Bonham's Retromobile sale in Paris in 2010.
Attempts at revival.
The company attempted a comeback under Roland Bugatti in the mid-1950s with the mid-engined Type 251 race car. Designed with help from Gioacchino Colombo, the car failed to perform to expectations and the company's attempts at automobile production were halted.
In the 1960s, Virgil Exner designed a Bugatti as part of his "Revival Cars" project. A show version of this car was actually built by Ghia using the last Bugatti Type 101 chassis, and was shown at the 1965 Turin Motor Show. Finance was not forthcoming, and Exner then turned his attention to a revival of Stutz.
Bugatti continued manufacturing airplane parts and was sold to Hispano-Suiza, also a former auto maker turned aircraft supplier, in 1963. Snecma took over Hispano-Suiza in 1968. After acquiring Messier, Snecma merged Messier and Bugatti into Messier-Bugatti in 1977.
Modern revivals.
Bugatti Automobili SpA 1987–1995.
Italian entrepreneur Romano Artioli acquired the Bugatti brand in 1987, and established Bugatti Automobili SpA. Bugatti commissioned architect Giampaolo Benedini to design the factory which was built in Campogalliano, Italy.
By 1989 the plans for the new Bugatti revival were presented by Paolo Stanzani and Marcello Gandini, designers of the Lamborghini Miura and Lamborghini Countach. Bugatti called their first production vehicle the Bugatti EB110 GT. Bugatti advertised the EB110 as the most technically advanced sports car ever produced.
Famed racing car designer Mauro Forghieri served as Bugatti's technical director from 1992 through 1994.
On 27 August 1993, through his holding company, ACBN Holdings S.A. of Luxembourg, Romano Artioli purchased Lotus Cars from General Motors. Bugatti made plans to list the company's shares on international stock exchanges.
Bugatti presented a prototype large saloon called the EB112 in 1993.
Perhaps the most famous Bugatti EB110 owner was seven-time Formula One World Champion racing driver Michael Schumacher who purchased an EB110 in 1994. Schumacher sold his EB110, which had been repaired after a severe 1994 crash, to Modena Motorsport, a Ferrari service and race preparation garage in Germany.
By the time the EB110 came to market, the North American and European economies were in recession. Poor economic conditions forced the company to fail and operations ceased in September 1995. A model specific to the US market called the "Bugatti America" was in the preparatory stages when the company ceased operations.
Bugatti's liquidators sold Lotus Cars to Proton of Malaysia. German firm Dauer Racing purchased the EB110 licence and remaining parts stock in 1997 in order to produce five more EB110 SS vehicles. These five SS versions of the EB110 were greatly refined by Dauer. The Campogalliano factory was sold to a furniture-making company, which subsequently collapsed before moving in, leaving the building unoccupied. After Dauer stopped producing cars in 2011, Toscana-Motors GmbH of Germany purchased the remaining parts stock from Dauer.
Bugatti Automobiles S.A.S. 1998–present.
Volkswagen AG acquired the Bugatti brand in 1998.
Bugatti Automobiles S.A.S. commissioned Giorgetto Giugiaro of ItalDesign to produce Bugatti's first concept vehicle, the EB118, a coupé that debuted at the 1998 Paris Auto Show. The EB118 concept featured a , W-18 engine. After its Paris debut, the EB118 concept was shown again in 1999 at the Geneva Auto Show and the Tokyo Motor Show.
Bugatti introduced its next concepts, the EB 218 at the 1999 Geneva Motor Show and the 18/3 Chiron at the 1999 Frankfurt Motor Show (IAA).
Bugatti Automobiles S.A.S. began assembling its first regular-production vehicle, the Bugatti Veyron 16.4 (the 1001 BHP super car with an 8-litre W-16 engine with four turbochargers) in September 2005 at the Bugatti Molsheim, France assembly "studio".

</doc>
<doc id="4498" url="http://en.wikipedia.org/wiki?curid=4498" title="Benchmark">
Benchmark

Benchmark may refer to:

</doc>
<doc id="4499" url="http://en.wikipedia.org/wiki?curid=4499" title="Band">
Band

Band may refer to:

</doc>
<doc id="4501" url="http://en.wikipedia.org/wiki?curid=4501" title="Black Death">
Black Death

The Black Death was one of the most devastating pandemics in human history, resulting in the deaths of an estimated people and peaking in Europe in the years 1346–53. Although there were several competing theories as to the etiology of the Black Death, analysis of DNA from victims in northern and southern Europe published in 2010 and 2011 indicates that the pathogen responsible was the "Yersinia pestis" bacterium, probably causing several forms of plague.
The Black Death is thought to have originated in the arid plains of central Asia, where it then travelled along the Silk Road, reaching the Crimea by 1346. From there, it was most likely carried by Oriental rat fleas living on the black rats that were regular passengers on merchant ships. Spreading throughout the Mediterranean and Europe, the Black Death is estimated to have killed 30–60% of Europe's total population. All in all, the plague reduced the world population from an estimated 450 million down to 350–375 million in the 14th century.
The aftermath of the plague created a series of religious, social, and economic upheavals, which had profound effects on the course of European history. It took for Europe's population to recover. The plague recurred occasionally in Europe until the 19th century.
Overview.
Major outbreaks.
There have been three major outbreaks of plague. The Plague of Justinian in the 6th and 7th centuries is the first known attack on record, and marks the first firmly recorded pattern of bubonic plague. From historical descriptions, as much as 40% of the population of Constantinople died from the plague. Modern estimates suggest half of Europe's population died as a result of the plague before it disappeared in the 700s. After 750, major epidemic diseases did not appear again in Europe until the Black Death of the 14th century. The Third Pandemic hit China in the 1890s and devastated India, but was confined to limited outbreaks in the west.
The Black Death originated in or near China and spread by way of the Silk Road or by ship. It may have reduced world population from an estimated 450 million down to 350–375 million by the year 1400.
The plague is thought to have returned at intervals with varying virulence and mortality until the 18th century. On its return in 1603, for example, the plague killed 38,000 Londoners. Other notable 17th-century outbreaks were the Italian Plague (1629–31); the Great Plague of Seville (1647–52); the Great Plague of London (1665–66); and the Great Plague of Vienna (1679). There is some controversy over the identity of the disease, but in its virulent form, after the Great Plague of Marseille in 1720–22, the Great Plague of 1738 (which hit Eastern Europe), and the Russian plague of 1770-1772, it seems to have gradually disappeared from Europe. By the early 19th century, the threat of plague had diminished, but it was quickly replaced by a new disease. The Asiatic cholera was the first of several cholera pandemics to sweep through Asia and Europe during the 19th and 20th centuries.
The 14th-century eruption of the Black Death had a drastic effect on Europe's population, irrevocably changing the social structure, and resulted in widespread persecution of minorities such as Jews, foreigners, beggars, and lepers (see Persecutions). The uncertainty of daily survival has been seen as creating a general mood of morbidity, influencing people to "live for the moment," as illustrated by Giovanni Boccaccio in "The Decameron" (1353).
Naming.
Medieval people called the catastrophe of the 14th century either the "Great Pestilence"' or the "Great Plague". Writers contemporary to the plague referred to the event as the "Great Mortality". Swedish and Danish chronicles of the 17th century described the events as "black" for the first time, not to describe the late-stage sign of the disease, in which the sufferer's skin would blacken due to subepidermal hemorrhages and the extremities would darken with a form of gangrene, acral necrosis, but more likely to refer to black in the sense of glum or dreadful and to denote the terror and gloom of the events. Gasquet (1908) claimed that the Latin name ""atra mors"" (Black Death) for an epidemic first appeared in modern times in 1631 in a book on Danish history by J.I. Pontanus, where Pontanus wrote about a disease that occurred in 1348: ""Vulgo & ab effectu "atram mortem" vocatibant."" (Commonly and from effects, they called [it the black death.). However, Gasquet doubted that Pontanus was referring to bubonic plague. Nevertheless, the name spread through Scandinavia and then Germany. In England, it was not until 1823 that the medieval epidemic was first called the Black Death.
Migration.
Populations in crisis.
In Europe, the Medieval Warm Period ended sometime towards the end of the 13th century, with the climate turning towards the "Little Ice Age" and harsher winters with reduced harvests. In most of Europe, new technological innovations such as the heavy plough with an improved mouldboard and pulled by oxen or horses and the three-field system had had a major impact over the previous five centuries; they had opened up more intense cultivation of the heavy and fertile clay soils of northern and central Europe and helped increase population by many millions, especially in Germany, England and France. These agricultural innovations had also boosted productivity and population in the western Mediterranean basin, and by 1300, in much of western and southern Europe, the population was most likely outgrowing what could be sustained given the technological limits of the age.
Food shortages and rapidly inflating (or fluctuating) prices were becoming a fact of life in some parts of France and Italy by the early 14th century, and over the next hundred years these would spread and worsen. Wheat, oats, hay and consequently livestock were all in short supply. Their scarcity resulted in malnutrition, which increases susceptibility to infections due to weakened immunity. Consistently high fertility rates, at five or more children per woman throughout Europe, had helped power high population growth rates and now contributed to these food shortages. In the autumn of 1314, heavy rains began to fall, followed by several years of cold and wet winters. The already weak harvests of the north suffered and a seven-year famine ensued. In the years 1315 to 1317, a catastrophic famine, known as the Great Famine, struck much of northwest Europe. It was the first major famine with this kind of severity in Europe for at least a hundred years.
Infection and migration.
The plague disease, caused by "Yersinia pestis", is enzootic (commonly present) in populations of fleas carried by ground rodents, including marmots, in various areas including Central Asia, Kurdistan, Western Asia, Northern India and Uganda. Nestorian graves dating to 1338–9 near Lake Issyk Kul in Kyrgizstan have inscriptions referring to plague and are thought by many epidemiologists to mark the outbreak of the epidemic, from which it could easily have spread to China and India. In October 2010, medical geneticists suggested that all three of the great waves of the plague originated in China. In China, the 13th century Mongol conquest caused a decline in farming and trading. However, economic recovery had been observed at the beginning of the 14th century. In the 1330s a large number of natural disasters and plagues led to widespread famine, starting in 1331, with a deadly plague arriving soon after. The 14th-century plague killed an estimated 25 million Chinese and other Asians during the 15 years before it reached Constantinople in 1347. However, according to George Sussman, the first obvious medical description of plague in China dates to 1644.
The disease may have travelled along the Silk Road with Mongol armies and traders or it could have come via ship. By the end of 1346, reports of plague had reached the seaports of Europe: "India was depopulated, Tartary, Mesopotamia, Syria, Armenia were covered with dead bodies".
Plague was reportedly first introduced to Europe at the trading city of Caffa in the Crimea in 1347. After a protracted siege, during which the Mongol army under Jani Beg was suffering the disease, the army catapulted the infected corpses over the city walls to infect the inhabitants. The Genoese traders fled, taking the plague by ship into Sicily and the south of Europe, whence it spread north. Whether or not this hypothesis is accurate, it is clear that several existing conditions such as war, famine, and weather contributed to the severity of the Black Death.
European outbreak.
There appear to have been several introductions into Europe. The plague reached Sicily in October 1347, carried by twelve Genoese galleys, and rapidly spread all over the island. Galleys from Caffa reached Genoa and Venice in January 1348, but it was the outbreak in Pisa a few weeks later that was the entry point to northern Italy. Towards the end of January, one of the galleys expelled from Italy arrived in Marseille.
From Italy, the disease spread northwest across Europe, striking France, Spain, Portugal and England by June 1348, then turned and spread east through Germany and Scandinavia from 1348–50. It was introduced in Norway in 1349 when a ship landed at Askøy, then spread to Bjørgvin (modern Bergen) and Iceland. Finally it spread to northwestern Russia in 1351. The plague spared some parts of Europe, including the Kingdom of Poland, the majority of the Basque Country and isolated parts of Belgium and the Netherlands.
Middle Eastern outbreak.
The plague struck various countries in the Middle East during the pandemic, leading to serious depopulation and permanent change in both economic and social structures. As it spread to western Europe, the disease entered the region from southern Russia also. By autumn 1347, the plague reached Alexandria in Egypt, probably through the port's trade with Constantinople, and ports on the Black Sea. During 1347, the disease travelled eastward to Gaza, and north along the eastern coast to cities in Lebanon, Syria and Palestine, including Ashkelon, Acre, Jerusalem, Sidon, Damascus, Homs, and Aleppo. In 1348–49, the disease reached Antioch. The city's residents fled to the north, most of them dying during the journey, but the infection had been spread to the people of Asia Minor.
Mecca became infected in 1349. During the same year, records show the city of Mawsil (Mosul) suffered a massive epidemic, and the city of Baghdad experienced a second round of the disease. In 1351 Yemen experienced an outbreak of the plague, coinciding with the return of King Mujahid of Yemen from imprisonment in Cairo. His party may have brought the disease with them from Egypt.
Symptoms.
Contemporary accounts of the plague are often varied or imprecise. The most commonly noted symptom was the appearance of buboes (or gavocciolos) in the groin, the neck and armpits, which oozed pus and bled when opened. Boccaccio's description is graphic:
Ziegler comments that the only medical detail that is questionable is the infallibility of approaching death, as if the bubo discharges, recovery is possible.
This was followed by acute fever and vomiting of blood. Most victims died two to seven days after initial infection. David Herlihy identifies another potential sign of the plague: freckle-like spots and rashes which could be caused by flea-bites.
Some accounts, like that of Louis Heyligen, a musician in Avignon who died of the plague in 1348, noted a distinct form of the disease which infected the lungs and led to respiratory problems and which is identified with pneumonic plague.
Causes.
Medical knowledge had stagnated during the Middle Ages. The most authoritative account at the time came from the medical faculty in Paris in a report to the king of France that blamed the heavens, in the form of a conjunction of three planets in 1345 that caused a "great pestilence in the air". This report became the first and most widely circulated of a series of "plague tracts" that sought to give advice to sufferers. That the plague was caused by "bad air" became the most widely accepted theory. The word "plague" had no special significance at this time, and only the recurrence of outbreaks during the Middle Ages gave it the name that has become the medical term.
The importance of hygiene was recognised only in the nineteenth century; until then it was common that the streets were filthy, with live animals of all sorts around and human parasites abounding. A transmissible disease will spread easily in such conditions. One development as a result of the Black Death was the establishment of the idea of quarantine in Dubrovnik in 1377 after continuing outbreaks.
The dominant explanation for the Black Death is the plague theory, which attributes the outbreak to "Yersinia pestis", also responsible for an epidemic that began in southern China in 1865, eventually spreading to India. The investigation of the pathogen that caused the 19th-century plague was begun by teams of scientists who visited Hong Kong in 1894, among whom was the French-Swiss bacteriologist Alexandre Yersin, after whom the pathogen was named "Yersinia pestis". The mechanism by which "Y. pestis" was usually transmitted was established in 1898 by Paul-Louis Simond and was found to involve the bites of fleas whose midguts had become obstructed by replicating "Y. pestis" several days after feeding on an infected host. This blockage results in starvation and aggressive feeding behaviour by the fleas, which repeatedly attempt to clear their blockage by regurgitation, resulting in thousands of plague bacteria being flushed into the feeding site, infecting the host. The bubonic plague mechanism was also dependent on two populations of rodents: one resistant to the disease, which act as hosts, keeping the disease endemic; and a second that lack resistance. When the second population dies, the fleas move on to other hosts, including people, thus creating a human epidemic.
The historian Francis Aidan Gasquet, who had written about the 'Great Pestilence' in 1893 and suggested that "it would appear to be some form of the ordinary Eastern or bubonic plague" was able to adopt the epidemiology of the bubonic plague for the Black Death for the second edition in 1908, implicating rats and fleas in the process, and his interpretation was widely accepted for other ancient and medieval epidemics, such as the Justinian plague that was prevalent in the Eastern Roman Empire from 541 to 700 CE.
More recently other forms of plague have been implicated. The modern bubonic plague has a mortality rate of 30–75% and symptoms including fever of , headaches, painful aching joints, nausea and vomiting, and a general feeling of malaise. Left untreated, of those that contract the bubonic plague, 80 percent die within eight days. Pneumonic plague has a mortality rate of 90 to 95 percent. Symptoms include fever, cough, and blood-tinged sputum. As the disease progresses, sputum becomes free flowing and bright red. Septicemic plague is the least common of the three forms, with a mortality rate near 100%. Symptoms are high fevers and purple skin patches (purpura due to disseminated intravascular coagulation). In cases of pneumonic and particularly septicemic plague the progress of the disease is so rapid that there would often be no time for the development of the enlarged lymph nodes that were noted as buboes.
"Many modern scholars accept that the lethality of the Black Death stemmed from the combination of bubonic and pneumonic plague with other diseases and warn that every historical mention of 'pest' was not necessarily bubonic plague...In her study of 15th-century outbreaks, Ann Carmichael states that worms, the pox, fevers and dysentery clearly accompanied bubonic plague."
Alternative explanations.
This interpretation was first significantly challenged by the work of British bacteriologist J. F. D. Shrewsbury in 1970, who noted that the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague, leading him to conclude that contemporary accounts were exaggerations. In 1984 zoologist Graham Twigg produced the first major work to challenge the bubonic plague theory directly, and his doubts about the identity of the Black Death have been taken up by a number of authors, including Samuel K. Cohn, Jr. (2002), David Herlihy (1997), and Susan Scott and Christopher Duncan (2001).
It is recognised that an epidemiological account of the plague is as important as an identification of symptoms, but researchers are hampered by the lack of reliable statistics from this period. Most work has been done on the spread of the plague in England, and even estimates of overall population at the start vary by over 100% as no census was undertaken between the time of publication of the Domesday Book and the year 1377. Estimates of plague victims are usually extrapolated from figures from the clergy.
In addition to arguing that the rat population was insufficient to account for a bubonic plague pandemic, sceptics of the bubonic plague theory point out that the symptoms of the Black Death are not unique (and arguably in some accounts may differ from bubonic plague); that transference via fleas in goods was likely to be of marginal significance and that the DNA results may be flawed and might not have been repeated elsewhere, despite extensive samples from other mass graves. Other arguments include the lack of accounts of the death of rats before outbreaks of plague between the 14th and 17th centuries; temperatures that are too cold in northern Europe for the survival of fleas; that, despite primitive transport systems, the spread of the Black Death was much faster than that of modern bubonic plague; that mortality rates of the Black Death appear to be very high; that, while modern bubonic plague is largely endemic as a rural disease, the Black Death indiscriminately struck urban and rural areas; and that the pattern of the Black Death, with major outbreaks in the same areas separated by 5 to 15 years, differs from modern bubonic plague—which often becomes endemic for decades with annual flare-ups.
Walløe complains that all of these authors "take it for granted that Simond's infection model, black rat → rat flea → human, which was developed to explain the spread of plague in India, is the only way an epidemic of "Yersinia pestis" infection could spread", whilst pointing to several other possibilities.
A variety of alternatives to the "Y. pestis" have been put forward. Twigg suggested that the cause was a form of anthrax and N. F. Cantor (2001) thought it may have been a combination of anthrax and other pandemics. Scott and Duncan have argued that the pandemic was a form of infectious disease that characterise as "hemorrhagic" plague similar to Ebola. Archaeologist Barney Sloane has argued that there is insufficient evidence of the extinction of large number of rats in the archaeological record of the medieval waterfront in London and that the plague spread too quickly to support the thesis that the "Y. pestis" was spread from fleas on rats and argues that transmission must have been person to person. However, no single alternative solution has achieved widespread acceptance. Many scholars arguing for the "Y. pestis" as the major agent of the pandemic suggest that its extent and symptoms can be explained by a combination of bubonic plague with other diseases, including typhus, smallpox and respiratory infections. In addition to the bubonic infection, others point to additional septicemic (a type of "blood poisoning") and pneumonic (an airborne plague that attacks the lungs before the rest of the body) forms of the plague, which lengthen the duration of outbreaks throughout the seasons and help account for its high mortality rate and additional recorded symptoms. In 2014, scientists with Public Health England announced the results of an examination of 25 bodies exhumed from the Clerkenwell area of London, as well as of wills registered in London during the period, which supported the pneumonic hypothesis.
DNA evidence.
In October 2010, the open-access scientific journal "PLoS Pathogens" published a paper by a multinational team who undertook a new investigation into the role of "Yersinia pestis" in the Black Death following the disputed identification by Drancourt and Raoult in 1998. Their surveys tested for DNA and protein signatures specific for "Y. pestis" in human skeletons from widely distributed mass graves in northern, central and southern Europe that were associated archaeologically with the Black Death and subsequent resurgences. The authors concluded that this new research, together with prior analyses from the south of France and Germany
The study also found that there were two previously unknown but related clades (genetic branches) of the "Y. pestis" genome associated with medieval mass graves. These clades (which are thought to be extinct) were found to be ancestral to modern isolates of the modern "Y. pestis" strains "Y. p. orientalis" and "Y. p. medievalis", suggesting the plague may have entered Europe in two waves. Surveys of plague pit remains in France and England indicate the first variant entered Europe through the port of Marseille around November 1347 and spread through France over the next two years, eventually reaching England in the spring of 1349, where it spread through the country in three epidemics. Surveys of plague pit remains from the Dutch town of Bergen op Zoom showed the "Y. pestis" genotype responsible for the pandemic that spread through the Low Countries from 1350 differed from that found in Britain and France, implying Bergen op Zoom (and possibly other parts of the southern Netherlands) was not directly infected from England or France in 1349 and suggesting a second wave of plague, different from those in Britain and France, may have been carried to the Low Countries from Norway, the Hanseatic cities or another site.
The results of the Haensch study have since been confirmed and amended. Based on genetic evidence derived from Black Death victims in the East Smithfield burial site in England, Schuenemann et al. concluded in 2011 "that the Black Death in medieval Europe was caused by a variant of "Y. pestis" that may no longer exist." A study published in "Nature" in October 2011 sequenced the genome of "Y. pestis" from plague victims and indicated that the strain that caused the Black Death is ancestral to most modern strains of the disease.
DNA taken from 25 skeletons in London that died in the 14th century, have shown the plague is a strain of "Y. pestis" that is almost identical to that which hit Madagascar in 2013.
Consequences.
Death toll.
There are no exact figures for the death toll; the rate varied widely by locality. It killed some people in Eurasia.
The most widely accepted estimate for the Middle East, including Iraq, Iran and Syria, during this time, is for a death rate of about a third. The Black Death killed about 40% of Egypt's population. Half of Paris's population of 100,000 people died. In Italy, Florence's population was reduced from 110–120 thousand inhabitants in 1338 down to 50 thousand in 1351. At least 60% of Hamburg's and Bremen's population perished, and a similar percentage of Londoners may have died from the disease as well. Before 1350, there were about 170,000 settlements in Germany, and this was reduced by nearly 40,000 by 1450. In 1348, the plague spread so rapidly that before any physicians or government authorities had time to reflect upon its origins, about a third of the European population had already perished. In crowded cities, it was not uncommon for as much as 50% of the population to die. The disease bypassed some areas, and the most isolated areas were less vulnerable to contagion. Monks and priests were especially hard hit since they cared for the Black Death's victims.
Persecutions.
Renewed religious fervor and fanaticism bloomed in the wake of the Black Death. Some Europeans targeted "various groups such as Jews, friars, foreigners, beggars, pilgrims", lepers and Romani, thinking that they were to blame for the crisis. Lepers, and other individuals with skin diseases such as acne or psoriasis, were singled out and exterminated throughout Europe.
Because 14th-century healers were at a loss to explain the cause, Europeans turned to astrological forces, earthquakes, and the poisoning of wells by Jews as possible reasons for the plague's emergence. The governments of Europe had no apparent response to the crisis because no one knew its cause or how it spread. The mechanism of infection and transmission of diseases was little understood in the 14th century; many people believed only God's anger could produce such horrific displays.
There were many attacks against Jewish communities. In August 1349, the Jewish communities of Mainz and Cologne were exterminated. In February of that same year, the citizens of Strasbourg murdered 2,000 Jews. By 1351, 60 major and 150 smaller Jewish communities were destroyed.
Recurrence.
The plague repeatedly returned to haunt Europe and the Mediterranean throughout the 14th to 17th centuries. According to Biraben, plague was present somewhere in Europe in every year between 1346 and 1671. The was particularly widespread in the following years: 1360–63; 1374; 1400; 1438–39; 1456–57; 1464–66; 1481–85; 1500–03; 1518–31; 1544–48; 1563–66; 1573–88; 1596–99; 1602–11; 1623–40; 1644–54; and 1664–67. Subsequent outbreaks, though severe, marked the retreat from most of Europe (18th century) and northern Africa (19th century). According to Geoffrey Parker, "France alone lost almost a million people to plague in the epidemic of 1628–31."
In England, in the absence of census figures, historians propose a range of preincident population figures from as high as 7 million to as low as 4 million in 1300, and a postincident population figure as low as 2 million. By the end of 1350, the Black Death subsided, but it never really died out in England. Over the next few hundred years, further outbreaks occurred in 1361–62, 1369, 1379–83, 1389–93, and throughout the first half of the 15th century. An outbreak in 1471 took as much as 10–15% of the population, while the death rate of the plague of 1479–80 could have been as high as 20%. The most general outbreaks in Tudor and Stuart England seem to have begun in 1498, 1535, 1543, 1563, 1589, 1603, 1625, and 1636, and ended with the Great Plague of London in 1665.
In 1466, perhaps 40,000 people died of plague in Paris. During the 16th and 17th centuries, plague visited Paris for almost one year out of every three. The Black Death ravaged Europe for three years before it continued on into Russia, where the disease hit somewhere once every five or six years from 1350 to 1490. Plague epidemics ravaged London in 1563, 1593, 1603, 1625, 1636, and 1665, reducing its population by 10 to 30% during those years. Over 10% of Amsterdam's population died in 1623–25, and again in 1635–36, 1655, and 1664. There were 22 outbreaks of plague in Venice between 1361 and 1528. The plague of 1576–77 killed 50,000 in Venice, almost a third of the population. Late outbreaks in central Europe included the Italian Plague of 1629–1631, which is associated with troop movements during the Thirty Years' War, and the Great Plague of Vienna in 1679. Over 60% of Norway's population died in 1348–50. The last plague outbreak ravaged Oslo in 1654.
In the first half of the 17th century, a plague claimed some 1.7 million victims in Italy, or about 14% of the population. In 1656, the plague killed about half of Naples' 300,000 inhabitants. More than 1.25 million deaths resulted from the extreme incidence of plague in 17th-century Spain. The plague of 1649 probably reduced the population of Seville by half. In 1709–13, a plague epidemic that followed the Great Northern War (1700–21, Sweden v. Russia and allies) killed about 100,000 in Sweden, and 300,000 in Prussia. The plague killed two-thirds of the inhabitants of Helsinki, and claimed a third of Stockholm's population. Europe's last major epidemic occurred in 1720 in Marseille.
The Black Death ravaged much of the Islamic world. Plague was present in at least one location in the Islamic world virtually every year between 1500 and 1850. Plague repeatedly struck the cities of North Africa. Algiers lost 30 to 50 thousand inhabitants to it in 1620–21, and again in 1654–57, 1665, 1691, and 1740–42. Plague remained a major event in Ottoman society until the second quarter of the 19th century. Between 1701 and 1750, thirty-seven larger and smaller epidemics were recorded in Constantinople, and an additional thirty-one between 1751 and 1800. Baghdad has suffered severely from visitations of the plague, and sometimes two-thirds of its population has been wiped out.
Third plague pandemic.
The Third plague pandemic (1855–1859) started in China in the middle of the 19th century, spreading to all inhabited continents and killing 10 million people in India alone. Twelve plague outbreaks in Australia in 1900–25 resulted in well over , chiefly in Sydney. This led to the establishment of a Public Health Department there which undertook some leading-edge research on plague transmission from rat fleas to humans via the bacillus "Yersinia pestis".
The first North American plague epidemic was the San Francisco plague of 1900–1904, followed by another outbreak in 1907–08.
</ref> From 1944 through 1993, 362 cases of human plague were reported in the United States; approximately 90% occurred in four western states: Arizona, California, Colorado, and New Mexico. Plague was confirmed in the United States from 9 western states during 1995. Currently, 5 to 15 people in the United States are estimated to catch the disease each year—typically in western states.
The plague bacterium could develop drug-resistance and again become a major health threat. One case of a drug-resistant form of the bacterium was found in Madagascar in 1995.

</doc>
<doc id="4502" url="http://en.wikipedia.org/wiki?curid=4502" title="Biotechnology">
Biotechnology

Biotechnology is the use of living systems and organisms to develop or make useful products, or "any technological application that uses biological systems, living organisms or derivatives thereof, to make or modify products or processes for specific use" (UN Convention on Biological Diversity, Art. 2). Depending on the tools and applications, it often overlaps with the (related) fields of bioengineering and biomedical engineering.
For thousands of years, humankind has used biotechnology in agriculture, food production, and medicine. The term itself is largely believed to have been coined in 1919 by Hungarian engineer Károly Ereky. In the late 20th and early 21st century, biotechnology has expanded to include new and diverse sciences such as genomics, recombinant gene technologies, applied immunology, and development of pharmaceutical therapies and diagnostic tests.
Definitions.
The wide concept of "biotech" or "biotechnology" encompasses a wide range of procedures (and history) for modifying living organisms according to human purposes, going back to domestication of animals, cultivation of plants, and "improvements" to these through breeding programs that employ artificial selection and hybridization. Modern usage also includes genetic engineering as well as cell and tissue culture technologies. The American Chemical Society defines biotechnology as the application of biological organisms, systems, or processes by various industries to learning about the science of life and the improvement of the value of materials and organisms such as pharmaceuticals, crops, and livestock. Biotechnology also writes on the pure biological sciences (animal cell culture, biochemistry, cell biology, embryology, genetics, microbiology, and molecular biology). In many instances, it is also dependent on knowledge and methods from outside the sphere of biology including:
Conversely, modern biological sciences (including even concepts such as molecular ecology) are intimately entwined and heavily dependent on the methods developed through biotechnology and what is commonly thought of as the life sciences industry. Biotechnology is the research and development in the laboratory using bioinformatics for exploration, extraction, exploitation and production from any living organisms and any source of biomass by means of biochemical engineering where high value-added products could be planned (reproduced by biosynthesis, for example), forecasted, formulated, developed, manufactured and marketed for the purpose of sustainable operations (for the return from bottomless initial investment on R & D) and gaining durable patents rights (for exclusives rights for sales, and prior to this to receive national and international approval from the results on animal experiment and human experiment, especially on the pharmaceutical branch of biotechnology to prevent any undetected side-effects or safety concerns by using the products).
By contrast, bioengineering is generally thought of as a related field that more heavily emphasizes higher systems approaches (not necessarily the altering or using of biological materials "directly") for interfacing with and utilizing living things. Bioengineering is the application of the principles of engineering and natural sciences to tissues, cells and molecules. This can be considered as the use of knowledge from working with and manipulating biology to achieve a result that can improve functions in plants and animals. Relatedly, biomedical engineering is an overlapping field that often draws upon and applies "biotechnology" (by various definitions), especially in certain sub-fields of biomedical and/or chemical engineering such as tissue engineering, biopharmaceutical engineering, and genetic engineering.
History.
Although not normally what first comes to mind, many forms of human-derived agriculture clearly fit the broad definition of "'using a biotechnological system to make products". Indeed, the cultivation of plants may be viewed as the earliest biotechnological enterprise.
Agriculture has been theorized to have become the dominant way of producing food since the Neolithic Revolution. Through early biotechnology, the earliest farmers selected and bred the best suited crops, having the highest yields, to produce enough food to support a growing population. As crops and fields became increasingly large and difficult to maintain, it was discovered that specific organisms and their by-products could effectively fertilize, restore nitrogen, and control pests. Throughout the history of agriculture, farmers have inadvertently altered the genetics of their crops through introducing them to new environments and breeding them with other plants — one of the first forms of biotechnology.
These processes also were included in early fermentation of beer. These processes were introduced in early Mesopotamia, Egypt, China and India, and still use the same basic biological methods. In brewing, malted grains (containing enzymes) convert starch from grains into sugar and then adding specific yeasts to produce beer. In this process, carbohydrates in the grains were broken down into alcohols such as ethanol. Later other cultures produced the process of lactic acid fermentation which allowed the fermentation and preservation of other forms of food, such as soy sauce. Fermentation was also used in this time period to produce leavened bread. Although the process of fermentation was not fully understood until Louis Pasteur's work in 1857, it is still the first use of biotechnology to convert a food source into another form.
For thousands of years, humans have used selective breeding to improve production of crops and livestock to use them for food. In selective breeding, organisms with desirable characteristics are mated to produce offspring with the same characteristics. For example, this technique was used with corn to produce the largest and sweetest crops.
In the early twentieth century scientists gained a greater understanding of microbiology and explored ways of manufacturing specific products. In 1917, Chaim Weizmann first used a pure microbiological culture in an industrial process, that of manufacturing corn starch using "Clostridium acetobutylicum," to produce acetone, which the United Kingdom desperately needed to manufacture explosives during World War I.
Biotechnology has also led to the development of antibiotics. In 1928, Alexander Fleming discovered the mold "Penicillium". His work led to the purification of the antibiotic compound formed by the mold by Howard Florey, Ernst Boris Chain and Norman Heatley - to form what we today know as penicillin. In 1940, penicillin became available for medicinal use to treat bacterial infections in humans.
The field of modern biotechnology is generally thought of as having been born in 1971 when Paul Berg's (Stanford) experiments in gene splicing had early success. Herbert W. Boyer (Univ. Calif. at San Francisco) and Stanley N. Cohen (Stanford) significantly advanced the new technology in 1972 by transferring genetic material into a bacterium, such that the imported material would be reproduced. The commercial viability of a biotechnology industry was significantly expanded on June 16, 1980, when the United States Supreme Court ruled that a genetically modified microorganism could be patented in the case of "Diamond v. Chakrabarty". Indian-born Ananda Chakrabarty, working for General Electric, had modified a bacterium (of the "Pseudomonas" genus) capable of breaking down crude oil, which he proposed to use in treating oil spills. (Chakrabarty's work did not involve gene manipulation but rather the transfer of entire organelles between strains of the "Pseudomonas" bacterium.
Revenue in the industry is expected to grow by 12.9% in 2008. Another factor influencing the biotechnology sector's success is improved intellectual property rights legislation—and enforcement—worldwide, as well as strengthened demand for medical and pharmaceutical products to cope with an ageing, and ailing, U.S. population.
Rising demand for biofuels is expected to be good news for the biotechnology sector, with the Department of Energy estimating ethanol usage could reduce U.S. petroleum-derived fuel consumption by up to 30% by 2030. The biotechnology sector has allowed the U.S. farming industry to rapidly increase its supply of corn and soybeans—the main inputs into biofuels—by developing genetically modified seeds which are resistant to pests and drought. By boosting farm productivity, biotechnology plays a crucial role in ensuring that biofuel production targets are met.
Applications.
Biotechnology has applications in four major industrial areas, including health care (medical), crop production and agriculture, non food (industrial) uses of crops and other products (e.g. biodegradable plastics, vegetable oil, biofuels), and environmental uses.
For example, one application of biotechnology is the directed use of organisms for the manufacture of organic products (examples include beer and milk products). Another example is using naturally present bacteria by the mining industry in bioleaching. Biotechnology is also used to recycle, treat waste, cleanup sites contaminated by industrial activities (bioremediation), and also to produce biological weapons.
A series of derived terms have been coined to identify several branches of biotechnology; for example:
The investment and economic output of all of these types of applied biotechnologies is termed as "bioeconomy".
Medicine.
In medicine, modern biotechnology finds applications in areas such as pharmaceutical drug discovery and production, pharmacogenomics, and genetic testing (or genetic screening).
Pharmacogenomics (a combination of pharmacology and genomics) is the technology that analyses how genetic makeup affects an individual's response to drugs. It deals with the influence of genetic variation on drug response in patients by correlating gene expression or single-nucleotide polymorphisms with a drug's efficacy or toxicity. By doing so, pharmacogenomics aims to develop rational means to optimize drug therapy, with respect to the patients' genotype, to ensure maximum efficacy with minimal adverse effects. Such approaches promise the advent of "personalized medicine"; in which drugs and drug combinations are optimized for each individual's unique genetic makeup.
Biotechnology has contributed to the discovery and manufacturing of traditional small molecule pharmaceutical drugs as well as drugs that are the product of biotechnology - biopharmaceutics. Modern biotechnology can be used to manufacture existing medicines relatively easily and cheaply. The first genetically engineered products were medicines designed to treat human diseases. To cite one example, in 1978 Genentech developed synthetic humanized insulin by joining its gene with a plasmid vector inserted into the bacterium "Escherichia coli". Insulin, widely used for the treatment of diabetes, was previously extracted from the pancreas of abattoir animals (cattle and/or pigs). The resulting genetically engineered bacterium enabled the production of vast quantities of synthetic human insulin at relatively low cost. Biotechnology has also enabled emerging therapeutics like gene therapy. The application of biotechnology to basic science (for example through the Human Genome Project) has also dramatically improved our understanding of biology and as our scientific knowledge of normal and disease biology has increased, our ability to develop new medicines to treat previously untreatable diseases has increased as well. 
Genetic testing allows the genetic diagnosis of vulnerabilities to inherited diseases, and can also be used to determine a child's parentage (genetic mother and father) or in general a person's ancestry. In addition to studying chromosomes to the level of individual genes, genetic testing in a broader sense includes biochemical tests for the possible presence of genetic diseases, or mutant forms of genes associated with increased risk of developing genetic disorders. Genetic testing identifies changes in chromosomes, genes, or proteins. Most of the time, testing is used to find changes that are associated with inherited disorders. The results of a genetic test can confirm or rule out a suspected genetic condition or help determine a person's chance of developing or passing on a genetic disorder. As of 2011 several hundred genetic tests were in use. Since genetic testing may open up ethical or psychological problems, genetic testing is often accompanied by genetic counseling.
Agriculture.
Genetically modified crops ("GM crops", or "biotech crops") are plants used in agriculture, the DNA of which has been modified using genetic engineering techniques. In most cases the aim is to introduce a new trait to the plant which does not occur naturally in the species. 
Examples in food crops include resistance to certain pests, diseases, stressful environmental conditions, resistance to chemical treatments (e.g. resistance to a herbicide), reduction of spoilage, or improving the nutrient profile of the crop. Examples in non-food crops include production of pharmaceutical agents, biofuels, and other industrially useful goods, as well as for bioremediation.
Farmers have widely adopted GM technology. Between 1996 and 2011, the total surface area of land cultivated with GM crops had increased by a factor of 94, from to 1,600,000 km2 (395 million acres). 10% of the world's crop lands were planted with GM crops in 2010. As of 2011, 11 different transgenic crops were grown commercially on 395 million acres (160 million hectares) in 29 countries such as the USA, Brazil, Argentina, India, Canada, China, Paraguay, Pakistan, South Africa, Uruguay, Bolivia, Australia, Philippines, Myanmar, Burkina Faso, Mexico and Spain.
Genetically modified foods are foods produced from organisms that have had specific changes introduced into their DNA using the methods of genetic engineering. These techniques have allowed for the introduction of new crop traits as well as a far greater control over a food's genetic structure than previously afforded by methods such as selective breeding and mutation breeding. Commercial sale of genetically modified foods began in 1994, when Calgene first marketed its Flavr Savr delayed ripening tomato. To date most genetic modification of foods have primarily focused on cash crops in high demand by farmers such as soybean, corn, canola, and cotton seed oil. These have been engineered for resistance to pathogens and herbicides and better nutrient profiles. GM livestock have also been experimentally developed, although as of November 2013 none are currently on the market.
There is broad scientific consensus that food on the market derived from GM crops poses no greater risk to human health than conventional food. GM crops also provide a number of ecological benefits, if not used in excess. However, opponents have objected to GM crops per se on several grounds, including environmental concerns, whether food produced from GM crops is safe, whether GM crops are needed to address the world's food needs, and economic concerns raised by the fact these organisms are subject to intellectual property law.
Industrial biotechnology.
Industrial biotechnology (known mainly in Europe as white biotechnology) is the application of biotechnology for industrial purposes, including industrial fermentation. It includes the practice of using cells such as micro-organisms, or components of cells like enzymes, to generate industrially useful products in sectors such as chemicals, food and feed, detergents, paper and pulp, textiles and biofuels. In doing so, biotechnology uses renewable raw materials and may contribute to lowering greenhouse gas emissions and moving away from a petrochemical-based economy.
Regulation.
The regulation of genetic engineering concerns the approaches taken by governments to assess and manage the risks associated with the use of genetic engineering technology and the development and release of genetically modified organisms (GMO), including genetically modified crops and genetically modified fish. There are differences in the regulation of GMOs between countries, with some of the most marked differences occurring between the USA and Europe. Regulation varies in a given country depending on the intended use of the products of the genetic engineering. For example, a crop not intended for food use is generally not reviewed by authorities responsible for food safety. The European Union differentiates between approval for cultivation within the EU and approval for import and processing. While only a few GMOs have been approved for cultivation in the EU a number of GMOs have been approved for import and processing. The cultivation of GMOs has triggered a debate about coexistence of GM and nonGM crops. Depending on the coexistence regulations incentives for cultivation of GM crops differ.
Education.
In 1988, after prompting from the United States Congress, the National Institute of General Medical Sciences (National Institutes of Health) (NIGMS) instituted a funding mechanism for biotechnology training. Universities nationwide compete for these funds to establish Biotechnology Training Programs (BTPs). Each successful application is generally funded for five years then must be competitively renewed. Graduate students in turn compete for acceptance into a BTP; if accepted, then stipend, tuition and health insurance support is provided for two or three years during the course of their Ph.D. thesis work. Nineteen institutions offer NIGMS supported BTPs. Biotechnology training is also offered at the undergraduate level and in community colleges.

</doc>
