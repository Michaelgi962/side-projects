<doc id="5712" url="http://en.wikipedia.org/wiki?curid=5712" title="Cumin">
Cumin

Cumin ( or , ; sometimes spelled cummin; Cuminum cyminum) is a flowering plant in the family Apiaceae, native from the east Mediterranean to India. Its seeds (each one contained within a fruit, which is dried) are used in the cuisines of many different cultures, in both whole and ground form.
Etymology.
The English "cumin" derives from the Old English, from Latin "cuminum", which is the Latinisation of the Greek ("kyminon"), cognate with Hebrew ("kammon") and Arabic ("kammun"). The earliest attested form of the word in Greek is the Mycenaean , "ku-mi-no", written in Linear B syllabic script. Forms of this word are attested in several ancient Semitic languages, including "kamūnu" in Akkadian. The ultimate source is thought to be the Sumerian word "gamun".
Description.
Cumin is the dried seed of the herb "Cuminum cyminum", a member of the parsley family. The cumin plant grows to tall and is harvested by hand. It is an annual herbaceous plant, with a slender, glabrous, branched stem which is tall and has a diameter of 3–5 cm (–2 in). Each branch has two to three sub-branches. All the branches attain the same height, therefore the plant has a uniform canopy. The stem is coloured grey or dark green. The leaves are long, pinnate or bipinnate, with thread-like leaflets. The flowers are small, white or pink, and borne in umbels. Each umbel has five to seven umbellts. The fruit is a lateral fusiform or ovoid achene 4–5 mm (– in) long, containing two mericarps with a single seed. Cumin seeds have eight ridges with oil canals. They resemble caraway seeds, being oblong in shape, longitudinally ridged, and yellow-brown in colour, like other members of the umbelliferae family such as caraway, parsley and dill.
History.
Cumin has been in use since ancient times. Seeds excavated in India have been dated to the second millennium BC. They have also been reported from several New Kingdom levels of ancient Egyptian archaeological sites. In the ancient Egyptian civilisation cumin was used as spice and as preservative in mummification.
Originally cultivated in Iran and the Mediterranean region, cumin is mentioned in the Bible in both the Old Testament (Isaiah 28:27) and the New Testament (Matthew 23:23). The ancient Greeks kept cumin at the dining table in its own container (much as pepper is frequently kept today), and this practice continues in Morocco. Cumin was also used heavily in ancient Roman cuisine. In India, it has been used for millennia as a traditional ingredient of innumerable "kormas"," masalas", soups, and other spiced gravies.
Cumin was introduced to the Americas by Spanish and Portuguese colonists. There are several different types of cumin but the most famous ones are black and green cumin, both of which are used in Persian cuisine.
Today, the plant is mostly grown in China, Uzbekistan, Tajikistan, Iran, Turkey, Morocco, Egypt, Syria, Mexico, Chile and India. Since cumin is often used as part of birdseed and exported to many countries, the plant can occur as a rare casual in many territories including Britain. Cumin occurs as a rare casual in the British Isles, mainly in Southern England; but the frequency of its occurrence has declined greatly. According to the Botanical Society of the British Isles' most recent Atlas, only one record has been confirmed since 2000.
Cultivation and production.
Cultivation areas.
The main producer and consumer of cumin is India. It produces 70% of the world production and consumes 90% of its own production (which is 63% of the world production). Other producers are Syria (7%), Turkey (6%) and Iran (6%). The remaining 11% production is assigned to other countries. Totally, around 300,000 tons of cumin per year are produced worldwide. 2007 India produced around 175,000 tons of cumin on an area of about 410,000 ha. I.e. the average yield is 0.43 tons per hectare.
Climatic requirements.
Cumin is a drought tolerant, tropic or semi-tropic crop. Its origin is most probably Egypt, Turkmenistan and the east Mediterranean. Cumin has a short growth season of 100 – 120 days. The optimum growth temperature ranges are between 25° and 30°C. The Mediterranean climate is most suitable for its growth; cumin requires a moderately cool and dry climate. Cultivation of cumin requires a long, hot summer of three to four months. Upon low temperatures leaf colour changes from green to purple. High temperature might reduce growth period and induce early ripening. In India, Cumin is sown from October until the begin of December and harvesting starts in February. In Syria and Iran Cumin is sown from mid-November until mid-December (extensions up to mid-January are possible) and harvested in June/July.
Cultivation parameters.
Cumin is grown from seeds. The seeds need for emergence, an optimum of is suggested. Cumin is vulnerable to frost damage, especially at flowering and early seed formation stages. Methods to reduce frost damage are spraying with sulfuric acid (0.1%), irrigating the crop prior to frost incidence, setting up windbreaks or creating an early morning smoke cover. The seedlings of cumin are rather small and their vigor is low. Soaking the seeds for 8 hours before sowing enhances germination. For an optimal plant population a sowing density of is recommended. Fertile, sandy, loamy soils with good aeration, proper drainage and high oxygen availability are preferred. The pH optimum of the soil ranges from pH 6.8 to 8.3. Cumin seedlings are sensitive to salinity and emergence from heavy soils is rather difficult for cumin. Therefore a proper seedbed preparation (smooth seed bed) is crucial for optimal establishment of cumin.
Two sowing methods are used for cumin, broadcasting and line sowing. For broadcast sowing, the field is divided into beds and the seeds are uniformly broadcast in this bed. Afterwards they are covered with soil using a rake. For line sowing shallow furrows are prepared with hooks in a distance of . The seeds are afterwards placed in these furrows and covered with soil. Line sowing offers advantages for intercultural operations such as weeding, hoeing or spraying. The recommended sowing depth is 1–2 cm and the recommended sowing density is around 120 plants per square metre. The water requirements of cumin are lower than those of many other species. Despite, cumin is often irrigated after sowing to be sure that enough moisture is available for seedling development. The amount and frequency of irrigation depends on the climate conditions.
Cultivation management.
The relative humidity in the center of origin of cumin is rather low. High relative humidity (i.e. wet years) favours fungal diseases. Cumin is especially sensitive to "Alternaria" blight and "Fusarium" wilt. Early sown crops are exhibit stronger disease effects than late sown crops. The most important disease is wilt caused by "Fusarium" resulting in yield losses up to 80%. "Fusarium" is seed- or soil-borne and it requires distinct soil temperatures for development of epidemics. Inadequate fertilization might favour "Fusarium" epidemics. Cumin blight ("Alternaria") appears in the form of dark brown spots on leaves and stems. When the weather is cloudy after flowering the incidence of the disease is increased. Another, but less important disease is powdery mildew. Incidence of powdery mildew in early development can cause drastic yield losses because no seeds are formed. Later in development powdery mildew causes discoloured, small seeds.
Pathogens can lead to high reductions in crop yield. Cumin can be attacked by aphids ("Myzus persicae") at flowering stage. They suck the sap of the plant from tender parts and flowers. The plant becomes yellow, the seed formation is reduced (yield reduction) and the quality of the harvested product decreases. Heavily infested plant parts should be removed. Other important pests are the mites ("Petrobia latens") which frequently attack the crop. Since the mites mostly feed on young leaves, the infestation is more severe on young inflorescences.
The open canopy of cumin is another problem. Only a low proportion of the incoming light is absorbed. The Leaf Area Index (LAI) of cumin is low (approximately 1.5). This might be a problem because weeds can compete with cumin for essential resources such as water and light and thereby lower yield. The slow growth and a short stature of cumin favours weed competition additionally. Two hoeing and weeding sessions (30 and 60 days after sowing) are needed for the control of weeds. During the first weeding session (30 days after sowing) thinning should be done as well to remove excess plants. The use of pre-plant or pre-emergence herbicides is very effective in India. But this kind of herbicide application requires soil moisture for a successful weed control.
"Fertilization recommendations in India"
"Fertilization recommendations in Syria"
Breeding of cumin.
Cumin is a diploid species and with 14 chromosomes (i.e. 2n = 14). The chromosomes of the different varieties have morphological similarities and there is no distinct variation in length and volume. Most of the varieties available today are selections. The variabilities of yield and yield components are high. Varieties are developed by sib mating in enclosed chambers or by biotechnoloy. 
Cumin is a cross-pollinator, i.e. the breeds are already hybrids. Therefore, methods used for breeding are in vitro regenerations, DNA technologies and gene transfers. The in vitro cultivation of cumin allows the production of genetically identical plants. The main sources for the explants used in vitro regenerations are embryos, hypocotyl, shoot internodes, leaves and cotyledons. 
One goal of cumin breeding is to improve its resistance to biotic (fungal diseases) and abiotic (cold, drought, salinity) stresses. The potential genetic variability for conventional breeding of cumin is limited and research about cumin genetics is scarce.
Uses.
Cumin seeds are used as a spice for their distinctive flavour and aroma. It is globally popular and an essential flavouring in many cuisines, particularly South Asian, Northern African and Latin American cuisines. Cumin can be found in some cheeses, such as Leyden cheese, and in some traditional breads from France. It is commonly used in traditional Brazilian cuisine. Cumin can be an ingredient in chili powder (often Tex-Mex or Mexican-style), and is found in "achiote" blends, "adobos", "sofrito", "garam masala", curry powder, and "bahaarat". Cumin is also popularly used by cannabilistic tribes as part of the ritual.
Cumin can be used ground or as whole seeds. It helps to add an earthy and warming feeling to food, making it a staple in certain stews and soups, as well as spiced gravies such as chili. It is also used as an ingredient in some pickles and pastries.
Medicinal uses.
In Sanskrit, Cumin is known as "Jira". Jira means “that which helps digestion". In Ayurvedic system of medicine, dried Cumin seeds are used for medicinal purposes. The dried cumin seeds are powdered and used in different forms like "kashaya "(decoction), "arishta" (fermented decoction), "vati"(tablet/pills), and processed with" "ghee (a semi-fluid clarified butter). It is used internally and sometimes for external application also. It is known for its actions like enhancing appetite, taste perception, digestion, vision, strength, and lactation. It is used to treat diseases like fever, loss of appetite, diarrhea, vomiting,  abdominal distension, edema and puerperal disorders.
A popular drink in southern India such as Kerala and Tamil Nadu is called jira water, made by boiling cumin seeds in water. It is understood that cumin is beneficial for heart disease, swellings, tastelessness, vomiting, poor digestion and chronic fever.
Ahmad Reza Gohari and Soodabeh Saeidnia have reviewed the Phytochemistry of Cuminum cyminum seeds and its standards. They have reported many pharmacological effects such as anti-diabetic, Immunologic, anti-epileptic, anti-tumor and antimicrobial activities.' A study by researchers at Mysore University in India reports the potential anti-diabetic properties of cumin.
Efraim Lev and Zohar Amar have reported several medicinal properties and health benefits of cumin seeds. According to the authors, cumin seeds and warm jeera water is believed to improve saliva secretion, provide relief in digestive disorders.
Secondary metabolites.
Cuminaldehyde, cymene and terpenoids are the major volatile components of cumin. Results of a study conducted in India showed that cumin can be used as an antioxidant. The antioxidative potential is correlated with the phenol content of cumin. Cuminaldehyde has also antimicrobial and antifungal properties which could be shown e.g. with "Escherichia coli" and "Penicillium chrysogenum".
Nutritional value.
Although cumin seeds contain a relatively large percentage of iron, extremely large quantities of cumin would need to be consumed for it to serve as a significant dietary source (see nutrition data).
According to the USDA, one tablespoon of cumin spice contains
Confusion with other spices.
Cumin is sometimes confused with caraway ("Carum carvi"), another umbelliferous spice. Cumin, though, is hotter to the taste, lighter in colour, and larger. Many European languages do not distinguish clearly between the two. Many Slavic and Uralic languages refer to cumin as "Roman caraway". Examples include Czech: "kmín" – caraway, "římský kmín" -cumin; Polish: "kminek" – caraway, "kmin rzymski" – cumin; Hungarian: "kömény" – caraway, "római kömény" – cumin. Finnish: "kumina" – caraway, "roomankumina" – cumin, although sometimes also called "juustokumina", cheese caraway. In Norwegian, caraway is called both "karve" and "kummin"/"kømming" while cumin is "spisskummen", from the word "spise", to eat. Similarly in Swedish and Danish, caraway is "kummin"/"kommen", while cumin is "spiskummin"/"spidskommen". In German, "Kümmel" stands for caraway and "Kreuzkümmel" denotes cumin. In Icelandic, caraway is "kúmen", while cumin is "kúmín". In Romanian, "chimen" is caraway, while "chimion" is cumin.
The distantly related "Bunium persicum" and the unrelated "Nigella sativa" are both sometimes called black cumin ("q.v.").
Aroma profile.
Cumin's distinctive flavour and strong, warm aroma are due to its essential oil content. Its main constituent aroma compounds are cuminaldehyde (a promising agent against alpha-synuclein aggregation) and cuminic alcohol. Other important aroma compounds of toasted cumin are the substituted pyrazines, 2-ethoxy-3-isopropylpyrazine, 2-methoxy-3-"sec"-butylpyrazine, and 2-methoxy-3-methylpyrazine. Other components include γ-terpinene, safranal, p-cymene and β-pinene.

</doc>
<doc id="5714" url="http://en.wikipedia.org/wiki?curid=5714" title="Cornish Nationalist Party">
Cornish Nationalist Party

The Cornish Nationalist Party (CNP), , was a political party in England, United Kingdom, led by Dr James Whetter which campaigned for independence for Cornwall. It was formed by people who left Cornwall's main nationalist party Mebyon Kernow on 28 May 1975.
A separate party with a similar name (the Cornish National Party) existed from 1969. 
The split with Mebyon Kernow was down to the same debate that was occurring in most of the political parties campaigning for autonomy from the United Kingdom at the time (for example the Scottish National Party and Plaid Cymru), whether to be a centre-left party appealing to the electorate on a social democratic line, or whether to appeal emotionally on a centre-right cultural line. Another subject of the split was whether to embrace devolution as a first step to full independence (or as the sole step if this was what the electorate wished) or for it to be "all or nothing".
The CNP essentially represented a more right wing outlook, who were not willing to accept that economic arguments were more likely to win votes than cultural. The CNP worked to preserve the identity of Cornwall and improve its economy, and encouraged links with Cornish people overseas and with other regions which have distinct identities. It also gave support to the Cornish language, and commemorated Thomas Flamank, a leader of the Cornish Rebellion in 1497, at an annual ceremony at Bodmin on 27 June each year.
While the CNP were not a racist organisation there was a perceived image problem relating to the similarly-styled British National Party (BNP). Today, the Cornish Nationalist Party is seen as more of a pressure group as they do not put up candidates for any elections, although their visibility and influence within Cornwall is negligible. , they are not registered on the UK political parties register; Mebyon Kernow are the only registered political party based in Cornwall. A news story appeared in April 2009 claiming that the CNP had reformed following a conference in Bodmin, however it did not contest any elections that year.
Dr Whetter and the CNP still publish a quarterly journal, "The Cornish Banner" ("An Baner Kernewek"). This is done within the actions of the Roseland Institute.

</doc>
<doc id="5715" url="http://en.wikipedia.org/wiki?curid=5715" title="Cryptanalysis">
Cryptanalysis

Cryptanalysis (from the Greek "kryptós", "hidden", and "analýein", "to loosen" or "to untie") is the study of analyzing information systems in order to study the hidden aspects of the systems. Cryptanalysis is used to breach cryptographic security systems and gain access to the contents of encrypted messages, even if the cryptographic key is unknown. 
In addition to mathematical analysis of cryptographic algorithms, cryptanalysis also includes the study of side-channel attacks that do not target weaknesses in the cryptographic algorithms themselves, but instead exploit weaknesses in their implementation.
Even though the goal has been the same, the methods and techniques of cryptanalysis have changed drastically through the history of cryptography, adapting to increasing cryptographic complexity, ranging from the pen-and-paper methods of the past, through machines like the British Bombes and Colossus computers at Bletchley Park in World War II, to the mathematically advanced computerized schemes of the present. Methods for breaking modern cryptosystems often involve solving carefully constructed problems in pure mathematics, the best-known being integer factorization.
Overview.
Given some encrypted data (""ciphertext""), the goal of the "cryptanalyst" is to gain as much information as possible about the original, unencrypted data (""plaintext"").
Amount of information available to the attacker.
Attacks can be classified based on what type of information the attacker has available. As a basic starting point it is normally assumed that, for the purposes of analysis, the general algorithm is known; this is Shannon's Maxim "the enemy knows the system"—in its turn, equivalent to Kerckhoffs' principle. This is a reasonable assumption in practice — throughout history, there are countless examples of secret algorithms falling into wider knowledge, variously through espionage, betrayal and reverse engineering. (And on occasion, ciphers have been reconstructed through pure deduction; for example, the German Lorenz cipher and the Japanese Purple code, and a variety of classical schemes).:
Computational resources required.
Attacks can also be characterised by the resources they require. Those resources include:
It's sometimes difficult to predict these quantities precisely, especially when the attack isn't practical to actually implement for testing. But academic cryptanalysts tend to provide at least the estimated "order of magnitude" of their attacks' difficulty, saying, for example, "SHA-1 collisions now 252."
Bruce Schneier notes that even computationally impractical attacks can be considered breaks: "Breaking a cipher simply means finding a weakness in the cipher that can be exploited with a complexity less than brute force. Never mind that brute-force might require 2128 encryptions; an attack requiring 2110 encryptions would be considered a break...simply put, a break can just be a certificational weakness: evidence that the cipher does not perform as advertised."
Partial breaks.
The results of cryptanalysis can also vary in usefulness. For example, cryptographer Lars Knudsen (1998) classified various types of attack on block ciphers according to the amount and quality of secret information that was discovered:
Academic attacks are often against weakened versions of a cryptosystem, such as a block cipher or hash function with some rounds removed. Many, but not all, attacks become exponentially more difficult to execute as rounds are added to a cryptosystem, so it's possible for the full cryptosystem to be strong even though reduced-round variants are weak. Nonetheless, partial breaks that come close to breaking the original cryptosystem may mean that a full break will follow; the successful attacks on DES, MD5, and SHA-1 were all preceded by attacks on weakened versions.
In academic cryptography, a "weakness" or a "break" in a scheme is usually defined quite conservatively: it might require impractical amounts of time, memory, or known plaintexts. It also might require the attacker be able to do things many real-world attackers can't: for example, the attacker may need to choose particular plaintexts to be encrypted or even to ask for plaintexts to be encrypted using several keys related to the secret key. Furthermore, it might only reveal a small amount of information, enough to prove the cryptosystem imperfect but too little to be useful to real-world attackers. Finally, an attack might only apply to a weakened version of cryptographic tools, like a reduced-round block cipher, as a step towards breaking of the full system.
History of cryptanalysis.
Cryptanalysis has coevolved together with cryptography, and the contest can be traced through the history of cryptography—new ciphers being designed to replace old broken designs, and new cryptanalytic techniques invented to crack the improved schemes. In practice, they are viewed as two sides of the same coin: in order to create secure cryptography, you have to design against possible cryptanalysis.
Successful cryptanalysis has undoubtedly influenced history; the ability to read the presumed-secret thoughts and plans of others can be a decisive advantage. For example, in England in 1587, Mary, Queen of Scots was tried and executed for treason as a result of her involvement in three plots to assassinate Elizabeth I of England. The plans came to light after her coded correspondence with fellow conspirators was deciphered by Thomas Phelippes.
In World War I, the breaking of the Zimmermann Telegram was instrumental in bringing the United States into the war. In World War II, the Allies benefitted enormously from their joint success cryptanalysis of the German ciphers — including the Enigma machine and the Lorenz cipher — and Japanese ciphers, particularly 'Purple' and JN-25. 'Ultra' intelligence has been credited with everything between shortening the end of the European war by up to two years, to determining the eventual result. The war in the Pacific was similarly helped by 'Magic' intelligence. 
Governments have long recognized the potential benefits of cryptanalysis for intelligence, both military and diplomatic, and established dedicated organizations devoted to breaking the codes and ciphers of other nations, for example, GCHQ and the NSA, organizations which are still very active today. In 2004, it was reported that the United States had broken Iranian ciphers. (It is unknown, however, whether this was pure cryptanalysis, or whether other factors were involved:).
Classical ciphers.
Although the actual word ""cryptanalysis"" is relatively recent (it was coined by William Friedman in 1920), methods for breaking codes and ciphers are much older. The first known recorded explanation of cryptanalysis was given by 9th-century Arabian polymath, Al-Kindi (also known as "Alkindus" in Europe), in "A Manuscript on Deciphering Cryptographic Messages". This treatise includes a description of the method of frequency analysis (Ibrahim Al-Kadi, 1992- ref-3). Italian scholar Giambattista della Porta was author of a seminal work on cryptanalysis ""De Furtivis Literarum Notis"."
Frequency analysis is the basic tool for breaking most classical ciphers. In natural languages, certain letters of the alphabet appear more frequently than others; in English, "E" is likely to be the most common letter in any sample of plaintext. Similarly, the digraph "TH" is the most likely pair of letters in English, and so on. Frequency analysis relies on a cipher failing to hide these statistics. For example, in a simple substitution cipher (where each letter is simply replaced with another), the most frequent letter in the ciphertext would be a likely candidate for "E". Frequency analysis of such a cipher is therefore relatively easy, provided that the ciphertext is long enough to give a reasonably representative count of the letters of the alphabet that it contains.
In Europe during the 15th and 16th centuries, the idea of a polyalphabetic substitution cipher was developed, among others by the French diplomat Blaise de Vigenère (1523–96). For some three centuries, the Vigenère cipher, which uses a repeating key to select different encryption alphabets in rotation, was considered to be completely secure ("le chiffre indéchiffrable"—"the indecipherable cipher"). Nevertheless, Charles Babbage (1791–1871) and later, independently, Friedrich Kasiski (1805–81) succeeded in breaking this cipher. During World War I, inventors in several countries developed rotor cipher machines such as Arthur Scherbius' Enigma, in an attempt to minimise the repetition that had been exploited to break the Vigenère system.
Ciphers from World War I and World War II.
Cryptanalysis of enemy messages played a significant part in the Allied victory in World War II. F. W. Winterbotham, quoted the western Supreme Allied Commander, Dwight D. Eisenhower, at the war's end as describing Ultra intelligence as having been "decisive" to Allied victory. Sir Harry Hinsley, official historian of British Intelligence in World War II, made a similar assessment about Ultra, saying that it shortened the war "by not less than two years and probably by four years"; moreover, he said that in the absence of Ultra, it is uncertain how the war would have ended.
In practice, frequency analysis relies as much on linguistic knowledge as it does on statistics, but as ciphers became more complex, mathematics became more important in cryptanalysis. This change was particularly evident before and during World War II, where efforts to crack Axis ciphers required new levels of mathematical sophistication. Moreover, automation was first applied to cryptanalysis in that era with the Polish Bomba device, the British Bombe, the use of punched card equipment, and in the Colossus computers — the first electronic digital computers to be controlled by a program.
Indicator.
With reciprocal machine ciphers such as the Lorenz cipher and the Enigma machine used by Nazi Germany during World War II, each message had its own key. Usually, the transmitting operator informed the receiving operator of this message key by transmitting some plaintext and/or ciphertext before the enciphered message. This is termed the "indicator", as it indicates to the receiving operator how to set his machine to decipher the message.
Poorly designed and implemented indicator systems allowed first the Poles and then the British at Bletchley Park to break the Enigma cipher system. Similar poor indicator systems allowed the British to identify "depths" that led to the diagnosis of the Lorenz SZ40/42 cipher system, and the comprehensive breaking of its messages without the cryptanalysts seeing the cipher machine.
Depth.
Sending two or more messages with the same key is an insecure process. To a cryptanalyst the messages are then said to be ""in depth"". This may be detected by the messages having the same "indicator" by which the sending operator informs the receiving operator about the key generator initial settings for the message.
Generally, the cryptanalyst may benefit from lining up identical enciphering operations among a set of messages. For example the Vernam cipher enciphers by bit-for-bit combining plaintext with a long key using the "exclusive or" operator, which is also known as "modulo-2 addition" (symbolized by ⊕ ):
Deciphering combines the same key bits with the ciphertext to reconstruct the plaintext:
(In modulo-2 arithmetic, addition is the same as subtraction.) When two such ciphertexts are aligned in depth, combining them eliminates the common key, leaving just a combination of the two plaintexts: 
The individual plaintexts can then be worked out linguistically by trying "probable words" (or phrases) at various locations; a correct guess, when combined with the merged plaintext stream, produces intelligible text from the other plaintext component:
The recovered fragment of the second plaintext can often be extended in one or both directions, and the extra characters can be combined with the merged plaintext stream to extend the first plaintext. Working back and forth between the two plaintexts, using the intelligibility criterion to check guesses, the analyst may recover much or all of the original plaintexts. (With only two plaintexts in depth, the analyst may not know which one corresponds to which ciphertext, but in practice this is not a large problem.) When a recovered plaintext is then combined with its ciphertext, the key is revealed: 
Knowledge of a key of course allows the analyst to read other messages encrypted with the same key, and knowledge of a set of related keys may allow cryptanalysts to diagnose the system used for constructing them.
The development of modern cryptography.
Even though computation was used to great effect in Cryptanalysis of the Lorenz cipher and other systems during World War II, it also made possible new methods of cryptography orders of magnitude more complex than ever before. Taken as a whole, modern cryptography has become much more impervious to cryptanalysis than the pen-and-paper systems of the past, and now seems to have the upper hand against pure cryptanalysis. The historian David Kahn notes: 
Kahn goes on to mention increased opportunities for interception, bugging, side channel attacks, and quantum computers as replacements for the traditional means of cryptanalysis. In 2010, former NSA technical director Brian Snow said that both academic and government cryptographers are "moving very slowly forward in a mature field."
However, any postmortems for cryptanalysis may be premature. While the effectiveness of cryptanalytic methods employed by intelligence agencies remains unknown, many serious attacks against both academic and practical cryptographic primitives have been published in the modern era of computer cryptography:
Thus, while the best modern ciphers may be far more resistant to cryptanalysis than the Enigma, cryptanalysis and the broader field of information security remain quite active.
Cryptanalysis of asymmetric ciphers.
Asymmetric cryptography (or public key cryptography) is cryptography that relies on using two keys; one private, and one public. Such ciphers invariably rely on "hard" mathematical problems as the basis of their security, so an obvious point of attack is to develop methods for solving the problem. The security of two-key cryptography depends on mathematical questions in a way that single-key cryptography generally does not, and conversely links cryptanalysis to wider mathematical research in a new way.
Asymmetric schemes are designed around the (conjectured) difficulty of solving various mathematical problems. If an improved algorithm can be found to solve the problem, then the system is weakened. For example, the security of the Diffie-Hellman key exchange scheme depends on the difficulty of calculating the discrete logarithm. In 1983, Don Coppersmith found a faster way to find discrete logarithms (in certain groups), and thereby requiring cryptographers to use larger groups (or different types of groups). RSA's security depends (in part) upon the difficulty of integer factorization — a breakthrough in factoring would impact the security of RSA.
In 1980, one could factor a difficult 50-digit number at an expense of 1012 elementary computer operations. By 1984 the state of the art in factoring algorithms had advanced to a point where a 75-digit number could be factored in 1012 operations. Advances in computing technology also meant that the operations could be performed much faster, too. Moore's law predicts that computer speeds will continue to increase. Factoring techniques may continue to do so as well, but will most likely depend on mathematical insight and creativity, neither of which has ever been successfully predictable. 150-digit numbers of the kind once used in RSA have been factored. The effort was greater than above, but was not unreasonable on fast modern computers. By the start of the 21st century, 150-digit numbers were no longer considered a large enough key size for RSA. Numbers with several hundred digits were still considered too hard to factor in 2005, though methods will probably continue to improve over time, requiring key size to keep pace or other methods such as elliptic curve cryptography to be used.
Another distinguishing feature of asymmetric schemes is that, unlike attacks on symmetric cryptosystems, any cryptanalysis has the opportunity to make use of knowledge gained from the public key.
Quantum computing applications for cryptanalysis.
Quantum computers, which are still in the early phases of research, have potential use in cryptanalysis. For example, Shor's Algorithm could factor large numbers in polynomial time, in effect breaking some commonly used forms of public-key encryption.
By using Grover's algorithm on a quantum computer, brute-force key search can be made quadratically faster. However, this could be countered by doubling the key length.

</doc>
<doc id="5716" url="http://en.wikipedia.org/wiki?curid=5716" title="Chicano">
Chicano

The terms Chicano or Chicana (also spelled Xicano or Xicana) is a chosen identity of Mexican-Americans in the United States. The term "Chicano" is interchangeable with Mexican-American. Both names are chosen identities within the Mexican-American community in the United States. However, these terms have a wide range of meanings in various parts of the Southwest. The term became widely used during the Chicano Movement, mainly among Mexicans-Americans who wanted to express an identity, of cultural, ethnic and community pride.
The term "Chicano" had negative connotations before the Chicano Movement, and still is viewed negatively by more conservative members of this community, but it is gaining more acceptance as an identity of pride within the Mexican-American community in the United States. As people from Mexico could be of any ethnic origin or race, including European (mostly Spanish), the pro indigenous/mestizo Chicano identity, movement or ideology is not necessarily supported by and/or representative in many cases, the interest of all people who consider themselves as being of "Mexican" descent.
Etymology.
The origin of the word "chicano" is disputed. Some critics claim it is a shorterned form of ""Mexicano"" (the Nahuatl name for a member of the Mexica pueblo). The word "Mexico" as spoken in its original Nahuatl, and by the Spaniards at the time of the conquest, was pronounced originally with a "sh" sound ("Meh-shee-co") and was transcribed with an "x" during this time period. The difference between the pronunciation and spelling of "chicano" and "mexicano" stems from the fact that the modern-day Spanish language experienced a change in pronunciation regarding a majority of words containing the "x" (for example: México, Ximenez, Xavier, Xarabe). In most cases the "sh" sound has been replaced with the "he" sound ("Meh-he-co") and a change of spelling ("x" to "j"). The word "Chicano" was also affected by this change. Many Chicanos replace the "ch" with the letter "x" as Xicanos, due to the original spelling of the Mexica Empire (pronounced: Meshica).
In Mexico's indigenous regions, ladinos (mestizos) and westernized natives are referred to as "mexichanos," referring to the modern nation, rather than the pueblo identification of the speaker, be it Mayan, Zapotec, Mixtec, Huasteco, or the hundreds of other native pueblos. Thus, a newly emigrated Nahuatl speaker would have referred to his cultural relatives in this country, different than himself, as "mexicanos," shortened to "chicano."
Derived from "mexhicano," "chicano" thus has its roots in the identification of the Mexica, also known as the Aztecs. "Mexica" is the name given members of the Aztec who settled in the Valley of Mexico (Anahuac).
The Chicano poet and writer Tino Villanueva traces the first documented use of the term to 1911, as referenced in a then-unpublished essay by University of Texas anthropologist José Limón. Linguists Edward R. Simmen and Richard F. Bauerle report the use of the term in an essay by Mexican American writer, Mario Suárez, published in the "Arizona Quarterly" in 1947.
Mexican Americans were not identified as a racial/ethnic category prior to the 1980 US Census, when the term "Hispanic" was first used in census reports.
In 1857 there was a mention of a sale of the gun boat, “Chicana” being sold to Jose Maria Carvajal to ship arms on the Rio Grande. The King and Kenedy firm submitted a voucher to the Joint Claims Commission of the United States in 1870 to cover the costs to convert the “Chicana” from a river steamer to a “gunboat”.
Some believe that the word ""chicamo"" somehow became ""chicano"", which (unlike ""chicamo"") reflects the grammatical conventions of Spanish-language ethno- and demonyms, such as ""americano"" or ""castellano"" or ""peruano"". However, Chicanos generally do not agree that "chicamo" was ever a word used within the culture, as its assertion is thus far entirely unsubstantiated. Therefore, most Chicanos do not agree that "Chicano" was ever derived from the word "chicamo". There is ample literary evidence to further substantiate that "Chicano" is a self-declaration, as a large body of Chicano literature exists with publication dates far predating the 1950s. There is also a substantial body of Chicano literature that predates both Raso and the Federal Census Bureau.
As stated in the Handbook of Texas:
Thus far, the origins of the word remain inconclusive, as the term is not used outside Mexican-American communities, further indicating that the term is primarily self-identifying.
Meanings.
The term's meanings are highly debatable, but self-described Chicanos view the term as a positive self-identifying social construction. Outside of Mexican-American communities, the term has been considered pejorative and takes on subjective view but usually consists of one or more of the following elements.
Ethnic identity.
From a popular perspective, the term "Chicano" became widely visible outside of Chicano communities during the American civil rights movement. It was commonly used during the mid-1960s by Mexican-American activists, who, in attempt to reassert their civil rights, tried to rid the word of its polarizing negative connotation by reasserting a unique ethnic identity and political consciousness, proudly identifying themselves as "Chicanos".
The term "chicano" may have came from the word Mexica pronounced (Me-shi-ca) or from the Guanajuato indios the Chichimecas combined with the word Mexicano .
Others believe it was a corrupted term of "Chilango", meaning an inhabitant from Mexico City or Central Mexico (i.e. the highland states of Mexico (state), Sinaloa, Jalisco, Puebla (state) and Michoacán); and even from the term "Chileno" by the Chilean presence in mid 19th-century California, when miners from Chile arrived in the California Gold Rush (1848–51 .
Political identity.
According to the Handbook of Texas:
At certain points in the 1970s, ""Chicano"" was the preferred term for reference to Mexican-Americans, particularly in the scholarly literature. However, as the term became politicized, its use fell out of favor as a means of referring to the entire population. Since then, ""Chicano"" has tended to refer to politicized Mexican-Americans.
Sabine Ulibarri, an author from Tierra Amarilla, New Mexico, once attempted to note that ""Chicano"" was a politically "loaded" term, although Ulibarri has recanted that assessment. "Chicano" is considered to be a positive term of honor by many.
Ambiguous identity.
The identity is seen as uncertain:
For Chicanos, the term usually implies being "neither from here, nor from there" in reference to the US and Mexico. As a mixture of cultures from both countries, being Chicano represents the struggle of being accepted into the Anglo-dominated society of the United States, while maintaining the cultural sense developed as a Latino-cultured, US-born Mexican child.
Indigenous identity.
The identity is seen as native to the land:
One theory is the origin of such terminology is from the Mayan temple "Chichen Itza" in the Yucatán Peninsula, a ruin of an ancient MesoAmerican civilization about 1,500 years ago. "Chicano" may be a Hispanized word for ""Chichen"" or the Mayan descendants, not limited to Aztec descendants or Nahuatl people.
But essentially Chicanos, like some Mexicans, are Mestizo Indians who were influenced by the Spanish/European culture through conquest, while Latino or Hispanic refers to race/genetics. Therefore Latinos are Americans who are descendants of the Latin group (namely Spain and Portugal) and Hispanic refers to the descendants from the Iberian Peninsula (also known as Ibero-American particularly Spain and Portugal).
Political device.
Reies Tijerina was a vocal claimant to the rights of Hispanics and Mexican Americans, and he remains a major figure of the early Chicano Movement.
Historic origins.
The earliest known record of "Chicano" was thought to be in the late 19th or turn of the 20th century in Northwest Indiana. Mexican factory workers and railroad crews first arrived in the area and used the term among themselves, probably to mean working man. But in the Spanish language, the words "Chic"o (small) -"a-no" (man) stands for "the little people".
Term of derision.
Long a disparaging term in Mexico, the term "Chicano" gradually transformed from a class-based term of derision to one of ethnic pride and general usage within Mexican-American communities, beginning with the rise of the Chicano movement in the 1960s. In their "Latinas in the United States: A Historical Encyclopedia", Vicki Ruíz and Virginia Sánchez report that demographic differences in the adoption of the term existed; because of the prior vulgar connotations, it was more likely to be used by males than females, and as well, less likely to be used among those in a higher socioeconomic status. Usage was also generational, with the more assimilated third-generation members (again, more likely male) likely to adopt the usage. This group was also younger, of more radical persuasion, and less-connected to a Mexican cultural heritage.
In his essay "Chicanismo" in "The Oxford Encyclopedia of Mesoamerican Cultures" (2002), Jose Cuellar dates the transition from derisive to positive to the late 1950s, with a usage by young Mexican-American high school students.
Outside of Mexican American communities, the term might assume a negative meaning if it is used in a manner that embodies the prejudices and bigotries long directed at Mexican and Mexican-American people in the United States. For example, in one case, a prominent Chicana feminist writer and poet has indicated the following subjective meaning through her creative work.
Ana Castillo has referred to herself as a Chicana, and her literary work reflects that she primarily considers the term to be a positive one of self-determination and political solidarity.
The Mexican archeologist and anthropologist Manuel Gamio reported in 1930 that the term ""chicamo"" (with an "m") was used as a derogatory term used by Hispanic Texans for recently arrived Mexican immigrants displaced during the Mexican revolution in the beginning of the early 20th century. At this time, the term "Chicano" began to reference those who resisted total assimilation, while the term "Pochos" referred (often pejoratively) to those who strongly advocated assimilation.
In Mexico, which by American standards would be considered classist or racist, the term is associated with a Mexican-American person of low importance class and poor morals. The term "Chicano" is widely known and used in Mexico.
While some Mexican-Americans may embrace the term "Chicano", others prefer to identify themselves as:
When it comes to the use of loanwords, pay attention to the fact that unlike e.g. French, Iberian orthographies do not use uppercase for non-name nouns, such as those used for nationalities or ethnic groups, of whatever sort – even the originally English neologism theme for this article is best written with lowercase as in formal orthography of Spanish and other languages such as Portuguese and Catalan.
Some of them might be used more commonly in English and others in Spanish: e.g. one says to be a Mexican in a mixed American context, in which English would generally be expected, and to be of the white segment of the ethnic Mexican population in a Mexican and Mexican-American context, in which one can use their own vernacular language, in the case Spanish.
Anyone from the United States is referred in Spanish as or , given how in many languages other than English, Romance languages conserved the former standard formerly shared with English of counting the entire New World as a single continent – as was the consensus in the Age of Discovery – and to Spanish- and Portuguese-speakers in the Americas, they are just as American (wider sense) as someone from Belgium would be European (geological validation of the current English norm is bound by controversies and potential inconsistency, so the best explanation for both cases is indeed mere tradition).
Social aspects.
Chicanos, regardless of their generational status, tend to connect their culture to the indigenous peoples of North America and to a nation of Aztlán. According to the Aztec legend, Aztlán is a region; Chicano nationalists have equated it with the Southwestern United States.
Some historians may place Aztlán in Nayarit or the Caribbean while other historians entirely disagree, and make a distinction between legend and the contemporary socio-political ideology.
Political aspects.
Many currents came together to produce the revived Chicano political movement of the 1960s and 1970s. Early struggles were against school segregation, but the Mexican-American cause, or ""La Causa"" as it was called, soon came under the banner of the United Farm Workers and César Chávez. However, Corky Gonzales and Reies Tijerina stirred up old tensions about New Mexican land claims with roots going back to before the Mexican-American War. Simultaneous movements like the Young Lords, to empower youth, question patriarchy, democratize the Church, end police brutality, and end the Vietnam War, all intersected with other ethnic nationalist, peace, countercultural, and feminist movements.
Sim Chicanismo covers a wide array of political, religious and ethnic beliefs, and not everybody agrees with what exactly a Chicano is, most new Latino immigrants see it as a lost cause, as a lost culture, because Chicanos do not identify with Mexico or wherever their parents migrated from as new immigrants do. Chicanoism is an appreciation of a historical movement, but also is used by many to bring a new revived politicized feeling to voters young and old in the defense of Mexican and Mexican-American rights. People descended from Aztlan (both in the contemporary U.S. and in Mexico) use the Chicano ideology to create a platform for fighting for immigration reform and equality for all people.
Rejection of borders.
For some, Chicano ideals involve a rejection of borders. The 1848 Treaty of Guadalupe Hidalgo transformed the Rio Grande region from a rich cultural center to a rigid border poorly enforced by the United States government. At the end of the Mexican-American War, 80,000 Spanish-Mexican-Indian people were forced into sudden U.S. habitation. As a result, Chicano identification is aligned with the idea of Aztlán, which extends to the Aztec period of Mexico, celebrating a time preceding land division.
Paired with the dissipation of militant political efforts of the Chicano movement in the 1960s was the emergence of the Chicano generation. Like their political predecessors, the Chicano generation rejects the "immigrant/foreigner" categorization status. Chicano identity has expanded from its political origins to incorporate a broader community vision of social integration and nonpartisan political participation.
The shared Spanish language, Catholic faith, close contact with their political homeland (Mexico) to the south, a history of labor segregation, ethnic exclusion and racial discrimination encourage a united "Chicano" or Mexican folkloric tradition in the United States. Ethnic cohesiveness is a resistance strategy to assimilation and the accompanying cultural dissolution.
Mexican nationalists in Mexico, however, condemn the advocates of Chicanoism for attempting to create a new identity for the Mexican American population, distinct from that of the Mexican nation.
Cultural aspects.
The term "Chicano" is also used to describe the literary, artistic, and musical movements that emerged with the Chicano Movement.
Literature.
Chicano literature tends to focus on themes of identity, discrimination, and culture, with an emphasis on validating Mexican American and Chicano culture in the United States. Rodolfo "Corky" Gonzales's "Yo Soy Joaquin" is one of the first examples of Chicano poetry, while José Antonio Villarreal's "Pocho" is widely recognized as the first major Chicano/a novel. The novel, "Chicano" by Richard Vasquez, was the first novel about Mexican-Americans to be released by a major publisher (Doubleday, 1970).
It was widely read in high schools and Universities during the 1970s, and has now been recognized as a literary classic. Vasquez's writing has been compared to Upton Sinclair and John Steinbeck. Other important writers include Rudolfo Anaya, Sandra Cisneros, Gary Soto, Sergio Troncoso, Rigoberto González, Raul Salinas, Oscar Zeta Acosta, Daniel Olivas, John Rechy, Ana Castillo, Denise Chávez, Benjamin Alire Sáenz, Luis Alberto Urrea, Dagoberto Gilb, Alicia Gaspar de Alba, Luis J. Rodriguez and Gloria Anzaldua.
Visual arts.
In the visual arts, works by Chicanos address similar themes as works in literature. The preferred media for Chicano art are murals and graphic arts. San Diego's Chicano Park, home to the largest collection of murals in the world, was created as an outgrowth of the city's political movement by Chicanos. Rasquache art is a unique style subset of the Chicano Arts movement.
Chicano art emerged in the mid-60s as a necessary component to the urban and agarian civil rights movement in the Southwest, known as "la causa chicana", or the Chicano Renaissance. The artistic spirit, based on historical and traditional cultural evolution, within the movement has continued into the present millennium. There are artists, for example, who have chosen to do work within ancestral/historical references or who have mastered traditional techniques. Some artists and crafters have transcended the motifs, forms, functions, and context of Chicano references in their work but still acknowledge their identity as Chicano. These emerging artists are incorporating new materials to present mixed-media, digital media, and transmedia works.
Chicano performance art blends humor and pathos for tragi-comic effect as shown by Los Angeles' comedy troupe Culture Clash and Mexican-born performance artist Guillermo Gomez-Pena and Nao Bustamante is a Chicana Artist known internationally for her conceptual art pieces and as a participant in Work of Art: The next Great Artist produced by Sarah Jessica Parker. Lalo Alcaraz often depicts the issues of Chicanos in his cartoons called "La Cucaracha".
One of the most powerful and far-reaching cultural aspects of Chicano culture is the indigenous current that strongly roots Chicano culture to the American continent. It also unifies Chicanismo within the larger "Pan Indian Movement". Since its arrival in 1974, what is known as "Danza Azteca" in the U.S., (and known by several names in its homeland of the central States of Mexico: danza Conchera, De la Conquista, Chichimeca, and so on.) has had a deep impact in Chicano muralism, graphic design, tattoo art (flash), poetry, music, and literature. Lowrider cars also figure prominently as functional art in the Chicano community.
Music.
Lalo Guerrero is regarded as the "father of Chicano music". Beginning in the 1930s, he wrote songs in the big band and swing genres that were popular at the time. He expanded his repertoire to include songs written in traditional genres of the Mexican music, and during the farmworkers' rights campaign, wrote music in support of César Chávez and the United Farm Workers.
Jeffrey Lee Pierce of The Gun Club often spoke about being half Mexican and growing up with the Chicano culture.
Other Chicano/Mexican American singers include Selena, who sang a variety of Mexican, Tejano, and American popular music, but was killed in 1995 at the age of 23; Zack de la Rocha, lead vocalist of Rage Against the Machine and social activist; and Los Lonely Boys, a Texas style country rock band who have not ignored their Mexican American roots in their music. In recent years, a growing Tex-Mex polka band trend from Mexican immigrants (i.e. Conjunto or Norteño) has influenced much of new Chicano folk music, especially in large market Spanish language radio stations and on television music video programs in the U.S. The band Quetzal is known for its political songs.
Rock.
In the 1950s, 1960s and 1970s, a wave of Chicano pop music surfaced through innovative musicians Johnny Rodriguez, Ritchie Valens and Linda Ronstadt. Joan Baez, who was also of Mexican-American descent, included Hispanic themes in some of her protest folk songs. Chicano rock is rock music performed by Chicano groups or music with themes derived from Chicano culture.
There are two undercurrents in Chicano rock. One is a devotion to the original rhythm and blues roots of Rock and roll including Ritchie Valens, Sunny and the Sunglows, and ? and the Mysterians. Groups inspired by this include Sir Douglas Quintet, Thee Midniters, Los Lobos, War, Tierra, and El Chicano, and, of course, the Chicano Blues Man himself, the late Randy Garribay.
The second theme is the openness to Latin American sounds and influences. Trini Lopez, Santana, Malo, Azteca, Toro, Ozomatli and other Chicano Latin Rock groups follow this approach. Chicano rock crossed paths of other Latin rock genres (Rock en espanol) by Cubans, Puerto Ricans, such as Joe Bataan, and Ralphi Pagan and South America (La Nueva Cancion). Rock band The Mars Volta combines elements of progressive rock with traditional Mexican folk music and Latin rhythms along with Cedric Bixler-Zavala's Spanglish lyrics.
Chicano punk is a branch of Chicano rock. Examples of the genre include music by the bands The Zeros, Los Illegals, The Brat, The Plugz, Manic Hispanic, Los Crudos, The Casualties, and the Cruzados; these bands emerged from the California punk scene. Some music historians argue that Chicanos of Los Angeles in the late 1970s might have independently co-founded punk rock along with the already-acknowledged founders from British-European sources when introduced to the US in major cities. The rock band ? and the Mysterians, which was composed primarily of Mexican American musicians, was the first band to be described as punk rock. The term was reportedly coined in 1971 by rock critic Dave Marsh in a review of their show for Creem magazine.
Jazz.
Although Latin Jazz is most popularly associated with artists from the Caribbean (particularly Cuba) and Brazil, young Mexican Americans have played a role in its development over the years, going back to the 1930s and early 1940s, the era of the zoot suit, when young Mexican American musicians in Los Angeles and San Jose began to experiment with banda, a Jazz-like Mexican music that has grown recently in popularity among Mexican Americans such as Jenni Rivera.
Rap.
Chicano rap is a unique style of hip hop music which started with Kid Frost, who saw some mainstream exposure in the early 1990s. While Mellow Man Ace was the first mainstream rapper to use Spanglish, Frost's song "La Raza" paved the way for its use in American hip hop. Chicano rap tends to discuss themes of importance to young urban Chicanos. Some of today's Chicano artists include A.L.T., Lil Rob, Psycho Realm, Sick Symphonies, Street Platoon, El Vuh, Baby Bash, Serio, and Lighter Shade Of Brown as well as A.K.A. Down Kilo with "Definition of an Ese" which denotes a historical account of Chicano popularity in Southern California .
Pop and R&B.
Paula DeAnda, Frankie J, Victor Ivan Santos, old member of the Kumbia Kings and associated with Baby Bash.

</doc>
<doc id="5717" url="http://en.wikipedia.org/wiki?curid=5717" title="Canary Islands">
Canary Islands

The Canary Islands (; , ), also known as "the Canaries" (), are a Spanish archipelago located just off the northwest coast of mainland Africa, west of the southern border of Morocco. The Canaries are one of Spain's 17 autonomous communities and are among the outermost region of the European Union proper. The islands include (from largest to smallest): Tenerife, Fuerteventura, Gran Canaria, Lanzarote, La Palma, La Gomera, El Hierro, La Graciosa, Alegranza, Isla de Lobos, Montaña Clara, Roque del Este and Roque del Oeste.
The archipelago's beaches, climate and important natural attractions, especially Maspalomas in Gran Canaria and Teide National Park and Mount Teide in Tenerife (the third tallest volcano in the world measured from its base on the ocean floor), make it a major tourist destination with over 12 million visitors per year, especially Tenerife, Fuerteventura, Gran Canaria and Lanzarote. The islands have a subtropical climate, with long warm summers and moderately warm winters. Due to their location above the temperature inversion layer, the high mountains of these islands are ideal for astronomical observation. For this reason, two professional observatories, Teide Observatory on the island of Tenerife and Roque de los Muchachos Observatory on the island of La Palma, have been built on the islands.
The capital of the Autonomous Community is shared by the cities of Santa Cruz de Tenerife and Las Palmas de Gran Canaria, which in turn are the capitals of the provinces of Santa Cruz de Tenerife and Province of Las Palmas. Las Palmas de Gran Canaria has been the largest city in the Canaries since 1768, except for a brief period in 1910. Between the 1833 territorial division of Spain and 1927 Santa Cruz de Tenerife was the sole capital of the Canary Islands. In 1927 a decree ordered that the capital of the Canary Islands be shared, as it remains at present. The third largest city of the Canary Islands is La Laguna (a World Heritage Site) on Tenerife.
During the times of the Spanish Empire the Canaries were the main stopover for Spanish galleons on their way to the Americas because of the prevailing winds from the northeast.
Etymology.
The name "Islas Canarias" is likely derived from the Latin name "Canariae Insulae", meaning "Island of the Dogs", a name applied originally only to Gran Canaria. According to the historian Pliny the Elder, the Mauretanian king Juba II named the island "Canaria" because it contained "vast multitudes of dogs of very large size".
Another speculation is that the so-called dogs were actually a species of monk seal ("canis marinus" or "sea dog" was a Latin term for 'seal'), critically endangered and no longer present in the Canary Islands. The dense population of seals may have been the characteristic that most struck the few ancient Romans who established contact with these islands by sea.
Alternatively, it is said that the original inhabitants of the island, Guanches, used to worship dogs, mummified them and treated dogs generally as holy animals. The ancient Greeks also knew about a people, living far to the west, who are the "dog-headed ones", who worshipped dogs on an island. Some hypothesize that the Canary Islands dog-worship and the ancient Egyptian cult of the dog-headed god, Anubis are closely connected but there is no explanation given as to which one was first.
Other theories speculate that the name comes from a reported Berber tribe living in the Moroccan Atlas, named in Roman sources as "Canarii", though Pliny again mentions the relation of this term with dogs.
The connection to dogs is retained in their depiction on the islands' coat-of-arms (shown above).
What is certain is that the name of the islands does not derive from the canary bird; rather, the birds are named after the islands.
Geography.
Tenerife is the most populous island, and also the largest island of the archipelago. Gran Canaria, with 865,070 inhabitants, is both the Canary Islands' second most populous island, and the third most populous one in Spain after Majorca. The island of Fuerteventura is the second largest in the archipelago and located from the African coast.
The islands form the Macaronesia ecoregion with the Azores, Cape Verde, Madeira, and the Savage Isles. The archipelago consists of seven large and several smaller islands, all of which are volcanic in origin. The Teide volcano on Tenerife is the highest mountain in Spain, and the third tallest volcano on Earth on a volcanic ocean island. All the islands except La Gomera have been active in the last million years; four of them (Lanzarote, Tenerife, La Palma and El Hierro) have historical records of eruptions since European discovery. The islands rise from Jurassic oceanic crust associated with the opening of the Atlantic. Underwater magmatism commenced during the Cretaceous, and reached the ocean's surface during the Miocene. The islands are considered as a distinct physiographic section of the Atlas Mountains province, which in turn is part of the larger African Alpine System division.
In the summer of 2011 a series of low magnitude earthquakes occurred beneath El Hierro. These had a linear trend of northeast-southwest. In October a submarine eruption occurred about south of Restinga. This eruption produced gases and pumice but no explosive activity was reported.
According to the position of the islands with respect to the north-east trade winds, the climate can be mild and wet or very dry. Several native species form laurisilva forests.
As a consequence, the individual islands in the canary archipelago tend to have distinct microclimates. Those islands such as El Hierro, La Palma and La Gomera lying to the west of the archipelago have a climate which is influenced by the moist gulf stream. They are well vegetated even at low levels and have extensive tracts of sub-tropical laurisilva forest. As one travels east toward the African coast, the influence of the gulf stream diminishes, and the islands become increasingly arid. Fuerteventura and Lanzarote the islands which are closest to the African mainland are effectively desert or semi desert. Gran Canaria is known as a "continent in miniature" for its diverse landscapes like Maspalomas and Roque Nublo. In terms of its climate Tenerife is particularly interesting. The north of the island lies under the influence of the moist Atlantic winds and is well vegetated, while the south of the island around the tourist resorts of Playa de las Americas and Los Cristianos is arid. The island rises to almost 4000 m above sea level, and at altitude, in the cool relatively wet climate, forests of the endemic pine "Pinus canariensis" thrive. Many of the plant species in the Canary Islands, like the Canary Island pine and the dragon tree, "Dracaena draco" are endemic, as noted by Sabin Berthelot and Philip Barker Webb in their epic work, "L'Histoire Naturelle des Îles Canaries" (1835–50).
Four of Spain's thirteen national parks are located in the Canary Islands, more than any other autonomous community. Teide National Park is the most visited in Spain, and the oldest and largest within the Canary Islands. The parks are:
The following table shows the highest mountains in each of the islands:
Geology.
The originally volcanic islands –seven major islands, one minor island, and several small islets– were formed by the Canary hotspot. The Canary Islands is the only place in Spain where volcanic eruptions have been recorded during the Modern Era, with some volcanoes still active (El Hierro, 2011).
Volcanic islands such as the those in the Canary chain often have steep ocean cliffs caused by catastrophic debris avalanches and landslides.
Political geography.
The Autonomous Community of the Canary Islands consists of two provinces, Las Palmas and Santa Cruz de Tenerife, whose capitals (Las Palmas de Gran Canaria and Santa Cruz de Tenerife) are capitals of the autonomous community. Each of the seven major islands is ruled by an island council named "Cabildo Insular".
The international boundary of the Canaries is the subject of dispute between Spain and Morocco. Morocco's official position is that international laws regarding territorial limits do not authorise Spain to claim seabed boundaries based on the territory of the Canaries, since the Canary Islands enjoy a high degree of autonomy. 
The boundary determines the ownership of seabed oil deposits and other ocean resources. Morocco and Spain have therefore been unable to agree on a compromise regarding the territorial boundary, since neither nation wants to cede its claimed right to the vast resources whose ownership depends upon the boundary. In 2002, for example, Morocco rejected a unilateral Spanish proposal.
The Islands have 13 seats in the Spanish Senate. Of these, 11 seats are directly elected, 3 for Gran Canaria, 3 for Tenerife, 1 for each other island; 2 seats are indirectly elected by the regional Autonomous Government. The local government is presided over by Paulino Rivero, the current President of the Canary Islands.
History.
Ancient and pre-colonial times.
Before the arrival of the aborigines, the Canaries were inhabited by prehistoric animals; for example, the giant lizard ("Gallotia goliath"), or giant rats ("Canariomys bravoi" and "Canariomys tamarani").
The islands were visited by the Phoenicians, the Greeks, and the Carthaginians. According to the 1st century AD Roman author and philosopher Pliny the Elder, the archipelago was found to be uninhabited when visited by the Carthaginians under Hanno the Navigator, but that they saw ruins of great buildings. This story may suggest that the islands were inhabited by other peoples prior to the Guanches. King Juba, Augustus's Numidian protégé, is credited with discovering the islands for the Western world. He dispatched a naval contingent to re-open the dye production facility at Mogador in what is now western Morocco in the early 1st century AD. That same naval force was subsequently sent on an exploration of the Canary Islands, using Mogador as their mission base.
The Romans called each of the islands; "Ninguaria" or "Nivaria" (Tenerife), "Canaria" (Gran Canaria), "Pluvialia" or "Invale" (Lanzarote), "Ombrion" (La Palma), "Planasia" (Fuerteventura), "Iunonia" or "Junonia" (El Hierro) and "Capraria" (La Gomera).
When the Europeans began to explore the islands, they encountered several indigenous populations living at a Neolithic level of technology. Although the history of the settlement of the Canary Islands is still unclear, linguistic and genetic analyses seem to indicate that at least some of these inhabitants shared a common origin with the Berbers of northern Africa. The pre-colonial inhabitants came to be known collectively as the Guanches, although "Guanches" was originally the name for the indigenous inhabitants of Tenerife. From the 14th century onward, numerous visits were made by sailors from Majorca, Portugal and Genoa. Lancelotto Malocello settled on Lanzarote in 1312. The Majorcans established a mission with a bishop in the islands that lasted from 1350 to 1400.
Castilian conquest.
There may have been a Portuguese expedition that attempted to colonize the islands as early as 1336, but there is not enough hard evidence to support this. In 1402, the Castilian conquest of the islands began, with the expedition of French explorers Jean de Béthencourt and Gadifer de la Salle, nobles and vassals of Henry III of Castile, to Lanzarote. From there, they conquered Fuerteventura (1405) and El Hierro. Béthencourt received the title King of the Canary Islands, but still recognized King Henry III as his overlord.
Béthencourt also established a base on the island of La Gomera, but it would be many years before the island was truly conquered. The natives of La Gomera, and of Gran Canaria, Tenerife, and La Palma, resisted the Castilian invaders for almost a century. In 1448 Maciot de Béthencourt sold the lordship of Lanzarote to Portugal's Prince Henry the Navigator, an action that was not accepted by the natives nor by the Castilians. Despite Pope Nicholas V ruling that the Canary Islands were under Portuguese control, a crisis swelled to a revolt which lasted until 1459 with the final expulsion of the Portuguese. In 1479, Portugal and Castile signed the Treaty of Alcáçovas. The treaty settled disputes between Castile and Portugal over the control of the Atlantic, in which Castilian control of the Canary Islands was recognized but which also confirmed Portuguese possession of the Azores, Madeira, the Cape Verde islands and gave them rights to lands discovered and to be discovered...and any other island which might be found and conquered from the Canary islands beyond toward Guinea.
The Castilians continued to dominate the islands, but due to the topography and the resistance of the native Guanches, complete pacification was not achieved until 1495, when Tenerife and La Palma were finally subdued by Alonso Fernández de Lugo. After that, the Canaries were incorporated into the Kingdom of Castile.
After the conquest.
After the conquest, the Castilians imposed a new economic model, based on single-crop cultivation: first sugar cane; then wine, an important item of trade with England. In this era, the first institutions of colonial government were founded. Both Gran Canaria, a colony of Castile since March 6, 1480 (from 1556, of Spain), and Tenerife, a Spanish colony since 1495, had separate governors.
The cities of Santa Cruz de Tenerife and Las Palmas de Gran Canaria became a stopping point for the Spanish conquerors, traders, and missionaries on their way to the New World. This trade route brought great prosperity to some of the social sectors of the islands. The islands became quite wealthy and soon were attracting merchants and adventurers from all over Europe. Magnificent palaces and churches were built on La Palma during this busy, prosperous period. The Church of El Salvador survives as one of the island's finest examples of the architecture of the 16th century.
The Canaries' wealth invited attacks by pirates and privateers. Ottoman Turkish admiral and privateer Kemal Reis ventured into the Canaries in 1501, while Murat Reis the Elder captured Lanzarote in 1585.
The most severe attack took place in 1599, during the Dutch War of Independence. A Dutch fleet of 74 ships and 12,000 men, commanded by Pieter van der Does, attacked the capital Las Palmas de Gran Canaria (the city had 3,500 of Gran Canaria's 8,545 inhabitants). The Dutch attacked the Castillo de la Luz, which guarded the harbor. The Canarians evacuated civilians from the city, and the Castillo surrendered (but not the city). The Dutch moved inland, but Canarian cavalry drove them back to Tamaraceite, near the city.
The Dutch then laid siege to the city, demanding the surrender of all its wealth. They received 12 sheep and 3 calves. Furious, the Dutch sent 4,000 soldiers to attack the Council of the Canaries, who were sheltering in the village of Santa Brígida. 300 Canarian soldiers ambushed the Dutch in the village of Monte Lentiscal, killing 150 and forcing the rest to retreat. The Dutch concentrated on Las Palmas de Gran Canaria, attempting to burn it down. The Dutch pillaged Maspalomas, on the southern coast of Gran Canaria, San Sebastian on La Gomera, and Santa Cruz on La Palma, but eventually gave up the siege of Las Palmas de Gran Canaria and withdrew.
In 1618 the Algerian pirates attacked Lanzarote and La Gomera taking 1000 captives to be sold as slaves. Another noteworthy attack occurred in 1797, when Santa Cruz de Tenerife was attacked by a British fleet under the future Lord Nelson on 25 July. The British were repulsed, losing almost 400 men. It was during this battle that Nelson lost his right arm.
18th to 19th century.
The sugar-based economy of the islands faced stiff competition from Spain's American colonies. Low prices in the sugar market in the 19th century caused severe recessions on the islands. A new cash crop, cochineal ("cochinilla"), came into cultivation during this time, saving the islands' economy.
By the end of the 18th century, Canary Islanders had already emigrated to Spanish American territories, such as Havana, Veracruz, Santo Domingo, San Antonio, Texas and St. Bernard Parish, Louisiana. These economic difficulties spurred mass emigration, primarily to the Americas, during the 19th and first half of the 20th century. Between 1840 and 1890 as many as 40,000 Canary Islanders emigrated to Venezuela. Also, thousands of Canarians moved to Puerto Rico where the Spanish monarchy felt that Canarians would adapt to island life better than other immigrants from the mainland of Spain. Deeply entrenched traditions, such as the Mascaras Festival in the town of Hatillo, Puerto Rico, are an example of Canarian culture still preserved in Puerto Rico. Similarly, many thousands of Canarians emigrated to the shores of Cuba. During the Spanish-American War of 1898, the Spanish fortified the islands against possible American attack, but an attack never came.
Romantic period and scientific expeditions.
Sirera and Renn (2004) distinguish two different types of expeditions, or voyages, during the period 1770–1830, which they term "the Romantic period":
First are "expeditions financed by the States, closely related with the official scientific Institutions. characterized by having strict scientific objectives (and inspired by) the spirit of Illustration and progress".
In this type of expedition, Sirera and Renn include the following travelers:
The second type of expedition identified by Sirera and Renn is one that took place starting from more or less private initiatives. Among these, the key exponents were the following:
Sirera and Renn identify the period 1770–1830 as one in which "In a panorama dominated until that moment by France and England enters with strength and brio Germany of the Romantic period whose presence in the islands will increase".
Early 20th century.
At the beginning of the 20th century, the British introduced a new cash-crop, the banana, the export of which was controlled by companies such as Fyffes.
The rivalry between the elites of the cities of Las Palmas de Gran Canaria and Santa Cruz de Tenerife for the capital of the islands led to the division of the archipelago into two provinces in 1927. This has not laid to rest the rivalry between the two cities, which continues to this day.
During the time of the Second Spanish Republic, Marxist and anarchist workers' movements began to develop, led by figures such as Jose Miguel Perez and Guillermo Ascanio. However, outside of a few municipalities, these organizations were a minority and fell easily to Nationalist forces during the Spanish Civil War.
Franco regime.
In 1936, Francisco Franco was appointed General Commandant of the Canaries. He joined the military revolt of July 17 which began the Spanish Civil War. Franco quickly took control of the archipelago, except for a few points of resistance on La Palma and in the town of Vallehermoso, on La Gomera. Though there was never a proper war in the islands, the post-war repression on the Canaries was most severe.
During the Second World War, Winston Churchill prepared plans for the British seizure of the Canary Islands as a naval base, in the event of Gibraltar being invaded from the Spanish mainland.
Opposition to Franco's regime did not begin to organize until the late 1950s, which experienced an upheaval of parties such as the Communist Party of Spain and the formation of various nationalist, leftist parties.
Self-governance.
After the death of Franco, there was a pro-independence armed movement based in Algeria, the MPAIAC. Now there are some pro-independence political parties, like the CNC and the Popular Front of the Canary Islands, but none of them calls for an armed struggle. Their popular support is almost insignificant, with no presence in either the autonomous parliament or the "cabildos insulares".
After the establishment of a democratic constitutional monarchy in Spain, autonomy was granted to the Canaries via a law passed in 1982. In 1983, the first autonomous elections were held. The Spanish Socialist Workers' Party (PSOE) won. In the 2007 elections, the PSOE gained a plurality of seats, but the nationalist Canarian Coalition and the conservative Partido Popular (PP) formed a ruling coalition government.
According to "Centro de Investigaciones Sociológicas" (Sociological Research Center) in 2010, 43.5% of the population of the Canary Islands feels more Canarian than Spanish (37.6%), only Canarian (7.6%), compared to 5.4% that feels more Spanish than Canarian (2.4%) or only Spanish (3%). The most popular choice of those who feel equally Spanish and Canarian, with 49.9%. With these data, one of the Canary recorded levels of identification with higher autonomy from Spain.
Demographics.
The Canary Islands have a population of 2,117,519 inhabitants (2011), making it the eighth most populous of Spain's autonomous communities, with a density of 282.6 inhabitants per km². The total area of the archipelago is .
The Canarian population includes long-tenured residents and new waves of mainland Spanish immigrants (including Galicians, Castilians, Catalans, Basques), as well as Portuguese, Italians, Flemings and Britons. Of the total Canarian population in 2009 (2,098,593) 1,799,373 were Spanish (1,547,611 native Canarian and 178,613 from the Spanish mainland) and 299,220 foreigners. Of these, the majority are Europeans (55%), including Germans (39,505), British (37,937) and Italians (24,177). There are also 86,287 inhabitants from the Americas, mainly Colombians (21,798), Venezuelans (11,958), Cubans (11,098) and Argentines (10,159). There are also 28,136 African residents, mostly Moroccans (16,240).
Population of the individual islands.
The population of the islands according the 2010 data are:
Religion.
The overwhelming majority of native Canarians are Roman Catholic with various smaller foreign-born populations of other Christian beliefs such as Protestants from northern Europe. Separate from the overwhelming Christian majority are a minority of Muslims, though no official (http://canaryislandsguide.net/pages/religion.html) mention is made of them. Other religious faiths represented include The Church of Jesus Christ of Latter-day Saints as well as Hinduism. Minority religions are also present such as the Church of the Guanche People which is classified as a neo-pagan religion. The appearance of the Virgin of Candelaria (Patron of Canary Islands) was credited with moving the Canary Islands toward Christianity.
The Canary Islands are divided into two Catholic dioceses, each governed by a bishop:
Islands.
The islands are arranged alphabetically:
El Hierro.
El Hierro, the westernmost island, covers , making it the smallest of the major islands, and the least populous with 10,753 inhabitants. The whole island was declared Reserve of the Biosphere in 2000. Its capital is Valverde. Also known as Ferro, it was once believed to be the westernmost land in the world.
Fuerteventura.
Fuerteventura, with a surface of , is the second-most extensive island of the archipelago. It has been declared a Biosphere reserve by Unesco. It has a population of 100,929. Being also the most ancient of the islands, it is the one that is more eroded: its highest point is the Peak of the Bramble, at a height of . Its capital is Puerto del Rosario.
Gran Canaria.
Gran Canaria has 845,676 inhabitants. The capital, Las Palmas de Gran Canaria (377,203 inhabitants), is the most populous city and shares the status of capital of the Canaries with Santa Cruz de Tenerife. Gran Canaria's surface area is . In center of the island lie the Roque Nublo and Pico de las Nieves ("Peak of Snow") . In the south of island are the Maspalomas Dunes (Gran Canaria), these are the biggest tourist attractions.
La Gomera.
La Gomera, has an area of and is the second least populous island with 22,622 inhabitants. Geologically it is one of the oldest of the archipelago. The insular capital is San Sebastian de La Gomera. Garajonay's National Park is here.
Lanzarote.
Lanzarote, is the easternmost island and one of the most ancient of the archipelago, and it has shown evidence of recent volcanic activity. It has a surface of , and a population of 139,506 inhabitants, including the adjacent islets of the Chinijo Archipelago. The capital is Arrecife, with 56,834 inhabitants.
Chinijo Archipelago.
The Chinijo Archipelago includes the islands La Graciosa, Alegranza, Montaña Clara, Roque del Este and Roque del Oeste. It has a surface of , and a population of 658 inhabitants all of them in the la Graciosa island. With , La Graciosa, is the smallest inhabited island of the Canaries, and the major island of the Chinijo Archipelago.
La Palma.
La Palma, with 86,528 inhabitants, covering an area of is in its entirety a biosphere reserve. It shows no recent signs of volcanic activity, even though the volcano Teneguía entered into eruption last in 1971. In addition, it is the second-highest island of the Canaries, with the Roque de los Muchachos as highest point. Santa Cruz de La Palma (known to those on the island as simply "Santa Cruz") is its capital.
Tenerife.
Tenerife is, with its area of , the most extensive island of the Canary Islands. In addition, with 906,854 inhabitants it is the most populated island of the archipelago and Spain. Two of the islands' principal cities are located on it: The capital, Santa Cruz de Tenerife and San Cristóbal de La Laguna (a World Heritage Site). San Cristóbal de La Laguna, the second city of the island is home to the oldest university in the Canary Islands, the University of La Laguna. The Teide, with its is the highest peak of Spain and also a World Heritage Site.
Economy.
The economy is based primarily on tourism, which makes up 32% of the GDP. The Canaries receive about 12 million tourists per year. Construction makes up nearly 20% of the GDP and tropical agriculture, primarily bananas and tobacco, are grown for export to Europe and the Americas. Ecologists are concerned that the resources, especially in the more arid islands, are being overexploited but there are still many agricultural resources like tomatoes, potatoes, onions, cochineal, sugarcane, grapes, vines, dates, oranges, lemons, figs, wheat, barley, maize, apricots, peaches and almonds.
The economy is € 25 billion (2001 GDP figures). The islands experienced continuous growth during a 20-year period, up until 2001, at a rate of approximately 5% annually. This growth was fueled mainly by huge amounts of Foreign Direct Investment, mostly to develop tourism real estate (hotels and apartments), and European Funds (near €11 billion euro in the period from 2000 to 2007), since the Canary Islands are labelled Region Objective 1 (eligible for euro structural funds). Additionally, the EU allows the Canary Islands Government to offer special tax concessions for investors who incorporate under the Zona Especial Canaria (ZEC) regime and create more than 5 jobs.
The Canary Islands have great natural attractions, climate and beaches make the islands a major tourist destination, being visited each year by about 12 million people (11,986,059 in 2007, noting 29% of Britons, 22% of Spanish, not residents of the Canaries, and 21% of Germans). Among the islands, Tenerife has the largest number of tourists received annually, followed by Gran Canaria and Lanzarote. The archipelago's principal tourist attraction is the Teide National Park (in Tenerife) where the highest mountain in Spain and third largest volcano in the world (Mount Teide), receives over 2.8 million visitors annually.
The combination of high mountains, proximity to Europe, and clean air has made the Roque de los Muchachos peak (on La Palma island) a leading location for telescopes like the Grantecan.
The islands are outside the European Union customs territory and VAT area, though politically within the EU. Instead of VAT there is a local Sales Tax (IGIC) which has a general rate of 7%, an increased tax rate of 13.5%, a reduced tax rate of 3% and a zero tax rate for certain basic need products and services.
Canarian time is Western European Time (WET) (or GMT; in summer one hour ahead of GMT). So Canarian time is one hour behind that of mainland Spain and the same as that of the UK, Ireland and Portugal all year round.
Transport.
The Canary Islands have eight airports altogether, two of the main ports of Spain, and an extensive network of autopistas (highways) and other roads. For a road map see multimap.
There are large ferry boats that link islands as well as fast ferries linking most of the islands. Both types can transport large numbers of passengers and cargo (including vehicles). Fast ferries are made of aluminium and powered by modern and efficient diesel engines, while conventional ferries have a steel hull and are powered by heavy oil. Fast ferries travel relatively quickly (in excess of 30 knots) and are a faster method of transportation than the conventional ferry (some 20 knots). A typical ferry ride between La Palma and Tenerife may take up to eight hours or more while a fast ferry takes about 2 and a half hours and between Tenerife and Gran Canaria can be about one hour.
The largest airport is the Gran Canaria airport, with about 10,000,000 passengers. It is also the 5th largest airport in Spain. The biggest port is in Las Palmas de Gran Canaria. It is an important port for commerce with Europe, Africa and the Americas. It is the 4th biggest commercial port in Spain with more than 1,400,000 TEU's. The largest commercial companies of the world, including MSC and Maersk, operate here. In this port there is an international post of the Red Cross, one of only four points like this all around the world. Tenerife has two airports, Tenerife North Airport (4,048,281 passengers) and Tenerife South Airport (6,939,168 passengers).
Canary Islands has an input of 16,874,532 passengers. The two main islands (Tenerife and Gran Canaria) receive the greatest number of passengers; Tenerife 6,204,499 passengers and Gran Canaria 5,011,176 passengers.
The port of Las Palmas is first in freight traffic in the islands, while the port of Santa Cruz de Tenerife is the first fishing port with approximately 7,500 tons of fish caught, according to the Spanish government publication Statistical Yearbook of State Ports. Similarly, it is the second port in Spain as regards ship traffic, only surpassed by the Port of Algeciras Bay. The port's facilities include a border inspection post (BIP) approved by the European Union, which is responsible for inspecting all types of imports from third countries or exports to countries outside the European Economic Area. The port of Los Cristianos (Tenerife) has the greatest number of passengers recorded in the Canary Islands, followed by the port of Santa Cruz de Tenerife. The Port of Las Palmas is the third port in the islands in passengers and first in number of vehicles transported.
Rail transport.
The Tenerife Tram opened in 2007 and the only one in the Canary Islands, travelling between the cities of Santa Cruz de Tenerife and San Cristóbal de La Laguna. It is currently planned to have three lines in the Canary Islands (two in Tenerife and one in Gran Canaria). Tenerife trains travel north and south on the island, connecting the cities of Santa Cruz (capital) and Costa Adeje in Adeje (south), and the cities of Santa Cruz and Los Realejos (north). The planned Gran Canaria tram route will be from Las Palmas de Gran Canaria to Maspalomas (south).
Wildlife.
The official symbols from nature associated with Canary Islands are the bird "Serinus canaria" (Canary) and the "Phoenix canariensis" palm.
Terrestrial wildlife.
With a range of habitats, the Canary Islands exhibit diverse plant species. The bird life includes European and African species, such as the Black-bellied Sandgrouse; and a rich variety of endemic (local) taxa including the:
Terrestrial fauna includes geckos (such as the striped Canary Islands Gecko) and wall lizards, and three endemic species of recently rediscovered and critically endangered giant lizard: the El Hierro Giant Lizard (or Roque Chico de Salmor Giant Lizard), La Gomera Giant Lizard, and La Palma Giant Lizard. Mammals include the Canarian Shrew, Canary Big-Eared Bat, the Algerian Hedgehog (which may have been introduced) and the more recently introduced Mouflon. Some endemic mammals, the Lava Mouse, Tenerife Giant Rat and Gran Canaria Giant Rat, are extinct, as are the Canary Islands Quail, Long-legged Bunting, and the Eastern Canary Islands Chiffchaff.
Marine life.
The marine life found in the Canary Islands is also varied, being a combination of North Atlantic, Mediterranean and endemic species. In recent years, the increasing popularity of both scuba diving and underwater photography have provided biologists with much new information on the marine life of the islands.
Fish species found in the islands include many species of shark, ray, moray eel, bream, jack, grunt, scorpionfish, triggerfish, grouper, goby, and blenny. In addition, there are many invertebrate species, including sponge, jellyfish, anemone, crab, mollusc, sea urchin, starfish, sea cucumber and coral.
There are a total of 5 different species of marine turtle that are sighted periodically in the islands, the most common of these being the endangered loggerhead sea turtle. The other four are the green sea turtle, hawksbill sea turtle, leatherback sea turtle and Kemp's ridley sea turtle. Currently, there are no signs that any of these species breed in the islands, and so those seen in the water are usually migrating. However, it is believed that some of these species may have bred in the islands in the past, and there are records of several sightings of leatherback sea turtle on beaches in Fuerteventura, adding credibility to the theory.
Marine mammals include the Short-Finned Pilot Whale, Common and Bottlenose dolphins. Hooded Seals have also been known to be vagrant in the Canary Islands every now and then. The Canary Islands were also formerly home to a population of the rarest Pinniped in the world, the Mediterranean Monk Seal.
National parks of the Canary Islands.
The Canary Islands officially has four national parks, of which two have been declared World Heritage Site by UNESCO, and the other two declared a World Biosphere Reserve, these national parks are:
Sports.
A unique form of wrestling known as Canarian wrestling ("lucha canaria") has opponents stand in a special area called a "terrero" and try to throw each other to the ground using strength and quick movements.
Another sport is the "game of the sticks" where opponents fence with long sticks. This may have come about from the shepherds of the islands who would challenge each other using their long walking sticks.
Another sport is called the shepherd's jump ("salto del pastor"). This involves using a long stick to vault over an open area. This sport possibly evolved from the shepherd's need to occasionally get over an open area in the hills as they were tending their sheep.
The two main football teams in the archipelago are: the CD Tenerife (founded in 1912) and UD Las Palmas (founded in 1949).
Carnival.
The Carnival of Santa Cruz de Tenerife and Carnival of Las Palmas are one of the most famous Carnivals in Spain. It is celebrated on the streets between the months of February and March.

</doc>
<doc id="5718" url="http://en.wikipedia.org/wiki?curid=5718" title="Chuck D">
Chuck D

Carlton Douglas Ridenhour (born August 1, 1960), better known by his stage name Chuck D, is an American rapper, author, and producer. He helped create politically and socially conscious rap music in the mid-1980s as the leader of the rap group Public Enemy. About.com ranked him #9 on their list of the Top 50 MCs of Our Time, while "The Source" ranked him #12 on their list of the Top 50 Hip-Hop Lyricists of All Time.
Early life.
Ridenhour was born in Queens, New York. After graduating from Roosevelt Junior-Senior High School, he went to Adelphi University on Long Island to study graphic design. He is the son of Lorenzo Ridenhour.
Career.
Upon hearing Ridenhour's demo track "Public Enemy Number One", fledgling producer/upcoming music-mogul Rick Rubin insisted on signing him to his Def Jam label.
Their major label albums were "Yo! Bum Rush the Show" (1987), "It Takes a Nation of Millions to Hold Us Back" (1988), "Fear of a Black Planet" (1990), "Apocalypse 91... The Enemy Strikes Black" (1991), "Greatest Misses" (1992), and "Muse Sick-n-Hour Mess Age" (1994). They also released a full length album soundtrack for the film "He Got Game" in 1998. Ridenhour also contributed (as Chuck D) to several episodes of the PBS documentary series "The Blues". He has appeared as a featured artist on many other songs and albums, having collaborated with artists such as Janet Jackson, Kool Moe Dee, The Dope Poet Society, Run-DMC, Ice Cube, Boom Boom Satellites, Rage Against the Machine, Anthrax, John Mellencamp and many others. In 1990, he appeared on "Kool Thing", a song by the alternative rock band Sonic Youth, and along with Flavor Flav, he sang on George Clinton's song "Tweakin'", which appears on his 1989 album "The Cinderella Theory". In 1993, he executive produced "Got 'Em Running Scared", an album by Ichiban Records group Chief Groovy Loo and the Chosen Tribe.
Later career.
In 1996, Ridenhour released "Autobiography of Mistachuck" on Mercury Records. Chuck D made a rare appearance at the 1998 MTV Video Music Awards, presenting the Video Vanguard Award to the Beastie Boys, whilst commending their musicianship. In November 1998, he settled out of court with Christopher "The Notorious B.I.G." Wallace's estate over the latter's sampling of his voice in the song "Ten Crack Commandments". The specific sampling is Ridenhour counting off the numbers one to nine on the track "Shut 'Em Down".
In September 1999, he launched a multi-format "supersite" on the web site Rapstation.com. A home for the vast global hip hop community, the site boasts a TV and radio station with original programming, many of hip hop's most prominent DJs, celebrity interviews, free MP3 downloads (the first was contributed by multi-platinum rapper Coolio), downloadable ringtones by ToneThis, social commentary, current events, and regular features on turning rap careers into a viable living. Since 2000, he has been one of the most vocal supporters of peer-to-peer file sharing in the music industry.
He loaned his voice to ' as DJ Forth Right MC for the radio station Playback FM. In 2000, he collaborated with Public Enemy's Gary G-Whiz and MC Lyte on the theme music to the television show "Dark Angel". He appeared with Henry Rollins in a cover of Black Flag's "Rise Above" for the album '. He was also featured on Z-Trip's album "Shifting Gears" on a track called "Shock and Awe"; a 12-inch of the track was released featuring artwork by Shepard Fairey. In 2008 he contributed a chapter to "Sound Unbound: Sampling Digital Music and Culture" (The MIT Press, 2008) edited by Paul D. Miller a.k.a. DJ Spooky, and also turned up on The Go! Team's album "Proof of Youth" on the track "Flashlight Fight." He also fulfilled his childhood dreams of being a sports announcer by performing the play-by-play commentary in the video game "NBA Ballers: Chosen One" on Xbox 360 and PlayStation 3.
In 2009, Ridenhour wrote the foreword to the book "The Love Ethic: The Reason Why You Can't Find and Keep Beautiful Black Love" by Kamau and Akilah Butler. He also appeared on Brother Ali's album, "Us".
In March 2011, Chuck D re-recorded vocals with The Dillinger Escape Plan for a cover of "Fight the Power".
Chuck D duetted with Rock singer Meat Loaf on his 2011 album "Hell in a Handbasket" on the song "Mad Mad World/The Good God Is a Woman and She Don't Like Ugly".
Rapping technique and creative process.
Chuck D is known for his powerful rapping voice - "How to Rap" says, “Chuck D of Public Enemy has a powerful, resonant voice that is often acclaimed as one of the most distinct and impressive in hip-hop”. Chuck D says this was based on listening to Melle Mel and sportscasters such as Marv Albert.
Chuck D often comes up with a title for a song first and that he writes on paper, though he sometimes edits using a computer. He also prefers to not punch in vocals, and he prefers to not overdub vocals.
Politics.
Ridenhour is politically active; he co-hosted "Unfiltered" on Air America Radio, testified before Congress in support of peer-to-peer MP3 sharing, and was involved in a 2004 rap political convention. He continues to be an activist, publisher, lecturer, and producer. Addressing the negative views associated with rap music, he co-wrote the essay book "Fight the Power: Rap, Race, and Reality", along with Yusuf Jah. He argues that "music and art and culture is escapism, and escapism sometimes is healthy for people to get away from reality", but sometimes the distinction is blurred and that's when "things could lead a young mind in a direction." He also founded the record company Slam Jamz and acted as narrator in Kareem Adouard's short film "Bling: Consequences and Repercussions", which examines the role of conflict diamonds in bling fashion. Despite Chuck D and Public Enemy's success, Chuck D claims that popularity or public approval was never a driving motivation behind their work. He is admittedly skeptical of celebrity status, revealing in a 1999 interview with BOMB Magazine that, "The key for the record companies is to just keep making more and more stars, and make the ones who actually challenge our way of life irrelevant. The creation of celebrity has clouded the minds of most people in America, Europe and Asia. It gets people off the path they need to be on as individuals." 
In an interview with "Le Monde" published January 29, 2008, Chuck D stated that rap is devolving so much into a commercial enterprise, that the relationship between the rapper and the record label is that of slave to a master. He believes that nothing has changed for African-Americans since the debut of Public Enemy and, although he thinks that an Obama-Clinton alliance is great, he does not feel that the establishment will allow anything of substance to be accomplished. He also stated that French President Sarkozy is like any other European elite: he has profited through the murder, rape, and pillaging of those less fortunate and he refuses to allow equal opportunity for those men and women from Africa. In this article, he also defended a comment made by Professor Griff in the past that he says was taken out of context by the media. The real statement was a critique of the Israeli government and its treatment of the Palestinian people. Chuck D stated that it is Public Enemy's belief that all human beings are equal.
In an interview with the magazine "N'Digo" published in late June 2008, he spoke of today's mainstream urban music seemingly relishing the addictive euphoria of materialism and sexism, perhaps being the primary cause of many people harboring resentment towards the genre and its future. However he has expressed hope for its resurrection, saying "It's only going to be dead if it doesn’t talk about the messages of life as much as the messages of death and non-movement", citing artists such as NYOil, M.I.A. and the The Roots as socially conscious artists who push the envelope creatively. "A lot of cats are out there doing it, on the Web and all over. They’re just not placing their career in the hands of some major corporation."
Most recently Chuck D became involved in "Let Freedom Sing: The Music of the Civil Rights", a 3-CD box set from Time Life. He wrote the introduction to the liner notes and is visiting colleges across the nation discussing the significance of the set. He's also set to appear in a follow up movie called "Let Freedom Sing: The Music That Inspired the Civil Rights Movement".
In 2010, Chuck D released a track entitled "Tear Down That Wall". He said, “I talked about the wall not only just dividing the U.S. and Mexico but the states of California, New Mexico and Texas. But Arizona, it's like, come on. Now they're going to enforce a law that talks about basically racial profiling.”
He is on the board of the TransAfrica Forum a Pan African organization that works for the right of Africa, Caribbean and Latin American issues.
Personal life.
Chuck D is married to Gaye Theresa Johnson, an associate professor in the Department of Black Studies at the University of California, Santa Barbara.
He is a pescatarian.
Discography.
Chuck D.
Studio albums
Compilation albums

</doc>
<doc id="5719" url="http://en.wikipedia.org/wiki?curid=5719" title="Cutaway (filmmaking)">
Cutaway (filmmaking)

In film and video, a cutaway shot is the interruption of a continuously filmed action by inserting a view of something else. It is usually, although not always, followed by a cut back to the first shot, when the cutaway avoids a jump cut. The cutaway shot does not necessarily contribute any dramatic content of its own, but is used to help the editor assemble a longer sequence. For this reason, editors choose cutaway shots related to the main action, such as another action or object in the same location. For example, if the main shot is of a man walking down an alley, possible cutaways may include a shot of a cat on a nearby dumpster or a shot of a person watching from a window overhead.
Similarly, a cutaway scene is the interruption of a scene with the insertion of another scene, generally unrelated or only peripherally related to the original scene. The interruption is usually quick, and is usually, although not always, ended by a return to the original scene. The effect is of commentary to the original scene, frequently comic in nature.
Usage.
The most common use of cutaway shots in dramatic films is to adjust the pace of the main action, to conceal the deletion of some unwanted part of the main shot, or to allow the joining of parts of two versions of that shot. For example, a scene may be improved by cutting a few frames out of an actor's pause; a brief view of a listener can help conceal the break. Or the actor may fumble some of his lines in a group shot; rather than discarding a good version of the shot, the director may just have the actor repeat the lines for a new shot, and cut to that alternate view when necessary.
Cutaways are also used often in older horror films in place of special effects. For example, a shot of a zombie getting its head cut off may, for instance, start with a view of an axe being swung through the air, followed by a close-up of the actor swinging it, then followed by a cut back to the now severed head. George A. Romero, creator of the Dead Series, and Tom Savini pioneered effects that removed the need for cutaways in horror films.
In news broadcasting and documentary work, the cutaway is used much as it would be in fiction. On location, there is usually just one camera to film an interview, and it's usually trained on the interviewee. Often there is also only one microphone. After the interview, the interviewer will usually repeat his questions while he himself is being filmed, with pauses as they act as if to listen to the answers. These shots can be used as cutaways. Cutaways to the interviewer, called noddies, can also be used to cover cuts.
References.
Notes

</doc>
<doc id="5721" url="http://en.wikipedia.org/wiki?curid=5721" title="Coma">
Coma

In medicine, a coma (from the Greek "koma", meaning "deep sleep") is a state of unconsciousness lasting more than six hours, in which a person: cannot be awakened; fails to respond normally to painful stimuli, light, or sound; lacks a normal sleep-wake cycle; and, does not initiate voluntary actions. A person in a state of coma is described as being comatose.
A comatose person exhibits a complete absence of wakefulness and is unable to consciously feel, speak, hear, or move. For a patient to maintain consciousness, two important neurological components must function. The first is the cerebral cortex—the gray matter that covers the outer layer of the brain. The other is a structure located in the brainstem, called reticular activating system (RAS).
Injury to either or both of these components is sufficient to cause a patient to experience a coma. The cerebral cortex is a group of tight, dense, "gray matter" composed of the nucleus of the neurons whose axons then form the "white matter", and is responsible for perception, relay of the sensory input (sensation) via the thalamic pathway, and many other neurological functions, including complex thinking.
RAS, on the other hand, is a more primitive structure in the brainstem that is tightly in connection with reticular formation (RF). The RAS area of the brain has two tracts, the ascending and descending tract. Made up of a system of acetylcholine-producing neurons, the ascending track, or ascending reticular activating system (ARAS), works to arouse and wake up the brain, from the RF, through the thalamus, and then finally to the cerebral cortex. A failure in ARAS functioning may then lead to a coma.
Signs and symptoms.
Generally, a person who is unable to voluntarily open the eyes, does not have a sleep-wake cycle, is unresponsive in spite of strong tactile (painful), or verbal stimuli and who generally scores between 3 to 8 on the Glasgow Coma Scale is considered in a coma. Coma may have developed in humans as a response to injury to allow the body to pause bodily actions and heal the most immediate injuries - if at all - before waking. It therefore could be a compensatory state in which the body's expenditure of energy is not superfluous. The severity and mode of onset of coma depends on the underlying cause. For instance, severe hypoglycemia (low blood sugar) or hypercapnia (increased carbon dioxide levels in the blood) initially cause mild agitation and confusion, but progress to obtundation, stupor and finally complete unconsciousness. In contrast, coma resulting from a severe traumatic brain injury or subarachnoid hemorrhage can be instantaneous. The mode of onset may therefore be indicative of the underlying cause.
Causes.
Coma may result from a variety of conditions, including intoxication (such as drug abuse, overdose or misuse of over the counter medications, prescribed medication, or controlled substances), metabolic abnormalities, central nervous system diseases, acute neurologic injuries such as strokes or herniations, hypoxia, hypothermia, hypoglycemia or traumatic injuries such as head trauma caused by falls or vehicle collisions. It may also be deliberately induced by pharmaceutical agents during major neurosurgery, to preserve higher brain functions following brain trauma, or to save the patient from extreme pain during healing of injuries or diseases.
Forty percent of comatose states result from drug poisoning. Drugs damage or weaken the synaptic functioning in the ARAS and keep the system from properly functioning to arouse the brain. Secondary effects of drugs, which include abnormal heart rate and blood pressure, as well as abnormal breathing and sweating, may also indirectly harm the functioning of the ARAS and lead to a coma. Seizures and hallucinations have shown to also play a major role in ARAS malfunction. Given that drug poisoning causes a large portion of patients in a coma, hospitals first test all comatose patients by observing pupil size and eye movement, through the vestibular-ocular reflex.
The second most common cause of coma, which makes up about 25% of comatose patients, occurs from lack of oxygen, generally resulting from cardiac arrest. The Central Nervous System (CNS) requires a great deal of oxygen for its neurons. Oxygen deprivation in the brain, also known as hypoxia, causes neuronal extracellular sodium and calcium to decrease and intracellular calcium to increase, which harms neuron communication. Lack of oxygen in the brain also causes ATP exhaustion and cellular breakdown from cytoskeleton damage and nitric oxide production.
Twenty percent of comatose states result from the side effects of a stroke. During a stroke, blood flow to part of the brain is restricted or blocked. An ischemic stroke, brain hemorrhage, or tumor may cause such cessation of blood flow. Lack of blood to cells in the brain prevents nutrients and oxygen from getting to the neurons, and consequently causes cells to become disrupted and eventually die. As brain cells die, brain tissue continues to deteriorate, which may affect functioning of the ARAS.
The remaining 15% of comatose cases result from trauma, excessive blood loss, malnutrition, hypothermia, hyperthermia, abnormal glucose levels, and many other biological disorders.
Diagnosis.
Diagnosis of coma is simple, but diagnosing the cause of the underlying disease process is often challenging. The first priority in treatment of a comatose patient is stabilization following the basic ABCs (standing for airway, breathing, and circulation). Once a person in a coma is stable, investigations are performed to assess the underlying cause. Investigative methods are divided into physical examination findings and imaging (such as CAT scan, MRI, etc.) and special studies (EEG, etc.)
Diagnostic steps.
When an unconscious patient enters a hospital, the hospital utilizes a series of diagnostic steps to identify the cause of unconsciousness. According to Young, the following steps should be taken when dealing with a patient possibly in a coma:
Initial assessment and evaluation.
In the initial assessment of coma, it is common to gauge the level of consciousness by spontaneously exhibited actions, response to vocal stimuli ("Can you hear me?"), and painful stimuli; this is known as the AVPU (alert, vocal stimuli, painful stimuli, unresponsive) scale. More elaborate scales, such as the Glasgow Coma Scale, quantify an individual's reactions such as eye opening, movement and verbal response on a scale; Glasgow Coma Scale (GCS) is an indication of the extent of brain injury varying from 3 (indicating severe brain injury and death) to a maximum of 15 (indicating mild or no brain injury).
In those with deep unconsciousness, there is a risk of asphyxiation as the control over the muscles in the face and throat is diminished. As a result, those presenting to a hospital with coma are typically assessed for this risk ("airway management"). If the risk of asphyxiation is deemed high, doctors may use various devices (such as an oropharyngeal airway, nasopharyngeal airway or endotracheal tube) to safeguard the airway.
Physical examination findings.
Physical examination is critical after stabilization. It should include vital signs, a general portion dedicated to making observations about the patient's respiration (breathing pattern), body movements (if any), and of the patient's body habitus (physique); it should also include assessment of the brainstem and cortical function through special reflex tests such as the oculocephalic reflex test (doll's eyes test), oculovestibular reflex test (cold caloric test), nasal tickle, corneal reflex, and the gag reflex.
Vital signs in medicine are temperature (rectal is most accurate), blood pressure, heart rate (pulse), respiratory rate, and oxygen saturation. It should be easy to evaluate these vitals quickly to gain insight into a patient's metabolism, fluid status, heart function, vascular integrity, and tissue oxygenation.
Respiratory pattern (breathing rhythm) is significant and should be noted in a comatose patient. Certain stereotypical patterns of breathing have been identified including Cheyne-Stokes a form of breathing in which the patient's breathing pattern is described as alternating episodes of hyperventilation and apnea. This is a dangerous pattern and is often seen in pending herniations, extensive cortical lesions, or brainstem damage. Another pattern of breathing is apneustic breathing, which is characterized by sudden pauses of inspiration and is due to a lesion of the pons. Ataxic breathing is irregular and is due to a lesion (damage) of the medulla.
Assessment of posture and body habitus is the next step. It involves general observation about the patient's positioning. There are often two stereotypical postures seen in comatose patients. Decorticate posturing is a stereotypical posturing in which the patient has arms flexed at the elbow, and arms adducted toward the body, with both legs extended. Decerebrate posturing is a stereotypical posturing in which the legs are similarly extended (stretched), but the arms are also stretched (extended at the elbow). The posturing is critical since it indicates where the damage is in the central nervous system. A decorticate posturing indicates a lesion (a point of damage) at or above the red nucleus, whereas a decerebrate posturing indicates a lesion at or below the red nucleus. In other words, a decorticate lesion is closer to the cortex, as opposed to a decerebrate cortex that is closer to the brainstem.
Oculocephalic reflex also known as the doll's eye is performed to assess the integrity of the brainstem. Patient's eyelids are gently elevated and the cornea is visualized. The patient's head is then moved to the patient's left, to observe if the eyes stay or deviate toward the patient's right; same maneuver is attempted on the opposite side. If the patient's eyes move in a direction opposite to the direction of the rotation of the head, then the patient is said to have an intact brainstem. However, failure of both eyes to move to one side, can indicate damage or destruction of the affected side. In special cases, where only one eye deviates and the other does not, this often indicates a lesion (or damage) of the medial longitudinal fasciculus (MLF), which is a brainstem nerve tract. Caloric reflex test also evaluates both cortical and brainstem function; cold water is injected into one ear and the patient is observed for eye movement; if the patient's eyes slowly deviate toward the ear where the water was injected, then the brainstem is intact, however failure to deviate toward the injected ear indicates damage of the brainstem on that side. Cortex is responsible for a rapid nystagmus away from this deviated position and is often seen in patients who are conscious or merely lethargic.
An important part of the physical exam is also assessment of the cranial nerves. Due to the unconscious status of the patient, only a limited number of the nerves can be assessed. These include the cranial nerves number 2 (CN II), number 3 (CN III), number 5 (CN V), number 7 (CN VII), and cranial nerves 9 and 10 (CN IX, CN X). Gag reflex helps assess cranial nerves 9 and 10. Pupil reaction to light is important because it shows an intact retina, and cranial nerve number 2 (CN II); if pupils are reactive to light, then that also indicates that the cranial nerve number 3 (CN III) (or at least its parasympathetic fibers) are intact. Corneal reflex assess the integrity of cranial nerve number 7 (CN VII), and cranial nerve number 5 (CN V). Cranial nerve number 5 (CN V), and its ophthalmic branch (V1) are responsible for the afferent arm of the reflex, and the cranial nerve number 7 (CN VII) also known a facial nerve, is responsible for the efferent arm, causing contraction of the muscle orbicularis oculi resulting in closing of the eyes.
Pupil assessment is often a critical portion of a comatose examination, as it can give information as to the cause of the coma; the following table is a technical, medical guideline for common pupil findings and their possible interpretations:
Imaging and special tests findings.
Imaging basically encompasses computed tomography (CAT or CT) scan of the brain, or MRI for example, and is performed to identify specific causes of the coma, such as hemorrhage in the brain or herniation of the brain structures. Special tests such as an EEG can also show a lot about the activity level of the cortex such as semantic processing, The autonomous responses such as the Skin conductance response may also provide further insight on the patient's emotional processing.
History.
When diagnosing any neurological condition, history and examination are fundamental. History is obtained by family, friends or EMS. The Glasgow Coma Scale is a helpful system used to examine and determine the depth of coma, track patients progress and predict outcome as best as possible. In general a correct diagnosis can be achieved by combining findings from physical exam, imaging, and history components and directs the appropriate therapy.
Severity and classification.
Plum and Posner classify coma as either (1) supratentoral (above Tentorium cerebelli), (2) infratentoral (below Tentorium cerebelli), or (3) metabolic or (4) diffused. This classification is merely dependent on the position of the original damage that caused the coma, and does not correlate with severity or the prognosis.
The severity of coma impairment however is categorized into several levels. Patients may or may not progress through these levels. In the first level, the brain responsiveness lessens, normal reflexes are lost, the patient no longer responds to pain and cannot hear.
The Rancho Los Amigos Scale is a complex scale that has eight separate levels, and is often used in the first few weeks or months of coma while the patient is under closer observation, and when shifts between levels are more frequent.
Treatment.
Medical treatment.
The treatment hospitals use on comatose patients depends on both the severity and cause of the comatose state. Although the best treatment for comatose patients remains unknown, hospitals usually place comatose patients in an Intensive Care Unit (ICU) immediately. In the ICU, the hospital monitors a patient’s breathing and brain activity through CT scans. Attention must first be directed to maintaining the patient's respiration and circulation, using intubation and ventilation, administration of intravenous fluids or blood and other supportive care as needed. Once a patient is stable and no longer in immediate danger, the medical staff may concentrate on maintaining the health of patient’s physical state. The concentration is directed to preventing infections such as pneumonias, bedsores (decubitus ulcers), and providing balanced nutrition. These infections may appear from the patient not being able to move around, and being confined to the bed. The nursing staff moves the patient every 2–3 hours from side to side and depending on the state of consciousness sometimes to a chair. The goal is to move the patient as much as possible to try to avoid bedsores, atelectasis and pneumonia. Pneumonia can occur from the person’s inability to swallow leading to aspiration, lack of gag reflex or from feeding tube, (aspiration pneumonia). Physical therapy may also be used to prevent contractures and orthopedic deformities that would limit recovery for those patients who emerge from coma.
A person in a coma may become restless, or seize and need special care to prevent them from hurting themselves. Medicine may be given to calm such individuals. Patients who are restless may also try to pull on tubes or dressings so soft cloth wrist restraints may be put on. Side rails on the bed should be kept up to prevent the patient from falling.
In attempt to wake comatose patients, some hospitals treat their patients by either reversing the cause of comatose (i.e., glucose shock if low sugar), giving medication to stop brain swelling, or inducing hypothermia. Inducing hypothermia on comatose patients provides one of the main treatments for patients after suffering from cardiac arrest. In this treatment, medical personnel expose patients to “external or intravascular cooling” at 32-34 °C for 24 h.; this treatment cools patients down about 2-3 °C less than normal body temperature. In 2002, Baldursdottir and her coworkers found that in the hospital, more comatose patients survived after induced hypothermia than patients that remained at normal body temperature. For this reason, the hospital chose to continue the induced hypothermia technique for all of its comatose patients that suffered from cardiac arrest.
Emotional challenges.
Coma has a wide variety of emotional reactions from the family members of the affected patients, as well as the primary care givers taking care of the patients. Common reactions, such as desperation, anger, frustration, and denial are possible. The focus of the patient care should be on creating an amicable relationship with the family members or dependents of a comatose patient as well as creating a rapport with the medical staff.
Prognosis.
Comas can last from several days to several weeks. In more severe cases a coma may last for over five weeks, while some have lasted as long as several years. After this time, some patients gradually come out of the coma, some progress to a vegetative state, and others die. Some patients who have entered a vegetative state go on to regain a degree of awareness. Others remain in a vegetative state for years or even decades (the longest recorded period being 42 years).
The outcome for coma and vegetative state depends on the cause, location, severity and extent of neurological damage. A deeper coma alone does not necessarily mean a slimmer chance of recovery, because some people in deep coma recover well while others in a so-called milder coma sometimes fail to improve.
People may emerge from a coma with a combination of physical, intellectual and psychological difficulties that need special attention. Recovery usually occurs gradually—patients acquire more and more ability to respond. Some patients never progress beyond very basic responses, but many recover full awareness. Regaining consciousness is not instant: in the first days, patients are only awake for a few minutes, and duration of time awake gradually increases. This is unlike the situation in many movies where people who awake from comas are instantly able to continue their normal lives. In reality, the coma patient awakes sometimes in a profound state of confusion, not knowing how they got there and sometimes suffering from dysarthria, the inability to articulate any speech, and with many other disabilities.
Predicted chances of recovery are variable owing to different techniques used to measure the extent of neurological damage. All the predictions are based on statistical rates with some level of chance for recovery present: a person with a low chance of recovery may still awaken. Time is the best general predictor of a chance of recovery: after four months of coma caused by brain damage, the chance of partial recovery is less than 15%, and the chance of full recovery is very low.
The most common cause of death for a person in a vegetative state is secondary infection such as pneumonia, which can occur in patients who lie still for extended periods.
Occasionally people come out of coma after long periods of time. After 19 years in a minimally conscious state, Terry Wallis spontaneously began speaking and regained awareness of his surroundings. Similarly, Polish railroad worker Jan Grzebski woke up from a 19-year coma in 2007.
A brain-damaged man, trapped in a coma-like state for six years, was brought back to consciousness in 2003 by doctors who planted electrodes deep inside his brain. The method, called deep brain stimulation (DBS) successfully roused communication, complex movement and eating ability in the 38-year-old American man who suffered a traumatic brain injury. His injuries left him in a minimally conscious state (MCS), a condition akin to a coma but characterized by occasional, but brief, evidence of environmental and self-awareness that coma patients lack.
Comas lasting seconds to minutes result in post-traumatic amnesia (PTA) that lasts hours to days; recovery plateau occurs over days to weeks.
Comas that last hours to days result in PTA lasting days to weeks; recovery plateau occurs over months.
Comas lasting weeks result in PTA that lasts months; recovery plateau occurs over months to years.
Society and culture.
Research by Dr. Eelco Wijdicks on the depiction of comas in movies was published in Neurology in May 2006. Dr. Wijdicks studied 30 films (made between 1970 and 2004) that portrayed actors in prolonged comas, and he concluded that only two films accurately depicted the state of a coma victim and the agony of waiting for a patient to awaken: "Reversal of Fortune" (1990) and "The Dreamlife of Angels" (1998). The remaining 28 were criticized for portraying miraculous awakenings with no lasting side effects, unrealistic depictions of treatments and equipment required, and comatose patients remaining muscular and tanned.
For decades, medical personnel and others have argued, and continue to argue, to define the circumstances under which a patient is considered dead. Society places a lot of importance on the idea of “brain death” because most “industrialized countries have equated this with death of the individual”. However, according to Rady and coworkers, “human death is a singular phenomenon characterized by irreversible cessation of all vital functions (circulation, respiration, and consciousness)”. This means that death may be consisted of much more than just the brain’s inability to function. For example, although a patient may be “brain dead”, they may still be considered alive because they can still grow and even reproduce.

</doc>
<doc id="5722" url="http://en.wikipedia.org/wiki?curid=5722" title="Call of Cthulhu (role-playing game)">
Call of Cthulhu (role-playing game)

Call of Cthulhu is a horror fiction role-playing game based on H. P. Lovecraft's story of the same name and the associated Cthulhu Mythos. The game, often abbreviated as "CoC", is published by Chaosium. The game was first released in 1981, and seven editions have been published to date.
Gameplay.
The setting of "Call of Cthulhu" is a darker version of our world, based on H. P. Lovecraft's observation (from his essay, "Supernatural Horror in Literature") that "The oldest and strongest emotion of mankind is fear, and the strongest kind of fear is fear of the unknown." The original game, first published in 1981, uses mechanics from Basic Role-Playing, and is set in the 1920s, the setting of many of Lovecraft's stories. Additional settings were developed in the 1890s "Cthulhu by Gaslight" supplement, a blend of occult and Holmesian mystery and mostly set in England, and modern/1980s conspiracy with "Cthulhu Now." More recent additions include 1000 AD ("Cthulhu: Dark Ages"), 23rd century ("Cthulhu Rising") and Ancient Roman times ("Cthulhu Invictus"). The protagonists may also travel to places that are not of this earth, represented in the Dreamlands (which can be accessed through dreams as well as being physically connected to the earth), to other planets, or into the voids of space.
"Call of Cthulhu" uses the Basic Role-Playing system used by other Chaosium games (first seen in "RuneQuest"). For as long as they stay functionally healthy and sane, characters grow and develop. "Call of Cthulhu" does not use levels, but is completely skill-based, with player characters getting better with their skills by succeeding at them. They do not, however, gain "hit points" and do not become significantly harder to kill.
The players take the roles of ordinary people drawn into the realm of the mysterious: detectives, criminals, scholars, artists, war veterans, etc. Often, happenings begin innocently enough, until more and more of the workings behind the scenes are revealed. As the characters learn more of the true horrors of the world and the irrelevance of humanity, their sanity (represented by "Sanity Points", abbreviated SAN) inevitably withers away. The game includes a mechanism for determining how damaged a character's sanity is at any given point; encountering the horrific beings usually triggers a loss of SAN points. To gain the tools they need to defeat the horrors – mystic knowledge and magic – the characters may end up losing some of their sanity, though other means such as pure firepower or simply outsmarting one's opponents also exist. "Call of Cthulhu" has a reputation as a game in which it is quite common for a player character to die in gruesome circumstances or end up in a mental institution. Unlike most other role-playing games, eventual triumph of the players is not assumed.
History.
The original conception of "Call of Cthulhu" was "Dark Worlds", a game commissioned by the publisher Chaosium but never published. Sandy Petersen, now best known for his work on the "Doom" computer game, contacted them regarding writing a supplement for their popular fantasy game "RuneQuest" set in Lovecraft's Dreamlands. He took over the writing of "Call of Cthulhu", and the game was released in 1981, using a version of the Basic Role-Playing system used in "RuneQuest".
Editions.
Since Petersen's departure, continuing development of "Call of Cthulhu" has passed to Lynn Willis, who since the fifth edition has been credited as co-author. The game is now in its sixth edition, but the rules have changed little over the years. A 7th edition in development, first announced in 2012, to ship in 2013, is reported to offer much greater changes to the rules than previous revisions.
Early releases.
For those grounded in the RPG tradition, the very first release of "Call of Cthulhu" created a brand new framework for table-top gaming. Rather than the traditional format established by "Dungeons & Dragons", which often involved the characters wandering through caves or tunnels and fighting different types of monsters, Sandy Petersen introduced the concept of the "Onion Skin": Interlocking layers of information and nested clues that lead the Player Characters from seemingly minor investigations into a missing person to discovering mind-numbingly awful, global conspiracies to destroy the world. Unlike its predecessor games, "CoC" assumed that most investigators would not survive, alive or sane, and that the only safe way to deal with the vast majority of nasty things described in the rule books was to run away. A well-run "CoC" campaign should engender a sense of foreboding and inevitable doom in its players. The style and setting of the game, in a relatively modern time period, created an emphasis on real-life settings, character research, and thinking one's way around trouble.
The first book of "Call of Cthulhu" adventures was "Shadows of Yog-Sothoth". In this work, the characters come upon a secret society's foul plot to destroy mankind, and pursue it first near to home and then in a series of exotic locations. This template was to be followed in many subsequent campaigns, including "Fungi from Yuggoth" (later known as "Curse of Cthulhu" and "Day of the Beast"), "Spawn of Azathoth", and possibly the most highly acclaimed, "Masks of Nyarlathotep". Many of these seem closer in tone to the pulp adventures of "Indiana Jones" than H. P. Lovecraft, but they are nonetheless beloved by many gamers.
"Shadows of Yog-Sothoth" is important not only because it represents the first published addition to the boxed first edition of "Call of Cthulhu", but because its format defined a new way of approaching a campaign of linked RPG scenarios involving actual clues for the would-be detectives amongst the players to follow and link in order to uncover the dastardly plots afoot. Its format has been used by every other campaign-length "Call of Cthulhu" publication. The standard of "CoC" scenarios was well received by independent reviewers. "The Asylum and Other Tales", a series of stand alone articles released in 1983, rated an overall 9/10 in Issue 47 of "White Dwarf" magazine.
The standard of the included 'clue' material varies from scenario to scenario, but reached its zenith in the original boxed versions of the "Masks of Nyarlathotep" and "Horror on the Orient Express" campaigns. Inside these one could find matchbooks and business cards apparently defaced by non-player characters, newspaper cuttings and (in the case of "Orient Express") period passports to which players could attach their photographs, bringing a Live Action Role Playing feel to a tabletop game. Indeed, during the period that these supplements were produced, third party campaign publishers strove to emulate the quality of the additional materials, often offering separately-priced 'deluxe' clue packages for their campaigns.
Additional milieu were provided by Chaosium with the release of "Dreamlands", a boxed supplement containing additional rules needed for playing within the Lovecraft Dreamlands, a large map and a scenario booklet, and "Cthulhu By Gaslight", another boxed set which moved the action from the 1920s to the 1890s.
"Cthulhu Now".
In 1987, Chaosium issued the supplement titled "Cthulhu Now", a collection of rules, supplemental source materials and scenarios for playing "Call of Cthulhu" in the present day. This proved to be a very popular alternative milieu, so much so that much of the supplemental material is now included in the core rule book.
"Delta Green".
Pagan Publishing has released a series of supplements in a similar vein, by the name "Delta Green", that is set in the 1990s (although later supplements add support for playing closer to the present day).
Lovecraft Country.
"Lovecraft Country" was a line of supplements for "Call of Cthulhu" released in 1990. These supplements were overseen by Keith Herber and provided backgrounds and adventures set in Lovecraft's fictional towns of Arkham, Kingsport, Innsmouth, Dunwich, and their environs. The intent was to give investigators a common base, as well as to center the action on well-drawn characters with clear motivations. With the departure of Herber, Chaosium's line ended.
Recent history.
In the years since the collapse of the "Mythos" collectible card game (production ceased in 1997), the release of "CoC" books has been very sporadic with up to a year between releases. Chaosium struggled with near bankruptcy for many years before finally starting their upward climb again.
2005 was Chaosium's busiest year for many years with ten releases for the game and many more scheduled for release in the near future. Chaosium has taken to marketing "monographs"—short books by individual writers with editing and layout provided out-of-house—directly to the consumer, allowing the company to gauge market response to possible new works, though the long-term effects of this program remain uncertain. The range of times and places in which the horrors of the Mythos can be encountered was also expanded in late 2005 onwards with the addition of "Cthulhu Dark Ages" by Stéphane Gesbert, which gives a framework for playing games set in 11th century Europe, "Secrets of Japan" by Michael Dziesinski for gaming in modern day Japan, and "Secrets of Kenya" by David Conyers for gaming in interwar period Africa.
In July 2011, Chaosium has announced it will re-release a 30th anniversary edition of the "CoC" 6th edition role-playing game. This 320-page book will feature thick (3 mm) leatherette hard-covers with the front cover and spine stamped with gold foil. The interior pages will be printed in black ink, on 90 gsm matte art paper. The binding will be thread sewn, square backed. Chaosium will offer a one-time printing of this Collector's Edition.
In May 28, 2013, a kickstarter for the 7th Edition of Call of Cthulhu was launched, it ended in June 29 of the same year and collected $561,836.
Licenses.
Chaosium has licensed other publishers to create supplements using their rule system, notably including "Delta Green" by Pagan Publishing. Other licensees have included Miskatonic River Press, Theater of the Mind Enterprises, Triad Entertainment, Games Workshop, Fantasy Flight Games, RAFM, Grenadier Models Inc. and Yog-Sothoth.com. These supplements may be set in different time frames or even different game universes from the original game.
"D20 Call of Cthulhu".
In 2001, a stand-alone version of "Call of Cthulhu" was released by Wizards of the Coast, for the d20 system. Intended to preserve the feeling of the original game, the d20 conversion of the game rules were supposed to make the game more accessible to the large "D&D" player base. The d20 system also made it possible to use "Dungeons & Dragons" characters in "Call of Cthulhu", as well as to introduce the Cthulhu Mythos into "Dungeons & Dragons" games. The d20 version of the game is no longer supported by Wizards as per their contract with Chaosium. Chaosium included d20 stats as an appendix in three releases (see Lovecraft Country), but have since dropped the "dual stat" idea.
"Dark Corners of the Earth".
A licensed first-person shooter adventure game by Headfirst Productions, based on "Call of Cthulhu" campaign "Escape from Innsmouth" and released by Bethesda Softworks in 2005/2006 for the PC and Xbox.
"Trail of Cthulhu".
In February 2008, Pelgrane Press published "Trail of Cthulhu", a stand-alone game created by Kenneth Hite using the GUMSHOE System developed by Robin Laws. "Trail of Cthulhu"s system is more mystery oriented and focuses mostly on interpreting clues.
"Shadows of Cthulhu".
In September 2008, Reality Deviant Publications published "Shadows of Cthulhu", a supplement that brings Lovecraftian gaming to Green Ronin's True20 system.
"Realms of Cthulhu".
In October 2009, Reality Blurs published "Realms of Cthulhu", a supplement for Pinnacle Entertainment's Savage Worlds system.
"The Wasted Land".
In April 2011, Chaosium and new developer Red Wasp Design announced a joint project to produce a mobile video game based on the "Call of Cthulhu" RPG, entitled "Call of Cthulhu: The Wasted Land". The game was released on 30 January 2012.
Card games.
"Mythos" was a collectible card game (CCG) based on the Cthulhu Mythos that Chaosium produced and marketed during the mid-1990s. While generally praised for its fast gameplay and unique mechanics, it ultimately failed to gain a very large market presence. It bears mention because its eventual failure brought the company to hard times that affected its ability to produce material for "Call of Cthulhu". "Call of Cthulhu: The Card Game" is a second collectible card game, produced by Fantasy Flight Games.
Miniatures.
The first licensed "Call of Cthulhu" 25mm gaming miniatures were sculpted by Andrew Chernack and released by Grenadier Models in boxed sets and blister packs in 1983. The license was later transferred to RAFM. As of 2011, RAFM still produce licensed C"all of Cthulhu" models sculpted by Bob Murch. Both lines include investigator player character models and the iconic monsters of the Cthulhu mythos.
Reception.
The game won several major awards in the following years:
In 2002, the "Call of Cthulhu 20th Anniversary Edition" won the Origins Award for "Best Graphic Presentation of a Book Product 2001".

</doc>
<doc id="5723" url="http://en.wikipedia.org/wiki?curid=5723" title="Constellations (journal)">
Constellations (journal)

Constellations: An International Journal of Critical and Democratic Theory is a quarterly peer-reviewed academic journal of critical and democratic theory and successor of "Praxis International". It is edited by Andrew Arato, Amy Allen, and Andreas Kalyvas. Seyla Benhabib is a co-founding former editor and Nancy Fraser a former co-editor.

</doc>
<doc id="5724" url="http://en.wikipedia.org/wiki?curid=5724" title="Cape Breton Island">
Cape Breton Island

Cape Breton Island ( - formerly "Île Royale", Scottish Gaelic: "Ceap Breatainn" or "Eilean Cheap Bhreatainn", Míkmaq: "Únamakika", simply: "Cape Breton") is an island on the Atlantic coast of North America. The name most likely corresponds to the word "Breton", the French adjective referring to the Atlantic province of Brittany.
Cape Breton Island is part of the province of Nova Scotia, Canada. The island accounts for 18.7% of the total area of Nova Scotia. Although physically separated from the Nova Scotia peninsula by the Strait of Canso, it is artificially connected to mainland Nova Scotia by the long rock-fill Canso Causeway. The island is located east-northeast of the mainland with its northern and western coasts fronting on the Gulf of Saint Lawrence; its western coast also forming the eastern limits of the Northumberland Strait. The eastern and southern coasts front the Atlantic Ocean; its eastern coast also forming the western limits of the Cabot Strait. Its landmass slopes upward from south to north, culminating in the highlands of its northern cape. One of the world's larger salt water lakes, Bras d'Or ("Arm of Gold" in French), dominates the centre of the island.
The island is divided into four of Nova Scotia's eighteen counties: Cape Breton, Inverness, Richmond, and Victoria. Their total population at the 2011 census numbered 135,974 "Cape Bretoners"; this is approximately 15% of the provincial population. Cape Breton Island has experienced a decline in population of approximately 4.4% since the previous census in 2006. Approximately 75% of the island's population is located in the Cape Breton Regional Municipality (CBRM) which includes all of Cape Breton County and is often referred to as Industrial Cape Breton, given the history of coal mining and steel manufacturing in this area, which was Nova Scotia's industrial heartland throughout the 20th Century.
The island contains five reserves of the Mi'kmaq Nation, these being: Eskasoni, Membertou, Wagmatcook, Waycobah, and Potlotek/Chapel Island. Eskasoni is the largest in both population and land area.
History.
Cape Breton Island's first residents were probably Archaic maritime natives, ancestors of the Mi'kmaq, the people who were inhabiting the island at the time of European arrival. John Cabot reportedly visited the island in 1497. However, historians are unclear as to whether Cabot first visited Newfoundland or Cape Breton Island. This discovery is commemorated by Cape Breton's Cabot Trail, and by the "Cabot's Landing Historic Site & Provincial Park", located near the village of Dingwall.
In about 1521–22, the Portuguese under João Álvares Fagundes established a fishing colony on the island. As many as two hundred settlers lived in a village, the name of which is not known, located according to some historians at what is now present day Ingonish on the island's northeastern peninsula. The fate of this Portuguese colony is unknown, but it is mentioned as late as 1570.
During the Anglo-French War of 1627 to 1629, under Charles I, by 1629 the Kirkes took Quebec City; Sir James Stewart of Killeith, Lord Ochiltree planted a colony on Cape Breton Island at Baleine, Nova Scotia; and Alexander’s son, William Alexander, 1st Earl of Stirling, established the first incarnation of "New Scotland" at Port Royal. This set of Scottish triumphs which left Cape Sable as the only major French holding in North America was not destined to last. Charles I’s haste to make peace with France on the terms most beneficial to him meant that the new North American gains would be bargained away in the Treaty of Saint-Germain-en-Laye (1632).
The French quickly defeated the Scottish at Baleine, and established the first permanent settlements on Île Royale: present day Englishtown (1629) and St. Peter's (1630). These settlements lasted almost continuously until Nicolas Denys left in 1659. Île Royale then remained vacant for more than fifty years, until the communities along with Louisbourg were established in 1713.
Île Royale.
Known as "Île Royale" to the French, the island also saw active settlement by France. After the French ceded their colonies on Newfoundland and the Acadian mainland to the British by the Treaty of Utrecht in 1713, the French relocated the population of Plaisance, Newfoundland, to Île Royale and the French garrison was established in the central eastern part at Sainte Anne. As the harbour at Sainte Anne experienced icing problems, it was decided to construct a much larger fortification at Louisbourg to improve defences at the entrance to the Gulf of Saint Lawrence and to defend France's fishing fleet on the Grand Banks. The French also built the Louisbourg Lighthouse in 1734, the first lighthouse in Canada and one of the first in North America. In addition to Cape Breton Island, the French colony of Île Royale also included Île Saint-Jean, today called Prince Edward Island.
Louisbourg itself was one of the most important commercial and military centres in New France. Although Louisbourg was captured by New Englanders with British naval assistance in 1745 and by the British again in 1758, Île Royale remained formally part of New France until it was ceded to Great Britain by the Treaty of Paris in 1763. Britain merged the island with its adjacent colony of Nova Scotia (present day peninsular Nova Scotia and New Brunswick).
Some of the first British-sanctioned settlers on the island following the Seven Years' War were Irish, although upon settlement they merged with local French communities to form a culture rich in music and tradition. From 1763 to 1784, the island was administratively part of the colony of Nova Scotia and was governed from Halifax.
The first permanently settled Scottish community on Cape Breton Island was Judique, settled in 1775 by Michael Mor MacDonald. He spent his first winter using his upside-down boat for shelter, which is reflected in the architecture of the village's Community Centre. He composed a song about the area called "O's alainn an t-aite", or "Fair is the Place."
Colony of Cape Breton.
In 1784, Britain split the colony of Nova Scotia into three separate colonies: New Brunswick, Cape Breton Island, and present-day peninsular Nova Scotia, in addition to the adjacent colonies of St. John's Island (renamed Prince Edward Island in 1798) and Newfoundland. The colony of Cape Breton Island had its capital at Sydney on its namesake harbour fronting on Spanish Bay and the Cabot Strait. Its first Lieutenant-Governor was Joseph Frederick Wallet DesBarres (1784–1787) and his successor was William Macarmick (1787).
A number of United Empire Loyalists emigrated to the Canadian colonies, including Cape Breton. David Mathews, the former Mayor of New York City during the American Revolution, emigrated with his family to Cape Breton in 1783. He succeeded Macarmick as head of the colony and served from 1795 to 1798.
From 1799 to 1807, the military commandant was John Despard [http://www.biographi.ca/009004-119.01-e.php?&id_nbr=2837], brother of Edward.
An order forbidding the granting of land in Cape Breton, issued in 1763, was removed in 1784. The mineral rights to the island were given over to the Duke of York by an order-in-council. The British government had intended that the Crown take over the operation of the mines when Cape Breton was made a colony, but this was never done, probably because of the rehabilitation cost of the mines. The mines were in a neglected state, caused by careless operations dating back at least to the time of the final fall of Louisbourg.
Large-scale shipbuilding began in the 1790s, beginning with schooners for local trade moving in the 1820s to larger brigs and brigantines, mostly built for British shipowners. Shipbuilding peaked in the 1850s, marked in 1851 by the full rigged ship "Lord Clarendon", the largest wooden ship ever built in Cape Breton.
Merger with Nova Scotia.
In 1820, the colony of Cape Breton Island was merged for the second time with Nova Scotia. This development is one of the factors which led to large-scale industrial development in the Sydney Coal Field of eastern Cape Breton County. By the late 19th century, as a result of the faster shipping, expanding fishery and industrialization of the island, exchanges of people between the island of Newfoundland and Cape Breton increased, beginning a cultural exchange that continues to this day.
During the first half of the 19th century, Cape Breton Island experienced an influx of Highland Scots numbering approximately 50,000 as a result of the Highland Clearances. Today, the descendants of the Highland Scots dominate Cape Breton Island's culture, particularly in rural communities. To this day, Gaelic is still the first language of a number of elderly Cape Bretoners. A campaign of violence and intimidation by the provincial school board led to the near extermination of Gaelic culture. The growing influence of English-dominated media from outside the Scottish communities saw the use of this language erode quickly during the 20th century. Many of the Scots who immigrated there were either Roman Catholics or Presbyterians, which can be seen in a number of island landmarks and place names.
The 1920s were some of the most violent times in Cape Breton. They were marked by several severe labour disputes. The famous murder of William Davis by strike breakers, and the seizing of the New Waterford power plant by striking miners led to a major union sentiment that persists to this day in some circles. William Davis Miners' Memorial Day is celebrated in coal mining towns to commemorate the deaths of miners at the hands of the coal companies.
20th century.
The turn of the 20th century saw Cape Breton Island at the forefront of scientific achievement with the now-famous activities launched by inventors Alexander Graham Bell and Guglielmo Marconi.
Following his successful invention of the telephone and being relatively wealthy, Bell acquired land near Baddeck in 1885, largely due to surroundings reminiscent of his early years in Scotland. He established a summer estate complete with research laboratories, working with deaf people—including Helen Keller—and continued to invent. Baddeck would be the site of his experiments with hydrofoil technologies as well as the Aerial Experiment Association, financed by his wife, which saw the first powered flight in the British Empire when the AEA "Silver Dart" took off from the ice-covered waters of Bras d'Or Lake. Bell also built the forerunner to the iron lung and experimented with breeding sheep.
Marconi's contributions to Cape Breton Island were also quite significant, as he used the island's geography to his advantage in transmitting the first North American trans-Atlantic radio message from a station constructed at Table Head in Glace Bay to a receiving station at Poldhu in Cornwall, England. Marconi's pioneering work in Cape Breton marked the beginning of modern radio technology. Marconi's station at Marconi Towers, on the outskirts of Glace Bay, became the chief communication centre for the Royal Canadian Navy in World War I through to the early years of World War II.
Promotions for tourism beginning in the 1950s recognized the importance of the Scottish culture to the province, and the provincial government started encouraging the use of Gaelic once again. The establishment of funding for the Gaelic College of Celtic Arts and Crafts and formal Gaelic language courses in public schools are intended to address the near-loss of this culture to English assimilation.
In the 1960s, the Fortress of Louisbourg was partially reconstructed by Parks Canada. Today this National Historic Site of Canada is one of the island's dominant economic engines, employing many residents and attracting thousands of tourists every year. The Fortress has also led to the revival and pride of the Acadian community who were the first settlers along with the Mi'kmaq. Isle Madame and Chéticamp are well known for their hospitality and fine food.
Geography.
The island measures in area, making it the 77th largest island in the world and Canada's 18th largest island. Cape Breton Island is composed mainly of rocky shores, rolling farmland, glacial valleys, barren headlands, mountains, woods and plateaus. Geological evidence suggests that at least part of the island was originally joined with present-day Scotland and Norway, now separated by millions of years of continental drift.
The northern portion of Cape Breton Island is dominated by the Cape Breton Highlands, commonly shortened to simply the "Highlands", which are an extension of the Appalachian mountain chain. The Highlands comprise the northern portions of Inverness and Victoria counties. In 1936 the federal government established the Cape Breton Highlands National Park covering across the northern third of the Highlands. The Cabot Trail scenic highway also encircles the coastal perimeter of the plateau.
Cape Breton Island's hydrological features include the Bras d'Or Lake system, a salt-water fjord at the heart of the island, and freshwater features including Lake Ainslie, the Margaree River system, and the Mira River. Innumerable smaller rivers and streams drain into the Bras d'Or Lake estuary and onto the Gulf of St. Lawrence and Atlantic coasts.
Cape Breton Island is joined to the mainland by the Canso Causeway, which was completed in 1955, enabling direct road and rail traffic to and from the island, but requiring marine traffic to pass through the Canso Canal at the eastern end of the causeway.
Cape Breton Island is divided into four counties: Cape Breton, Inverness, Richmond, and Victoria.
Demographics.
The island's residents can be grouped into five main cultures; Scottish, Mi'kmaq, Acadian, Irish, and English, with respective languages Gaelic (Scottish and Irish), Mi'kmaq, French, and English. English is now the primary spoken language, though Mi'kmaq, Gaelic and French are still heard.
Later migrations of Black Loyalists, Italians, and Eastern Europeans mostly settled in the eastern part of the island around the Industrial Cape Breton region. The population of Cape Breton Island has been in decline for almost two decades with an increasing population exodus in recent years due to economic conditions.
According to the Census of Canada, the population of Cape Breton Island in 2011 was 135,974, a 4.4% decline from 142,298 in 2006, and a 14.1% decline from 158,260 in 1996.
Religious groups
Statistics Canada in 2001 reported a "religion" total of 145,525 for Cape Breton, including 5,245 with "no religious affiliation." Major categories included:
A Synagogue in Sydney serves a small historic Jewish community which was once one of the largest ones in eastern Canada with four shuls: one in Glace Bay, one in New Waterford, one in Whitney Pier, and the one in Sydney. While more recent Muslim immigrants hold Friday prayers at Cape Breton University and the former Holy Redeemer Hall in Whitney Pier. Buddhists are a tiny minority (105 in 2001, according to Statistics Canada), although Gampo Abbey in Pleasant Bay has been operational since 1984.
Economy.
Much of the recent economic history of Cape Breton Island can be tied to the coal industry.
The island has two major coal deposits:
Sydney has traditionally been the main port, with various facilities in a large, sheltered, natural harbour. It is the island's largest commercial centre and home to the "Cape Breton Post" daily newspaper, as well as one television station, CJCB-TV (CTV), and several radio stations. The Marine Atlantic terminal at North Sydney is the terminal for large ferries traveling to Channel-Port aux Basques and seasonally to Argentia, both on the island of Newfoundland.
Point Edward on the west side of Sydney Harbour is the location of Sydport, a former navy base () now converted to commercial use. The Canadian Coast Guard College is located nearby at Westmount. Petroleum, bulk coal, and cruise ship facilities are also located in Sydney Harbour.
Glace Bay is the second largest urban community in population and was the island's main coal mining centre until its last mine ceased operation in the 1980s. Glace Bay served as the hub of the Sydney & Louisburg Railway and also as a major fishing port. At one time, Glace Bay was known as the largest town in Nova Scotia, based on population.
Port Hawkesbury has risen to prominence since the completion of the Canso Causeway and Canso Canal created an artificial deep-water port, allowing extensive petrochemical, pulp and paper, and gypsum handling facilities to be established. The Strait of Canso is completely navigable to Seawaymax vessels, and Port Hawkesbury is open to the deepest-draught vessels on the world's oceans. Large marine vessels may also enter Bras d'Or Lake through the Great Bras d'Or channel, whereas small craft have the additional use of the Little Bras d'Or channel or St. Peters Canal. The St. Peters Canal is no longer used by commercial shipping on Cape Breton Island, but is an important waterway for recreational vessels.
The industrial Cape Breton area faced several challenges with the closure of the Cape Breton Development Corporation's (DEVCO) coal mines and the Sydney Steel Corporation's (SYSCO) steel mill. In recent years, the Island's residents have been attempting to diversify the area economy by investing in tourism developments, call centres, and small businesses, as well as manufacturing ventures in such fields as auto parts, pharmaceuticals, and window glazings.
While the Cape Breton Regional Municipality is in transition from an industrial to a service-based economy, the rest of Cape Breton Island outside the industrial area surrounding Sydney-Glace Bay has been more stable, with a mixture of fishing, forestry, small-scale agriculture, and tourism.
Tourism in particular has grown throughout the post-Second World War era, especially the growth in vehicle-based touring, which was furthered by the creation of the Cabot Trail scenic drive. The scenery of the island is rivalled in northeastern North America by only Newfoundland; and Cape Breton Island tourism marketing places a heavy emphasis on its Scottish Gaelic heritage through events such as the Celtic Colours Festival, held each October, as well as promotions through the Gaelic College of Celtic Arts and Crafts.
Whale-watching is a popular attraction for tourists. Whale-watching cruises are operated by numerous vendors from Baddeck to Cheticamp. The most popular species of whale found in Cape Breton's waters is the Pilot whale.
The primary east-west road on the island is Highway 105, the Trans-Canada Highway, although Trunk 4 is also heavily used. Highway 125 is an important arterial route around Sydney Harbour in the Cape Breton Regional Municipality. The Cabot Trail, circling the Cape Breton Highlands, and Trunk 19, along the western coast of the island, are important secondary roads. Railway connections between the port of Sydney to Canadian National Railway in Truro are maintained by the Cape Breton and Central Nova Scotia Railway.
The Cabot Trail is a scenic road circuit around and over the Cape Breton Highlands with spectacular coastal vistas; over 400,000 visitors drive the Cabot Trail each summer and fall. Coupled with the Fortress of Louisbourg, it has driven the growth of the tourism industry on the island in recent decades. The "Condé Nast" travel guide has rated Cape Breton Island as one of the best island destinations in the world.
Traditional music.
Cape Breton is well known for its traditional fiddle music, which was brought to North America by Scottish immigrants during the Highland Clearances. The traditional style has been well preserved in Cape Breton, and céilidhs have become a popular attraction for summer tourists. Inverness County in particular has a heavy concentration of musical activity, with regular performances in communities such as Mabou and Judique. Judique is recognized as 'Baile nam Fonn', (literally: Village of Tunes) or the 'Home of Celtic Music', featuring the Celtic Music Interpretive Centre. Performers who have received significant recognition outside of Cape Breton include Bruce Guthro, Buddy MacMaster, Natalie MacMaster, Ashley MacIsaac, The Rankin Family, Aselin Debison, Lee Cremo, and the Barra MacNeils.
The Men of the Deeps are a male choral group of current and former miners from the industrial Cape Breton area.

</doc>
<doc id="5725" url="http://en.wikipedia.org/wiki?curid=5725" title="Cthulhu Mythos">
Cthulhu Mythos

The Cthulhu Mythos is a shared fictional universe, based on the work of American horror writer H. P. Lovecraft.
The term was first coined by August Derleth, a contemporary correspondent of Lovecraft, who used the name of the creature "Cthulhu"—a central figure in Lovecraft literature and the focus of Lovecraft's short story "The Call of Cthulhu" (first published in pulp magazine "Weird Tales" in 1928)—to identify the system of lore employed by Lovecraft and his literary successors. The writer Richard L. Tierney later applied the term "Derleth Mythos" to distinguish between Lovecraft's works and Derleth's later stories.
Authors of Lovecraftian horror use elements of the Mythos in an ongoing expansion of the fictional universe.
History.
Robert M. Price described, in his essay "H. P. Lovecraft and the Cthulhu Mythos," two stages in the development of the Cthulhu Mythos. The first stage, termed the "Cthulhu Mythos proper" by Price, was formulated during Lovecraft's lifetime and was subject to his guidance. The second stage was guided by August Derleth who, in addition to publishing Lovecraft's stories after his death, attempted to categorize and expand the Mythos.
First stage.
An ongoing theme in Lovecraft's work is the complete irrelevance of mankind in the face of the cosmic horrors that apparently exist in the universe. Lovecraft made frequent reference to the "Great Old Ones": a loose pantheon of ancient, powerful deities from space who once ruled the Earth and who have since fallen into a deathlike sleep. This was first established in "The Call of Cthulhu", in which the minds of the human characters deteriorated when afforded a glimpse of what exists outside their perceived reality. Lovecraft emphasised the point by stating in the opening sentence of the story that "The most merciful thing in the world, I think, is the inability of the human mind to correlate all its contents."
Writer Dirk W. Mosig notes that Lovecraft was a "mechanistic materialist" who embraced the philosophy of "cosmic indifferentism". Lovecraft believed in a purposeless, mechanical, and uncaring universe that human beings, with their limited faculties, could never fully understand, and the cognitive dissonance caused by this leads to insanity. Lovecraft's viewpoint made no allowance for religious belief which could not be supported scientifically, with the incomprehensible, cosmic forces of his tales having as little regard for humanity as humans have for insects.
There have been attempts at categorizing this fictional group of beings, and Phillip A. Schreffler argues that by carefully scrutinizing Lovecraft's writings a workable framework emerges that outlines the entire "pantheon" – from the unreachable "Outer Ones" (e.g. Azathoth, who apparently occupies the centre of the universe) and "Great Old Ones" (e.g. Cthulhu, imprisoned on Earth in the sunken city of R'lyeh) to the lesser castes (the lowly slave shoggoths and the Mi-go).
David E. Schultz, however, believes Lovecraft never meant to create a canonical Mythos but rather intended his imaginary pantheon to merely serve as a background element. Lovecraft himself humorously referred to his mythos as "Yog Sothothery" (Mosig coincidentally suggested the term "Yog-Sothoth Cycle of Myth" be substituted for "Cthulhu Mythos") and at times had to remind readers his mythos creations were entirely fictional.
The view that there was no rigid structure is reinforced by S. T. Joshi, who stated "Lovecraft's imaginary cosmogony was never a static system but rather a sort of aesthetic construct that remained ever adaptable to its creator's developing personality and altering interests... was never a rigid system that might be posthumously appropriated... [The essence of the mythos lies not in a pantheon of imaginary deities nor in a cobwebby collection of forgotten tomes, but rather in a certain convincing cosmic attitude."
Price, however, believed that Lovecraft's writings could at least be divided into categories and identified three distinct themes: the "Dunsanian" (written in the vein of Lord Dunsany), "Arkham" (occurring in Lovecraft's fictionalized New England setting), and "Cthulhu" (the cosmic tales) cycles. Writer Will Murray noted that while Lovecraft often used his fictional pantheon in the stories he ghostwrote for other authors, he reserved Arkham and its environs exclusively for those tales he wrote under his own name.
Although not formalised and acknowledged as a mythos per se, Lovecraft did correspond with contemporary writers (Clark Ashton Smith, Robert E. Howard, Robert Bloch, Frank Belknap Long, Henry Kuttner, and Fritz Leiber – a group referred to as the "Lovecraft Circle") – and shared story elements: Robert E. Howard's character Friedrich Von Junzt reads Lovecraft's "Necronomicon" in the short story "The Children of the Night" (1931), and in turn Lovecraft mentions Howard's "Unaussprechlichen Kulten" in the stories "Out of the Aeons" (1935) and "The Shadow Out of Time" (1936). Many of Howard's original unedited Conan stories also form part of the Cthulhu Mythos.
Second stage.
Price's dichotomy dictates the second stage commenced with August Derleth, the principal difference between Lovecraft and Derleth being the latter's use of hope and that the Cthulhu mythos essentially represented a struggle between good and evil. Derleth is credited with creating the "Elder Gods", and stated:
As Lovecraft conceived the deities or forces of his mythos, there were, initially, the Elder Gods... hese Elder Gods were benign deities, representing the forces of good, and existed peacefully...very rarely stirring forth to intervene in the unceasing struggle between the powers of evil and the races of Earth. These powers of evil were variously known as the Great Old Ones or the Ancient Ones...
—August Derleth, "The Cthulhu Mythos"
Price suggests that the basis of Derleth's systemization are found in Lovecraft, stating: "Was Derleth's use of the rubric 'Elder Gods' so alien to Lovecraft's in "At the Mountains of Madness"? Perhaps not. In fact, this very story, along with some hints from 'The Shadow over Innsmouth', provides the key to the origin of the 'Derleth Mythos'. For in "At the Mountains of Madness" we find the history of a conflict between two interstellar races (among others): the Elder Ones and the Cthulhu-spawn." Derleth himself believed that Lovecraft wished for other authors to actively write about the myth-cycle as opposed to it being a discrete plot device. Derleth expanded the boundaries of the Mythos by including any passing reference to another author's story elements by Lovecraft as part of the genre: just as Lovecraft made passing reference to Clark Ashton Smith's "Book of Eibon", Derleth in turn added Smith's Ubbo-Sathla to the Mythos.
Derleth also attempted to connect the deities of the Mythos to the four elements ("air", "earth", "fire", and "water"), but was forced to adopt artistic license and create beings to represent certain elements ("air" and "fire") to legitimise his system of classification. In applying the elemental theory to beings that function on a cosmic scale (e.g. Yog-Sothoth) some authors created a separate category termed "aethyr". Derleth matched "earth" against "fire" and "air" against "water", which is inconsistent with the classical elements pairings.
"Lovecraft" mythos.
A lesser known term employed by the scholar S. T. Joshi to describe the works of Lovecraft. Joshi identified four key elements in Lovecraft's mythos (that Price would later condense to three themes), being the fundamental principle of cosmicism (which once again highlighted the irrelevance of mankind), the imaginary New England setting, a pantheon of recurring "pseudomythological" entities and a collection of arcane books that supposedly yield insights into the mythology.

</doc>
<doc id="5726" url="http://en.wikipedia.org/wiki?curid=5726" title="Crane shot">
Crane shot

In filmmaking and video production, a crane shot is a shot taken by a camera on a crane or jib. The most obvious uses are to view the actors from above or to move up and away from them, a common way of ending a movie. Camera cranes go back to the dawn of movie-making, and were frequently used in silent films to enhance the epic nature of large sets and massive crowds.
The major supplier of cranes in Hollywood throughout the 1940s, 1950s, and 1960s was the Chapman Company (later Chapman-Leonard of North Hollywood), supplanted by dozens of similar manufacturers around the world. The typical design provided seats for both the director and the camera operator, and sometimes a third seat for the cinematographer as well. Large weights on the back of the crane provided a perfect balance to compensate for the weight of the people riding the crane. The weights had to be adjusted carefully to avoid the possibility of accidents. During the 1960s, the tallest Hollywood crane was the Chapman Titan crane, a massive design over 20' high that won an Academy Scientific & Engineering award. Most cranes like this were manually operated, requiring an experienced boom operator who knew how to raise, lower, and "crab" the camera alongside actors while the crane platform rolled on separate tracks. The crane operator and camera operator had to precisely coordinate their moves so that focus, pan, and camera position all started and stopped at the same time, requiring great skill and rehearsal.
Some filmmakers like to have the camera on a boom arm just to make it easier to move around between ordinary set-ups. Most cranes accommodate both the camera and an operator, but some can be operated by remote control. They are usually, but not always, found in what are supposed to be emotional or suspenseful scenes. One example of this technique is the shots taken by remote cranes in the car-chase sequence of "To Live and Die in L.A.".
During the last few years, camera cranes have been miniaturized and costs have dropped so dramatically that most aspiring film makers have access to these tools. What was once a "Hollywood" effect is now available for under $400.

</doc>
<doc id="5729" url="http://en.wikipedia.org/wiki?curid=5729" title="Chariots of Fire">
Chariots of Fire

Chariots of Fire is a 1981 British historical drama film. It tells the fact-based story of two athletes in the 1924 Olympics: Eric Liddell, a devout Scottish Christian who runs for the glory of God, and Harold Abrahams, an English Jew who runs to overcome prejudice.
The film was conceived and produced by David Puttnam, written by Colin Welland, and directed by Hugh Hudson. It was nominated for seven Academy Awards and won four, including Best Picture and Best Screenplay. It is ranked 19th in the British Film Institute's list of Top 100 British films. The film is also notable for its memorable instrumental theme tune by Vangelis, who won the Academy Award for Best Original Score.
The film's title was inspired by the line, "Bring me my chariot of fire," from the William Blake poem adapted into the popular British hymn "Jerusalem"; the hymn is heard at the end of the film. The original phrase "chariot(s) of fire" is from 2 Kings and in the Bible.
Plot.
In 1919, Harold Abrahams (Ben Cross) enters the University of Cambridge, where he experiences anti-Semitism from the staff, but enjoys participating in the Gilbert and Sullivan club. He becomes the first person to ever complete the Trinity Great Court Run – running around the college courtyard in the time it takes for the clock to strike 12. Abrahams achieves an undefeated string of victories in various national running competitions. Although focused on his running, he falls in love with a leading Gilbert and Sullivan soprano, Sybil (Alice Krige).
Eric Liddell (Ian Charleson), born in China of Scottish missionary parents, is in Scotland. His devout sister Jennie (Cheryl Campbell) disapproves of Liddell's plans to pursue competitive running. But Liddell sees running as a way of glorifying God before returning to China to work as a missionary.
When they first race against each other, Liddell beats Abrahams. Abrahams takes it poorly, but Sam Mussabini (Ian Holm), a professional trainer whom he had approached earlier, offers to take him on to improve his technique. This attracts criticism from the Cambridge college masters (John Gielgud and Lindsay Anderson). They allege it is not gentlemanly for an amateur to "play the tradesman" by employing a professional coach. Abrahams realises this is a cover for their anti-Semitism and class-based sense of superiority, and dismisses their concern.
When Eric Liddell accidentally misses a church prayer meeting because of his running, his sister Jennie upbraids him and accuses him of no longer caring about God. Eric tells her that though he intends to eventually return to the China mission, he feels divinely inspired when running, and that not to run would be to dishonour God, saying, "I believe that God made me for a purpose. But He also made me fast, and when I run, I feel His pleasure."
The two athletes, after years of training and racing, are accepted to represent Great Britain in the 1924 Olympics in Paris. Also accepted are Abrahams' Cambridge friends, Lord Andrew Lindsay (Nigel Havers), Aubrey Montague (Nicholas Farrell), and Henry Stallard (Daniel Gerroll). While boarding the boat to Paris for the Olympics, Liddell learns the news that the heat for his 100 metre race will be on a Sunday. He refuses to run the race – despite strong pressure from the Prince of Wales and the British Olympic committee – because his Christian convictions prevent him from running on the Sabbath.
Hope appears when Liddell's teammate Lindsay, having already won a silver medal in the 400 metres hurdles, proposes to yield his place in the 400 metre race on the following Thursday to Liddell, who gratefully agrees. His religious convictions in the face of national athletic pride make headlines around the world.
Liddell delivers a sermon at the Paris Church of Scotland that Sunday, and quotes from , ending with:
Abrahams is badly beaten by the heavily favoured United States runners in the 200 metre race. He knows his last chance for a medal will be the 100 metres. He competes in the race, and wins. His coach Sam Mussabini is overcome that the years of dedication and training have paid off with an Olympic gold medal. Now Abrahams can get on with his life and reunite with his girlfriend Sybil, whom he had neglected for the sake of running. Before Liddell's race, the American coach remarks dismissively to his runners that Liddell has little chance of doing well in his now far longer 400 metre race. But one of the American runners, Jackson Scholz, hands Liddell a note of support for his convictions. Liddell defeats the American favourites and wins the gold medal.
The British team returns home triumphant. As the film ends, onscreen text explains that Abrahams married Sybil, and became the elder statesman of British athletics. Liddell went on to missionary work in China. All of Scotland mourned his death in 1945 in Japanese-occupied China.
Historical accuracy.
Characters.
The film depicts Abrahams as attending Gonville and Caius College, Cambridge with three other Olympic athletes: Henry Stallard, Aubrey Montague, and Lord Andrew Lindsay. Abrahams and Stallard were in fact students there and competed in the 1924 Olympics. Montague also competed in the Olympics as depicted, but he attended Oxford, not Cambridge. Aubrey Montague sent daily letters to his mother about his time at Oxford and the Olympics; these letters were the basis of Montague's narration in the film.
The character of Lindsay was based partially on Lord Burghley, a significant figure in the history of British athletics. Although Burghley did attend Cambridge, he was not a contemporary of Harold Abrahams, as Abrahams was an undergraduate from 1919 to 1923 and Burghley was at Cambridge from 1923 to 1927. One scene in the film depicts the Burghley-based "Lindsay" as practising hurdles on his estate with full champagne glasses placed on each hurdle – this was something the wealthy Burghley did, although he used matchboxes instead of champagne glasses. The fictional character of Lindsay was created when Douglas Lowe, who was Britain's third athletics gold medallist in the 1924 Olympics, was not willing to be involved with the film.
Another scene in the film recreates the Great Court Run, in which the runners attempt to run around the perimeter of the Great Court at Trinity College, Cambridge in the time it takes the clock to strike 12 at midday. The film shows Abrahams performing the feat for the first time in history. In fact, Abrahams never attempted this race, and at the time of filming the only person on record known to have succeeded was Lord Burghley, in 1927. In "Chariots of Fire", Lindsay, who is based on Lord Burghley, runs the Great Court Run with Abrahams in order to spur him on, and crosses the finish line just a moment too late. Since the film's release, the Great Court Run has also been successfully run by Trinity undergraduate Sam Dobin, in October 2007.
In the film, Eric Liddell is tripped up by a Frenchman in the 400 metre event of a Scotland–France international athletic meeting. He recovers, makes up a 20 metre deficit, and wins. This was based on fact; the actual race was the 440 yards at a Triangular Contest meet between Scotland, England, and Ireland at Stoke-on-Trent in England in July 1923. His achievement was remarkable as he had already won the 100- and 220-yard events that day. Also unmentioned with regard to Liddell is that it was he who introduced Abrahams to Sam Mussabini. This is alluded to: In the film Abrahams first encounters Mussabini while he is watching Liddell race. The film, however, suggests that Abrahams himself sought Mussabini's assistance.
Abrahams' fiancée is misidentified as Sybil Gordon, a soprano at the D'Oyly Carte Opera Company. In fact, in 1936, Abrahams married Sybil Evers, who sang at the D'Oyly Carte, but they did not meet until 1934. Also, in the film, Sybil is depicted as singing the role of Yum-Yum in "The Mikado", but neither Sybil Gordon nor Sybil Evers ever sang that role with D'Oyly Carte, although Evers was known for her charm in singing Peep-Bo, one of the two other "little maids from school". Harold Abrahams' love of and heavy involvement with Gilbert and Sullivan, as depicted in the film, is factual.
Liddell's sister was several years younger than she was portrayed in the film. Her disapproval of Liddell's track career was creative licence; she actually fully supported his sporting work. Jenny Liddell Somerville cooperated fully with the making of the film and has a brief cameo in the Paris Church of Scotland during Liddell's sermon.
At the memorial service for Harold Abrahams, which opens the film, Lord Lindsay mentions that he and Aubrey Montague are the only members of the 1924 Olympic team still alive. However, Montague died in 1948, 30 years before Abrahams' death.
1924 Olympics.
The film takes some liberties with the events at the 1924 Olympics, including the events surrounding Liddell's refusal to race on a Sunday. In the film, he doesn't learn that the 100 metre heat is to be held on the Christian Sabbath until he is boarding the boat to Paris. In fact, the schedule was made public several months in advance. Liddell did however face immense pressure to run on that Sunday and to compete in the 100 metres, getting called before a grilling by the British Olympic Committee, the Prince of Wales, and other grandees; and his refusal to run made headlines around the world. The decision to change races was, even so, made well before embarking to Paris, and Liddell spent the intervening months training for the 400 metres, an event in which he had previously excelled. It is true, nonetheless, that Liddell's success in the Olympic 400m was largely unexpected.
The film depicts Lindsay, having already won a medal in the 400 metre hurdles, giving up his place in the 400 metre race for Liddell. In fact Burghley, on whom Lindsay is loosely based, was eliminated in the heats of the 110 hurdles (he would go on to win a gold medal in the 400 hurdles at the 1928 Olympics), and was not entered for the 400 metres.
The film reverses the order of Abrahams' 100m and 200m races at the Olympics. In reality, after winning the 100 metres race, Abrahams ran the 200 metres but finished last, Jackson Scholz taking the gold medal. In the film, before his triumph in the 100m, Abrahams is shown losing the 200m and being scolded by Mussabini. And during the following scene in which Abrahams speaks with his friend Montague while receiving a massage from Mussabini, there is a French newspaper clipping showing Scholz and Charlie Paddock with a headline which states that the 200 metres was a triumph for the United States. In the same conversation, Abrahams laments getting "beaten out of sight" in the 200. The film thus has Abrahams overcoming the disappointment of losing the 200 by going on to win the 100, a reversal of the real order.
Eric Liddell actually also ran in the 200m race, and finished third, behind Paddock and Scholz. This was the only time in reality that Liddell and Abrahams competed in the same race. Their meeting in the 1923 AAA Championship in the film was fictitious, though Liddell's record win in that race did spur Abrahams to train even harder.
Abrahams also won a silver medal as an opening runner for the 4 x 100 metres relay team, not shown in the film. Aubrey Montague placed sixth in the steeplechase, as depicted.
Personal inaccuracies at the Olympics.
In the film, the 100m bronze medallist is a character called "Tom Watson"; the real medallist was Arthur Porritt of New Zealand, who refused permission for his name to be used in the film, allegedly out of modesty. His wish was accepted by the film's producers, even though his permission was not necessary. However, the brief back-story given for Watson, who is called up to the New Zealand team from the University of Oxford, substantially matches Porritt's history. With the exception of Porritt, all the runners in the 100m final are identified correctly when they line up for inspection by the Prince of Wales.
Jackson Scholz is depicted as handing Liddell an inspirational Bible-quotation message before the 400 metres final: "It says in the good Book, 'He that honors me, I will honor.' Good luck." In reality, the note was from members of the British team, and was handed to Liddell before the race by his attending masseur at the team's Paris hotel. For dramatic purposes, screenwriter Welland asked Scholz if he could be depicted handing the note, and Scholz readily agreed, saying "Yes, great, as long as it makes me look good."
Production.
Script and direction.
Producer David Puttnam was looking for a story in the mould of "A Man for All Seasons" (1966), regarding someone who follows his conscience, and felt sports provided clear situations in this sense. He discovered Eric Liddell's story by accident in 1977, when he happened upon a reference book on the Olympics while housebound from the flu in a rented house in Los Angeles.
Screenwriter Colin Welland, commissioned by Puttnam, did an enormous amount of research for his Academy Award-winning script. Among other things, he took out advertisements in London newspapers seeking memories of the 1924 Olympics, went to the National Film Archives for pictures and footage of the 1924 Olympics, and interviewed everyone involved who was still alive. Welland just missed Abrahams, who died 14 January 1978, but he did attend Abrahams' February 1978 memorial service, which inspired the present-day framing device of the film. Aubrey Montague's son saw Welland's newspaper ad and sent him copies of the letters his father had sent home – which gave Welland something to use as a narrative bridge in the film. Except for changes in the greetings of the letters from "Darling Mummy" to "Dear Mum" and the change from Oxford to Cambridge, all of the readings from Montague's letters are from the originals.
Welland's original script also featured, in addition to Eric Liddell and Harold Abrahams, a third protagonist, 1924 Olympic gold medallist Douglas Lowe, who was presented as a privileged aristocratic athlete. However, Lowe refused to have anything to do with the film, and his character was written out and replaced by the fictional character of Lord Andrew Lindsay.
Ian Charleson himself wrote Eric Liddell's speech to the post-race workingmen's crowd at the Scotland v. Ireland races. Charleson, who had studied the Bible intensively in preparation for the role, told director Hugh Hudson that he didn't feel the portentous and sanctimonious scripted speech was either authentic or inspiring. Hudson and Welland allowed him to write words he personally found inspirational instead.
The film was slightly altered for the U.S. audience. A brief scene depicting a pre-Olympics cricket game between Abrahams, Liddell, Montague, and the rest of the British track team appears shortly after the beginning of the original film. For the American audience, this brief scene was deleted. In the U.S., to avoid the initial child's G rating, which might have hindered box office sales, a different scene was used – one depicting Abrahams and Montague arriving at a Cambridge railway station and encountering two World War I veterans who use an obscenity – in order to be given a PG rating.
Puttnam chose Hugh Hudson, a multiple award-winning advertising and documentary filmmaker who had never helmed a feature film, to direct "Chariots of Fire". Hudson and Puttnam had known each other since the 1960s, when Puttnam was an advertising executive and Hudson was making films for ad agencies. In 1977, Hudson had also been second-unit director on the Puttnam-produced film "Midnight Express".
Casting.
Director Hugh Hudson was determined to cast young, unknown actors in all the major roles of the film, and to back them up by using veterans like John Gielgud, Lindsay Anderson, and Ian Holm as their supporting cast. Hudson and producer David Puttnam did months of fruitless searching for the perfect actor to play Eric Liddell. They then saw Scottish stage actor Ian Charleson performing the role of Pierre in the Royal Shakespeare Company's production of "Piaf", and knew immediately they had found their man. Unbeknownst to them, Charleson had heard about the film from his father, and desperately wanted to play the part, feeling it would "fit like a kid glove".
Ben Cross, who plays Harold Abrahams, was discovered while playing Billy Flynn in "Chicago". In addition to having a natural pugnaciousness, he had the desired ability to sing and play the piano. Cross was thrilled to be cast, and said he was moved to tears by the film's script.
20th Century Fox, which put up half of the production budget in exchange for distribution rights outside of North America, insisted on having a couple of notable American names in the cast. Thus the small parts of the two American champion runners, Jackson Scholz and Charlie Paddock, were cast with recent headliners: Brad Davis had recently starred in "Midnight Express" (also produced by Puttnam), and Dennis Christopher had recently starred, as a young bicycle racer, in the popular indie film "Breaking Away".
All of the actors portraying runners underwent a gruelling three-month training intensive, with renowned running coach Tom McNab. This training and isolation of the actors also created a strong bond and sense of camaraderie among them.
Music.
Although the film is a period piece, set in the 1920s, the Academy Award-winning original soundtrack composed by Vangelis uses a modern 1980s electronic sound, with a strong use of synthesizer and piano among other instruments. This was a bold and significant departure from earlier period films, which employed sweeping orchestral instrumentals. The title theme of the film has become iconic, and has been used in subsequent films and television shows during slow-motion segments.
Vangelis, a Greek-born electronic composer who moved to Paris in the late 1960s, had been living in London since 1974. Director Hugh Hudson had collaborated with him on documentaries and commercials, and was also particularly impressed with his 1979 albums "Opera Sauvage" and "China". David Puttnam also greatly admired Vangelis's body of work, having originally selected his compositions for his previous film "Midnight Express". Hudson made the choice for Vangelis and for a modern score: "I knew we needed a piece which was anachronistic to the period to give it a feel of modernity. It was a risky idea but we went with it rather than have a period symphonic score." The soundtrack had a personal significance to Vangelis: After composing the iconic theme tune he told Puttnam, "My father is a runner, and this is an anthem to him."
Hudson originally wanted Vangelis's 1977 tune "L'Enfant", from his "Opera Sauvage" album, to be the title theme of the film, and the beach running sequence was actually filmed with "L'Enfant" playing on loudspeakers for the runners to pace to. Vangelis finally convinced Hudson he could create a new and better piece for the film's main theme – and when he played the now-iconic "Chariots of Fire" theme for Hudson, it was agreed the new tune was unquestionably better. The "L'Enfant" melody still made it into the film: When the athletes reach Paris and enter the stadium, a brass band marches through the field, and first plays a modified, acoustic performance of the piece. Vangelis's electronic "L'Enfant" track eventually was used prominently in the 1982 film "The Year of Living Dangerously".
Some pieces of Vangelis's music in the film did not end up on the film's soundtrack album. One of them is the background music to the race Eric Liddell runs in the Scottish highlands. This piece is a version of "Hymn", the original version of which appears on Vangelis's 1979 album, "Opéra sauvage". Various versions are also included on Vangelis's compilation albums "Themes", "Portraits", and "", though none of these include the version used in the film.
Five lively Gilbert and Sullivan tunes also appear in the soundtrack, and serve as jaunty period music which nicely counterpoints Vangelis's modern electronic score. These are: "He is an Englishman" from "H.M.S. Pinafore", "Three Little Maids from School Are We" from "The Mikado", "With Catlike Tread" from "The Pirates of Penzance", "The Soldiers of Our Queen" from "Patience", and "There Lived a King" from "The Gondoliers".
The film also incorporates a major traditional work: "Jerusalem", sung by a British choir at the 1978 funeral of Harold Abrahams. The words, written by William Blake in 1804-8, were set to music by Parry in 1916 as a celebration of England. This hymn has been described as "England's unofficial national anthem", concludes the film and inspired its title. A handful of other traditional anthems and hymns and period-appropriate instrumental ballroom-dance music round out the film's soundtrack.
Filming locations.
The beach scenes associated with the theme tune were filmed at West Sands, St. Andrews. A plaque commemorating the filming can be found there today. The very last scene of the opening titles crosses the 1st and 18th holes of the Old Course at St. Andrews Links.
All of the Cambridge scenes were actually filmed at Hugh Hudson's alma mater Eton College, because Cambridge refused filming rights, fearing depictions of anti-Semitism. The Cambridge administration greatly regretted the decision after the film's enormous success.
Liverpool Town Hall was the setting for the scenes depicting the British Embassy in Paris. The Colombes Olympic Stadium in Paris was represented by The Oval Sports Centre, Bebington, Merseyside. The nearby Woodside ferry terminal was used to represent the embarkation scenes set in Dover. The railway station scenes were filmed at the National Railway Museum in York. The scene depicting a performance of "The Mikado" was filmed in the Savoy Theatre with members of the D'Oyly Carte Opera Company.
Revival for the 2012 Olympics.
"Chariots of Fire" became a recurring theme in promotions for the 2012 Summer Olympics in London. The film's theme tune was featured at the opening of the 2012 London New Years fireworks celebrating the Olympics, and the film's iconic beach-running scene and theme tune were used in "The Sun"'s "Let's Make It Great, Britain" Olympic ads. The runners who first tested the new Olympic Park were spurred on by the "Chariots of Fire" theme tune, and the iconic music was also used to fanfare the carriers of the Olympic flame on parts of its route through the UK. The film's theme was also performed by the London Symphony Orchestra, conducted by Simon Rattle, during the Opening Ceremony of the games; the performance was accompanied by a comedy skit by Rowan Atkinson (in persona as Mr. Bean) which included the opening beach-running footage from the film. The film's theme tune was also played during each medal ceremony of the 2012 Olympics.
Stage adaptation.
A stage adaptation of "Chariots of Fire" was mounted in honour of the 2012 Olympics. The play, "Chariots of Fire", which was adapted by playwright Mike Bartlett and included the iconic Vangelis score, ran from 9 May to 16 June 2012 at London's Hampstead Theatre, and transferred to the Gielgud Theatre in the West End on 23 June, where it ran until 5 January 2013. It starred Jack Lowden as Eric Liddell and James McArdle as Harold Abrahams, and Edward Hall directed. Stage designer Miriam Buether transformed each theatre into an Olympic stadium, and composer Jason Carr wrote additional music. Vangelis also created several new pieces of music for the production. The stage version for the London Olympic year was the idea of the film's director, Hugh Hudson, who co-produced the play; he stated, "Issues of faith, of refusal to compromise, standing up for one's beliefs, achieving something for the sake of it, with passion, and not just for fame or financial gain, are even more vital today."
The production will also be touring various cities throughout the UK in 2014.
Another play, "Running for Glory", written by Philip Dart, based on the 1924 Olympics, and focusing on Abrahams and Liddell, toured parts of Britain from 25 February to 1 April 2012. It starred Nicholas Jacobs as Harold Abrahams, and Tom Micklem as Eric Liddell.
UK cinematic re-release, Blu-ray.
As an official part of the London 2012 Festival celebrations, a new digitally re-mastered version of the film screened in 150 cinemas throughout the UK. The re-release began 13 July 2012, two weeks before the opening ceremony of the London Olympics.
A Blu-ray of the film was released on 10 July 2012 in North America, and was released 16 July 2012 in the UK. The release includes nearly an hour of special features, a CD sampler, and a 32-page "digibook".
Accolades.
"Chariots of Fire" was very successful at the 54th Academy Awards, winning four of seven nominations. When accepting his Oscar for Best Original Screenplay, Colin Welland famously announced "The British are coming". At the 1981 Cannes Film Festival the film won two awards and competed for the Palme d'Or.
American Film Institute recognition

</doc>
<doc id="5734" url="http://en.wikipedia.org/wiki?curid=5734" title="Consequentialism">
Consequentialism

Consequentialism is the class of normative ethical theories holding that the consequences of one's conduct are the ultimate basis for any judgment about the rightness or wrongness of that conduct. Thus, from a consequentialist standpoint, a morally right act (or omission from acting) is one that will produce a good outcome, or consequence. In an extreme form, the idea of consequentialism is commonly encapsulated in the English saying, "the ends justify the means", meaning that if a goal is morally important enough, any method of achieving it is acceptable.
Consequentialism is usually distinguished from deontological ethics (or "deontology"), in that deontology derives the rightness or wrongness of one's conduct from the character of the behaviour itself rather than the outcomes of the conduct. It is also distinguished from virtue ethics, which focuses on the character of the agent rather than on the nature or consequences of the act (or omission) itself, and pragmatic ethics which treats morality like science: advancing socially over the course of many lifetimes, such that any moral criterion is subject to revision. Consequentialist theories differ in how they define moral goods.
Some argue that consequentialist and deontological theories are not necessarily mutually exclusive. For example, T.M. Scanlon advances the idea that human rights, which are commonly considered a "deontological" concept, can only be justified with reference to the consequences of having those rights. Similarly, Robert Nozick argues for a theory that is mostly consequentialist, but incorporates inviolable "side-constraints" which restrict the sort of actions agents are permitted to do.
Consequentialist philosophies.
State consequentialism.
Mohist consequentialism, also known as state consequentialism, is an ethical theory which evaluates the moral worth of an action based on how much it contributes to the welfare of a state. According to the "Stanford Encyclopedia of Philosophy", Mohist consequentialism, dating back to the 5th century BCE, is the "world's earliest form of consequentialism, a remarkably sophisticated version based on a plurality of intrinsic goods taken as constitutive of human welfare." Unlike utilitarianism, which views utility as the sole moral good, "the basic goods in Mohist consequentialist thinking are... order, material wealth, and increase in population". During Mozi's era, war and famines were common, and population growth was seen as a moral necessity for a harmonious society. The "material wealth" of Mohist consequentialism refers to basic needs like shelter and clothing, and the "order" of Mohist consequentialism refers to Mozi's stance against warfare and violence, which he viewed as pointless and a threat to social stability. Stanford sinologist David Shepherd Nivison, in the "The Cambridge History of Ancient China", writes that the moral goods of Mohism "are interrelated: more basic wealth, then more reproduction; more people, then more production and wealth... if people have plenty, they would be good, filial, kind, and so on unproblematically." The Mohists believed that morality is based on "promoting the benefit of all under heaven and eliminating harm to all under heaven." In contrast to Jeremy Bentham's views, state consequentialism is not utilitarian because it is not hedonistic or individualistic. The importance of outcomes that are good for the community outweigh the importance of individual pleasure and pain. The term state consequentialism has also been applied to the political philosophy of the Confucian philosopher Xunzi.
Utilitarianism.
In summary, Jeremy Bentham states that people are driven by their interests and their fears, but their interests take precedence over their fears, and their interests are carried out in accordance with how people view the consequences that might be involved with their interests. "Happiness" on this account is defined as the maximization of pleasure and the minimization of pain.
Historically, hedonistic utilitarianism is the paradigmatic example of a consequentialist moral theory. This form of utilitarianism holds that what matters is the aggregate happiness; the happiness of everyone and not the happiness of any particular person. John Stuart Mill, in his exposition of hedonistic utilitarianism, proposed a hierarchy of pleasures, meaning that the pursuit of certain kinds of pleasure is more highly valued than the pursuit of other pleasures. However, some contemporary utilitarians, such as Peter Singer, are concerned with maximizing the satisfaction of preferences, hence "preference utilitarianism". Other contemporary forms of utilitarianism mirror the forms of consequentialism outlined below.
Ethical egoism.
Ethical egoism can be understood as a consequentialist theory according to which the consequences for the individual agent are taken to matter more than any other result. Thus, egoism will prescribe actions that may be beneficial, detrimental, or neutral to the welfare of others. Some, like Henry Sidgwick, argue that a certain degree of egoism "promotes" the general welfare of society for two reasons: because individuals know how to please themselves best, and because if everyone were an austere altruist then general welfare would inevitably decrease.
Ethical altruism.
Ethical altruism can be seen as a consequentialist ethic which prescribes that an individual take actions that have the best consequences for everyone except for himself. This was advocated by Auguste Comte, who coined the term "altruism," and whose ethics can be summed up in the phrase: Live for others.
Rule consequentialism.
In general, consequentialist theories focus on actions. However, this need not be the case. Rule consequentialism is a theory that is sometimes seen as an attempt to reconcile deontology and consequentialism—and in some cases, this is stated as a criticism of rule consequentialism. Like deontology, rule consequentialism holds that moral behavior involves following certain rules. However, rule consequentialism chooses rules based on the consequences that the selection of those rules have. Rule consequentialism exists in the forms of rule utilitarianism and rule egoism.
Various theorists are split as to whether the rules are the only determinant of moral behavior or not. For example, Robert Nozick holds that a certain set of minimal rules, which he calls "side-constraints", are necessary to ensure appropriate actions. There are also differences as to how absolute these moral rules are. Thus, while Nozick's side-constraints are absolute restrictions on behavior, Amartya Sen proposes a theory that recognizes the importance of certain rules, but these rules are not absolute. That is, they may be violated if strict adherence to the rule would lead to much more undesirable consequences.
One of the most common objections to rule-consequentialism is that it is incoherent, because it is based on the consequentialist principle that what we should be concerned with is maximizing the good, but then it tells us not to act to maximize the good, but to follow rules (even in cases where we know that breaking the rule could produce better results).
Brad Hooker avoided this objection by not basing his form of rule-consequentialism on the ideal of maximizing the good. He writes:
"…the best argument for rule-consequentialism is not that it derives from an overarching commitment to maximise the good. The best argument for rule-consequentialism is that it does a better job than its rivals of matching and tying together our moral convictions, as well as offering us help with our moral disagreements and uncertainties" 
Derek Parfit described Brad Hooker's book on rule-consequentialism "Ideal Code, Real World" as the "best statement and defence, so far, of one of the most important moral theories."
Two-level consequentialism.
The two-level approach involves engaging in critical reasoning and considering all the possible ramifications of one's actions before making an ethical decision, but reverting to generally reliable moral rules when one is not in a position to stand back and examine the dilemma as a whole. In practice, this equates to adhering to rule consequentialism when one can only reason on an intuitive level, and to act consequentialism when in a position to stand back and reason on a more critical level.
This position can be described as a reconciliation between act consequentialism - in which the morality of an action is determined by that action's effects - and rule consequentialism - in which moral behavior is derived from following rules that lead to positive outcomes.
The two-level approach to consequentialism is most often associated with R.M. Hare and Peter Singer.
Motive consequentialism.
Another consequentialist version is motive consequentialism which looks if the state of affairs that results from the motive to choose an action is better or at least as good as each of the alternative state of affairs that would have resulted from alternative actions. This version gives relevance to the motive of an act and links it to its consequences. An act can therefore not be wrong if the decision to act was based on a right motive. A possible inference is, that one can not be blamed for mistaken judgements if the motivation was to do good.
Negative consequentialism.
Most consequentialist theories focus on "promoting" some sort of good consequences. However, one could equally well lay out a consequentialist theory that focuses solely on minimizing bad consequences. (Negative utilitarianism is an actual example.)
One major difference between these two approaches is the agent's responsibility. Positive consequentialism demands that we bring about good states of affairs, whereas negative consequentialism requires that we avoid bad ones. Stronger versions of negative consequentialism will require active intervention to prevent bad and ameliorate existing harm. In weaker versions, simple forbearance from acts tending to harm others is sufficient.
Often "negative" consequentialist theories assert that reducing suffering is more important than increasing pleasure. Karl Popper, for example, claimed "…from the moral point of view, pain cannot be outweighed by pleasure...". (While Popper is not a consequentialist per se, this is taken as a classic statement of negative utilitarianism.) When considering a theory of justice, negative consequentialists may use a state-wide or global-reaching principle: the reduction of suffering (for the disadvantaged) is more valuable than increased pleasure (for the affluent or luxurious).
Teleological ethics.
Teleological ethics (Greek telos, “end”; logos, “science”) is an ethical theory that holds that the ends or consequences of an act determine whether an act is good or evil. Teleological theories are often discussed in opposition to deontological ethical theories, which hold that acts themselves are "inherently" good or evil, regardless of the consequences of acts.
Teleological theories differ on the nature of the end that actions ought to promote. Eudaemonist theories (Greek eudaimonia, "happiness") hold that the goal of ethics consists in some function or activity appropriate to man as a human being, and thus tend to emphasize the cultivation of virtue or excellence in the agent as the end of all action. These could be the classical virtues—courage, temperance, justice, and wisdom—that promoted the Greek ideal of man as the "rational animal", or the theological virtues—faith, hope, and love—that distinguished the Christian ideal of man as a being created in the image of God.
Utilitarian-type theories hold that the end consists in an experience or feeling produced by the action. Hedonism, for example, teaches that this feeling is pleasure—either one's own, as in egoism (the 17th-century English philosopher Thomas Hobbes), or everyone's, as in universalistic hedonism, or utilitarianism (the 19th-century English philosophers Jeremy Bentham, John Stuart Mill, and Henry Sidgwick), with its formula of the "greatest pleasure of the greatest number."
Other utilitarian-type views include the claims that the end of action is survival and growth, as in evolutionary ethics (the 19th-century English philosopher Herbert Spencer); the experience of power, as in despotism (the 16th-century Italian political philosopher Niccolò Machiavelli and the 19th-century German Friedrich Nietzsche); satisfaction and adjustment, as in pragmatism (20th-century American philosophers Ralph Barton Perry and John Dewey); and freedom, as in existentialism (the 20th-century French philosopher Jean-Paul Sartre).
The chief problem for eudaemonist theories is to show that leading a life of virtue will also be attended by happiness—by the winning of the goods regarded as the chief end of action. That Job should suffer and Socrates and Jesus die while the wicked prosper, then seems unjust. Eudaemonists generally reply that the universe is moral and that, in Socrates' words, “No evil can happen to a good man, either in life or after death,” or, in Jesus' words, “But he who endures to the end will be saved.”
Utilitarian theories, on the other hand, must answer the charge that ends do not justify the means. The problem arises in these theories because they tend to separate the achieved ends from the action by which these ends were produced. One implication of utilitarianism is that one's intention in performing an act may include all of its foreseen consequences. The goodness of the intention then reflects the balance of the good and evil of these consequences, with no limits imposed upon it by the nature of the act itself—even if it be, say, the breaking of a promise or the execution of an innocent man. Utilitarianism, in answering this charge, must show either that what is apparently immoral is not really so or that, if it really is so, then closer examination of the consequences will bring this fact to light. Ideal utilitarianism (G.E. Moore and Hastings Rashdall) tries to meet the difficulty by advocating a plurality of ends and including among them the attainment of virtue itself, which, as John Stuart Mill affirmed, "may be felt a good in itself, and desired as such with as great intensity as any other good."
Acts and omissions, and the "act and omissions doctrine".
Since pure consequentialism holds that an action is to be judged solely by its result, most consequentialist theories hold that a deliberate action is no different from a deliberate decision not to act. This contrasts with the "acts and omissions doctrine", which is upheld by some medical ethicists and some religions: it asserts there is a significant moral distinction between acts and deliberate non-actions which lead to the same outcome. This contrast is brought out in issues such as voluntary euthanasia – a pure consequentialist would see no moral difference between allowing a patient to die by, for example, withholding food; switching off their life-support machine; or actively killing them with harmful drugs.
Issues in consequentialism.
Action guidance.
One important characteristic of many normative moral theories such as consequentialism is the ability to produce practical moral judgements. At the very least, any moral theory needs to define the standpoint from which the goodness of the consequences are to be determined. What is primarily at stake here is the "responsibility" of the agent.
The ideal observer.
One common tactic among consequentialists, particularly those committed to an altruistic (selfless) account of consequentialism, is to employ an ideal, neutral observer from which moral judgements can be made. John Rawls, a critic of utilitarianism, argues that utilitarianism, in common with other forms of consequentialism, relies on the perspective of such an ideal observer. The particular characteristics of this ideal observer can vary from an omniscient observer, who would grasp all the consequences of any action, to an ideally informed observer, who knows as much as could reasonably be expected, but not necessarily all the circumstances or all the possible consequences. Consequentialist theories that adopt this paradigm hold that right action is the action that will bring about the best consequences from this ideal observer's perspective.
The real observer.
In practice, it is very difficult, and at times arguably impossible, to adopt the point of view of an ideal observer. Individual moral agents do not know everything about their particular situations, and thus do not know all the possible consequences of their potential actions. For this reason, some theorists have argued that consequentialist theories can only require agents to choose the best action in line with what they know about the situation. However, if this approach is naïvely adopted, then moral agents who, for example, recklessly fail to reflect on their situation, and act in a way that brings about terrible results, could be said to be acting in a morally justifiable way. Acting in a situation without first informing oneself of the circumstances of the situation can lead to even the most well-intended actions yielding miserable consequences. As a result, it could be argued that there is a moral imperative for an agent to inform himself as much as possible about a situation before judging the appropriate course of action. This imperative, of course, is derived from consequential thinking: a better-informed agent is able to bring about better consequences.
Consequences for whom.
Moral action always has an effect on certain people or things, the consequences. Various kinds of consequentialism can be differentiated by beneficiary of the good consequences. That is, one might ask "Consequences for whom?"
Agent-focused or agent-neutral.
A fundamental distinction can be drawn between theories which require that agents act for ends perhaps disconnected from their own interests and drives and theories which permit that agents act for ends in which they have some personal interest or motivation. These are called "agent-neutral" and "agent-focused" theories respectively.
Agent-neutral consequentialism ignores the specific value a state of affairs has for any particular agent. Thus, in an agent-neutral theory, an actor's personal goals do not count any more than anyone else's goals in evaluating what action the actor should take. Agent-focused consequentialism, on the other hand, focuses on the particular needs of the moral agent. Thus, in an agent-focused account, such as one that Peter Railton outlines, the agent might be concerned with the general welfare, but the agent is "more" concerned with the immediate welfare of herself and her friends and family.
These two approaches could be reconciled by acknowledging the tension between an agent's interests as an individual and as a member of various groups, and seeking to somehow optimize among all of these interests. For example, it may be meaningful to speak of an action as being good for someone as an individual but bad for them as a citizen of their town.
Human-centered?
Many consequentialist theories may seem primarily concerned with human beings and their relationships with other human beings. However, some philosophers argue that we should not limit our ethical consideration to the interests of human beings alone. Jeremy Bentham, who is regarded as the founder of utilitarianism, argues that animals can experience pleasure and pain, thus demanding that 'non-human animals' should be a serious object of moral concern. More recently, Peter Singer has argued that it is unreasonable that we do not give equal consideration to the interests of animals as to those of human beings when we choose the way we are to treat them. Such equal consideration does not necessarily imply identical treatment of humans and non-humans, any more than it necessarily implies identical treatment of all humans.
Kinds of consequences.
One way to divide various consequentialisms is by the types of consequences that are taken to matter most, that is, which consequences count as good states of affairs. According to utilitarianism, a good action is one that results in an increase in pleasure, and the best action is one that results in the most pleasure for the greatest number. Closely related is eudaimonic consequentialism, according to which a full, flourishing life, which may or may not be the same as enjoying a great deal of pleasure, is the ultimate aim. Similarly, one might adopt an aesthetic consequentialism, in which the ultimate aim is to produce beauty. However, one might fix on non-psychological goods as the relevant effect. Thus, one might pursue an increase in material equality or political liberty instead of something like the more ephemeral "pleasure". Other theories adopt a package of several goods, all to be promoted equally.
Virtue ethics.
Consequentialism can also be contrasted with aretaic moral theories such as virtue ethics. Whereas consequentialist theories posit that consequences of action should be the primary focus of our thinking about ethics, virtue ethics insists that it is the character rather than the consequences of actions that should be the focal point. Some virtue ethicists hold that consequentialist theories totally disregard the development and importance of moral character. For example, Philippa Foot argues that consequences in themselves have no ethical content, unless it has been provided by a virtue such as benevolence.
However, consequentialism and virtue ethics need not be entirely antagonistic. Philosopher Iain King has developed an approach which reconciles the two schools. Other consequentialists consider effects on the character of people involved in an action when assessing consequence. Similarly, a consequentialist theory may aim at the maximization of a particular virtue or set of virtues. Finally, following Foot's lead, one might adopt a sort of consequentialism that argues that virtuous activity ultimately produces the best consequences.
Ultimate end.
The "ultimate end" is a concept in the moral philosophy of Max Weber, in which individuals act in a faithful, rather than rational, manner.
Etymology.
The term "consequentialism" was coined by G. E. M. Anscombe in her essay "Modern Moral Philosophy" in 1958, to describe what she saw as the central error of certain moral theories, such as those propounded by Mill and Sidgwick.
Criticisms.
G. E. M. Anscombe objects to consequentialism on the grounds that it does not provide guidance in what one ought to do because there is no distinction between consequences that are foreseen and those that are intended (see Principle of double effect).
Bernard Williams has argued that consequentialism is alienating because it requires moral agents to put too much distance between themselves and their own projects and commitments. Williams argues that consequentialism requires moral agents to take a strictly impersonal view of all actions, since it is only the consequences, and not who produces them, that is said to matter. Williams argues that this demands too much of moral agents—since (he claims) consequentialism demands that they be willing to sacrifice any and all personal projects and commitments in any given circumstance in order to pursue the most beneficent course of action possible. He argues further that consequentialism fails to make sense of intuitions that it can matter whether or not someone is personally the author of a particular consequence. For example, that participating in a crime can matter, even if the crime would have been committed anyway, or would even have been worse, without the agent's participation.
Some consequentialists—most notably Peter Railton—have attempted to develop a form of consequentialism that acknowledges and avoids the objections raised by Williams. Railton argues that Williams's criticisms can be avoided by adopting a form of consequentialism in which moral decisions are to be determined by the sort of life that they express. On his account, the agent should choose the sort of life that will, on the whole, produce the best overall effects.

</doc>
<doc id="5735" url="http://en.wikipedia.org/wiki?curid=5735" title="Conscription">
Conscription

Conscription is the compulsory enlistment of people in some sort of national service, most often military service. Conscription dates back to antiquity and continues in some countries to the present day under various names. The modern system of near-universal national conscription for young men dates to the French Revolution in the 1790s, where it became the basis of a very large and powerful military. Most European nations later copied the system in peacetime, so that men at a certain age would serve 1–8 years on active duty and then transfer to the reserve force.
In China, the State of Qin instituted universal military service following the registration of every household. This allowed huge armies to be levied, and was instrumental in the creation of the Qin Empire that conquered a large area of what is now China in 221 BC.
Conscription is controversial for a range of reasons, including conscientious objection to military engagements on religious or philosophical grounds; political objection, for example to service for a disliked government or unpopular war; and ideological objection, for example, to a perceived violation of individual rights. Those conscripted may evade service, sometimes by leaving the country. Some selection systems accommodate these attitudes by providing alternative service outside combat-operations roles or even outside the military, such as Zivildienst (civil service) in Austria and Switzerland. Most post-Soviet countries conscript soldiers not only for Armed Forces but also for paramilitary organizations which are dedicated to police-like "domestic only" service (Internal Troops) or "non-combat" rescue duties (Civil Defence Troops) – none of which is considered alternative to the military conscription.
As of the early 21st century, many states no longer conscript soldiers, relying instead upon professional militaries with volunteers enlisted to meet the demand for troops. The ability to rely on such an arrangement, however, presupposes some degree of predictability with regard to both war-fighting requirements and the scope of hostilities. Many states that have abolished conscription therefore still reserve the power to resume it during wartime or times of crisis.
History.
Ilkum.
Around the reign of Hammurabi (1791–1750 BC), the Babylonian Empire used a system of conscription called "Ilkum". Under that system those eligible were required to serve in the royal army in time of war. During times of peace they were instead required to provide labour for other activities of the state. In return for this service, people subject to it gained the right to hold land. It is possible that this right was not to hold land "per se" but specific land supplied by the state.
Various forms of avoiding military service are recorded. While it was outlawed by the Code of Hammurabi, the hiring of substitutes appears to have been practiced both before and after the creation of the code. Later records show that Ilkum commitments could become regularly traded. In other places, people simply left their towns to avoid their Ilkum service. Another option was to sell Ilkum lands and the commitments along with them. With the exception of a few exempted classes, this was forbidden by the Code of Hammurabi.
China.
Universal conscription in China dates back to the State of Qin, which eventually became the Qin Empire of 221 BC. Following unification, historical records show that a total of 300,000 conscript soldiers and 500,000 conscript labourers constructed the Great Wall of China.
In the following dynasties, universal conscription was abolished and reintroduced on numerous occasions.
, universal military conscription is theoretically mandatory in the People's Republic of China, and reinforced by law. However, due to the large population of China and large pool of candidates available for recruitment, the People's Liberation Army has always had sufficient volunteers, so conscription has not been required in practice at all.
Medieval levies.
Under the feudal conditions for holding land in the medieval period, most peasants and freemen were liable to provide one man of suitable age per family for military duty when required by either the king or the local lord. The levies raised in this way fought as infantry under local superiors. Although the exact laws varied greatly depending on the country and the period, generally these levies were only obliged to fight for one to three months. Most were subsistence farmers, and it was in everyone's interest to send the men home for harvest-time.
In medieval Scandinavia the "leiðangr" (Old Norse), "leidang" (Norwegian), "leding", (Danish), "ledung" (Swedish), "lichting" (Dutch), "expeditio" (Latin) or sometimes "leþing" (Old English), was a levy of free farmers conscripted into coastal fleets for seasonal excursions and in defence of the realm.
The bulk of the Anglo-Saxon English army, called the "fyrd", was composed of part-time English soldiers drawn from the landowning minor nobility. These thegns were the land-holding aristocracy of the time and were required to serve with their own armour and weapons for a certain number of days each year. The historian David Sturdy has cautioned about regarding the "fyrd" as a precursor to a modern national army composed of all ranks of society, describing it as a "ridiculous fantasy":The persistent old belief that peasants and small farmers gathered to form a national army or "fyrd" is a strange delusion dreamt up by antiquarians in the late eighteenth or early nineteenth centuries to justify universal military conscription.
Medieval levy in Poland was known as the "pospolite ruszenie".
Military slavery.
The system of military slaves was widely used in the Middle East, beginning with the Egyptians training Mamluks from the 9th century, to the Turks and Ottoman Empire throughout the 19th century.
In the middle of the 14th century, Ottoman Sultan Murad I developed personal troops to be loyal to him, with a slave army called the "Kapıkulu". The new force was built by taking Christian children from newly conquered lands, especially from the far areas of his empire, in a system known as the "devşirme" (translated "gathering" or "converting"). The captive children were forced to convert to Islam. The Sultans had the young boys trained over several years. Those who showed special promise in fighting skills were trained in advanced warrior skills, put into the sultan's personal service, and turned into the Janissaries, the elite branch of the "Kapıkulu". A number of distinguished military commanders of the Ottomans, and most of the imperial administrators and upper-level officials of the Empire, such as Pargalı İbrahim Pasha and Sokollu Mehmet Paşa, were recruited in this way. By 1609, the Sultan's "Kapıkulu" forces increased to about 100,000.
In later years, Sultans turned to the Barbary Pirates to supply their Jannissaries corps. Their attacks on ships off the coast of Africa or in the Mediterranean, and subsequent capture of able-bodied men for ransom or sale provided some captives for the Sultan's system. Starting in the 17th century, Christian families living under the Ottoman rule began to submit their sons into the Kapikulu system willingly, as they saw this as a potentially invaluable career opportunity for their children. Eventually the Sultan turned to foreign volunteers from the warrior clans of Circassians in southern Russia to fill his Janissary armies. As a whole the system began to break down, the loyalty of the Jannissaries became increasingly suspect. Mahmud II forcibly disbanded the Janissary corps in 1826.
Similar to the Janissaries in origin and means of development were the Mamluks of Egypt in the Middle Ages. The Mamluks were usually captive non-Muslim Iranian and Turkish children who had been kidnapped or bought as slaves from the Barbary coasts. The Egyptians assimilated and trained the boys and young men to become Islamic soldiers who served the Muslim caliphs and the Ayyubid sultans during the Middle Ages. The first mamluks served the Abbasid caliphs in 9th century Baghdad. Over time they became a powerful military caste. On more than one occasion, they seized power, for example, ruling Egypt from 1250–1517.
From 1250 Egypt had been ruled by the Bahri dynasty of Kipchak origin. Slaves from the Caucasus served in the army and formed an elite corp of troops. They eventually revolted in Egypt to form the Burgi dynasty. The Mamluks' excellent fighting abilities, massed Islamic armies, and overwhelming numbers succeeded in overcoming the Christian Crusader fortresses in the Holy Land. The Mamluks were the most successful defense against the Mongol Ilkhanate of Persia and Iraq from entering Egypt.
On the western coast of Africa, Berber Muslims captured non-Muslims to put to work as laborers. They generally converted the younger people to Islam and many became quite assimilated. In Morocco, the Berber looked south rather than north. The Moroccan Sultan Moulay Ismail, called "the Bloodthirsty" (1672–1727), employed a corps of 150,000 black slaves, called his Black Guard. He used them to coerce the country into submission.
Invention of modern conscription.
Modern conscription, the massed military enlistment of national citizens, was devised during the French Revolution, to enable the Republic to defend itself from the attacks of European monarchies. Deputy Jean-Baptiste Jourdan gave its name to the 5 September 1798 Act, whose first article stated: "Any Frenchman is a soldier and owes himself to the defense of the nation." It enabled the creation of the "Grande Armée", what Napoleon Bonaparte called "the nation in arms," which overwhelmed European professional armies that often numbered only into the low tens of thousands. More than 2.6 million men were inducted into the French military in this way between the years 1800 and 1813.
The defeat of the Prussian Army in particular shocked the Prussian establishment, which had believed it was invincible after the victories of Frederick the Great. The Prussians were used to relying on superior organization and tactical factors such as order of battle to focus superior troops against inferior ones. Given approximately equivalent forces, as was generally the case with professional armies, these factors showed considerable importance. However, they became considerably less important when the Prussian armies faced forces that outnumbered their own in some cases by more than ten to one. Scharnhorst advocated adopting the "levée en masse", the military conscription used by France. The "Krümpersystem" was the beginning of short-term compulsory service in Prussia, as opposed to the long-term conscription previously used.
In the Russian Empire, the military service time "owed" by serfs was 25 years at the beginning of the 19th century. In 1834 it was decreased to 20 years. The recruits were to be not younger than 17 and not older than 35. In 1874 Russia introduced universal conscription in the modern pattern, an innovation only made possible by the abolition of serfdom in 1861. New military law decreed that all male Russian subjects, when they reached the age of 20, were eligible to serve in the military for six years.
World Wars.
The range of eligible ages for conscripting was expanded to meet national demand during the World Wars.
In the United States, the Selective Service System drafted men for World War I initially in an age range from 21 to 30 but expanded its eligibility in 1918 to an age range of 18 to 45. In the case of a widespread mobilization of forces where service includes homefront defense, ages of conscripts may range much higher, with the oldest conscripts serving in roles requiring lesser mobility. Expanded-age conscription was common during the Second World War: in Britain, it was commonly known as "call-up" and extended to age 51. Nazi Germany termed it Volkssturm ("People's Storm") and included men as young as 16 and as old as 60. During the Second World War, both Britain and the Soviet Union conscripted women. The United States was on the verge of drafting women into the Nurse Corps because it anticipated it would need the extra personnel for its planned invasion of Japan. However, the Japanese surrendered and the idea was abandoned.
The Netherlands.
Conscription, which was called "Service Duty" () in the Netherlands, was first employed in 1810 by French occupying forces. Napoleon's brother Louis Bonaparte, who was King of Holland from 1806 to 1810, had tried to introduce conscription a few years earlier, unsuccessfully. Every man aged 20 years or older had to enlist. By means of drawing lots it was decided who had to undertake service in the French army. It was possible to arrange a substitute against payment.
Later on, conscription was used for all men over the age of 18. Postponement was possible, due to study, for example. Conscientious objectors could perform an alternative civilian service instead of military service. For various reasons, this forced military service was criticized at the end of the twentieth century. Since the Cold War was over, so was the direct threat of a war. Instead, the Dutch army was employed in more and more peacekeeping operations. The complexity and danger of these missions made the use of conscripts controversial. Furthermore the conscription system was thought to be unfair as only men were drafted.
In the European part of Netherlands, compulsory attendance has been officially suspended since 1 May 1997. Between 1991 and 1996, the Dutch armed forces phased out their conscript personnel and converted to an all-volunteer force. The last conscript troops were inducted in 1995, and demobilized in 1996. The suspension means that citizens are no longer forced to serve in the armed forces, as long as it is not required for the safety of the country. Since then, the Dutch army is an all-volunteer force. However, to this day, every male citizen aged 17 gets a letter in which he is told that he has been registered but does not have to present himself for service. The Dutch army allowed its male soldiers to have long hair from the early 1970s to the end of conscription in the mid-1990s.
Even though it is generally thought that conscription has been abolished in the Netherlands, it is compulsory attendance that was abolished, not conscription. The laws and systems which provide for the conscription of armed forces personnel still remain in place.
United Kingdom.
Britain introduced conscription for the first time in 1916 (halfway through World War I) and abolished it in 1920, and reintroduced it again in 1939 on the outbreak of World War II. It remained in force until 1960.
In all, 8,000,000 men were drafted, as well as several hundred thousand women. The introduction of conscription in May 1939, before the war began, was largely due to pressure from the French, who emphasized the need for a large British army to oppose the Germans. Starting in early 1942 unmarried women age 19–30 were conscripted. Most were sent to the factories, but they could volunteer for the Auxiliary Territorial Service (ATS) and other women's services. None were assigned to combat roles unless they volunteered. By 1943 women were liable to some form of directed labour up to age 51. During the Second World War, 1.4 million British men volunteered for service and 3.2 million were conscripted. Volunteers comprised 20% of the Army, 40% of the Royal Navy, and 50% of the Royal Air Force.
The abolition of conscription in Britain was announced on 4 April 1957, by recently elected prime minister Harold Macmillan, with the last conscripts being recruited three years later.
United States.
In the United States, conscription, also called "the draft", ended in 1973, but males between 18 and 25 are required to register with the Selective Service System to enable a reintroduction of conscription if necessary. President Gerald Ford suspended mandatory draft registration in 1975, but President Jimmy Carter reinstated that requirement when the Soviet Union invaded Afghanistan. Selective Service registration is still required of all young men, although the draft has not been used since 1973.
Colonial and Early National.
In America before 1862, combat duty was always voluntary, but white men aged 18 to 45 were usually required to join local militia units. Colonial militia laws—and after 1776 those of the states—required able-bodied white men to enroll in the militia and to undergo a minimum of military training, all without pay. Colonial Pennsylvania (controlled by Quakers) did not have such laws. Members of pacifist religious denominations were exempt. When combat troops were needed some of the militiamen volunteered for short terms of service, for which they were paid. Following this system in its essentials, the Continental Congress in 1778 recommended that the states draft men from their militias for one year's service in the Continental army; this first national conscription was irregularly applied and failed to fill the Continental ranks.
In 1814, President James Madison proposed conscription of 40,000 men for the army, but the War of 1812 ended before Congress took any action. An 1840 proposal for a standing army of 200,000 men included conscription, but it never passed and military service was voluntary before 1862.
American Civil War.
Although both North and South resorted to conscription during the Civil War, in neither region did the system work effectively. The Confederate Congress on April 16, 1862, passed an act requiring military service for three years from all males aged eighteen to thirty-five not legally exempt, and it later extended the obligation so that all soldiers were required to serve for the duration of the conflict. The U.S. Congress followed on July 17, 1862, with an act authorizing a militia draft within a state when it could not meet its quota with volunteers. However, this failed to produce adequate enlistees, and with few men still volunteering by late 1862, it became necessary for the first time to impose national conscription. This met with considerable outcry among states rights advocates who wrote numerous letters to President Lincoln pleading against the unconstitutionality of such an action. In the end however, their complaints were ignored as Congress approved the first national conscription act on March 1, 1863, which made all white males between 20 and 44 liable for military service.
Quotas were assigned in each state, the deficiencies in volunteers to be met by conscription. But men drafted could provide substitutes or, until mid-1864, avoid service by paying commutation money. Many eligibles pooled their money to cover the cost of anyone drafted. Families used the substitute provision to select which man should go into the army and which should stay home. There was much evasion and overt resistance to the draft, especially in Catholic areas. The great draft riot in New York City in July 1863 involved Irish immigrants who had been signed up as citizens to swell the machine vote, not realizing it made them liable for the draft. Of the 168,649 men procured for the Union through the draft, 117,986 were substitutes, leaving only 50,663 who had their personal services conscripted.
In the end, conscription was largely a failure. The draft failed to bring in high-quality soldiers to the Union armies and instead most draftees were lazy, unmotivated men, men with physical or mental disabilities, and even criminals. They frequently met with contempt from the volunteer soldiers and required extra amounts of discipline and surveillance to prevent them from committing desertion and petty crimes.
The problem of Confederate desertion was aggravated by the inequitable inclinations of conscription officers and local judges. The three conscription acts of the Confederacy exempted certain categories, most notably the planter class, and enrolling officers and local judges often practiced favoritism, sometimes accepting bribes. Attempts to effectively deal with the issue were frustrated by conflict between state and local governments on the one hand and the national government of the Confederacy.
World War I.
In 1917, the administration of Woodrow Wilson decided to rely primarily on conscription, rather than voluntary enlistment, to raise military manpower for World War I. The Selective Service Act of 1917 was carefully drawn to remedy the defects in the Civil War system and—by allowing exemptions for dependency, essential occupations, and religious scruples—to place each man in his proper niche in a national war effort. The act established a "liability for military service of all male citizens"; authorized a selective draft of all those between twenty-one and thirty-one years of age (later from eighteen to forty-five); and prohibited all forms of bounties, substitutions, or purchase of exemptions. Administration was entrusted to local boards composed of leading civilians in each community. These boards issued draft calls in order of numbers drawn in a national lottery and determined exemptions. In 1917 and 1918 some 24 million men were registered and nearly 3 million inducted into the military services, with little of the resistance that characterized the Civil War.
World War II.
In 1940 Congress passed the first peacetime draft legislation, which was led by Grenville Clark. It was renewed (by one vote) in summer 1941. It involved questions as to who should control the draft, the size of the army, and the need for deferments. The system worked through local draft boards comprising community leaders who were given quotas and then decided how to fill them. There was very little draft resistance.
The nation went from a surplus manpower pool with high unemployment and relief in 1940 to a severe manpower shortage by 1943. Industry realized that the Army urgently desired production of essential war materials and foodstuffs more than soldiers. (Large numbers of soldiers were not used until the invasion of Europe in summer 1944.) In 1940 to 1943, the Army often transferred soldiers to civilian status in the Enlisted Reserve Corps in order to increase production. Those transferred would return to work in essential industry, although they could be called back to active duty if the Army needed them. Others were discharged if their civilian work was deemed absolutely essential. There were instances of mass releases of men to increase production in various industries. Blacks and Asians were drafted under the same terms as whites. Over ten million men were drafted for combat in World War II, more than twice the amount drafted for World War One, the Korean War, and the Vietnam War combined.
One contentious issue involved the drafting of fathers, which was avoided as much as possible. Farmers demanded and were generally given occupational deferments (many volunteered anyway, and those who stayed at home were not eligible for postwar veteran's benefits). The draft law as established in 1940 exempted males under 21 from mandatory service due to public opposition against the idea of drafting 18-year olds.
Later in the war, as the need for manpower grew more and more pressing, many earlier deferment categories became draft eligible.
Drafting of women.
Traditionally conscription has been limited to the male population. Women and handicapped males have been exempt from conscription. Many societies have traditionally considered military service as a test of manhood and a rite of passage from boyhood into manhood.
, countries that were drafting women into military service included
Bolivia,
Chad,
Cuba,
Eritrea,
Israel,
Libya,
North Korea,
Sudan,
and Tunisia.
In the United Kingdom during World War II, beginning in 1941, women were brought into the scope of conscription but, as all women with dependent children were exempt and many women were informally left in occupations such as nursing or teaching, the number conscripted was relatively few.
^ </ref>
In 2002, Sweden considered female conscription on the grounds that excluding them goes against the ideology of equality.
In June 2013, the parliament of Norway made a principal resolution to introduce female conscription, being the first country in NATO and Europe to do so. If further laws are passed, female consciption may be implemented in 2015.
In the USSR, there was no systematic conscription of women for the armed forces, but the severe disruption of normal life and the high proportion of civilians affected by World War II after the German invasion attracted many volunteers for what was termed "The Great Patriotic War". Medical doctors of both sexes could and would be conscripted (as officers). Also, the free Soviet university education system required Department of Chemistry students of both sexes to complete an ROTC course in NBC defense, and such female reservist officers could be conscripted in times of war. The United States came close to drafting women into the Nurse Corps in preparation for a planned invasion of Japan.
In 1981 in the United States, several men filed lawsuit in the case "Rostker v. Goldberg", alleging that the Selective Service Act of 1948 violates the Due Process Clause of the Fifth Amendment by requiring that only men register with the Selective Service System (SSS). The Supreme Court eventually upheld the Act, stating that "the argument for registering women was based on considerations of equity, but Congress was entitled, in the exercise of its constitutional powers, to focus on the question of military need, rather than 'equity.'"
On October 1, 1999 in the Taiwan Area, the Judicial Yuan of the Republic of China in its Interpretation 490 considered that the physical differences between males and females and the derived role differentiation in their respective social functions and lives would not make drafting only males a violation of the Constitution of the Republic of China. Though women are conscripted in Taiwan, transsexual persons are exempt.
Conscientious objection.
A conscientious objector is an individual whose personal beliefs are incompatible with military service, or, more often, with any role in the armed forces. In some countries, conscientious objectors have special legal status, which augments their conscription duties. For example, Sweden used to allow conscientious objectors to choose a service in the "weapons-free" branch, such as an airport fireman, nurse or telecommunications technician.
Most refuse such service, as they feel that such roles are a part of the military complex. The reasons for refusing to serve are varied. Some conscientious objectors are so for religious reasons — notably, the members of the historic peace churches, pacifist by doctrine; Jehovah's Witnesses, while not strictly pacifists, refuse to participate in the armed forces on the ground that they believe Christians should be neutral in worldly conflicts.
Evading the draft in the United States.
The New York Draft Riots (July 11 to July 16, 1863; known at the time as "Draft Week"), were violent disturbances in New York City that were the culmination of discontent with new laws passed by Congress to draft men to fight in the ongoing American Civil War. The Central Asian Revolt started in the summer of 1916, when the Russian Empire government ended its exemption of Muslims from military service. The conscription also became unpopular in Grand Duchy of Finland during the reign of Nicholas II and was suspended; instead Finland paid a levy tax, "military millions" as compensation for abolition of conscription.
In the United States and some other countries, the Vietnam War saw new levels of opposition to conscription and the Selective Service System. Many people opposed to and facing conscription chose to either apply for classification and assignment to civilian alternative service or noncombatant service within the military as conscientious objectors, or to evade the draft by fleeing to a neutral country. A small proportion, like Muhammad Ali, chose to resist the draft by publicly and politically fighting conscription. Some people resist at the point of registration for the draft. In the United States around 1970, for example, the draft resistance movement has focused on mandatory draft registration. Others resist at the point of induction, when they are ordered to put on a uniform, when they are ordered to carry or use a weapon, or when they are ordered into combat.
In the United States, especially during the Vietnam War, some used political connections to ensure that they were placed well away from any potential harm, serving in what was termed a Champagne unit. Many would avoid military service altogether through college deferments, by becoming fathers, or serving in various exempt jobs (teaching was one possibility). Others used educational exemptions, became conscientious objectors or pretended to be conscientious objectors, although they might then be drafted for non-combat work, such as serving as a combat medic. It was also possible they could be asked to do similar civilian work, such as being a hospital orderly.
It was, in fact, quite easy for those with some knowledge of the system to avoid being drafted. A simple route, widely publicized, was to get a medical rejection. While a person could claim to have symptoms (or feign homosexuality) if enough physicians sent letters that a person had a problem, he might well be rejected. It often wasn't worth the Army's time to dispute this claim. Such an approach worked best in a larger city where there was no stigma to not serving, and the potential draftee was not known to those reviewing him.
For others, the most common method of avoiding the draft was to cross the border into another country. People who have been "called up" for military service and who attempted to avoid it in some way were known as "draft-dodgers". Particularly during the Vietnam War, U.S. draft-dodgers usually made their way to Canada, Mexico, or Sweden.
Many people looked upon draft-dodgers with scorn as being "cowards", but some supported them in their efforts. In the late years of the Vietnam War, objections against it and support for draft-dodgers was much more outspoken, because of the casualties suffered by American troops, and the actual cause and purpose of the war being heavily questioned.
Toward the end of the U.S. draft, an attempt was made to make the system somewhat fairer by turning it into a lottery, with each of the year's calendar dates randomly assigned a number. Men born on lower-numbered dates were called up for review. For the reasons given above, this did not make the system any fairer, and the entire system ended in 1973. By 1975, the draft was no longer mandatory. Today, American men aged 18–25 are encouraged to sign up for the Military, but there has not been a call-up since the Vietnam Era.
In Israel, the Muslim and Christian Arab minority are exempt from mandatory service, as are permanent residents such as the Druze of the Golan Heights. Male Ultra-Orthodox Jews may apply for a deferment of draft to study in Yeshiva, and the deferment tends to become an exemption, while female religious Jews can be exempted after presenting "religious declaration" to the IDF authorities, and some (primarily National Religious or Modern Orthodox) choose to volunteer for national service instead. Male Druze and Circassian Israeli citizens are liable, by agreement with their community leaders (Female Druze and Circassian are exempt from service). Members of the exempted groups can still volunteer, but very few do, except that Bedouin have a relatively large number who tend to volunteer (usually for financial reasons).
Arguments against conscription.
Sexism.
Historically, the vast majority of conscription measures involve male-only participation. Even today, most countries mandating conscription only do so for males. Men who opt out of military service must often perform alternative service, such as Zivildienst in Austria and Switzerland, whereas women do not have even these obligations.
Nominally gender-equal societies such as Finland and Denmark also employ male-only conscription, as have the Netherlands and Sweden in contemporary times. The onerous time and other commitments involved with conscription, spanning two years in many cases, raises serious questions about the fairness of such programs and how they fit in with expectations of equal treatment irrespective of sex.
While women, almost always exempt from conscription, are free to pursue work, study and other activities, men's early career and life prospects can be impeded by conscription. The argument in favour of male-only conscription is that most women also face impediments to their early careers because of child-bearing and -raising, and children are as at least as important to the long-term survival of the nation as military service.
Slavery.
American libertarians oppose conscription and call for the abolition of the Selective Service System, believing that impressment of individuals into the armed forces is involuntary servitude. Ron Paul, a former leader of the Libertarian Party has said, "Conscription is wrongly associated with patriotism, when it really represents slavery and involuntary servitude." The philosopher Ayn Rand opposed it because "Of all the statist violations of individual rights in a mixed economy, the military draft is the worst. It is an abrogation of rights. It negates man’s fundamental right—the right to life—and establishes the fundamental principle of statism: that a man’s life belongs to the state, and the state may claim it by compelling him to sacrifice it in battle."
In 1917, a number of radicals and anarchists, including Emma Goldman, challenged the new draft law in federal court arguing that it was a direct violation of the Thirteenth Amendment's prohibition against slavery and involuntary servitude. However, the Supreme Court unanimously upheld the constitutionality of the draft act in the case of "Arver v. United States" on January 7, 1918. The decision said the Constitution gave Congress the power to declare war and to raise and support armies. The Court emphasized the principle of the reciprocal rights and duties of citizens:
Economics.
It can be argued that in a cost-to-benefit ratio, conscription during peace time is not worthwhile. Months or years of service amongst the most fit and capable subtracts from the productivity of the economy; add to this the cost of training them, and in some countries paying them. Compared to these extensive costs, some would argue there is very little benefit; if there ever was a war then conscription and basic training could be completed quickly, and in any case there is little threat of a war in most countries with conscription. In the United States, every male resident must register with the Selective Service System on his 18th birthday and is available for a draft.
The cost of conscription can be related to the parable of the broken window. The cost of the work, military service, does not disappear even if no salary is paid. The work effort of the conscripts is effectively wasted, as an unwilling workforce is extremely inefficient. The impact is especially severe in wartime, when civilian professionals are forced to fight as amateur soldiers. Not only is the work effort of the conscripts wasted and productivity lost, but professionally skilled conscripts are also difficult to replace in the civilian workforce. Every soldier conscripted in the army is taken away from his civilian work, and away from contributing to the economy which funds the military. This is not a problem in an agrarian or pre-industrialized state where the level of education is universally low, and where a worker is easily replaced by another. However, this proves extremely problematic in a post-industrial society where educational levels are high and where the workforce is highly sophisticated and a replacement for a conscripted specialist is difficult to find. Even direr economic consequences result if the professional conscripted as an amateur soldier is killed or maimed for life; his work effort and productivity is irrevocably lost.
Arguments for conscription.
Political and moral motives.
Jean Jacques Rousseau argued vehemently against professional armies, feeling it was the right and privilege of every citizen to participate to the defense of the whole society and a mark of moral decline to leave this business to professionals. He based this view on the development of the Roman republic, which came to an end at the same time as the Roman army changed from a conscript to professional force. Similarly, Aristotle linked the division of armed service among the populace intimately with the political order of the state. Niccolò Machiavelli argued strongly for conscription, seeing the professional armies as the cause of the failure of societal unity in Italy.
Other proponents, such as William James, consider both mandatory military and national service as ways of instilling maturity in young adults. Some proponents, such as Jonathan Alter and Mickey Kaus, support a draft in order to reinforce social equality, create social consciousness, break down class divisions and for young adults to immerse themselves in public enterprise.
Economic and resource efficiency.
It is estimated by the British military that in a professional military, a company deployed for active duty in peacekeeping corresponds to three inactive companies at home. Salaries for each are paid from the military budget. In contrast, volunteers from a trained reserve are in their civilian jobs when they are not deployed.

</doc>
<doc id="5736" url="http://en.wikipedia.org/wiki?curid=5736" title="Catherine Coleman">
Catherine Coleman

Catherine Grace "Cady" Coleman (born December 14, 1960) is an American chemist, a former United States Air Force officer, and a current NASA astronaut. She is a veteran of two Space Shuttle missions, and departed the International Space Station on May 23, 2011, as a crew member of Expedition 27 after logging 159 days in space.
Education.
Coleman graduated from Wilbert Tucker Woodson High School, Fairfax, Virginia, in 1978; in 1978–1979 she was an exchange student at Røyken upper secondary school in Norway with the AFS Intercultural Programs. She received a bachelor of science degree in chemistry from the Massachusetts Institute of Technology in 1983, and a doctorate in polymer science and engineering from the University of Massachusetts Amherst in 1991 as a member of the Air Force ROTC. She was a member of the intercollegiate crew and was a resident of Baker House.
Military career.
After completing her regular education, Coleman joined the U.S. Air Force as a Second Lieutenant while continuing her graduate work for a PhD at the University of Massachusetts Amherst. In 1988 she entered active duty at Wright-Patterson Air Force Base as a research chemist. During her work she participated as a surface analysis consultant on the NASA Long Duration Exposure Facility experiment. In 1991 she received her doctorate in polymer science and engineering. She retired from the Air Force in November 2009.
NASA career.
Coleman was selected by NASA in 1992 to join the NASA Astronaut Corps. In 1995 she was a member of the STS-73 crew on the scientific mission USML-1 with experiments including biotechnology, combustion science and the physics of fluids. During the flight, she reported to Houston Mission Control that she had spotted an unidentified flying object. She also trained for the mission STS-83 to be the backup for Donald A. Thomas; however, as he recovered on time, she did not fly that mission. STS-93 was Coleman's second space flight in 1999. She was mission specialist in charge of deploying the Chandra X-ray Observatory and its Inertial Upper Stage out of the shuttle's cargo bay.
Coleman served as Chief of Robotics for the Astronaut Office, to include robotic arm operations and training for all Space Shuttle and International Space Station missions. In October 2004, Coleman served as an aquanaut during the mission aboard the Aquarius underwater laboratory, living and working underwater for eleven days.
Coleman was assigned as a backup U.S. crew member for Expeditions 19, 20 and 21 and served as a backup crew member for Expeditions 24 and 25 as part of her training for Expedition 26.
Coleman launched on December 15, 2010 (December 16 Baikonur time), aboard Soyuz TMA-20 to join the Expedition 26 mission aboard the International Space Station.
Spaceflight experience.
STS-73 on Space Shuttle "Columbia" (October 20 to November 5, 1995) was the second United States Microgravity Laboratory mission. The mission focused on materials science, biotechnology, combustion science, the physics of fluids, and numerous scientific experiments housed in the pressurized Spacelab module. In completing her first space flight, Coleman orbited the Earth 256 times, traveled over 6 million miles, and logged a total of 15 days, 21 hours, 52 minutes and 21 seconds in space.
STS-93 on "Columbia" (July 22 to 27, 1999) was a five-day mission during which Coleman was the lead mission specialist for the deployment of the Chandra X-ray Observatory. Designed to conduct comprehensive studies of the universe, the telescope will enable scientists to study exotic phenomena such as exploding stars, quasars, and black holes. Mission duration was 118 hours and 50 minutes.
Soyuz TMA-20 / Expedition 26/27 (December 15, 2010, to May 23, 2011) was an extended duration mission to the International Space Station.
Personal.
Coleman is married to glass artist Josh Simpson who lives in Massachusetts. They have one son. She is part of the band Bandella, which also includes fellow NASA astronaut Steven Robinson, Canadian astronaut Chris Hadfield, and Micki Pettit (astronaut Don Pettit's wife). Coleman is a flute player and has taken several flutes with her to the ISS, including a pennywhistle from Paddy Moloney of the Chieftains, an old Irish flute from Matt Molloy of the Chieftains, and a flute from Ian Anderson of Jethro Tull. On February 15, 2011, she played one of the instruments live from orbit on National Public Radio. On April 12, 2011, she played live through video link for the audience of Jethro Tull's show in Russia in honour of the 50th anniversary of Yuri Gagarin's flight. She played the duet from orbit while Anderson played on the ground in Russia. On May 13 of that year, Coleman delivered a taped commencement address to the class of 2011 at the University of Massachusetts Amherst.
As do many other astronauts, Coleman holds an amateur radio license (callsign: KC5ZTH).

</doc>
<doc id="5738" url="http://en.wikipedia.org/wiki?curid=5738" title="Cervix">
Cervix

The cervix (') or cervix uteri is the lower part of the uterus and part of the female reproductive system. In a non-pregnant woman, the cervix is usually between 2 and 3 cm long. Roughly cylindrical in shape, it has a narrow central canal called the cervical canal running along its entire length, connecting the cavity of the body of the uterus and the lumen of the vagina. The opening into the uterus is called the internal os and the opening into the vagina is called the external os. The lower part of the cervix, known as the vaginal portion of the cervix (or ectocervix), bulges into the top of the vagina. The cervix has been documented anatomically since at least the time of Hippocrates, over 2,000 years ago.
The cervical canal is a passage through which sperm must travel to fertilise an egg cell after sexual intercourse. Several methods of contraception, including cervical caps and cervical diaphragms aim to block or prevent the passage of sperm through the cervical canal. Cervical mucus is used in several methods of fertility awareness, such as the Creighton model and Billings method, due to its changes in consistency throughout the menstrual period. During vaginal childbirth, the cervix must flatten and dilate to allow the fetus to progress along the birth canal. Midwives and doctors use the extent of the dilation of the cervix to assist decision making during childbirth.
The endocervical canal is lined with a layer of column-shaped cells and the ectocervix is covered with multiple layers of cells topped with flat cells. The two types of epithelia meet the squamocolumnar junction. Infection with the human papillomavirus (HPV) can cause changes in the epithelium, which can lead to cancer of the cervix. Cervical cytology tests can often detect precursors of cervical cancer and enable early successful treatment. Ways to avoid HPV include avoiding sex, using condoms, and HPV vaccination. The HPV vaccines, developed in the early 21st century, reduce the risk of cervical cancer by preventing infections from the main cancer-causing strains of HPV.
Structure.
The cervix is part of the female reproductive system. Around in length, it is the lower narrower part of the uterus continuous above with the broader upper part—or body—of the uterus. The lower end of the cervix bulges into the anterior wall of the vagina, and is referred to as the vaginal portion of cervix (or ectocervix); the rest of the cervix above the vagina is called the supravaginal portion of cervix. A central canal, known as the cervical canal, runs along its length and connects the cavity of the body of the uterus with the lumen of the vagina. The openings are known as the internal os and external orifice of the uterus (or external os) respectively. The mucosal lining of the cervical canal is known as the endocervix and the mucosa covering the ectocervix is known as the exocervix. The cervix has an inner mucosal layer, a thick layer of smooth muscle, and posteriorly the supravaginal portion has a serosal covering consisting of connective tissue and overlying peritoneum.
In front of the upper part of the cervix lies the bladder, separated from it by cellular connective tissue known as parametrium, which also extends over the sides of the cervix. To the rear, the supravaginal cervix is covered by peritoneum, which runs onto the back of the vaginal wall and then turns upwards and onto the rectum forming the recto-uterine pouch. The cervix is more tightly connected to surrounding structures than the rest of the uterus.
The cervical canal varies greatly in length and width between women and over the course of a woman's life, and can measure 8 mm (0.3 in) at its widest diameter in premenopausal adults. It is wider in the middle and narrower at each end. The anterior and posterior walls of the canal each have a vertical fold, from which ridges run diagonally upwards and laterally. These are known as palmate folds due to their resemblance to a palm leaf. The anterior and posterior ridges are arranged in such a way that they interlock with each other and close the canal. They are often effaced after pregnancy.
The ectocervix has a convex, elliptical surface and is divided into anterior and posterior lips. The size and shape of the external opening and the ectocervix can vary according to age, hormonal state, and whether natural or normal childbirth has taken place. In women who have not had a vaginal delivery, the external os is a small circular opening, and in women who have had a vaginal delivery, the external os is slit-like. On average, the ectocervix is long and wide.
The cervix is supplied blood by the descending branch of the uterine artery and drains into the uterine vein. The pelvic splanchnic nerves, emerging as S2–S3, transmit the sensation of pain from the cervix to the brain. These nerves travel along the uterosacral ligaments, which pass from the uterus to the anterior sacrum.
Three channels facilitate lymphatic drainage from the cervix. The anterior and lateral cervix drains to nodes along the uterine arteries, travelling along the cardinal ligaments at the base of the broad ligament to the external iliac lymph nodes and ultimately the paraaortic lymph nodes. The posterior and lateral cervix drains along the uterine arteries to the internal iliac lymph nodes and ultimately the paraaortic lymph nodes, and the posterior section of the cervix drains to the obturator and presacral lymph nodes. However, there are variations as lymphatic drainage from the cervix travels to different sets of pelvic nodes in some people. This has implications in scanning nodes for involvement in cervical cancer.
After menstruation and directly under the influence of estrogen, the cervix undergoes a series of changes in position and texture. During most of the menstrual cycle, the cervix remains firm, and is positioned low and closed. However, as ovulation approaches, the cervix becomes softer and rises to open in response to the higher levels of estrogen present. These changes are also accompanied by changes in cervical mucus, described below.
Development.
As a component of the female reproductive system, the cervix is derived from the two paramesonephric ducts (also called Müllerian ducts), which develop around the sixth week of embryogenesis. During development, the outer parts of the two ducts fuse, forming a single urogenital canal that will become the vagina, cervix and uterus. The cervix grows in size at a smaller rate than the body of the uterus, so the relative size of the cervix over time decreases, decreasing from being much larger than the body of the uterus in fetal life, twice as large during childhood, and decreasing to its adult size, smaller than the uterus, after puberty. In fetal development, the original squamous epithelium of the cervix is derived from the urogenital sinus and the original columnar epithelium is derived from the paramesonephric duct. The point at which these two original epithelia meet is called the original squamocolumnar junction.
Histology.
The endocervical mucosa is about 3 mm thick, lined with a single layer of columnar mucous cells, and contains numerous tubular mucous glands which empty viscous alkaline mucus into the lumen. In contrast, the exocervix is covered with nonkeratinized stratified squamous epithelium, which resembles the squamous epithelium lining the vaginal. The junction between these two types of epithlia is called the squamocolumnar junction. Underlying both types of epithelium is a tough layer of collagen. The mucosa of the endocervix is not shed during menstruation. The cervix has more fibrous tissue, including collagen and elastin, than the rest of the uterus.
In prepubertal girls, the functional squamocolumnar junction is present just within the entocervical canal. Upon entering puberty, due to hormonal influence, and during pregnancy, the columnar epithelium extends outwards over the ectocervix as the cervix everts. Hence, this also causes the squamocolumnar junction to move outwards onto the vaginal portion of the cervix, where it is exposed to the acidic vaginal environment. The exposed columnar epithelium can undergo physiological metaplasia and change to tougher metaplastic squamous epithelium in days or weeks, which when mature is very similar to the original squamous epithelium. The new squamocolumnar junction is therefore internal to the original squamocolumnar junction, and the zone of unstable epithelium between the two junctions is called the transformation zone of the cervix. After menopause, the uterine structures involute and the functional squamocolumnar junction moves into the endocervical canal.
Nabothian cysts (or Nabothian follicles) form in the transformation zone where the lining of metaplastic epithelium has replaced mucous epithelium and caused a strangulation of the outlet of some of the mucous glands. A build up of mucus in the glands forms Nabothian cysts, usually less than about 5 mm in diameter, which are considered physiological rather than pathological. Both gland openings and Nabothian cysts are helpful to identify the transformation zone.
Function.
Fertility.
The cervical canal is a pathway through which sperm enter the uterus after sexual intercourse. Some sperm remains in cervical crypts, infoldings of the endocervix, which act as a reservoir, releasing sperm over several hours and maximising the chances of fertilisation. A theory states the cervical and uterine contractions during orgasm draw semen into the uterus. Although the "upsuck theory" has been generally accepted for some years, it has been disputed due to lack of evidence, small sample size, and methodological errors.
Some methods of fertility awareness, such as the Creighton model and the Billings method involve estimating a woman's periods of fertility and infertility by observing physiological changes in her body. Among these changes are several involving the quality of her cervical mucus: the sensation it causes at the vulva, its elasticity ("Spinnbarkeit"), its transparency, and the presence of ferning.
Cervical mucus.
Several hundred glands in the endocervix produce 20–60 mg of cervical mucus a day, increasing to 600 mg around the time of ovulation. It is viscous as it contains large proteins known as mucins. The viscosity and water content varies during the menstrual cycle; mucus is composed of around 93% water, reaching 98% at midcycle. These changes allow it to function either as a barrier or a transport medium to spermatozoa. It contains electrolytes such as calcium, sodium, and potassium; organic components such as glucose, amino acids, and soluble proteins; trace elements including zinc, copper, iron, manganese, and selenium; free fatty acids; enzymes such as amylase; and prostaglandins. Its consistency is determined by the influence of the hormones estrogen and progesterone. At midcycle around the time of ovulation—a period of high estrogen levels— the mucus is thin and serous to allow sperm to enter the uterus, and is more alkaline and hence more hospitable to sperm. It is also higher in electrolytes, which results in the "ferning" pattern that can be observed in drying mucus under low magnification; as the mucus dries, the salts crystallize, resembling the leaves of a fern. The mucus has stretchy character described as "Spinnbarkeit" most prominent around the time of ovulation.
At other times in the cycle, the mucus is thick and more acidic due to the effects of progesterone. This "infertile" mucus acts as a barrier to sperm from entering the uterus. Women taking an oral contraceptive pill also have thick mucus from the effects of progesterone. Thick mucus also prevents pathogens from interfering with a nascent pregnancy.
A cervical mucus plug, called the operculum, forms inside the cervical canal during pregnancy. This provides a protective seal for the uterus against the entry of pathogens and against leakage of uterine fluids. The mucus plug is also known to have antibacterial properties. This plug is released as the cervix dilates, either during the first stage of childbirth or shortly before. It is visible as a blood-tinged mucous discharge.
Childbirth.
The cervix plays a major role in childbirth. As the foetus descends within the uterus in preparation for birth, the presenting part, usually the head, rests on and is supported by the cervix. As labour progresses, the cervix becomes softer and shorter, begins to dilate, and rotates to face anteriorly. The support the cervix provides to the foetal head starts to give way when the uterus begins its contractions. During childbirth, the cervix must dilate to a diameter of more than to accommodate the head of the foetus as it descends from the uterus to the vagina. In becoming wider, the cervix also becomes shorter, a phenomenon known as effacement.
Along with other factors, midwifes and doctors use the extent of cervical dilation to assist decision making during childbirth. Generally, the active first stage of labour, when the uterine contractions become strong and regular, begins when the cervical dilation is more than . The second phase of labor begins when the cervix has dilated to , which is regarded as its fullest dilation, and is when active pushing and contractions push the baby along the birth canal leading to the birth of the baby. The number of past vaginal deliveries is a strong factor in influencing how rapidly the cervix is able to dilate in labour. The time taken for the cervix to dilate and efface is one factor used in reporting systems such as the Bishop score, used to recommend whether interventions such as a forceps delivery, induction, or Caesarean section should be used in childbirth.
Cervical incompetence is a condition in which shortening of the cervix due to dilation and thinning occurs, before term pregnancy. Short cervical length is the strongest predictor of preterm birth.
Contraception.
Several methods of contraception involve the cervix. Cervical diaphragms are small, reusable, firm-rimmed plastic devices inserted by a woman prior to intercourse that cover the cervix. Pressure against the walls of the vagina maintain the position of the diaphragm, and it acts as a physical barrier to prevent the entry of sperm into the uterus, preventing fertilisation. Cervical caps are a similar method, although they are smaller and adhere to the cervix by suction. Diaphragms and caps are often used in conjunction with spermicides. In one year, 12% of women using the diaphragm will undergo an unintended pregnancy, and with optimal use this falls to 6%. Efficacy rates are lower for the cap, with 18% of women undergoing an unintended pregnancy, and 10–13% with optimal use. Most methods of hormonal contraception, such as the oral contraceptive pill, work primarily by preventing ovulation, but their effectiveness is increased because they prevent the production of the types of cervical mucus that are conducive to fertilisation.
Clinical significance.
Cancer.
In 2008, cervical cancer was the third-most common cancer in women worldwide, with rates varying geographically from less than one to more than 50 cases per 100,000 women. Cervical cancer nearly always involves human papillomavirus (HPV) infection, and generally involves the ectocervix at the transformation zone. HPV is a virus with numerous strains, several of which predispose to dysplasia of cervical tissue, particularly in the transformation zone. This dysplasia increases the risk of cancer forming in the transformation zone, which is the most common area for cervical cancer to occur.
Potentially precancerous changes in the cervix can be detected by cervical screening, using methods including a Pap smear (also called a cervical smear), in which epithelial cells are scraped from the surface of the cervix and examined under a microscope. The colposcope, used in a colposcopy to visualise the cervix, was invented in 1925. The Pap smear was developed by Georgios Papanikolaou in 1928. A LEEP procedure using a heated loop of platinum to excise a patch of cervical tissue was developed by Aurel Babes in 1927. In some parts of the developed world including the UK, the Pap test has been superseded with liquid-based cytology.
A result of dysplasia is usually further investigated, such as by taking a cone biopsy, which may also remove the cancerous lesion. Cervical intraepithelial neoplasia is a possible result of the biopsy, and represents dysplastic changes that may eventually progress to invasive cancer. Most cases of cervical cancer are detected in this way, without having caused any symptoms. When symptoms occur, they may include vaginal bleeding, discharge, or discomfort. The introduction of routine screening has resulted in fewer cases of (and deaths from) cervical cancer, however this has mainly taken place in developed countries. Most developing countries have limited or no screening, and 85% of the global burden occurring there. Most women who develop cervical cancer have never had a Pap smear, or have not had one within the last 10 years. Vaccines against HPV, such as Gardasil and Cervarix, also reduce the incidence of cervical cancer, by inoculating against the viral strains involved in cancer development.
Inflammation.
Inflammation of the cervix is referred to as cervicitis. This inflammation may be of the endocervix or ectocervix. When associated with the endocervix, it is associated with a mucous vaginal discharge and the sexually transmitted infections such as chlamydia and gonorrhoea. Other causes include overgrowth of the commensal flora of the vagina. When associated with the ectocervix, inflammation may be caused by the herpes simplex virus. Inflammation is often investigated through directly visualising the cervix using a speculum, which may appear whiteish due to exudate, and by taking a Pap smear and examining for causal bacteria. Special tests may be used to identify particular bacteria. If the inflammation is due to a bacterium, then antibiotics may be given as treatment.
Anatomical abnormalities.
Cervical stenosis refers to an abnormally narrow cervical canal, typically associated with trauma caused by removal of tissue for investigation or treatment of cancer, or cervical cancer itself. Diethylstilbestrol, used from 1938 to 1971 to prevent preterm labour and miscarriage, is also strongly associated with the development of cervical stenosis and other abnormalities in the daughters of the exposed women. Other abnormalities include vaginal adenosis, in which the squamous epithelium of the ectocervix becomes columnar, cancers such as clear cell adenocarcinomas, cervical ridges and hoods, and development of a "cockscomb" cervical appearance.
Cervical agenesis is a rare congenital condition in which the cervix completely fails to develop, often associated with the concurrent failure of the vagina to develop. Other congenital cervical abnormalities exist, often associated with abnormalities of the vagina and uterus. The cervix may be duplicated in situations such as bicornuate uterus and uterine didelphys.
Other.
Cervical polyps, which are benign overgrowths of endocervical tissue, if present, may cause bleeding, or a benign overgrowth may be present in the endocervical canal. Cervical ectropion refers to the horizontal overgrowth of the endocervical columnar lining in a one-cell-thick layer over the ectocervix.
History.
The name of the cervix comes from (neck) from the Proto-Indo-European root "ker-", referring to a "structure that projects". Thus, the word cervix is linguistically related to the English word "horn", the Persian word for "head" (sar), the Greek word for "head" (), and the Welsh word for "deer" ().
The cervix was documented in anatomical literature in at least the time of Hippocrates, although there was some variation in early writers, who used the term to refer to both the cervix and the internal uterine orifice. The first attested use of the word to refer to the cervix of the uterus was in 1702.
Cervical cancer has been described for over 2,000 years, with descriptions provided by both Hippocrates and Aretaeus, although the causal role played by HPV for cervical cancer was only elucidated in the late 20th century by Harald zur Hausen, who published a hypothesis in 1976, and whose hypothesis was confirmed in 1983 and 1984. Based on work done by Jian Zhou and Ian Fraser, a vaccine for four strains of HPV was released in 2006.
Other mammals.
Most Eutherian (placental) mammal species have a single cervix and single, bipartite or bicornuate uterus. Lagomorphs, rodents, aardvarks and hyraxes have a duplex uterus and two cervices. Female marsupials have paired uteri and cervices.

</doc>
<doc id="5739" url="http://en.wikipedia.org/wiki?curid=5739" title="Compiler">
Compiler

A compiler is a computer program (or set of programs) that transforms source code written in a programming language (the source language) into another computer language (the target language, often having a binary form known as object code). The most common reason for wanting to transform source code is to create an executable program.
The name "compiler" is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g., assembly language or machine code). If the compiled program can run on a computer whose CPU or operating system is different from the one on which the compiler runs, the compiler is known as a cross-compiler. A program that translates from a low level language to a higher level one is a decompiler. A program that translates between high-level languages is usually called a source-to-source compiler or transpiler. A language rewriter is usually a program that translates the form of expressions without a change of language. More generally, compilers are sometimes called translators.
A compiler is likely to perform many or all of the following operations: lexical analysis, preprocessing, parsing, semantic analysis (Syntax-directed translation), code generation, and code optimization.
Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementors invest significant effort to ensure compiler correctness.
The term compiler-compiler is sometimes used to refer to a parser generator, a tool often used to help create the lexer and parser.
History.
Software for early computers was primarily written in assembly language. Higher level programming languages were not invented until the benefits of being able to reuse software on different kinds of CPUs started to become significantly greater than the costs of writing a compiler. The limited memory capacity of early computers led to substantial technical challenges when the first compilers were being designed.
Towards the end of the 1950s, machine-independent programming languages were first proposed. Subsequently several experimental compilers were developed. The first compiler was written by Grace Hopper, in 1952, for the A-0 programming language.The A-0 functioned more as a loader or linker than the modern notion of a compiler. The first autocode and its compiler were developed by Alick Glennie in 1952 for the Mark 1 computer at the University of Manchester and is considered by some to be the first compiled programming language. The FORTRAN team led by John Backus at IBM is generally credited as having introduced the first complete compiler in 1957. COBOL was an early language to be compiled on multiple architectures, in 1960.
In many application domains the idea of using a higher level language quickly caught on. Because of the expanding functionality supported by newer programming languages and the increasing complexity of computer architectures, compilers have become more complex.
Early compilers were written in assembly language. The first "self-hosting" compiler – capable of compiling its own source code in a high-level language – was created in 1962 for Lisp by Tim Hart and Mike Levin at MIT. Since the 1970s it has become common practice to implement a compiler in the language it compiles, although both Pascal and C have been popular choices for implementation language. Building a self-hosting compiler is a bootstrapping problem—the first such compiler for a language must be compiled either by hand or by a compiler written in a different language, or (as in Hart and Levin's Lisp compiler) compiled by running the compiler in an interpreter.
Compilers in education.
Compiler construction and compiler optimization are taught at universities and schools as part of a computer science curriculum. Such courses are usually supplemented with the implementation of a compiler for an educational programming language. A well-documented example is Niklaus Wirth's PL/0 compiler, which Wirth used to teach compiler construction in the 1970s. In spite of its simplicity, the PL/0 compiler introduced several influential concepts to the field:
Compilation.
Compilers enabled the development of programs that are machine-independent. Before the development of FORTRAN, the first higher-level language, in the 1950s, machine-dependent assembly language was widely used. While assembly language produces more abstraction than machine code on the same architecture, just as with machine code, it has to be modified or rewritten if the program is to be executed on different computer hardware architecture.
With the advent of high-level programming languages that followed FORTRAN, such as COBOL, C, and BASIC, programmers could write machine-independent source programs. A compiler translates the high-level source programs into target programs in machine languages for the specific hardware. Once the target program is generated, the user can execute the program.
Structure of a compiler.
Compilers bridge source programs in high-level languages with the underlying hardware. A compiler verifies code syntax, generates efficient object code, performs run-time organization, and formats the output according to assembler and linker conventions. A compiler consists of:
Compiler output.
One classification of compilers is by the platform on which their generated code executes. This is known as the "target platform."
A "native" or "hosted" compiler is one which output is intended to directly run on the same type of computer and operating system that the compiler itself runs on. The output of a cross compiler is designed to run on a different platform. Cross compilers are often used when developing software for embedded systems that are not intended to support a software development environment.
The output of a compiler that produces code for a virtual machine (VM) may or may not be executed on the same platform as the compiler that produced it. For this reason such compilers are not usually classified as native or cross compilers.
The lower level language that is the target of a compiler may itself be a high-level programming language. C, often viewed as some sort of portable assembler, can also be the target language of a compiler. E.g.: Cfront, the original compiler for C++ used C as target language. The C created by such a compiler is usually not intended to be read and maintained by humans. So indent style and pretty C intermediate code are irrelevant. Some features of C turn it into a good target language. E.g.: C code with codice_1 directives can be generated to support debugging of the original source.
Compiled versus interpreted languages.
Higher-level programming languages usually appear with a type of translation in mind: either designed as compiled language or interpreted language. However, in practice there is rarely anything about a language that "requires" it to be exclusively compiled or exclusively interpreted, although it is possible to design languages that rely on re-interpretation at run time. The categorization usually reflects the most popular or widespread implementations of a language — for instance, BASIC is sometimes called an interpreted language, and C a compiled one, despite the existence of BASIC compilers and C interpreters.
Interpretation does not replace compilation completely. It only hides it from the user and makes it gradual. Even though an interpreter can itself be interpreted, a directly executed program is needed somewhere at the bottom of the stack (see machine language). Modern trends toward just-in-time compilation and bytecode interpretation at times blur the traditional categorizations of compilers and interpreters.
Some language specifications spell out that implementations "must" include a compilation facility; for example, Common Lisp. However, there is nothing inherent in the definition of Common Lisp that stops it from being interpreted. Other languages have features that are very easy to implement in an interpreter, but make writing a compiler much harder; for example, APL, SNOBOL4, and many scripting languages allow programs to construct arbitrary source code at runtime with regular string operations, and then execute that code by passing it to a special evaluation function. To implement these features in a compiled language, programs must usually be shipped with a runtime library that includes a version of the compiler itself.
Hardware compilation.
The output of some compilers may target computer hardware at a very low level, for example a Field Programmable Gate Array (FPGA) or structured Application-specific integrated circuit (ASIC). Such compilers are said to be "hardware compilers" or synthesis tools because the source code they compile effectively controls the final configuration of the hardware and how it operates; the output of the compilation is not instructions that are executed in sequence - only an interconnection of transistors or lookup tables.
For example, XST is the Xilinx Synthesis Tool used for configuring FPGAs. Similar tools are available from Altera, Synplicity, Synopsys and other vendors.
Compiler construction.
In the early days, the approach taken to compiler design used to be directly affected by the complexity of the processing, the experience of the person(s) designing it, and the resources available.
A compiler for a relatively simple language written by one person might be a single, monolithic piece of software. When the source language is large and complex, and high quality output is required, the design may be split into a number of relatively independent phases. Having separate phases means development can be parceled up into small parts and given to different people. It also becomes much easier to replace a single phase by an improved one, or to insert new phases later (e.g., additional optimizations).
The division of the compilation processes into phases was championed by the Production Quality Compiler-Compiler Project (PQCC) at Carnegie Mellon University. This project introduced the terms "front end", "middle end", and "back end".
All but the smallest of compilers have more than two phases. However, these phases are usually regarded as being part of the front end or the back end. The point at which these two "ends" meet is open to debate. The front end is generally considered to be where syntactic and semantic processing takes place, along with translation to a lower level of representation (than source code).
The middle end is usually designed to perform optimizations on a form other than the source code or machine code. This source code/machine code independence is intended to enable generic optimizations to be shared between versions of the compiler supporting different languages and target processors.
The back end takes the output from the middle. It may perform more analysis, transformations and optimizations that are for a particular computer. Then, it generates code for a particular processor and OS.
This front-end/middle/back-end approach makes it possible to combine front ends for different languages with back ends for different CPUs. Practical examples of this approach are the GNU Compiler Collection, LLVM, and the Amsterdam Compiler Kit, which have multiple front-ends, shared analysis and multiple back-ends.
One-pass versus multi-pass compilers.
Classifying compilers by number of passes has its background in the hardware resource limitations of computers. Compiling involves performing lots of work and early computers did not have enough memory to contain one program that did all of this work. So compilers were split up into smaller programs which each made a pass over the source (or some representation of it) performing some of the required analysis and translations.
The ability to compile in a single pass has classically been seen as a benefit because it simplifies the job of writing a compiler and one-pass compilers generally perform compilations faster than multi-pass compilers. Thus, partly driven by the resource limitations of early systems, many early languages were specifically designed so that they could be compiled in a single pass (e.g., Pascal).
In some cases the design of a language feature may require a compiler to perform more than one pass over the source. For instance, consider a declaration appearing on line 20 of the source which affects the translation of a statement appearing on line 10. In this case, the first pass needs to gather information about declarations appearing after statements that they affect, with the actual translation happening during a subsequent pass.
The disadvantage of compiling in a single pass is that it is not possible to perform many of the sophisticated optimizations needed to generate high quality code. It can be difficult to count exactly how many passes an optimizing compiler makes. For instance, different phases of optimization may analyse one expression many times but only analyse another expression once.
Splitting a compiler up into small programs is a technique used by researchers interested in producing provably correct compilers. Proving the correctness of a set of small programs often requires less effort than proving the correctness of a larger, single, equivalent program.
While the typical multi-pass compiler outputs machine code from its final pass, there are several other types:
Front end.
The "compiler frontend" analyzes the source code to build an internal representation of the program, called the intermediate representation or "IR". It also manages the symbol table, a data structure mapping each symbol in the source code to associated information such as location, type and scope.
While the frontend can be a single monolithic function or program, as in a scannerless parser, it is more commonly implemented and analyzed as several phases, which may execute sequentially or concurrently. This is particularly done for good engineering: modularity and separation of concerns. Most commonly today this is done as three phases: lexing, parsing, and semantic analysis. Lexing and parsing comprise the syntactic analysis (word syntax and phrase syntax, respectively), and in simple cases these modules (the lexer and parser) can be automatically generated from a grammar for the language, though in more complex cases these require manual modification or writing by hand. The lexical grammar and phrase grammar are usually context-free grammars, which simplifies analysis significantly, with context-sensitivity handled at the semantic analysis phase. The semantic analysis phase is generally more complex and written by hand, but can be partially or fully automated using attribute grammars. These phases themselves can be further broken down – lexing as scanning and evaluating, parsing as first building a concrete syntax tree (CST, parse tree), and then transforming it into an abstract syntax tree (AST, syntax tree).
In some cases additional phases are used, notably "line reconstruction" and "preprocessing," but these are rare. A detailed list of possible phases includes:
Back end.
The term "back end" is sometimes confused with "code generator" because of the overlapped functionality of generating assembly code. Some literature uses "middle end" to distinguish the generic analysis and optimization phases in the back end from the machine-dependent code generators.
The main phases of the back end include the following:
Compiler analysis is the prerequisite for any compiler optimization, and they tightly work together. For example, dependence analysis is crucial for loop transformation.
In addition, the scope of compiler analysis and optimizations vary greatly, from as small as a basic block to the procedure/function level, or even over the whole program (interprocedural optimization). Obviously, a compiler can potentially do a better job using a broader view. But that broad view is not free: large scope analysis and optimizations are very costly in terms of compilation time and memory space; this is especially true for interprocedural analysis and optimizations.
Interprocedural analysis and optimizations are common in modern commercial compilers from HP, IBM, SGI, Intel, Microsoft, and Sun Microsystems. The open source GCC was criticized for a long time for lacking powerful interprocedural optimizations, but it is changing in this respect. Another open source compiler with full analysis and optimization infrastructure is Open64, which is used by many organizations for research and commercial purposes.
Due to the extra time and space needed for compiler analysis and optimizations, some compilers skip them by default. Users have to use compilation options to explicitly tell the compiler which optimizations should be enabled.
Compiler correctness.
Compiler correctness is the branch of software engineering that deals with trying to show that a compiler behaves according to its language specification. Techniques include developing the compiler using formal methods and using rigorous testing (often called compiler validation) on an existing compiler.
Related techniques.
Assembly language is a type of low-level language and a program that compiles it is more commonly known as an "assembler", with the inverse program known as a disassembler.
A program that translates from a low level language to a higher level one is a decompiler.
A program that translates between high-level languages is usually called a language translator, source to source translator, language converter, or language rewriter. The last term is usually applied to translations that do not involve a change of language.
A program that translates into an object code format that is not supported on the compilation machine is called a cross compiler and is commonly used to prepare code for embedded applications.
International conferences and organizations.
A number of conferences in the field of programming languages present advances in compiler construction as one of their main topics.
ACM SIGPLAN supports a number of conferences, including:
The European Joint Conferences on Theory and Practice of Software (ETAPS) sponsors the International Conference on Compiler Construction, with papers from both the academic and industrial sectors.
Asian Symposium on Programming Languages and Systems (APLAS) is organized by the Asian Association for Foundation of Software (AAFS).

</doc>
<doc id="5742" url="http://en.wikipedia.org/wiki?curid=5742" title="Castrato">
Castrato

A castrato (Italian, plural: "castrati") is a type of classical male singing voice equivalent to that of a soprano, mezzo-soprano, or contralto. The voice is produced by castration of the singer before puberty, or it occurs in one who, due to an endocrinological condition, never reaches sexual maturity.
Castration before puberty (or in its early stages) prevents a boy's larynx from being transformed by the normal physiological events of puberty. As a result, the vocal range of prepubescence (shared by both sexes) is largely retained, and the voice develops into adulthood in a unique way. Prepubescent castration for this purpose diminished greatly in the late 18th century and was made illegal in Italy in 1870.
As the castrato's body grew, his lack of testosterone meant that his epiphyses (bone-joints) did not harden in the normal manner. Thus the limbs of the castrati often grew unusually long, as did the bones of their ribs. This, combined with intensive training, gave them unrivalled lung-power and breath capacity. Operating through small, child-sized vocal cords, their voices were also extraordinarily flexible, and quite different from the equivalent adult female voice. Their vocal range was higher than that of the uncastrated adult male (see soprano, mezzo-soprano, alto, sopranist, countertenor and contralto). Listening to the only surviving recordings of a castrato (see below), one can hear that the lower part of the voice sounds like a "super-high" tenor, with a more falsetto-like upper register above that.
Castrati were rarely referred to as such: in the 18th century, the euphemism "musico" (pl "musici") was much more generally used, although it usually carried derogatory implications; another synonym was "evirato" (literally meaning "emasculated"). Eunuch is a more general term, since historically many eunuchs were castrated after puberty, castration thus having no effect on their voices.
History.
Castration as a means of subjugation, enslavement or other punishment has a very long pedigree, dating back to ancient Sumer (see also Eunuch). In a Western context, eunuch singers are known to have existed from the early Byzantine Empire. In Constantinople around AD 400 the empress Aelia Eudoxia had a eunuch choir-master, Brison, who may have established the use of castrati in Byzantine choirs, though whether Brison himself was a singer and whether he had colleagues who were eunuch singers is not certain. By the 9th century, eunuch singers were well-known (not least in the choir of Hagia Sophia) and remained so until the sack of Constantinople by the Western forces of the Fourth Crusade in 1204. Their fate from then until their reappearance in Italy more than three hundred years later is not clear. It seems likely that the Spanish tradition of soprano falsettists may have "hidden" castrati. Much of Spain was under Muslim rulers during the Middle Ages, and castration had a history going back to the ancient Near East. Stereotypically, eunuchs served as harem "guards", but they were also valued as high-level political appointees since they could not start a dynasty which would threaten the ruler.
European classical tradition.
Castrati first appeared in Italy in the mid-16th century, though at first the terms describing them were not always clear. The phrase "soprano maschio" (male soprano), which could also mean falsettist, occurs in the "Due Dialoghi della Musica" of Luigi Dentice, an Oratorian priest, published in Rome in 1553. On 9 November 1555 Cardinal Ippolito II d'Este (famed as the builder of the Villa d'Este at Tivoli), wrote to Guglielmo Gonzaga, Duke of Mantua (1538–1587), that he has heard that His Grace is interested in his "cantoretti" and offered to send him two, so that he could choose one for his own service. This is a rare term but probably does equate to "castrato". The Cardinal's brother, Alfonso II d'Este, Duke of Ferrara, was another early enthusiast, enquiring about castrati in 1556. There were certainly castrati in the Sistine Chapel choir in 1558, although not described as such: on 27 April of that year, Hernando Bustamante, a Spaniard from Palencia, was admitted (the first castrati so termed who joined the Sistine choir were Pietro Paolo Folignato and Girolamo Rossini, admitted in 1599). Surprisingly, considering the later French distaste for castrati they certainly existed in France at this time also, being known of in Paris, Orléans, Picardy and Normandy, though they were not abundant: the King of France himself had difficulty in obtaining them. By 1574 there were castrati in the Ducal court chapel at Munich, where the Kapellmeister (music director) was the famous Orlando di Lasso. In 1589, by the bull "Cum pro nostro pastorali munere", Pope Sixtus V re-organised the choir of St Peter's, Rome specifically to include castrati. Thus the castrati came to supplant both boys (whose voices broke after only a few years) and falsettists (whose voices were weaker and less reliable) from the top line in such choirs. Women were banned by the Pauline dictum "mulieres in ecclesiis taceant" ("let women keep silent in church"; see I Corinthians, ch 14, v 34).
Opera.
Although the castrato (or musico) predates opera, there is some evidence that castrati had parts in the earliest operas. In the first performance of Monteverdi's "Orfeo" (1607), for example, they played subsidiary roles, including Speranza and (possibly) that of Euridice. Although female roles were performed by castrati in some of the papal states, this was increasingly rare; by 1680, they had supplanted "normal" male voices in lead roles, and retained their position as "primo uomo" for about a hundred years; an Italian opera not featuring at least one renowned castrato in a lead part would be doomed to fail. Because of the popularity of Italian opera throughout 18th-century Europe (except France), singers such as Ferri, Farinelli, Senesino and Pacchierotti became the first operatic superstars, earning enormous fees and hysterical public adulation. The strictly hierarchical organisation of "opera seria" favoured their high voices as symbols of heroic virtue, though they were frequently mocked for their strange appearance and bad acting. In his 1755 "Reflections upon theatrical expression in tragedy", Roger Pickering wrote: 
Farinelli drew every Body to the Haymarket. What a Pipe! What Modulation! What Extasy to the Ear! But, Heavens! What Clumsiness! What Stupidity! What Offence to the Eye! Reader, if of the City, thou mayest probably have seen in the Fields of Islington or Mile-End or, If thou art in the environs of St James', thou must have observed in the Park with what Ease and Agility a cow, heavy with calf, has rose up at the command of the milkwoman's foot: thus from the mossy bank sprang the DIVINE FARINELLI.
More modern objections to the existence of castrati in Europe might centre on the means by which the preparation of future singers could lead to premature death. To prevent the child from experiencing the intense pain of castration, many were inadvertently administered lethal doses of opium or some other narcotic, or were killed by overlong compression of the carotid artery in the neck (intended to render them unconscious during the castration procedure).
During the 18th century itself, the music historian Charles Burney was sent from pillar to post in search of places where "the operation" was carried out: 
I enquired throughout Italy at what place boys were chiefly qualified for singing by castration, but could get no certain intelligence. I was told at Milan that it was at Venice; at Venice that it was at Bologna; but at Bologna the fact was denied, and I was referred to Florence; from Florence to Rome, and from Rome I was sent to Naples ... it is said that there are shops in Naples with this inscription: 'QUI SI CASTRANO RAGAZZI' ("Here boys are castrated"); but I was utterly unable to see or hear of any such shops during my residence in that city.
The training of the boys was rigorous. The regime of one singing school in Rome (c. 1700) consisted of one hour of singing difficult and awkward pieces, one hour practising trills, one hour practising ornamented passaggi, one hour of singing exercises in their teacher's presence and in front of a mirror so as to avoid unnecessary movement of the body or facial grimaces, and one hour of literary study; all this, moreover, before lunch. After, half-an-hour would be devoted to musical theory, another to writing counterpoint, an hour copying down the same from dictation, and another hour of literary study. During the remainder of the day, the young castrati had to find time to practice their harpsichord playing, and to compose vocal music, either sacred or secular depending on their inclination. This demanding schedule meant that, if sufficiently talented, they were able to make a debut in their mid-teens with a perfect technique and a voice of a flexibility and power no woman or ordinary male singer could match.
In the 1720s and 1730s, at the height of the craze for these voices, it has been estimated that upwards of 4,000 boys were castrated annually in the service of art. Many came from poor homes and were castrated by their parents in the hope that their child might be successful and lift them from poverty (this was the case with Senesino). There are, though, records of some young boys asking to be operated on to preserve their voices (e.g. Caffarelli, who was from a wealthy family: his grandmother gave him the income from two vineyards to pay for his studies). Caffarelli was also typical of many castrati in being famous for tantrums on and off-stage, and for amorous adventures with noble ladies. Some, as described by Casanova, preferred gentlemen (noble or otherwise). Modern endocrinology would suggest that the castrati's much-vaunted sexual prowess was more the stuff of legend than reality – in addition to lacking a hormonal (but not a socio-psychological) sex drive, a castrato's remaining genitalia will not develop in size. Only a small percentage of boys castrated to preserve their voices had successful careers on the operatic stage; the better "also-rans" sang in cathedral or church choirs, but because of their marked appearance and the ban on their marrying, there was little room for them in society outside a musical context.
The castrati came in for a great amount of scurrilous and unkind abuse, and as their fame increased, so did the hatred of them. They were often castigated as malign creatures who lured men into homosexuality. There were homosexual castrati, as Casanova's accounts of 18th-century Italy bear witness. He mentions meeting an abbé whom he took for a girl in disguise, only later discovering that "she" was a famous castrato. In Rome in 1762 he attended a performance at which the prima donna was a castrato, "the favourite pathic" of Cardinal Borghese, who dined every evening with his protector. From his behaviour on stage "it was obvious that he hoped to inspire the love of those who liked him as a man, and probably would not have done so as a woman".
Decline.
By the late 18th century, changes in operatic taste and social attitudes spelled the end for castrati. They lingered on past the end of the "ancien régime" (which their style of opera parallels), and two of their number, Pacchierotti and Crescentini, even entranced the iconoclastic Napoleon. The last great operatic castrato was Giovanni Battista Velluti (1781–1861), who performed the last operatic castrato role ever written: Armando in "Il crociato in Egitto" by Meyerbeer (Venice, 1824). Soon after this they were replaced definitively as the first men of the operatic stage by a new breed of heroic tenor, as first incarnated by the Frenchman Gilbert-Louis Duprez, the earliest so-called "king of the high Cs". His successors have included such singers as Enrico Tamberlik, Jean de Reszke, Francesco Tamagno, Enrico Caruso, Giovanni Martinelli, Beniamino Gigli, Jussi Björling, Franco Corelli and Luciano Pavarotti, among others.
After the unification of Italy in 1861, castration for musical purposes was officially made illegal (the new Italian state had adopted a French legal code which expressly forbade the practice). In 1878, Pope Leo XIII prohibited the hiring of new castrati by the church: only in the Sistine Chapel and in other papal basilicas in Rome did a few castrati linger. A group photo of the Sistine Choir taken in 1898 shows that by then only six remained (plus the "Direttore Perpetuo", the fine soprano castrato Domenico Mustafà), and in 1902 a ruling was extracted from Pope Leo that no further castrati should be admitted. The official end to the castrati came on St. Cecilia's Day, 22 November 1903, when the new pope, Pius X, issued his "motu proprio", "Tra le Sollecitudini" ('Amongst the Cares'), which contained this instruction: "Whenever ... it is desirable to employ the high voices of sopranos and contraltos, these parts must be taken by boys, according to the most ancient usage of the Church."
The last Sistine castrato to survive was Alessandro Moreschi, the only castrato to have made solo recordings. While an interesting historical record, these discs of his give us only a glimpse of the castrato voice – although he had been renowned as "The Angel of Rome" at the beginning of his career, some would say he was past his prime when the recordings were made in 1902 and 1904 and he never attempted to sing opera. He retired officially in March 1913, and died in 1922.
The Catholic Church's involvement in the castrato phenomenon has long been controversial, and there have recently been calls for it to issue an official apology for its role. As early as 1748, Pope Benedict XIV tried to ban castrati from churches, but such was their popularity at the time that he realised that doing so might result in a drastic decline in church attendance.
The rumours of another castrato sequestered in the Vatican for the personal delectation of the Pontiff until as recently as 1959 have been proven false. The singer in question was a pupil of Moreschi's, Domenico Mancini, such a successful imitator of his teacher's voice that even Lorenzo Perosi, Direttore Perpetuo of the Sistine Choir from 1898 to 1956 and a lifelong opponent of castrati, thought he was a castrato. Mancini was in fact a moderately skilful falsettist and professional double-bass player.
Modern castrati and similar voices.
So-called "natural" or "endocrinological castrati" are born with hormonal anomalies such as Kallmann's syndrome or have undergone unusual physical or medical events during their early lives that reproduce the vocal effects of castration without being castrated. Jimmy Scott and Radu Marian are examples of this type of high male voice. Michael Maniaci is somewhat different, in that he has no hormonal or other anomalies, but for some unknown reason, his voice did not "break" in the usual manner, leaving him still able to sing in the soprano register. Other uncastrated male adults sing soprano, generally using some form of falsetto but in a much higher range than most countertenors. Examples are Aris Christofellis, Jörg Waschinski, and Ghio Nannini. However, it is believed the castrati possessed more of a tenorial chest register (the aria "Navigante che non spera" in Leonardo Vinci's opera "Il Medo", written for Farinelli, requires notes down to C3). Similar low-voiced singing can be heard from the jazz vocalist Jimmy Scott, whose range matches approximately that used by female blues singers, while the Turkish popular singer Cem Adrian has the ability to sing from bass to soprano, his vocal folds having been reported to be three times the average length. Actor Chris Colfer has a similar range. Colfer has stated in interviews that when his voice began to change at puberty he sang in a high voice "constantly" in an effort to retain his range. The late musician Jeff Buckley had a four-octave range, which allowed him to cover women's songs in a natural voice and reach notes from bass to soprano.
References.
Howard, P: "The Modern Castrato: Gaetano Guadagni and the coming of a new operatic age", (New York, 2014)

</doc>
<doc id="5743" url="http://en.wikipedia.org/wiki?curid=5743" title="Counting-out game">
Counting-out game

A counting-out game is a simple game intended to select a person to be "it", often for the purpose of playing another game. These games usually require no materials, and are played with spoken words or hand gestures.
Many such games involve one person pointing at each participant in a circle of players while reciting a rhyme. A new person is pointed at as each word is said. The player who is selected at the conclusion of the rhyme is "it" or "out". In an alternate version, the circle of players may each put two feet in and at the conclusion of the rhyme, that player removes one foot and the rhyme starts over with the next person. In this case, the first player that has both feet removed is "it" or "out". These are often accepted as random selections because the number of words has not been calculated beforehand, so the result is unknown right up until someone is selected.
A variant of counting-out game, known as Josephus problem, represents a famous theoretical problem in mathematics and computer science.
Counting-out games.
Several simple games can be played to select one person from a group, either as a straightforward winner, or as someone who is eliminated. Rock, Paper, Scissors, Odd or Even and Blue Shoe require no materials and are played using hand gestures, although with the former it is possible for a player to win or lose through skill rather than luck. Coin flipping and drawing straws are fair methods of randomly determining a player. Bizz Buzz is a spoken word game where if a player slips up and speaks a word out of sequence, they are eliminated.

</doc>
<doc id="5749" url="http://en.wikipedia.org/wiki?curid=5749" title="Key size">
Key size

In cryptography, key size or key length is the size measured in bits of the key used in a cryptographic algorithm (such as a cipher). An algorithm's key length is distinct from its cryptographic security, which is a logarithmic measure of the fastest known computational attack on the algorithm, also measured in bits. The security of an algorithm cannot exceed its key length (since any algorithm can be cracked by brute force), but it can be smaller. For example, Triple DES has a key size of 168 bits but provides at most 112 bits of security, since an attack of complexity 2112 is known. This property of Triple DES is not a weakness provided 112 bits of security is sufficient for an application. Most symmetric-key algorithms in common use are designed to have security equal to their key length. No asymmetric-key algorithms with this property are known; elliptic curve cryptography comes the closest with an effective security of roughly half its key length.
Significance.
Keys are used to control the operation of a cipher so that only the correct key can convert encrypted text (ciphertext) to plaintext. Many ciphers are actually based on publicly known algorithms or are open source, and so it is only the difficulty of obtaining the key that determines security of the system, provided that there is no analytic attack (i.e., a 'structural weakness' in the algorithms or protocols used), and assuming that the key is not otherwise available (such as via theft, extortion, or compromise of computer systems). The widely accepted notion that the security of the system should depend on the key alone has been explicitly formulated by Auguste Kerckhoffs (in the 1880s) and Claude Shannon (in the 1940s); the statements are known as Kerckhoffs' principle and Shannon's Maxim respectively.
A key should therefore be large enough that a brute force attack (possible against any encryption algorithm) is infeasible – i.e., would take too long to execute. Shannon's work on information theory showed that to achieve so called "perfect secrecy", the key length must be at least as large as the message and only used once (this algorithm is called the One-time pad). In light of this, and the practical difficulty of managing such long keys, modern cryptographic practice has discarded the notion of perfect secrecy as a requirement for encryption, and instead focuses on "computational security", under which the computational requirements of breaking an encrypted text must be infeasible for an attacker.
The preferred numbers commonly used as key sizes (in bits) are powers of two, potentially multiplied with a small odd integer.
Key size and encryption system.
Encryption systems are often grouped into families. Common families include symmetric systems (e.g. AES) and asymmetric systems (e.g. RSA); they may alternatively be grouped according to the central algorithm used (e.g. elliptic curve cryptography).
As each of these is of a different level of cryptographic complexity, it is usual to have different key sizes for the same level of security, depending upon the algorithm used. For example, the security available with a 1024-bit key using asymmetric RSA is considered approximately equal in security to an 80-bit key in a symmetric algorithm (Source: RSA Security).
The actual degree of security achieved over time varies, as more computational power and more powerful mathematical analytic methods become available. For this reason cryptologists tend to look at indicators that an algorithm or key length shows signs of potential vulnerability, to move to longer key sizes or more difficult algorithms. For example , a 1039 bit integer was factored with the special number field sieve using 400 computers over 11 months. The factored number was of a special form; the special number field sieve cannot be used on RSA keys. The computation is roughly equivalent to breaking a 700 bit RSA key. However, this might be an advance warning that 1024 bit RSA used in secure online commerce should be deprecated, since they may become breakable in the near future. Cryptography professor Arjen Lenstra observed that "Last time, it took nine years for us to generalize from a special to a nonspecial, hard-to-factor number" and when asked whether 1024-bit RSA keys are dead, said: "The answer to that question is an unqualified yes."
Brute force attack.
Even if a symmetric cipher is currently unbreakable by exploiting structural weaknesses in its algorithm, it is possible to run through the entire space of keys in what is known as a "brute force attack". Since longer symmetric keys require exponentially more work to brute force search, a sufficiently long symmetric key makes this line of attack impractical.
With a key of length "n" bits, there are 2n possible keys. This number grows very rapidly as "n" increases. Moore's law says that computing power doubles every 18 to 24 months, but even this doubling effect leaves the larger symmetric key lengths currently considered acceptable well out of reach. The large number of operations (2128) required to try all possible 128-bit keys is widely considered out of reach for conventional digital computing techniques for the foreseeable future. However, experts anticipate alternative computing technologies that may have processing power superior to current computer technology. If a suitably sized quantum computer capable of running Grover's algorithm reliably becomes available, it would reduce a 128-bit key down to 64-bit security, roughly a DES equivalent. This is one of the reasons why AES supports a 256-bit key length. See the discussion on the relationship between key lengths and quantum computing attacks at the bottom of this page for more information.
Symmetric algorithm key lengths.
US Government export policy has long restricted the 'strength' of cryptography that can be sent out of the country. For many years the limit was 40 bits. Today, a key length of 40 bits offers little protection against even a casual attacker with a single PC, a predictable and inevitable consequence of governmental restrictions limiting key length. In response, by the year 2000, most of the major US restrictions on the use of strong encryption were relaxed. However, not all regulations have been removed, and encryption registration with the U.S. Bureau of Industry and Security is still required to export "mass market encryption commodities, software and components with encryption exceeding 64 bits" ().
When the Data Encryption Standard cipher was released in 1977, a key length of 56 bits was thought sufficient. There was speculation at the time, however, that the NSA has deliberately reduced the key size from the original value of 112 bits (in IBM's Lucifer cipher) or 64 bits (in one of the versions of what was adopted as DES) so as to limit the strength of encryption available to non-US users. The NSA has major computing resources and a large budget; some thought that 56 bits was NSA-breakable in the late '70s. However, by the late 90s, it became clear that DES could be cracked in a few days' time-frame with custom-built hardware such as could be purchased by a large corporation. The book "Cracking DES" (O'Reilly and Associates) tells of the successful attempt to break 56-bit DES by a brute force attack mounted by a cyber civil rights group with limited resources; see EFF DES cracker. 56 bits is now considered insufficient length for symmetric algorithm keys, and may have been for some time. More technically and financially capable organizations were surely able to do the same long before the effort described in the book. Distributed.net and its volunteers broke a 64-bit RC5 key in several years, using about seventy thousand (mostly home) computers.
The NSA's Skipjack algorithm used in its Fortezza program employs 80 bit keys.
DES has been replaced in many applications by Triple DES, which has 112 bits of security with 168-bit keys.
The Advanced Encryption Standard published in 2001 uses a key size of (at minimum) 128 bits. It also can use keys up to 256 bits (a specification requirement for submissions to the AES contest). Many observers currently think 128 bits is sufficient for the foreseeable future for symmetric algorithms of AES's quality. The U.S. Government requires 192 or 256-bit AES keys for highly sensitive data.
In 2003, the U.S. National Institute for Standards and Technology, NIST proposed phasing out 80-bit keys by 2015. As of 2005, 80-bit keys were allowed only until 2010.
Asymmetric algorithm key lengths.
The effectiveness of public key cryptosystems depends on the intractability (computational and theoretical) of certain mathematical problems such as integer factorization. These problems are time consuming to solve, but usually faster than trying all possible keys by brute force. Thus, asymmetric algorithm keys must be longer for equivalent resistance to attack than symmetric algorithm keys. As of 2002, an asymmetric key length of 1024 bits was generally considered the minimum necessary for the RSA encryption algorithm.
The Finite Field Diffie-Hellman algorithm has roughly the same key strength as RSA for the same key sizes. The work factor for breaking Diffie-Hellman is based on the discrete logarithm problem, which is related to the integer factorization problem on which RSA's strength is based. Thus, a 3072-bit Diffie-Hellman key has about the same strength as a 3072-bit RSA key.
One of the asymmetric algorithm types, elliptic curve cryptography, or ECC, appears to be secure with shorter keys than other asymmetric key algorithms require. NIST guidelines state that ECC keys should be twice the length of equivalent strength symmetric key algorithms. So, for example, a 224-bit ECC key would have roughly the same strength as a 112-bit symmetric key. These estimates assume no major breakthroughs in solving the underlying mathematical problems that ECC is based on. A message encrypted with an elliptic key algorithm using a 109-bit long key has been broken by brute force.
The NSA specifies that "Elliptic Curve Public Key Cryptography using the 256-bit prime modulus elliptic curve as specified in FIPS-186-2 and SHA-256 are appropriate for protecting classified information up to the SECRET level. Use of the 384-bit prime modulus elliptic curve and SHA-384 are necessary for the protection of TOP SECRET information."
Effect of quantum computing attacks on key strength.
The two best known quantum computing attacks are based on Shor's algorithm and Grover's algorithm. Of the two, Shor's offers the greater risk to current security systems.
Derivatives of Shor's algorithm are widely conjectured to be effective against all mainstream public-key algorithms including RSA, Diffie-Hellman and elliptic curve cryptography. According to Professor Gilles Brassard, an expert in quantum computing: "The time needed to factor an RSA integer is the same order as the time needed to use that same integer as modulus for a single RSA encryption. In other words, it takes no more time to break RSA on a quantum computer (up to a multiplicative constant) than to use it legitimately on a classical computer." The general consensus is that these public key algorithms are insecure at any key size if sufficiently large quantum computers capable of running Shor's algorithm become available. The implication of this attack is that all data encrypted using current standards based security systems such as the ubiquitous SSL used to protect e-commerce and Internet banking and SSH used to protect access to sensitive computing systems is at risk. Encrypted data protected using public-key algorithms can be archived and may be broken at a later time.
Mainstream symmetric ciphers (such as AES or Twofish) and collision resistant hash functions (such as SHA) are widely conjectured to offer greater security against known quantum computing attacks. They are widely thought most vulnerable to Grover's algorithm. Bennett, Bernstein, Brassard, and Vazirani proved in 1996 that a brute-force key search on a quantum computer cannot be faster than roughly 2"n"/2 invocations of the underlying cryptographic algorithm, compared with roughly 2"n" in the classical case. Thus in the presence of large quantum computers an "n"-bit key can provide at least "n"/2 bits of security. Quantum brute force is easily defeated by doubling the key length, which has little extra computational cost in ordinary use. This implies that at least a 160-bit symmetric key is required to achieve 80-bit security rating against a quantum computer.

</doc>
<doc id="5750" url="http://en.wikipedia.org/wiki?curid=5750" title="Cognitive behavioral therapy">
Cognitive behavioral therapy

Cognitive behavioral therapy (CBT) is a psychotherapeutic approach that addresses dysfunctional emotions, maladaptive behaviors and cognitive processes and contents through a number of goal-oriented, explicit systematic procedures. The name refers to behavior therapy, cognitive therapy, and to therapy based upon a combination of basic behavioral and cognitive principles and research. Most therapists working with patients dealing with anxiety and depression use a blend of cognitive and behavioral therapy. This technique acknowledges that there may be behaviors that cannot be controlled through rational thought. CBT is "problem focused" (undertaken for specific problems) and "action oriented" (therapist tries to assist the client in selecting specific strategies to help address those problems).
CBT is thought to be effective for the treatment of a variety of conditions, including mood, anxiety, personality, eating, substance abuse, tic, and psychotic disorders. Many CBT treatment programs for specific disorders have been evaluated for efficacy; the health-care trend of evidence-based treatment, where specific treatments for symptom-based diagnoses are recommended, has favored CBT over other approaches such as psychodynamic treatments.
CBT was primarily developed through an integration of behavior therapy (the term "behavior modification" appears to have been first used by Edward Thorndike) with cognitive psychology research, first by Donald Meichenbaum and several other authors with the label of cognitive behavior modification in the late 1970s. This tradition thereafter merged with earlier work of a few clinicians, labeled as Cognitive Therapy (CT), developed first by Albert Ellis as Rational Emotive Therapy (RET) and later Aaron Beck. While rooted in rather different theories, these two traditions have been characterised by a constant reference to experimental research to test hypotheses, both at clinical and basic level. Common features of CBT procedures are the focus on the "here and now", a directive or guidance role of the therapist, a structuring of the psychotherapy sessions and path, and on alleviating both symptoms and patients' vulnerability.
Description.
Mainstream cognitive behavioral therapy assumes that changing maladaptive thinking leads to change in affect and behavior, but recent variants emphasize changes in one's relationship to maladaptive thinking rather than changes in thinking itself. Therapists or computer-based programs use CBT techniques to help individuals challenge their patterns and beliefs and replace "errors in thinking such as overgeneralizing, magnifying negatives, minimizing positives and catastrophizing" with "more realistic and effective thoughts, thus decreasing emotional distress and self-defeating behavior." These errors in thinking are known as cognitive distortions. CBT techniques may also be used to help individuals take a more open, mindful, and aware posture toward them so as to diminish their impact. Mainstream CBT helps individuals replace "maladaptive… coping skills, cognitions, emotions and behaviors with more adaptive ones", by challenging an individual's way of thinking and the way that he/she reacts to certain habits or behaviors, but there is still controversy about the degree to which these traditional cognitive elements account for the effects seen with CBT over and above the earlier behavioral elements such as exposure and skills training.
Modern forms of CBT include a number of diverse but related techniques such as exposure therapy, stress inoculation training, cognitive processing therapy, cognitive therapy, relaxation training, dialectical behavior therapy, and acceptance and commitment therapy.
CBT has six phases:
The reconceptualization phase makes up much of the "cognitive" portion of CBT. A summary of modern CBT approaches is given by Hofmann.
There are different protocols for delivering cognitive behavioral therapy, with important similarities among them. Use of the term "CBT" may refer to different interventions, including "self-instructions (e.g. distraction, imagery, motivational self-talk), relaxation and/or biofeedback, development of adaptive coping strategies (e.g. minimizing negative or self-defeating thoughts), changing maladaptive beliefs about pain, and goal setting". Treatment is sometimes manualized, with brief, direct, and time-limited treatments for individual psychological disorders that are specific technique-driven. CBT is used in both individual and group settings, and the techniques are often adapted for self-help applications. Some clinicians and researchers are cognitively oriented (e.g. cognitive restructuring), while others are more behaviorally oriented (e.g. "in vivo" exposure therapy). Interventions such as imaginal exposure therapy combine both approaches.
Specific applications.
CBT has been applied in both clinical and non-clinical environments to treat disorders such as personality conditions and behavioral problems. A systematic review of CBT in depression and anxiety disorders concluded that "CBT delivered in primary care, especially including computer- or Internet-based self-help programs, is potentially more effective than usual care and could be delivered effectively by primary care therapists."
Emerging evidence suggests a possible role for CBT in the treatment of attention deficit hyperactivity disorder (ADHD); hypochondriasis; coping with the impact of multiple sclerosis; sleep disturbances related to aging; dysmenorrhea; and bipolar disorder, but more study is needed and results should be interpreted with caution. CBT has been studied as an aid in the treatment of anxiety associated with stuttering. Initial studies have shown CBT to be effective in reducing social anxiety in adults who stutter, but not in reducing stuttering frequency.
Martinez-Devesa "et al." (2010) found no evidence that CBT is effective for tinnitus, although there appears to be an effect on management of associated depression and quality of life in this condition. Turner "et al." (2007) found no convincing evidence that CBT training helps foster care providers manage difficult behaviors in the youth under their care, and Smedslund "et al." (2007) found that it was not helpful in treating men who abuse their intimate partners.
In the case of metastatic breast cancer, Edwards "et al." (2008) maintained that the current body of evidence is not sufficient to rule out the possibility that psychological interventions may cause harm to women with this advanced neoplasm.
In adults, CBT has been shown to have a role in the treatment plans for anxiety disorders; depression; eating disorders; chronic low back pain; personality disorders; psychosis; schizophrenia; substance use disorders; in the adjustment, depression, and anxiety associated with fibromyalgia; and with post-spinal cord injuries. There is some evidence that CBT is superior in the long-term to benzodiazepines and the nonbenzodiazepines in the treatment and management of insomnia. CBT has been shown to be moderately effective for treating chronic fatigue syndrome.
In children or adolescents, CBT is an effective part of treatment plans for anxiety disorders; body dysmorphic disorder; depression and suicidality; eating disorders and obesity; obsessive–compulsive disorder; and posttraumatic stress disorder; as well as tic disorders, trichotillomania, and other repetitive behavior disorders. CBT-SP, an adaptation of CBT for suicide prevention (SP), was specifically designed for treating youth who are severely depressed and who have recently attempted suicide within the past 90 days, and was found to be effective, feasible, and acceptable.
Sparx is a video game to help young persons, using the CBT method to teach them how to resolve their own issues. That's a new way of therapy, which is quite effective for child and teenager.
CBT has also been shown to be effective for posttraumatic stress disorder in very young children (3 to 6 years of age).
Cognitive Behavior Therapy has also been applied to a variety of childhood disorders, including depressive disorders and various anxiety disorders.
In the United Kingdom, the National Institute for Health and Clinical Excellence (NICE) recommends CBT in the treatment plans for a number of mental health difficulties, including posttraumatic stress disorder, obsessive–compulsive disorder (OCD), bulimia nervosa, and clinical depression.
Anxiety disorders.
CBT has been shown to be effective in the treatment of adult anxiety disorders.
A basic concept in some CBT treatments used in anxiety disorders is "in vivo" exposure. The term refers to the direct confrontation of feared objects, activities, or situations by a patient. For example, a woman with PTSD who fears the location where she was assaulted may be assisted by her therapist in going to that location and directly confronting those fears. Likewise, a person with social anxiety disorder who fears public speaking may be instructed to directly confront those fears by giving a speech. This "two-factor" model is often credited to O. Hobart Mowrer. Through exposure to the stimulus, this harmful conditioning can be "unlearned" (referred to as extinction and habituation).
Schizophrenia, psychosis and mood disorders.
Cognitive behavioral therapy has been shown as an effective treatment for clinical depression. The American Psychiatric Association Practice Guidelines (April 2000) indicated that, among psychotherapeutic approaches, cognitive behavioral therapy and interpersonal psychotherapy had the best-documented efficacy for treatment of major depressive disorder. One etiological theory of depression is Aaron T. Beck's cognitive theory of depression. His theory states that depressed people think the way they do because their thinking is biased towards negative interpretations. According to this theory, depressed people acquire a negative schema of the world in childhood and adolescence as an effect of stressful life events, and the negative schema is activated later in life when the person encounters similar situations.
Beck also described a negative cognitive triad, made up of the negative schemata and cognitive biases of the person, theorizing that depressed individuals make negative evaluations of themselves, the world, and the future. Depressed people, according to this theory, have views such as, "I never do a good job", "It is impossible to have a good day", and "things will never get better." A negative schema helps give rise to the cognitive bias, and the cognitive bias helps fuel the negative schema. This is the negative triad. Beck further proposed that depressed people often have the following cognitive biases: arbitrary inference, selective abstraction, over-generalization, magnification, and minimization. These cognitive biases are quick to make negative, generalized, and personal inferences of the self, thus fueling the negative schema.
In long-term psychoses, CBT is used to complement medication and is adapted to meet individual needs. Interventions particularly related to these conditions include exploring reality testing, changing delusions and hallucinations, examining factors which precipitate relapse, and managing relapses. Several meta-analyses have shown CBT to be effective in schizophrenia, and the American Psychiatric Association includes CBT in its schizophrenia guideline as an evidence-based treatment. There is also some (limited) evidence of effectiveness for CBT in bipolar disorder and severe depression.
A 2010 meta-analysis found that no trial employing both blinding and psychological placebo has shown CBT to be effective in either schizophrenia or bipolar disorder, and that the effect size of CBT was small in major depressive disorder. They also found a lack of evidence to conclude that CBT was effective in preventing relapses in bipolar disorder. Evidence that severe depression is mitigated by CBT is also lacking, with anti-depressant medications still viewed as significantly more effective than CBT, although success with CBT for depression was observed beginning in the 1990s.
According to Cox, Abramson, Devine, and Hollon (2012), cognitive behavioral therapy can also be used to reduce prejudice towards others. This other-directed prejudice can cause depression in the "others," or in the self when a person becomes part of a group he or she previously had prejudice towards (i.e. deprejudice). "Devine and colleagues (2012) developed a successful Prejudice Perpetrator intervention with many conceptual parallels to CBT. Like CBT, their intervention taught Sources to be aware of their automative thoughts and to intentionally deploy a variety of cognitive techniques against automatic stereotyping."
With older adults.
CBT is used to help people of all ages, but the therapy should be adjusted based on the age of the patient with whom the therapist is dealing. Older individuals in particular have certain characteristics that need to be acknowledged and the therapy altered to account for these differences thanks to age. Some of the challenges to CBT because of age include the following:
Methods of access.
Therapist.
A typical CBT programme would consist of face-to-face sessions between patient and therapist, made up of 6-18 sessions of around an hour each with a gap of a 1–3 weeks between sessions. This initial programme might be followed by some booster sessions, for instance after one month and three months. CBT has also been found to be effective if patient and therapist type in real time to each other over computer links.
Cognitive behavioral therapy is most closely allied with the scientist–practitioner model in which clinical practice and research is informed by a scientific perspective, clear operationalization of the problem, and an emphasis on measurement, including measuring changes in cognition and behavior and in the attainment of goals. These are often met through "homework" assignments in which the patient and the therapist work together to craft an assignment to complete before the next session. The completion of these assignments – which can be as simple as a person suffering from depression attending some kind of social event – indicates a dedication to treatment compliance and a desire to change. The therapists can then logically gauge the next step of treatment based on how thoroughly the patient completes the assignment. Effective cognitive behavioral therapy is dependent on a therapeutic alliance between the healthcare practitioner and the person seeking assistance. Unlike many other forms of psychotherapy, the patient is very involved in CBT. For example, an anxious patient may be asked to talk to a stranger as a homework assignment, but if that is too difficult, he or she can work out an easier assignment first. The therapist needs to be flexible and willing to listen to the patient rather than acting as an authority figure.
Computerized.
Computerized cognitive behavioral therapy (CCBT) has been described by NICE as a ""generic term for delivering CBT via an interactive computer interface delivered by a personal computer, internet, or interactive voice response system"", instead of face-to-face with a human therapist. CCBT has potential to improve access to evidence-based therapies, and to overcome the prohibitive costs and lack of availability sometimes associated with retaining a human therapist.
CCBT has been found in meta-studies to be cost-effective and often cheaper than usual care. A key issue in CCBT use is low uptake and completion rates, even when it has been clearly made available and explained. CCBT completion rates and treatment efficacy have been found in some studies to be higher when use of CCBT is supported personally, with supporters not limited only to therapists, than when use is in a self-help form alone.
A wide range of software products incorporate CCBT, including, for example, products such as Sparx (video game).
In February 2006 NICE recommended that CCBT be made available for use within the NHS across England and Wales for patients presenting with mild-to-moderate depression, rather than immediately opting for antidepressant medication, and CCBT is made available by some health systems. The 2009 NICE guideline recognized that there are likely to be a number of computerized CBT products that are useful to patients, but removed endorsement of any specific product.
A relatively new avenue of research is the combination of artificial intelligence and CCBT. It has been proposed to use modern technology to create CCBT that simulates face-to-face therapy. This might be achieved in cognitive behaviour therapy for a specific disorders using the comprehensive domain knowledge of CBT. One area where this has been attempted, is the specific domain area of social anxiety in those who stutter.
Reading self-help materials.
Enabling patients to read self-help CBT guides has been shown to be effective by some studies. However one study found a negative effect in patients who tended to ruminate, and another meta-analysis found that the benefit was only significant when the self-help was guided (e.g. by a medical professional).
Group educational course.
Patient participation in group courses has been shown to be effective.
Brief cognitive behavioral therapy.
Brief cognitive behavioral therapy (BCBT) is a form of CBT which has been developed for situations in which there are time constraints on the therapy sessions. BCBT takes place over a couple of sessions that can last up to 12 accumulated hours by design. This technique was first implemented and developed on soldiers overseas in active duty by David M. Rudd to prevent suicide.
Breakdown of treatment
Cognitive emotional behavioral therapy.
Cognitive emotional behavioral therapy (CEBT) is a form of (CBT) developed initially for individuals with eating disorders but now used with a range of problems including anxiety, depression, obsessive compulsive disorder (OCD), post traumatic stress disorder (PTSD) and anger problems. It combines aspect of CBT and Dialectical Behavioural Therapy and aims to improve understanding and tolerance of emotions in order to facilitate the therapeutic process. It is frequently used as a 'pretreatment' to prepare and better equip individuals for longer term therapy.
Structured cognitive behavioral training.
Structured cognitive behavioral training (SCBT) is a cognitive-based process with core philosophies that draw heavily from CBT. Like CBT, SCBT asserts that behavior is inextricably related to beliefs, thoughts and emotions. SCBT also builds on core CBT philosophy by incorporating other well-known modalities in the fields of behavioral health and psychology: most notably, Albert Ellis's Rational Emotive Behavior Therapy. SCBT differs from CBT in two distinct ways. Firstly, SCBT is delivered in a highly regimented format. Secondly, SCBT is a predetermined and finite training process that becomes personalized by the input of the participant. SCBT is designed with the intention to bring a participant to a specific result in a specific period of time. SCBT has been used to challenge addictive behavior, particularly with substances such as tobacco, alcohol and food; and to manage diabetes and subdue stress and anxiety. SCBT has also been used in the field of criminal psychology in the effort to reduce recidivism.
Effectiveness.
In adults, CBT has been shown to have effectiveness and a role in the treatment plans for anxiety disorders, depression, eating disorders, chronic low back pain, personality disorders, psychosis, schizophrenia, substance use disorders, in the adjustment, depression, and anxiety associated with fibromyalgia, and with post-spinal cord injuries. Evidence has shown CBT is effective in helping treat schizophrenia, and it is now offered in most treatment guidelines.
In children or adolescents, CBT is an effective part of treatment plans for anxiety disorders, body dysmorphic disorder, depression and suicidality, eating disorders and obesity, obsessive–compulsive disorder, and posttraumatic stress disorder, as well as tic disorders, trichotillomania, and other repetitive behavior disorders.
Cochrane reviews have found no evidence that CBT is effective for tinnitus, although there appears to be an effect on management of associated depression and quality of life in this condition. Other recent Cochrane Reviews found no convincing evidence that CBT training helps foster care providers manage difficult behaviors in the youth under their care, nor was it helpful in treating men who abuse their intimate partners.
According to a 2004 review by INSERM of three methods, cognitive behavioral therapy was either "proven" or "presumed" to be an effective therapy on several specific mental disorders. According to the study, CBT was effective at treating schizophrenia, depression, bipolar disorder, panic disorder, post-traumatic stress, anxiety disorders, bulimia, anorexia, personality disorders and alcohol dependency.
Some meta-analyses find CBT more effective than psychodynamic therapy and equal to other therapies in treating anxiety and depression. However, psychodynamic therapy may provide better long-term outcomes.
Computerized CBT (CCBT) has been proven to be effective by randomized controlled and other trials in treating depression and anxiety disorders, including children, as well as insomnia. Some research has found similar effectiveness to an intervention of informational websites and weekly telephone calls. CCBT was found to be equally effective as face-to-face CBT in adolescent anxiety and insomnia.
Criticism of CBT sometimes focuses on implementations (such as the UK IAPT) which may result initially in low quality therapy being offered by poorly trained practitioners. However evidence supports the effectiveness of CBT for anxiety and depression.
Mounting evidence suggests that the addition of hypnotherapy as an adjunct to CBT improves treatment efficacy for a variety of clinical issues.
Criticisms.
One criticism of CBT theory, especially as applied to Major Depressive Disorder (MDD), is that it confounds the symptoms of the disorder with its causes.
A major criticism has been that clinical studies of CBT efficacy (or any psychotherapy) are not double-blind (i.e., neither subjects nor therapists in psychotherapy studies are blind to the type of treatment). They may be single-blinded, i.e. the rater may not know the treatment the patient received, but neither the patients nor the therapists are blinded to the type of therapy given (two out of three of the persons involved in the trial, i.e., all of the persons involved in the treatment, are unblinded). The patient is an active participant in correcting negative distorted thoughts, thus quite aware of the treatment group they are in.
The importance of double-blinding was shown in a meta-analysis that examined the effectiveness of CBT when placebo control and blindedness were factored in. Pooled data from published trials of CBT in schizophrenia, MDD, and bipolar disorder that used controls for non-specific effects of intervention were analyzed. This study concluded that CBT is no better than non-specific control interventions in the treatment of schizophrenia and does not reduce relapse rates, treatment effects are small in treatment studies of MDD, and it is not an effective treatment strategy for prevention of relapse in bipolar disorder. For MDD, the authors note that the pooled effect size was very low. Nevertheless, the methodological processes used to select the studies in the previously mentioned meta-analysis and the worth of its findings have been called into question.
The element of hope and expectation on the part of the patients to get better in non-blinded trials will bias the results in favor of CBT. The informed consent procedure required to enter a psychotherapy trial biases the subjects who enter to those that are favorably inclined to the psychotherapy. Taken together, trials using psychotherapy do not meet the qualifications of high quality evidence.
Use in prevention of mental illness.
For anxiety disorders, use of CBT with people at risk has significantly reduced the number of episodes of generalized anxiety disorder and other anxiety symptoms, and also given significant improvements in explanatory style, hopelessness, and dysfunctional attitudes. In another study, 3% of the group receiving the CBT intervention developed GAD by 12 months postintervention compared with 14% in the control group. Subthreshold panic disorder sufferers were found to significantly benefit from use of CBT. Use of CBT was found to significantly reduce social anxiety prevalence.
For depressive disorders, a stepped-care intervention (watchful waiting, CBT and medication if appropriate) achieved a 50% lower incidence rate in a patient group aged 75 or older. Another depression study found a neutral effect compared to personal, social, and health education, and usual school provision, and included a comment on potential for increased depression scores from people who have received CBT due to greater self recognition and acknowledgement of existing symptoms of depression and negative thinking styles. A further study also saw a neutral result. A meta-study of the Coping with Depression course, a cognitive behavioural intervention delivered by a psychoeducational method, saw a 38% reduction in risk of major depression.
For schizophrenia, one study of preventative CBT showed a positive effect and another showed neutral effect.
History.
Behavior therapy roots.
Precursors of certain fundamental aspects of CBT have been identified in various ancient philosophical traditions, particularly Stoicism. For example, Aaron T. Beck's original treatment manual for depression states, "The philosophical origins of cognitive therapy can be traced back to the Stoic philosophers". The modern roots of CBT can be traced to the development of behavior therapy in the early 20th century, the development of cognitive therapy in the 1960s, and the subsequent merging of the two. Behaviorally-centered therapeutic approaches appeared as early as 1924 with Mary Cover Jones' work on the unlearning of fears in children. In 1937, American psychiatrist Abraham Low developed cognitive training techniques for patient aftercare following psychiatric hospitalization.
It was during the period 1950 to 1970 that behavioral therapy became widely utilized by researchers in the United States, the United Kingdom, and South Africa, who were inspired by the behaviorist learning theory of Ivan Pavlov, John B. Watson, and Clark L. Hull. In Britain, this work was mostly focused on the neurotic disorders through the work of Joseph Wolpe, who applied the findings of animal experiments to his method of systematic desensitization, the precursor to today's fear reduction techniques. British psychologist Hans Eysenck, inspired by the writings of Karl Popper, criticized psychoanalysis in arguing that "if you get rid of the symptoms, you get rid of the neurosis", and presented behavior therapy as a constructive alternative. In the United States, psychologists were applying the radical behaviorism of B. F. Skinner to clinical use. Much of this work was concentrated on severe chronic psychiatric disorders, such as psychotic behavior and autism.
Other roots.
Although the early behavioral approaches were successful in many of the neurotic disorders, they had little success in treating depression. Behaviorism was also losing in popularity due to the so-called "cognitive revolution". The therapeutic approaches of Albert Ellis and Aaron T. Beck gained popularity among behavior therapists, despite the earlier behaviorist rejection of "mentalistic" concepts like thoughts and cognitions. Both of these systems included behavioral elements and interventions and primarily concentrated on problems in the present. Albert Ellis' system, originated in the early 1950s, was first called rational therapy, and can (arguably) be called one of the first forms of cognitive behavioral therapy. It was partly founded as a reaction against popular psychotherapeutic theories at the time (mainly psychoanalysis). Beck describes his therapeutic approach as originating in a realization he made while conducting free association with patients in the context of classical psychoanalysis. He noted that patients had not been reporting certain thoughts at the fringe of consciousness – thoughts which often preceded intense emotional reactions. This realization led Beck to begin viewing emotional reactions as resulting from "cognitions", rather than understanding emotion within the abstract psychoanalytic framework. He named these cognitions "automatic thoughts" because he believed that people were not necessarily aware that the cognitions existed, but that they could identify these types of thoughts when questioned closely. Beck believed that pushing his clients to identify these automatic thoughts was integral to overcoming a particular difficulty.
In initial studies, cognitive therapy was often contrasted with behavioral treatments to see which was most effective. During the 1980s and 1990s, cognitive and behavioral techniques were merged into cognitive behavioral therapy. Pivotal to this merging was the successful development of treatments for panic disorder by David M. Clark in the UK and David H. Barlow in the US.
Society and culture.
The UK's National Health Service announced in 2008 that more therapists would be trained to provide CBT at government expense as part of an initiative called Improving Access to Psychological Therapies (IAPT). NICE said that CBT would become the mainstay of treatment for non-severe depression, with medication used only in cases where CBT had failed. Therapists complained that the data does not fully support the attention and funding CBT receives. Psychotherapist and professor Andrew Samuels stated that this constitutes "a coup, a power play by a community that has suddenly found itself on the brink of corralling an enormous amount of money ... Everyone has been seduced by CBT's apparent cheapness." The UK Council for Psychotherapy issued a press release in 2012 saying that the IAPT's policies were undermining traditional psychotherapy and criticized proposals that would limit some approved therapies to CBT, claiming that they restricted patients to "a watered down version of cognitive behavioural therapy (CBT), often delivered by very lightly trained staff".
NICE also recommends offering CBT to all people with schizophrenia.

</doc>
<doc id="5751" url="http://en.wikipedia.org/wiki?curid=5751" title="Chinese language">
Chinese language

Chinese (; "Hànyǔ" or ; "Zhōngwén") is a group of related but in many cases mutually unintelligible language varieties, forming a branch of the Sino-Tibetan language family. Chinese is spoken by the Han majority and many other ethnic groups in China. More than one billion people, or about one-fifth of the world's population, speak some form of Chinese as their first language.
Varieties of Chinese are usually described by native speakers as dialects of a single Chinese language, but linguists note that they are as diverse as a language family.
The internal diversity of Chinese has been likened to that of the Romance languages. There are between 7 and 13 main regional groups of Chinese (depending on classification scheme), of which the most spoken, by far, is Mandarin (about 960 million), followed by Wu (80 million), Yue (60 million) and Min (50 million). Most of these groups are mutually unintelligible, although some, like Xiang and the Southwest Mandarin dialects, may share common terms and some degree of intelligibility. All varieties of Chinese are tonal and analytic.
Standard Chinese "(Putonghua/Guoyu/Huayu)" is a standardized form of spoken Chinese based on the Beijing dialect of Mandarin. It is the official language of the People's Republic of China (PRC) and the Republic of China (ROC, also known as Taiwan), as well as one of four official languages of Singapore. It is one of the six official languages of the United Nations. The written form of the standard language (; "Zhōngwén"), based on the logograms known as Chinese characters (; "hànzi"), is shared by literate speakers of otherwise unintelligible dialects.
Of the other varieties of Chinese, Cantonese (the prestige variety of Yue) is influential in Guangdong province and Cantonese-speaking overseas communities and remains one of the official languages of Hong Kong (together with English) and of Macau (together with Portuguese). Min Nan, part of the Min group, is widely spoken in southern Fujian, in neighbouring Taiwan (where it is known as Taiwanese or Hoklo) and in Southeast Asia (also known as Hokkien in the Philippines, Singapore, and Malaysia). There are also sizeable Hakka and Shanghainese diasporas, for example in Taiwan, where most Hakka communities are also conversant in Taiwanese and Standard Chinese.
History.
Chinese can be traced back over 3,000 years to the first written records, and even earlier to a hypothetical Sino-Tibetan proto-language. The language has evolved over time, with various local varieties becoming mutually unintelligible. In reaction, central governments have repeatedly sought to promulgate a unified standard.
Origins.
Most linguists classify all varieties of Chinese as part of the Sino-Tibetan language family, together with Burmese, Tibetan and many other languages spoken in the Himalayas and the Southeast Asian Massif.
Although the relationship was first proposed in the early 19th century and is now broadly accepted, reconstruction of Sino-Tibetan is much less developed than for families such as Indo-European or Austroasiatic.
Difficulties have included the great diversity of the languages, the lack of inflection in many of them, and the effects of language contact.
In addition, many of the smaller languages are spoken in mountainous areas that are difficult to access, and are often also sensitive border zones.
Without a secure reconstruction of proto-Sino-Tibetan, the higher-level structure of the family remains unclear.
A top-level branching into Chinese and Tibeto-Burman languages is often assumed, but has not been convincingly demonstrated.
Old and Middle Chinese.
The earliest examples of Chinese are divinatory inscriptions on oracle bones from around 1250 BCE in the late Shang dynasty.
Old Chinese was the language of the Western Zhou period (1046–771 BCE), recorded in inscriptions on bronze artifacts, the "Classic of Poetry" and portions of the "Book of Documents" and "I Ching".
Scholars have attempted to reconstruct the phonology of Old Chinese by comparing later varieties of Chinese with the rhyming practice of the "Classic of Poetry" and the phonetic elements found in the majority of Chinese characters.
Although many of the finer details remain unclear, most scholars agree that Old Chinese differed from Middle Chinese in lacking retroflex and palatal obstruents but having initial consonant clusters of some sort, and in having voiceless nasals and liquids.
Most recent reconstructions also describe an atonal language with consonant clusters at the end of the syllable, developing into tone distinctions in Middle Chinese.
Several derivational affixes have also been identified, but the language lacked inflection, and indicated grammatical relationships using word order and grammatical particles.
Middle Chinese was the language used during Southern and Northern Dynasties and the Sui, Tang, and Song dynasties (6th through 10th centuries CE).
It can be divided into an early period, reflected by the "Qieyun" rime book (601 CE), and a late period in the 10th century, reflected by rhyme tables such as the "Yunjing" constructed by ancient Chinese philologists as a guide to the "Qieyun" system.
These works define phonological categories, but with little hint of what sounds they represent.
Linguists have identified these sounds by comparing the categories with pronunciations in modern varieties of Chinese, borrowed Chinese words in Japanese, Vietnamese and Korean, and transcription evidence.
The resulting system is very complex, with a large number of consonants and vowels, but they were probably not all distinguished in any single dialect. Most linguists now believe it represents a diasystem encompassing 6th-century northern and southern standards for reading the classics.
Rise of northern dialects.
After the fall of the Northern Song dynasty, and during the reign of the Jin (Jurchen) and Yuan (Mongol) dynasties in northern China, a common speech (now called Old Mandarin) developed based on the dialects of the North China Plain around the capital.
The "Zhongyuan Yinyun" (1324) was a dictionary that codified the rhyming conventions of new "sanqu" verse form in this language.
Together with the slightly later "Menggu Ziyun", this dictionary describes a language with many of the features characteristic of modern Mandarin dialects.
Until the mid-20th century, most of the Chinese people living in many parts of southern China spoke only their local language.
As a practical measure, officials of the Ming and Qing dynasties carried out the administration of the empire using a common language based on Mandarin varieties, known as "Guānhuà" (官話, literally "language of officials").
For most of this period, this language was a koiné based on dialects spoken in the Nanjing area, though not identical to any single dialect.
By the middle of the 19th century, the Beijing dialect had become dominant and was essential for any business with the imperial court.
In the 1930s a standard national language "Guóyǔ" (国语/國語 "national language") was adopted.
After much dispute between proponents of northern and southern dialects and an abortive attempt at an artificial pronunciation, the National Language Unification Commission finally settled on the Beijing dialect in 1932.
The People's Republic founded in 1949 retained this standard, calling it "pǔtōnghuà" (普通话/普通 "common speech").
The national language is now used in education, the media, and formal situations in both Mainland China and Taiwan.
In Hong Kong and Macau, because of their colonial and linguistic history, the language of education, the media, formal speech and everyday life remains the local Cantonese, although the standard language is now very influential and taught in schools.
Influences.
The Chinese language has spread to neighbouring countries through a variety of means.
Northern Vietnam was incorporated into the Han empire in 111 BCE, beginning a period of Chinese control that ran almost continuously for a millennium.
The Four Commanderies were established in northern Korea in the first century BCE, but disintegrated in the following centuries.
Chinese Buddhism spread over East Asia between the 2nd and 5th centuries CE, and with it the study of scriptures and literature in Literary Chinese.
Later Korea, Japan and Vietnam developed strong central governments modelled on Chinese institutions, with Literary Chinese as the language of administration and scholarship, a position it would retain until the late 19th century in Korea and (to a lesser extent) Japan, and the early 20th century in Vietnam.
Scholars from different lands could communicate, albeit only in writing, using Literary Chinese.
Although they used Chinese solely for written communication, each country had its own tradition of reading texts aloud, the so-called Sino-Xenic pronunciations.
Chinese words with these pronunciations were also borrowed extensively into the Korean, Japanese and Vietnamese languages, and today comprise over half their vocabularies.
This massive influx led to changes in the phonological structure of the languages, contributing to the development of moraic structure in Japanese and the disruption of vowel harmony in Korean.
Borrowed Chinese morphemes have been used extensively in all these languages to coin compound words for new concepts, in a similar way to the use of Latin and Ancient Greek roots in European languages.
Many new compounds, or new meanings for old phrases, were created in the late 19th and early 20th centuries to name Western concepts and artifacts.
These coinages, written in shared Chinese characters, have then been borrowed freely between languages.
They have even been accepted into Chinese, a language usually resistant to loanwords, because their foreign origin was hidden by their written form.
Often different compounds for the same concept were in circulation for some time before a winner emerged, and sometimes the final choice differed between countries.
The proportion of vocabulary of Chinese origin thus tends to be greater in technical, abstract or formal language.
For example, Sino-Japanese words account for about 35% of the words in entertainment magazines, over half the words in newspapers, and 60% of the words in science magazines.
Vietnam, Korea and Japan each developed writing systems for their own languages, initially based on Chinese characters, but later replaced with the "Hangul" alphabet for Korean and supplemented with "kana" syllabaries for Japanese, while Vietnamese continued to be written with the complex "Chữ nôm" script.
However these were limited to popular literature until the late 19th century.
Today Japanese is written with a composite script using both Chinese characters ("Kanji") and kana, but Korean is written exclusively with Hangul in North Korea, and supplementary Chinese characters ("Hanja") are increasingly rarely used in the South.
Vietnamese is written with a Latin-based alphabet.
Examples of loan words in English include "tea", from Minnan "tê" (茶) and "kumquat", from Cantonese "gam1gwat1" (金橘).
Varieties of Chinese.
Jerry Norman estimated that there are hundreds of mutually unintelligible varieties of Chinese. These varieties form a dialect continuum, in which differences in speech generally become more pronounced as distances increase, though the rate of change varies immensely. Generally, mountainous South China displays more linguistic diversity than the North China Plain. In parts of South China, a major city's dialect may only be marginally intelligible to close neighbours. For instance, Wuzhou is about 120 miles upstream from Guangzhou, but its dialect is more like that of Guangzhou than is that of Taishan, 60 miles southwest of Guangzhou and separated from it by several rivers. In parts of Fujian the speech of neighbouring counties or even villages may be mutually unintelligible.
Until the late 20th century, Chinese emigrants to Southeast Asia and North America came from southeast coastal areas, where Min, Hakka and Yue dialects are spoken.
The vast majority of Chinese immigrants to North America spoke the Taishan dialect, from a small coastal area southwest of Guangzhou.
Classification.
Local varieties of Chinese are conventionally classified into seven dialect groups, largely on the basis of the different evolution of Middle Chinese voiced initials:
The classification of Li Rong, which is used in the "Language Atlas of China" (1987), distinguishes three further groups:
Numbers of first-language speakers (all countries):
Some varieties remain unclassified, including Danzhou dialect (spoken in Danzhou, on Hainan Island), Xianghua (spoken in western Hunan) and Shaozhou Tuhua (spoken in northern Guangdong). The Dungan language, spoken in Central Asia, is a Mandarin variety, but is politically not generally considered "Chinese" since it is written in Cyrillic and spoken by Dungan people outside China who are not considered ethnic Chinese.
Standard Chinese and diglossia.
Putonghua / Guoyu, often called "Mandarin", is the official standard language used by the People's Republic of China, the Republic of China (Taiwan), and Singapore (where it is called "Huayu" or simply Chinese). It is based on the Beijing dialect, which is the dialect of Mandarin as spoken in Beijing. The government intends for speakers of all Chinese speech varieties to use it as a common language of communication. Therefore it is used in government agencies, in the media, and as a language of instruction in schools.
In mainland China and Taiwan, diglossia has been a common feature: it is common for a Chinese to be able to speak two or even three varieties of the Sinitic languages (or "dialects") together with Standard Chinese. For example, in addition to "putonghua", a resident of Shanghai might speak Shanghainese; and, if he or she grew up elsewhere, then he or she may also be likely to be fluent in the particular dialect of that local area. A native of Guangzhou may speak both Cantonese and "putonghua", a resident of Taiwan, both Taiwanese and "putonghua/guoyu". A person living in Taiwan may commonly mix pronunciations, phrases, and words from Mandarin and Taiwanese, and this mixture is considered normal in daily or informal speech.
Nomenclature.
In common English usage, Chinese is considered a language and its varieties "dialects", a classification that agrees with Chinese speakers' self-perception. Most linguists prefer instead to call Chinese a family of languages, because of the lack of mutual intelligibility between its divisions. Measuring this mutual intelligibility is not precise, but Chinese is often compared to the Romance languages in this regard. Some linguists find the use of "Chinese languages" also problematic, because it can imply a set of disruptive "religious, economic, political, and other differences" between speakers that exist between for example between French Catholics and English Protestants in Canada, but not between speakers of Cantonese and Mandarin in China, owing to China's near-uninterrupted history of centralized government.
Chinese itself has a term for its unified writing system, "Zhōngwén" (), while the closest equivalent used to describe its spoken variants would be "Hànyǔ" (/, "spoken language of the Han Chinese")—this term could be translated to either "language" or "languages" since Chinese lacks grammatical number. For centuries in China, owing to the widespread use of a written standard in Classical Chinese, there was no uniform speech-and-writing continuum, as indicated by the employment of two separate morphemes "yǔ" / and "wén" . The characters used in written Chinese are logographs that denote morphemes as a whole rather than their phonemes, although most logographs are compounds of similar-sounding characters and semantic disambiguation (the "radical"). Modern-day Chinese speakers of all kinds communicate using the modern standard written language, the written form of Standard Chinese.
In Chinese, the major spoken varieties of Chinese are called "fāngyán" (, literally "regional speech"), and mutually intelligible variants within these are called "dìdiǎn fāngyán" (/ "local speech"). Both terms are customarily translated into English as "dialect". Ethnic Chinese often consider these spoken variations as one single language for reasons of nationality and as they inherit one common cultural and linguistic heritage in Classical Chinese. Han native speakers of Wu, Min, Hakka, and Cantonese, for instance, may consider their own linguistic varieties as separate spoken languages, but the Han Chinese as one—albeit internally very diverse—ethnicity. To Chinese nationalists, the idea of Chinese as a language family may suggest that the Chinese identity is much more fragmented and disunified than it actually is and as such is often looked upon as culturally and politically provocative. Additionally, in Taiwan it is closely associated with Taiwanese independence, some of whose supporters promote the local Taiwanese Minnan-based spoken language.
Writing.
The relationship between the Chinese spoken and written language is rather complex. Its spoken varieties evolved at different rates, while written Chinese itself has changed much less. Classical Chinese literature began in the Spring and Autumn period, although written records have been discovered as far back as the 14th to 11th centuries BCE Shang dynasty oracle bones using the oracle bone scripts.
The Chinese orthography centers on Chinese characters, "hanzi", which are written within imaginary rectangular blocks, traditionally arranged in vertical columns, read from top to bottom down a column, and right to left across columns. Chinese characters are morphemes independent of phonetic change. Thus the character 一 ("one") is uttered "yī"/"yāo" in Standard Chinese, "jat1" in Cantonese and "chi̍t"/"it" in Hokkien (form of Min). Vocabularies from different major Chinese variants have diverged, and colloquial non-standard written Chinese often makes use of unique "dialectal characters", such as 冇 and 係 for Cantonese and Hakka, which are considered archaic or unused in standard written Chinese.
Written colloquial Cantonese has become quite popular in online chat rooms and instant messaging amongst Hong-Kongers and Cantonese-speakers elsewhere. Use of it is considered highly informal, and does not extend to many formal occasions.
In Hunan, women in certain areas write their local language in Nü Shu, a syllabary derived from Chinese characters. The Dungan language, considered by many a dialect of Mandarin, is nowadays written in Cyrillic, and was previously written in the Arabic script. The Dungan people are primarily Muslim and live mainly in Kazakhstan, Kyrgyzstan, and Russia; some of the related Hui people also speak the language and live mainly in China.
Chinese characters.
Each Chinese character represents a monosyllabic Chinese word or morpheme. In 100 CE, the famed Han dynasty scholar Xu Shen classified characters into six categories, namely pictographs, simple ideographs, compound ideographs, phonetic loans, phonetic compounds and derivative characters. Of these, only 4% were categorized as pictographs, including many of the simplest characters, such as "rén" 人 (human), "rì" 日 (sun), "shān" 山 (mountain; hill), "shuǐ" 水 (water). Between 80% and 90% were classified as phonetic compounds such as "chōng" 沖 (pour), combining a phonetic component "zhōng" 中 (middle) with a semantic radical 氵 (water). Almost all characters created since have been of this type. The 18th-century Kangxi Dictionary recognized 214 radicals.
Modern characters are styled after the regular script. Various other written styles are also used in Chinese calligraphy, including seal script, cursive script and clerical script. Calligraphy artists can write in traditional and simplified characters, but they tend to use traditional characters for traditional art.
There are currently two systems for Chinese characters. The traditional system, still used in Hong Kong, Taiwan, Macau and Chinese speaking communities (except Singapore and Malaysia) outside mainland China, takes its form from standardized character forms dating back to the late Han dynasty. The Simplified Chinese character system, developed by the People's Republic of China in 1954 to promote mass literacy, simplifies most complex traditional glyphs to fewer strokes, many to common cursive shorthand variants.
Singapore, which has a large Chinese community, is the first—and at present the only—foreign nation to officially adopt simplified characters, although it has also become the "de facto" standard for younger ethnic Chinese in Malaysia. The Internet provides the platform to practice reading the alternative system, be it traditional or simplified.
A well-educated Chinese reader today recognizes approximately 4,000–6,000 characters; approximately 3,000 characters are required to read a Mainland newspaper. The PRC government defines literacy amongst workers as a knowledge of 2,000 characters, though this would be only functional literacy. A large unabridged dictionary, like the Kangxi Dictionary, contains over 40,000 characters, including obscure, variant, rare, and archaic characters; fewer than a quarter of these characters are now commonly used.
Homophones.
Standard Chinese has fewer than 1,700 distinct syllables but 4,000 common written characters, so there are many homophones. For example, the following characters (not necessarily words) are all pronounced "jī": 鸡／雞 "chicken", 机／機 "machine", 基 "basic", 击／擊 "to hit", 饥／饑 "hunger", and 积／積 "accumulate". In speech, the meaning of a syllable is determined by context (for example, in English, "some" as the opposite of "none" as opposed to "sum" in arithmetic) or by the word it is found in ("some" or "sum" vs. "summer"). Speakers may clarify which written character they mean by giving a word or phrase it is found in: 名字叫嘉英，嘉陵江的嘉，英國的英 "Míngzi jiào Jiāyīng, Jiālíng Jiāng de jiā, Yīngguó de yīng" – "My name is "Jiāyīng", 'Jia' as in 'Jialing River' and 'ying' as in 'England'."
Southern Chinese varieties like Cantonese and Hakka preserved more of the rimes of Middle Chinese and also have more tones. Several of the examples of Mandarin "jī" above have distinct pronunciations in Cantonese (romanized using jyutping): "gai1", "gei1", "gei1", "gik1", "gei1", and "zik1" respectively. For this reason, southern varieties tend to need to employ fewer multi-syllabic words.
Phonology.
The phonological structure of each syllable consists of a nucleus consisting of a vowel (which can be a monophthong, diphthong, or even a triphthong in certain varieties), preceded by an onset (a single consonant, or consonant+glide; zero onset is also possible), and followed (optionally) by a coda consonant; a syllable also carries a tone. There are some instances where a vowel is not used as a nucleus. An example of this is in Cantonese, where the nasal sonorant consonants and can stand alone as their own syllable.
Across all the spoken varieties, most syllables tend to be open syllables, meaning they have no coda (assuming that a final glide is not analyzed as a coda), but syllables that do have codas are restricted to , , , , , , , or . Some varieties allow most of these codas, whereas others, such as Standard Chinese, are limited to only , and .
The number of sounds in the different spoken dialects varies, but in general there has been a tendency to a reduction in sounds from Middle Chinese. The Mandarin dialects in particular have experienced a dramatic decrease in sounds and so have far more multisyllabic words than most other spoken varieties. The total number of syllables in some varieties is therefore only about a thousand, including tonal variation, which is only about an eighth as many as English.
Tones.
All varieties of spoken Chinese use tones. A few dialects of north China may have as few as three tones, while some dialects in south China have up to 6 or 10 tones, depending on how one counts. One exception from this is Shanghainese which has reduced the set of tones to a two-toned pitch accent system much like modern Japanese.
A very common example used to illustrate the use of tones in Chinese are the four tones of Standard Chinese (along with the neutral tone) applied to the syllable "ma". The tones are exemplified by the following five Chinese words:
Standard Cantonese, by contrast, has six tones in open syllables and three tones in syllables ending with stops:
Phonetic transcriptions.
The Chinese had no uniform phonetic transcription system until the mid-20th century, although enunciation patterns were recorded in early rime books and dictionaries. Early Indian translators, working in Sanskrit and Pali, were the first to attempt to describe the sounds and enunciation patterns of Chinese in a foreign language. After the 15th century, the efforts of Jesuits and Western court missionaries resulted in some rudimentary Latin transcription systems, based on the Nanjing Mandarin dialect.
Romanization.
Romanization is the process of transcribing a language into the Latin script. There are many systems of romanization for the Chinese languages due to the lack of a native phonetic transcription until modern times. Chinese is first known to have been written in Latin characters by Western Christian missionaries in the 16th century.
Today the most common romanization standard for Standard Chinese is "Hanyu Pinyin", often known simply as pinyin, introduced in 1956 by the People's Republic of China, and later adopted by Singapore and Taiwan. Pinyin is almost universally employed now for teaching standard spoken Chinese in schools and universities across America, Australia and Europe. Chinese parents also use Pinyin to teach their children the sounds and tones of new words. In school books that teach Chinese, the Pinyin romanization is often shown below a picture of the thing the word represents, with the Chinese character alongside.
The second-most common romanization system, the Wade–Giles, was invented by Thomas Wade in 1859 and modified by Herbert Giles in 1892. As this system approximates the phonology of Mandarin Chinese into English consonants and vowels, i.e. it is an Anglicization, it may be particularly helpful for beginner Chinese speakers of an English-speaking background. Wade–Giles was found in academic use in the United States, particularly before the 1980s, and until recently was widely used in Taiwan.
When used within European texts, the tone transcriptions in both pinyin and Wade–Giles are often left out for simplicity; Wade–Giles' extensive use of apostrophes is also usually omitted. Thus, most Western readers will be much more familiar with "Beijing" than they will be with "Běijīng" (pinyin), and with "Taipei" than "T'ai²-pei³" (Wade–Giles). This simplification presents syllables as homophones which really are none, and therefore exaggerates the number of homophones almost by a factor of four.
Here are a few examples of "Hanyu Pinyin" and Wade–Giles, for comparison:
Other systems of romanization for Chinese include Gwoyeu Romatzyh, the French EFEO, the Yale (invented during WWII for U.S. troops), as well as separate systems for Cantonese, Minnan, Hakka, and other Chinese languages or dialects.
Other phonetic transcriptions.
Chinese languages have been phonetically transcribed into many other writing systems over the centuries. The 'Phags-pa script, for example, has been very helpful in reconstructing the pronunciations of pre-modern forms of Chinese.
Zhuyin (also called "bopomofo"), a semi-syllabary is still widely used in Taiwan's elementary schools to aid standard pronunciation. Although bopomofo characters are reminiscent of katakana script, there is no source to substantiate the claim that Katakana was the basis for the zhuyin system. A comparison table of zhuyin to pinyin exists in the zhuyin article. Syllables based on pinyin and zhuyin can also be compared by looking at the following articles:
There are also at least two systems of cyrillization for Chinese. The most widespread is the Palladius system.
Grammar and morphology.
Chinese is often described as a "monosyllabic" language. However, this is only partially correct. It is largely accurate when describing Classical Chinese and Middle Chinese; in Classical Chinese, for example, perhaps 90% of words correspond to a single syllable and a single character. In the modern varieties, it is still usually the case that a morpheme (unit of meaning) is a single syllable; contrast English, with plenty of multi-syllable morphemes, both bound and free, such as "seven", "elephant", "para-" and "-able". Some of the conservative southern varieties of modern Chinese still have largely monosyllabic words, especially among the more basic vocabulary.
In modern Mandarin, however, most nouns, adjectives and verbs are largely disyllabic. A significant cause of this is phonological attrition. Sound change over time has steadily reduced the number of possible syllables. In modern Mandarin, there are now only about 1,200 possible syllables, including tonal distinctions, compared with about 5,000 in Vietnamese (still largely monosyllabic) and over 8,000 in English.
This phonological collapse has led to a corresponding increase in the number of homophones. As an example, the small Langenscheidt Pocket Chinese Dictionary lists six common words pronounced "shí" (tone 2): 十 "ten"; 实 "real, actual"; 识 "know (a person), recognize"; 石 "stone"; 时 "time"; 食 "food". These were all pronounced differently in Early Middle Chinese; in William H. Baxter's transcription they were "dzyip", "zyit", "syik", "dzyek", "dzyi" and "zyik" respectively. They are still pronounced differently in today's Cantonese; in Jyutping they are "sap9", "sat9", "sik7", "sek9", "si4", "sik9". In modern spoken Mandarin, however, tremendous ambiguity would result if all of these words could be used as-is; Yuen Ren Chao's modern poem Lion-Eating Poet in the Stone Den exploits this, consisting of 92 characters all pronounced "shi". As such, most of these words have been replaced (in speech, if not in writing) with a longer, less-ambiguous compound. Only the first one, 十 "ten", normally appears as such when spoken; the rest are normally replaced with, respectively, 实际 "shíjì" (lit. "actual-connection"); 认识 "rènshi" (lit. "recognize-know"); 石头 "shítou" (lit. "stone-head"); 时间 "shíjiān" (lit. "time-interval"); 食物 "shíwù" (lit. "food-thing"). In each case, the homophone was disambiguated by adding another morpheme, typically either a synonym or a generic word of some sort (for example, "head", "thing"), whose purpose is simply to indicate which of the possible meanings of the other, homophonic syllable should be selected.
However, when one of the above words forms part of a compound, the disambiguating syllable is generally dropped and the resulting word is still disyllabic. For example, 石 "shí" alone, not 石头 "shítou", appears in compounds meaning "stone-", for example, 石膏 "shígāo" "plaster" (lit. "stone cream"), 石灰 "shíhuī" "lime" (lit. "stone dust"), 石窟 "shíkū" "grotto" (lit. "stone cave"), 石英 "shíyīng" "quartz" (lit. "stone flower"), 石油 "shíyóu" "petroleum" (lit. "stone oil").
Most modern varieties of Chinese have the tendency to form new words through disyllabic, trisyllabic and tetra-character compounds. In some cases, monosyllabic words have become disyllabic without compounding, as in 窟窿 "kūlong" from 孔 "kǒng"; this is especially common in Jin.
Chinese morphology is strictly bound to a set number of syllables with a fairly rigid construction which are the morphemes, the smallest blocks of the language. While many of these single-syllable morphemes (字, "zì") can stand alone as individual words, they more often than not form multi-syllabic compounds, known as "cí" (词／詞), which more closely resembles the traditional Western notion of a word. A Chinese "cí" (“word”) can consist of more than one character-morpheme, usually two, but there can be three or more.
For example:
All varieties of modern Chinese are analytic languages, in that they depend on syntax (word order and sentence structure) rather than morphology—i.e., changes in form of a word—to indicate the word's function in a sentence. In other words, Chinese has very few grammatical inflections—it possesses no tenses, no voices, no numbers (singular, plural; though there are plural markers, for example for personal pronouns), and only a few articles (i.e., equivalents to "the, a, an" in English). There is, however, a gender difference in the written language (他 as "he" and 她 as "she"), but it should be noted that this is a relatively new introduction to the Chinese language in the twentieth century, and both characters are pronounced in exactly the same way.
They make heavy use of grammatical particles to indicate aspect and mood. In Mandarin Chinese, this involves the use of particles like "le" 了 (perfective), "hái" 还／還 ("still"), "yǐjīng" 已经／已經 ("already"), and so on.
Chinese features a subject–verb–object word order, and like many other languages in East Asia, makes frequent use of the topic–comment construction to form sentences. Chinese also has an extensive system of classifiers and measure words, another trait shared with neighbouring languages like Japanese and Korean. Other notable grammatical features common to all the spoken varieties of Chinese include the use of serial verb construction, pronoun dropping and the related subject dropping.
Although the grammars of the spoken varieties share many traits, they do possess differences.
Vocabulary.
The entire Chinese character corpus since antiquity comprises well over 20,000 characters, of which only roughly 10,000 are now commonly in use. However Chinese characters should not be confused with Chinese words; since most Chinese words are made up of two or more different characters, there are many times more Chinese words than there are characters.
Estimates of the total number of Chinese words and phrases vary greatly. The "Hanyu Da Zidian", a compendium of Chinese characters, includes 54,678 head entries for characters, including bone oracle versions. The "Zhonghua Zihai" (1994) contains 85,568 head entries for character definitions, and is the largest reference work based purely on character and its literary variants. The CC-CEDICT project (2010) contains 97,404 contemporary entries including idioms, technology terms and names of political figures, businesses and products. The 2009 version of the Webster's Digital Chinese Dictionary (WDCD), based on CC-CEDICT, contains over 84,000 entries.
The most comprehensive pure linguistic Chinese-language dictionary, the 12-volumed "Hanyu Da Cidian", records more than 23,000 head Chinese characters and gives over 370,000 definitions. The 1999 revised "Cihai", a multi-volume encyclopedic dictionary reference work, gives 122,836 vocabulary entry definitions under 19,485 Chinese characters, including proper names, phrases and common zoological, geographical, sociological, scientific and technical terms.
The latest 2012 6th edition of "Xiandai Hanyu Cidian", an authoritative one-volume dictionary on modern standard Chinese language as used in mainland China, has 69,000 entries and defines 13,000 head characters.
Loanwords.
Like any other language, Chinese has absorbed a sizable number of loanwords from other cultures. Most Chinese words are formed out of native Chinese morphemes, including words describing imported objects and ideas. However, direct phonetic borrowing of foreign words has gone on since ancient times.
Some early Indo-European loanwords in Chinese have been proposed, notably "mì" "honey", "shī" "lion," and perhaps also "mǎ" "horse", "zhū" "pig", "quǎn" "dog", and "é" "goose".
Ancient words borrowed from along the Silk Road since Old Chinese include 葡萄 "pútáo" "grape", 石榴 "shíliú" "pomegranate" and 狮子／獅子 "shīzi" "lion". Some words were borrowed from Buddhist scriptures, including 佛 "Fó" "Buddha" and 菩萨／菩薩 "Púsà" "bodhisattva." Other words came from nomadic peoples to the north, such as 胡同 "hútóng" "hutong". Words borrowed from the peoples along the Silk Road, such as 葡萄 "grape," generally have Persian etymologies. Buddhist terminology is generally derived from Sanskrit or Pāli, the liturgical languages of North India. Words borrowed from the nomadic tribes of the Gobi, Mongolian or northeast regions generally have Altaic etymologies, such as 琵琶 "pípa", the Chinese lute, or 酪 "lào"/"luò" "cheese" or "yoghurt", but from exactly which source is not always clear.
Modern borrowings and loanwords.
Modern neologisms are primarily translated into Chinese in one of three ways: free translation ("calque", or by meaning), phonetic translation (by sound), or a combination of the two. Today, it is much more common to use existing Chinese morphemes to coin new words in order to represent imported concepts, such as technical expressions and international scientific vocabulary. Any Latin or Greek etymologies are dropped and converted into the corresponding Chinese characters (for example, "anti-" typically becomes "反", literally "opposite"), making them more comprehensible for Chinese but introducing more difficulties in understanding foreign texts. For example, the word "telephone" was loaned phonetically as 德律风／德律風 (Shanghainese: "télífon" , Mandarin: "délǜfēng") during the 1920s and widely used in Shanghai, but later 电话／電話 "diànhuà" (lit. "electric speech"), built out of native Chinese morphemes, became prevalent (電話 is in fact from the Japanese "denwa"; see below for more Japanese loans). Other examples include 电视／電視 "diànshì" (lit. "electric vision") for television, 电脑／電腦 "diànnǎo" (lit. "electric brain") for computer; 手机／手機 "shǒujī" (lit. "hand machine") for mobile phone, 蓝牙／藍牙 "lányá" (lit. "blue tooth") for Bluetooth, and 网志/網誌 "wǎngzhì" (lit. "internet logbook") for blog in Hong Kong and Macau Cantonese. Occasionally half-transliteration, half-translation compromises (phono-semantic matching) are accepted, such as 汉堡包／漢堡包 "hànbǎobāo" (漢堡 "hànbǎo" "Hamburg" + 包 "bāo" "bun") for "hamburger". Sometimes translations are designed so that they sound like the original while incorporating Chinese morphemes, such as 拖拉机／拖拉機 "tuōlājī" "tractor" (lit. "dragging-pulling machine"), or 马利奥／馬利奧 Mǎlì'ào for the video game character Mario. This is often done for commercial purposes, for example 奔腾／奔騰 "bēnténg" (lit. "dashing-leaping") for Pentium and 赛百味／賽百味 "Sàibǎiwèi" (lit. "better-than hundred tastes") for Subway restaurants.
Foreign words, mainly proper nouns, continue to enter the Chinese language by transcription according to their pronunciations. This is done by employing Chinese characters with similar pronunciations. For example, "Israel" becomes 以色列 "Yǐsèliè", "Paris" becomes 巴黎 "Bālí". A rather small number of direct transliterations have survived as common words, including 沙发／沙發 "shāfā" "sofa", 马达／馬達 "mǎdá" "motor", 幽默 "yōumò" "humor", 逻辑／邏輯 "luójí" "logic", 时髦／時髦 "shímáo" "smart, fashionable", and 歇斯底里 "xiēsīdǐlǐ" "hysterics". The bulk of these words were originally coined in the Shanghai dialect during the early 20th century and were later loaned into Mandarin, hence their pronunciations in Mandarin may be quite off from the English. For example, 沙发／沙發 "sofa" and 马达／馬達 "motor" in Shanghainese sound more like their English counterparts. Cantonese differs from Mandarin with some transliterations, such as 梳化 "so1 faa3*2" "sofa" and 摩打 "mo1 daa2" "motor".
Western foreign words representing Western concepts have influenced Chinese since the 20th century through transcription. From French came 芭蕾 "bāléi" "ballet" and 香槟 "xiāngbīn", "champagne"; from Italian, 咖啡 "kāfēi" "caffè". English influence is particularly pronounced. From early 20th century Shanghainese, many English words are borrowed, such as 高尔夫／高爾夫 "gāoěrfū" "golf" and the above-mentioned 沙发／沙發 "shāfā" "sofa". Later, the United States soft influences gave rise to 迪斯科 "dísīkē" "disco", 可乐／可樂 "kělè" "cola", and 迷你 "mínǐ" "mini ". Contemporary colloquial Cantonese has distinct loanwords from English, such as 卡通 "kaa1 tung1" "cartoon", 基佬 "gei1 lou2" "gay people", 的士 "dik1 si6*2" "taxi", and 巴士 "baa1 si6*2" "bus". With the rising popularity of the Internet, there is a current vogue in China for coining English transliterations, for example, 粉丝／粉絲 "fěnsī" "fans", 黑客 "hēikè" "hacker" (lit. "black guest"), and 博客 "bókè". In Taiwan, some of these transliterations are different, such as 駭客 "hàikè" for "hacker" and 部落格 "bùluògé" for "blog" (lit. "interconnected tribes").
Another result of the English influence on Chinese is the appearance in Modern Chinese texts of so-called 字母词／字母詞 "zìmǔcí" (lit. "lettered words") spelled with letters from the English alphabet. This has appeared in magazines, newspapers, on web sites, and on TV: 三G手机／三G手機 "3rd generation cell phones" (三 "sān" "three" + G "generation" + 手机／手機 "shǒujī" "mobile phones"), IT界 "IT circles" (IT "information technology" + 界 "jiè" "industry"), HSK ("Hànyǔ Shuǐpíng Kǎoshì", 汉语水平考试／漢語水平考試), GB ("Guóbiāo", 国标／國標), CIF价／CIF價 (CIF "Cost, Insurance, Freight" + 价／價 "jià" "price"), e家庭 "e-home" (e "electronic" + 家庭 "jiātíng" "home"), W时代／W時代 "wireless era" (W "wireless" + 时代／時代 "shídài" "era"), TV族 "TV watchers" (TV "television" + 族 "zú" "social group; clan"), 后РС时代／後PC時代 "post-PC era" (后／後 "hòu" "after/post-" + PC "personal computer" + 时代／時代), and so on.
Since the 20th century, another source of words has been Japanese using existing kanji (Chinese characters used in Japanese). Japanese re-molded European concepts and inventions into , and many of these words have been re-loaned into modern Chinese. Other terms were coined by the Japanese by giving new senses to existing Chinese terms or by referring to expressions used in classical Chinese literature. For example, "jīngjì" (经济／經濟; "keizai" in Japanese), which in the original Chinese meant "the workings of the state", was narrowed to "economy" in Japanese; this narrowed definition was then re-imported into Chinese. As a result, these terms are virtually indistinguishable from native Chinese words: indeed, there is some dispute over some of these terms as to whether the Japanese or Chinese coined them first. As a result of this loaning, Chinese, Korean, Japanese, and Vietnamese share a corpus of linguistic terms describing modern terminology, paralleling the similar corpus of terms built from Greco-Latin and shared among European languages.
Education.
With the growing importance and influence of China's economy globally, Mandarin instruction is gaining popularity in schools in the USA, and has become an increasingly popular subject of study amongst the young in the Western world, as in the UK.
In 1991 there were 2,000 foreign learners taking China's official Chinese Proficiency Test (comparable to the English Cambridge Certificate), while in 2005, the number of candidates had risen sharply to 117,660. By 2010, 750,000 people had taken the Chinese Proficiency Test.

</doc>
<doc id="5759" url="http://en.wikipedia.org/wiki?curid=5759" title="Complex analysis">
Complex analysis

[[Image:Color complex plot.jpg|right|thumb|Plot of the function
. The hue represents the function argument, while the brightness represents the magnitude.]]
Complex analysis, traditionally known as the theory of functions of a complex variable, is the branch of mathematical analysis that investigates functions of complex numbers. It is useful in many branches of mathematics, including algebraic geometry, number theory, applied mathematics; as well as in physics, including hydrodynamics, thermodynamics, nuclear, aerospace, mechanical and electrical engineering.
Murray R. Spiegel described complex analysis as "one of the most beautiful as well as useful branches of Mathematics".
Complex analysis is particularly concerned with analytic functions of complex variables (or, more generally, meromorphic functions). Because the separate real and imaginary parts of any analytic function must satisfy Laplace's equation, complex analysis is widely applicable to two-dimensional problems in physics.
History.
Complex analysis is one of the classical branches in mathematics with roots in the 19th century and just prior. Important mathematicians associated with complex analysis include Euler, Gauss, Riemann, Cauchy, Weierstrass, and many more in the 20th century. Complex analysis, in particular the theory of conformal mappings, has many physical applications and is also used throughout analytic number theory. In modern times, it has become very popular through a new boost from complex dynamics and the pictures of fractals produced by iterating holomorphic functions. Another important application of complex analysis is in string theory which studies conformal invariants in quantum field theory.
Complex functions.
A complex function is one in which the independent variable and the dependent variable are both complex numbers. More precisely, a complex function is a function whose domain and range are subsets of the complex plane.
For any complex function, both the independent variable and the dependent variable may be separated into real and imaginary parts:
In other words, the components of the function "f"("z"),
can be interpreted as real-valued functions of the two real variables, "x" and "y".
The basic concepts of complex analysis are often introduced by extending the elementary real functions (e.g., exponential functions, logarithmic functions, and trigonometric functions) into the complex domain.
Holomorphic functions.
Holomorphic functions are complex functions defined on an open subset of the complex plane that are differentiable. Complex differentiability has much stronger consequences than usual (real) differentiability. For instance, holomorphic functions are infinitely differentiable, whereas some real differentiable functions are not. Most elementary functions, including the exponential function, the trigonometric functions, and all polynomial functions, are holomorphic.
"See also": analytic function, holomorphic sheaf and vector bundles.
Major results.
One central tool in complex analysis is the line integral. The integral around a closed path of a function that is holomorphic everywhere inside the area bounded by the closed path is always zero; this is the Cauchy integral theorem. The values of a holomorphic function inside a disk can be computed by a certain path integral on the disk's boundary (Cauchy's integral formula). Path integrals in the complex plane are often used to determine complicated real integrals, and here the theory of residues among others is useful (see methods of contour integration). If a function has a "pole" or isolated singularity at some point, that is, at that point where its values "blow up" and have no finite bound, then one can compute the function's residue at that pole. These residues can be used to compute path integrals involving the function; this is the content of the powerful residue theorem. The remarkable behavior of holomorphic functions near essential singularities is described by Picard's Theorem. Functions that have only poles but no essential singularities are called meromorphic. Laurent series are similar to Taylor series but can be used to study the behavior of functions near singularities.
A bounded function that is holomorphic in the entire complex plane must be constant; this is Liouville's theorem. It can be used to provide a natural and short proof for the fundamental theorem of algebra which states that the field of complex numbers is algebraically closed.
If a function is holomorphic throughout a connected domain then its values are fully determined by its values on any smaller subdomain. The function on the larger domain is said to be analytically continued from its values on the smaller domain. This allows the extension of the definition of functions, such as the Riemann zeta function, which are initially defined in terms of infinite sums that converge only on limited domains to almost the entire complex plane. Sometimes, as in the case of the natural logarithm, it is impossible to analytically continue a holomorphic function to a non-simply connected domain in the complex plane but it is possible to extend it to a holomorphic function on a closely related surface known as a Riemann surface.
All this refers to complex analysis in one variable. There is also a very rich theory of complex analysis in more than one complex dimension in which the analytic properties such as power series expansion carry over whereas most of the geometric properties of holomorphic functions in one complex dimension (such as conformality) do not carry over. The Riemann mapping theorem about the conformal relationship of certain domains in the complex plane, which may be the most important result in the one-dimensional theory, fails dramatically in higher dimensions.

</doc>
<doc id="5760" url="http://en.wikipedia.org/wiki?curid=5760" title="History of China">
History of China

The History of China encompasses the time period from prehistory to the present day. Yellow River is said to be the cradle of Chinese civilization, although cultures originated at various regional centers along both the Yellow River and the Yangtze River valleys in the Neolithic era. With thousands of years of continuous history, China is one of the world's oldest civilizations. Records of written history can be found as early as the Shang dynasty (c. 1700–1046 BC), although ancient historical texts such as the "Records of the Grand Historian" (ca. 100 BC) and "Bamboo Annals" assert the existence of a Xia dynasty before the Shang. Much of Chinese culture, literature and philosophy further developed during the Zhou dynasty (1045–256 BC).
The Zhou dynasty began to bow to external and internal pressures in the 8th century BC, and the kingdom eventually broke apart into smaller states, beginning in the Spring and Autumn period and reaching full expression in the Warring States period. This is one of multiple periods of failed statehood in Chinese history (the most recent of which was the Chinese Civil War).
In between eras of multiple kingdoms and warlordism, Chinese dynasties have ruled parts or all of China; in some eras, including the present, control has stretched as far as Xinjiang and/or Tibet. This practice began with the Qin dynasty: in 221 BC, Qin Shi Huang united the various warring kingdoms and created the first Chinese empire. Successive dynasties in Chinese history developed bureaucratic systems that enabled the Emperor of China to directly control vast territories. China's last dynasty was Qing, which was replaced by the Republic of China in 1912, and in the mainland by the People's Republic of China in 1949.
The conventional view of Chinese history is that of alternating periods of political unity and disunity, with China occasionally being dominated by steppe peoples, most of whom were in turn assimilated into the Han Chinese population. Cultural and political influences from other parts of Asia and the Western world, carried by successive waves of immigration, expansion, foreign contact, and cultural assimilation are part of the modern culture of China.
Prehistory.
Paleolithic.
What is now China was inhabited by "Homo erectus" more than a million years ago. Recent study shows that the stone tools found at Xiaochangliang site are magnetostratigraphically dated to 1.36 million years ago. The archaeological site of Xihoudu in Shanxi Province is the earliest recorded use of fire by "Homo erectus", which is dated 1.27 million years ago.
The excavations at Yuanmou and later Lantian show early habitation. Perhaps the most famous specimen of "Homo erectus" found in China is the so-called Peking Man discovered in 1923–27.
Neolithic.
The Neolithic age in China can be traced back to about 10,000 BC.
Early evidence for proto-Chinese millet agriculture is radiocarbon-dated to about 7000 BC. Farming gave rise to the Jiahu culture (7000 to 5800 BC). At Damaidi in Ningxia, 3,172 cliff carvings dating to 6000–5000 BC have been discovered, "featuring 8,453 individual characters such as the sun, moon, stars, gods and scenes of hunting or grazing." These pictographs are reputed to be similar to the earliest characters confirmed to be written Chinese. Excavation of a Peiligang culture site in Xinzheng county, Henan, found a community that flourished in 5,500–4,900 BC, with evidence of agriculture, constructed buildings, pottery, and burial of the dead. With agriculture came increased population, the ability to store and redistribute crops, and the potential to support specialist craftsmen and administrators. In late Neolithic times, the Yellow River valley began to establish itself as a center of Yangshao culture (5000 BC to 3000 BC), and the first villages were founded; the most archaeologically significant of these was found at Banpo, Xi'an. Later, Yangshao culture was superseded by the Longshan culture, which was also centered on the Yellow River from about 3000 BC to 2000 BC.
The early history of China is obscured by the lack of written documents from this period, coupled with the existence of later accounts that attempted to describe events that had occurred several centuries previously. In a sense, the problem stems from centuries of introspection on the part of the Chinese people, which has blurred the distinction between fact and fiction in regards to this early history.
Ancient China.
Xia dynasty (c. 2100 – c. 1600 BC).
The Xia dynasty of China (from c. 2100 to c. 1600 BC) is the first dynasty to be described in ancient historical records such as Sima Qian's "Records of the Grand Historian" and "Bamboo Annals".
Although there is disagreement as to whether the dynasty actually existed, there is some archaeological evidence pointing to its possible existence. Sima Qian, writing in the late 2nd century BC, dated the founding of the Xia dynasty to around 2200 BC, but this date has not been corroborated. Most archaeologists now connect the Xia to excavations at Erlitou in central Henan province, where a bronze smelter from around 2000 BC was unearthed. Early markings from this period found on pottery and shells are thought to be ancestral to modern Chinese characters. With few clear records matching the Shang oracle bones or the Zhou bronze vessel writings, the Xia era remains poorly understood.
According to mythology, the dynasty ended around 1600 BC as a consequence of the Battle of Mingtiao.
Shang dynasty (c. 1600–1046 BC).
Archaeological findings providing evidence for the existence of the Shang dynasty, c. 1600–1046 BC, are divided into two sets. The first set – from the earlier Shang period – comes from sources at Erligang, Zhengzhou, and Shangcheng. The second set – from the later Shang or Yin (殷) period – is at Anyang, in modern-day Henan, which has been confirmed as the last of the Shang's nine capitals (c. 1300–1046 BC). The findings at Anyang include the earliest written record of Chinese past so far discovered: inscriptions of divination records in ancient Chinese writing on the bones or shells of animals – the so-called "oracle bones", dating from around 1200 BC.
31 Kings reined over the Shang dynasty. During their rein, according to the Records of the Grand Historian, the capital city was moved six times. The final (and most important) move was to Yin in 1350 BC which led to the dynasty's golden age . The term Yin dynasty has been synonymous with the Shang dynasty in history, although it has lately been used to specifically refer to the latter half of the Shang dynasty.
Chinese historians living in later periods were accustomed to the notion of one dynasty succeeding another, but the actual political situation in early China is known to have been much more complicated. Hence, as some scholars of China suggest, the Xia and the Shang can possibly refer to political entities that existed concurrently, just as the early Zhou is known to have existed at the same time as the Shang.
Although written records found at Anyang confirm the existence of the Shang dynasty, Western scholars are often hesitant to associate settlements that are contemporaneous with the Anyang settlement with the Shang dynasty. For example, archaeological findings at Sanxingdui suggest a technologically advanced civilization culturally unlike Anyang. The evidence is inconclusive in proving how far the Shang realm extended from Anyang. The leading hypothesis is that Anyang, ruled by the same Shang in the official history, coexisted and traded with numerous other culturally diverse settlements in the area that is now referred to as China proper.
Zhou dynasty (1046–256 BC).
The Zhou dynasty was the longest-lasting dynasty in Chinese history, from 1066 BC to approximately 256 BC. By the end of the 2nd millennium BC, the Zhou dynasty began to emerge in the Yellow River valley, overrunning the territory of the Shang. The Zhou appeared to have begun their rule under a semi-feudal system. The Zhou lived west of the Shang, and the Zhou leader had been appointed "Western Protector" by the Shang. The ruler of the Zhou, King Wu, with the assistance of his brother, the Duke of Zhou, as regent, managed to defeat the Shang at the Battle of Muye.
The king of Zhou at this time invoked the concept of the Mandate of Heaven to legitimize his rule, a concept that would be influential for almost every succeeding dynasty. Like Shangdi, Heaven ("tian") ruled over all the other gods, and it decided who would rule China. It was believed that a ruler had lost the Mandate of Heaven when natural disasters occurred in great number, and when, more realistically, the sovereign had apparently lost his concern for the people. In response, the royal house would be overthrown, and a new house would rule, having been granted the Mandate of Heaven.
The Zhou initially moved their capital west to an area near modern Xi'an, on the Wei River, a tributary of the Yellow River, but they would preside over a series of expansions into the Yangtze River valley. This would be the first of many population migrations from north to south in Chinese history.
Spring and Autumn period (722–476 BC).
In the 8th century BC, power became decentralized during the Spring and Autumn period, named after the influential Spring and Autumn Annals. In this period, local military leaders used by the Zhou began to assert their power and vie for hegemony. The situation was aggravated by the invasion of other peoples from the northwest, such as the Qin, forcing the Zhou to move their capital east to Luoyang. This marks the second major phase of the Zhou dynasty: the Eastern Zhou. The Spring and Autumn period is marked by a falling apart of the central Zhou power. In each of the hundreds of states that eventually arose, local strongmen held most of the political power and continued their subservience to the Zhou kings in name only. Some local leaders even started using royal titles for themselves. China now consisted of hundreds of states, some of them only as large as a village with a fort.
The Hundred Schools of Thought of Chinese philosophy blossomed during this period, and such influential intellectual movements as Confucianism, Taoism, Legalism and Mohism were founded, partly in response to the changing political world.
Warring States period (476–221 BC).
After further political consolidation, seven prominent states remained by the end of 5th century BC, and the years in which these few states battled each other are known as the Warring States period. Though there remained a nominal Zhou king until 256 BC, he was largely a figurehead and held little real power.
As neighboring territories of these warring states, including areas of modern Sichuan and Liaoning, were annexed, they were governed under the new local administrative system of commandery and prefecture (郡縣/郡县). This system had been in use since the Spring and Autumn period, and parts can still be seen in the modern system of Sheng & Xian (province and county, 省縣/省县).
The final expansion in this period began during the reign of Ying Zheng, the king of Qin. His unification of the other six powers, and further annexations in the modern regions of Zhejiang, Fujian, Guangdong and Guangxi in 214 BC, enabled him to proclaim himself the First Emperor (Qin Shi Huang).
Imperial China.
Qin dynasty (221–206 BC).
Historians often refer to the period from Qin dynasty to the end of Qing dynasty as Imperial China. Though the unified reign of the First Qin Emperor lasted only 12 years, he managed to subdue great parts of what constitutes the core of the Han Chinese homeland and to unite them under a tightly centralized Legalist government seated at Xianyang (close to modern Xi'an). The doctrine of Legalism that guided the Qin emphasized strict adherence to a legal code and the absolute power of the emperor. This philosophy, while effective for expanding the empire in a military fashion, proved unworkable for governing it in peacetime. The Qin Emperor presided over the brutal silencing of political opposition, including the event known as the burning of books and burying of scholars. This would be the impetus behind the later Han synthesis incorporating the more moderate schools of political governance.
The Qin dynasty is well known for beginning the Great Wall of China, which was later augmented and enhanced during the Ming dynasty. The other major contributions of the Qin include the concept of a centralized government, the unification of the legal code, development of the written language, measurement, and currency of China after the tribulations of the Spring and Autumn and Warring States periods. Even something as basic as the length of axles for carts
had to be made uniform to ensure a viable trading system throughout the empire.
Han dynasty (202 BC–AD 220).
Western Han.
The Han dynasty was founded by Liu Bang, who emerged victorious in the civil war that followed the collapse of the unified but short-lived Qin dynasty. A golden age in Chinese history, the Han dynasty's long period of stability and prosperity consolidated the foundation of China as a unified state under a central imperial bureaucracy, which was to last intermittently for most of the next two millennium. During the Han dynasty, territory of China was extended to most of the China proper and to areas far west. Confucianism was officially elevated to orthodox status and was to shape the subsequent Chinese Civilization. Art, Culture and Science all advanced to unprecedented heights. With the profound and lasting impacts of this period of Chinese history, the dynasty name "Han" had been taken as the name of the Chinese people, now the dominant ethnic group in modern China, and had been commonly used to refer to Chinese language and written characters.
After the initial Laissez-faire policies of Emperors Wen and Jing, the ambitious Emperor Wu brought the empire to its zenith. To consolidate his power, Confucianism, which emphasizes stability and order in a well-structured society, was given exclusive patronage to be the guiding philosophical thoughts and moral principles of the empire. Imperial Universities were established to support its study and further development, while other schools of thoughts were discouraged.
Major military campaigns were launched to weaken the nomadic Xiongnu Empire, limiting their influence north of the Great Wall. Along with the diplomatic efforts led by Zhang Qian, the sphere of influence of the Han Empire extended to the states in the Tarim Basin, opened up the Silk Road that connected China to west, stimulating prosperous bilateral trades and cultural exchange. To the south, various small kingdoms far beyond the Yangtze River Valley were formally incorporated into the empire.
Emperor Wu also dispatched a series of military campaigns against the Baiyue tribes. The Han annexed Minyue in 135 BC and 111 BC, Nanyue in 111 BC, and Dian in 109 BC. Migration and military expeditions led to the cultural assimilation of the south. It also brought the Han into contact with kingdoms in Southeast Asia, introducing diplomacy and trade.
After Emperor Wu, the empire slipped into gradual stagnation and decline. Economically, the state treasury was strained by excessive campaigns and projects, while land acquisitions by elite families gradually drained the tax base. Various consort clans exerted increasing control over strings of incompetent emperors and eventually the dynasty was briefly interrupted by the usurpation of Wang Mang.
Xin dynasty.
In AD 9, the usurper Wang Mang claimed that the Mandate of Heaven called for the end of the Han dynasty and the rise of his own, and he founded the short-lived Xin ("New") dynasty. Wang Mang started an extensive program of land and other economic reforms, including the outlawing of slavery and land nationalization and redistribution. These programs, however, were never supported by the landholding families, because they favored the peasants. The instability of power brought about chaos, uprisings, and loss of territories. This was compounded by mass flooding of the Yellow River; silt buildup caused it to split into two channels and displaced large numbers of farmers. Wang Mang was eventually killed in Weiyang Palace by an enraged peasant mob in AD 23.
Eastern Han.
Emperor Guangwu reinstated the Han dynasty with the support of landholding and merchant families at Luoyang, "east" of the former capital Xi'an. Thus, this new era is termed the Eastern Han dynasty. With the capable administrations of Emperors Ming and Zhang, former glories of the dynasty was reclaimed, with brilliant military and cultural achievements. The Xiongnu Empire was decisively defeated. The diplomat and general Ban Chao further expanded the conquests across the Pamirs to the shores of the Caspian Sea, thus reopening the Silk Road, and bringing trade, foreign cultures, along with the arrival of Buddhism. With extensive connections with the west, the first of several Roman embassies to China were recorded in Chinese sources, coming from the sea route in AD 166, and a second one in AD 284.
The Eastern Han dynasty was one of the most prolific era of science and technology in ancient China, notably the historic invention of papermaking by Cai Lun, and the numerous contributions by the polymath Zhang Heng.
By the 2nd century, the empire declined amidst land acquisitions, invasions, and feuding between consort clans and eunuchs. The Yellow Turban Rebellion broke out in AD 184, ushering in an era of warlords. In the ensuing turmoil, three states tried to gain predominance in the period of the Three Kingdoms. This time period has been greatly romanticized in works such as "Romance of the Three Kingdoms".
Wei and Jin period (AD 265–420).
After Cao Cao reunified the north in 208, his son proclaimed the Wei dynasty in 220. Soon, Wei's rivals Shu and Wu proclaimed their independence, leading China into the Three Kingdoms period. This period was characterized by a gradual decentralization of the state that had existed during the Qin and Han dynasties, and an increase in the power of great families. Although the Three Kingdoms were reunified by the Jin dynasty in 280, this structure was essentially the same until the Wu Hu uprising.
Wu Hu period (AD 304–439).
Taking advantage of civil war in the Jin dynasty, the contemporary non-Han Chinese (Wu Hu) ethnic groups controlled much of the country in the early 4th century and provoked large-scale Han Chinese migrations to south of the Yangtze River. In 303 the Di people rebelled and later captured Chengdu, establishing the state of Cheng Han. Under Liu Yuan, the Xiongnu rebelled near today's Linfen County and established the state of Han Zhao. Liu Yuan's successor Liu Cong captured and executed the last two Western Jin emperors. Sixteen kingdoms were a plethora of short-lived non-Chinese dynasties that came to rule the whole or parts of northern China in the 4th and 5th centuries. Many ethnic groups were involved, including ancestors of the Turks, Mongols, and Tibetans. Most of these nomadic peoples had, to some extent, been "sinicized" long before their ascent to power. In fact, some of them, notably the Qiang and the Xiongnu, had already been allowed to live in the frontier regions within the Great Wall since late Han times.
Southern and Northern Dynasties (AD 420–589).
Signaled by the collapse of East Jin dynasty in 420, China entered the era of the Southern and Northern Dynasties. The Han people managed to survive the military attacks from the nomadic tribes of the north, such as the Xianbei, and their civilization continued to thrive.
In southern China, fierce debates about whether Buddhism should be allowed to exist were held frequently by the royal court and nobles. Finally, near the end of the Southern and Northern Dynasties era, both Buddhist and Taoist followers compromised and became more tolerant of each other.
In 589, Sui annexed the last Southern dynasty, Chen, through military force, and put an end to the era of Southern and Northern Dynasties.
Sui dynasty (AD 589–618).
The Sui dynasty, which managed to reunite the country in 589 after nearly four centuries of political fragmentation, played a role more important than its length of existence would suggest. The Sui brought China together again and set up many institutions that were to be adopted by their successors, the Tang. These included the government system of Three Departments and Six Ministries, standard coinage, improved defense and expansion of the Great Wall, and official support for Buddhism. Like the Qin, however, the Sui overused their resources and collapsed.
Tang dynasty (AD 618–907).
Tang dynasty was founded by Emperor Gaozu on 18 June 618. It was a golden age of Chinese civilization with significant developments in art, literature, particularly poetry, and technology. Buddhism became the predominant religion for common people. Chang'an (modern Xi'an), the national capital, was the largest city in the world of its time.
Started by the second emperor, Taizong, military campaigns were launched to dissolve threats from nomadic tribes, extend the border, and submit neighboring states into a tributary system. Military victories in the Tarim Basin kept the Silk Road open, connecting Chang'an to Central Asia and areas far to the west. In the south, lucrative maritime trade routes began from port cities such as Guangzhou. There was extensive trade with distant foreign countries, and many foreign merchants settled in China, boosting a vibrant cosmopolitan culture. The Tang culture and social systems were admired and adapted by neighboring countries like Japan. Internally, the Grand Canal linked the political heartland in Chang'an to the economic and agricultural centers in the eastern and southern parts of the empire.
Underlying the prosperity of the early Tang dynasty was a strong centralized bureaucracy with efficient policies. The government was organized as "Three Departments and Six Ministries" to separately draft, review, and implement policies. These departments were run by royal family members as well as scholar officials who were selected from imperial examinations. These practices, which matured in the Tang dynasty, were to be inherited by the later dynasties with some modifications.
Under the Tang "equal-field system", all land was owned by the Emperor and granted to people according to household size. The associated military policy ("Fubing system") conscripted all men for a fixed period each year in exchange for their land rights. These policies stimulated a rapid growth in productivity and boosted the army without much burden on the state treasury. By the dynasty's midpoint, however, standing armies had replaced conscription and land continued to fall into the hands of private owners.
The dynasty continued to flourish under Empress Wu Zetian, the only empress regnant in Chinese history, and reached its zenith during the reign of Emperor Xuanzong, who oversaw an empire that stretched from the Pacific to the Aral Sea with at least 50 million people.
At the zenith of prosperity of the empire, the An Lushan Rebellion from 755 to 763 was a watershed event that devastated the population and drastically weakened the central imperial government. Regional military governors, known as Jiedushi, gained increasingly autonomous status while formerly submissive states raided the empire. Nevertheless, after the An Lushan Rebellion, the Tang civil society recovered and thrived amidst the weakened imperial bureaucracy.
From about 860, the Tang dynasty declined due to a series of rebellions within China itself and in the former subject Kingdom of Nanzhao to the south. One warlord, Huang Chao, captured Guangzhou in 879, killing most of the 200,000 inhabitants, including most of the large colony of foreign merchant families there. In late 880, Luoyang surrendered to Huang Chao, and on 5 January 881 he conquered Chang'an. The emperor Xizong fled to Chengdu, and Huang established a new temporary regime which was eventually destroyed by Tang forces. Another time of political chaos followed.
Five Dynasties and Ten Kingdoms (AD 907–960).
The period of political disunity between the Tang and the Song, known as the Five Dynasties and Ten Kingdoms period, lasted little more than half a century, from 907 to 960. During this brief era, when China was in all respects a multi-state system, five regimes rapidly succeeded one another in control of the old Imperial heartland in northern China. During this same time, sections of southern and western China were occupied by ten, more stable, regimes so the period is also referred to as the Ten Kingdoms.
Song, Liao, Jin, and Western Xia dynasties (AD 960–1234).
In 960, the Song dynasty gained power over most of China and established its capital in Kaifeng (later known as Bianjing), starting a period of economic prosperity, while the Khitan Liao dynasty ruled over Manchuria, present-day Mongolia, and parts of Northern China. In 1115, the Jurchen Jin dynasty emerged to prominence, annihilating the Liao dynasty in 10 years. Meanwhile, in what are now the northwestern Chinese provinces of Gansu, Shaanxi, and Ningxia, there emerged a Western Xia dynasty from 1032 to 1227, established by Tangut tribes.
The Jin dynasty took power and conquered northern China in the Jin–Song Wars, capturing Kaifeng from the Song dynasty, which moved its capital to Hangzhou (杭州). The Southern Song dynasty also suffered the humiliation of having to acknowledge the Jin dynasty as formal overlords. In the ensuing years, China was divided between the Song dynasty, the Jin dynasty and the Tangut Western Xia. Southern Song experienced a period of great technological development which can be explained in part by the military pressure that it felt from the north. This included the use of gunpowder weapons, which played a large role in the Song dynasty naval victories against the Jin in the Battle of Tangdao and Battle of Caishi on the Yangtze River in 1161. Furthermore, China's first permanent standing navy was assembled and provided an admiral's office at Dinghai in 1132, under the reign of Emperor Renzong of Song.
The Song dynasty is considered by many to be classical China's high point in science and technology, with innovative scholar-officials such as Su Song (1020–1101) and Shen Kuo (1031–1095). There was court intrigue between the political rivals of the Reformers and Conservatives, led by the chancellors Wang Anshi and Sima Guang, respectively. By the mid-to-late 13th century, the Chinese had adopted the dogma of Neo-Confucian philosophy formulated by Zhu Xi. Enormous literary works were compiled during the Song dynasty, such as the historical work of the "Zizhi Tongjian" ("Comprehensive Mirror to Aid in Government"). Culture and the arts flourished, with grandiose artworks such as "Along the River During the Qingming Festival" and "Eighteen Songs of a Nomad Flute", along with great Buddhist painters like the prolific Lin Tinggui.
Yuan dynasty (AD 1271–1368).
The Jurchen-founded Jin dynasty was defeated by the Mongols, who then proceeded to defeat the Southern Song in a long and bloody war, the first war in which firearms played an important role. During the era after the war, later called the "Pax Mongolica", adventurous Westerners such as Marco Polo travelled all the way to China and brought the first reports of its wonders to Europe. In the Yuan dynasty, the Mongols were divided between those who wanted to remain based in the steppes and those who wished to adopt the customs of the Chinese.
Kublai Khan, grandson of Genghis Khan, wanting to adopt the customs of China, established the Yuan dynasty. This was the first dynasty to rule the whole of China from Beijing as the capital. Beijing had been ceded to Liao in AD 938 with the Sixteen Prefectures of Yan Yun. Before that, it had been the capital of the Jin, who did not rule all of China.
Before the Mongol invasion, Chinese dynasties reportedly had approximately 120 million inhabitants; after the conquest was completed in 1279, the 1300 census reported roughly 60 million people. While it is tempting to attribute this major decline solely to Mongol ferocity, scholars today have mixed sentiments regarding this subject. Scholars such as Frederick W. Mote argue that the wide drop in numbers reflects an administrative failure to record rather than an actual decrease; others such as Timothy Brook argue that the Mongols created a system of enserfment among a huge portion of the Chinese populace, causing many to disappear from the census altogether; other historians like William McNeill and David Morgan argue that the Bubonic Plague was the main factor behind the demographic decline during this period.
In the 14th century, China suffered additional depredations from epidemics of plague. The Black Death is estimated to have killed 25 million people or 30% of the population of China.
Ming dynasty (AD 1368–1644).
Throughout the Yuan dynasty, which lasted less than a century, there was relatively strong sentiment among the populace against the Mongol rule. The frequent natural disasters since the 1340s finally led to peasant revolts. The Yuan dynasty was eventually overthrown by the Ming dynasty in 1368.
Urbanization increased as the population grew and as the division of labor grew more complex. Large urban centers, such as Nanjing and Beijing, also contributed to the growth of private industry. In particular, small-scale industries grew up, often specializing in paper, silk, cotton, and porcelain goods. For the most part, however, relatively small urban centers with markets proliferated around the country. Town markets mainly traded food, with some necessary manufactures such as pins or oil.
Despite the xenophobia and intellectual introspection characteristic of the increasingly popular new school of neo-Confucianism, China under the early Ming dynasty was not isolated. Foreign trade and other contacts with the outside world, particularly Japan, increased considerably. Chinese merchants explored all of the Indian Ocean, reaching East Africa with the voyages of Zheng He.
Zhu Yuanzhang or Hong-wu, the founder of the dynasty, laid the foundations for a state interested less in commerce and more in extracting revenues from the agricultural sector. Perhaps because of the Emperor's background as a peasant, the Ming economic system emphasized agriculture, unlike that of the Song and the Mongolian dynasties, which relied on traders and merchants for revenue. Neo-feudal landholdings of the Song and Mongol periods were expropriated by the Ming rulers. Land estates were confiscated by the government, fragmented, and rented out. Private slavery was forbidden. Consequently, after the death of Emperor Yong-le, independent peasant landholders predominated in Chinese agriculture. These laws might have paved the way to removing the worst of the poverty during the previous regimes.
The dynasty had a strong and complex central government that unified and controlled the empire. The emperor's role became more autocratic, although Zhu Yuanzhang necessarily continued to use what he called the "Grand Secretaries" (内阁) to assist with the immense paperwork of the bureaucracy, including memorials (petitions and recommendations to the throne), imperial edicts in reply, reports of various kinds, and tax records. It was this same bureaucracy that later prevented the Ming government from being able to adapt to changes in society, and eventually led to its decline.
The Yong-le Emperor strenuously tried to extend China's influence beyond its borders by demanding other rulers send ambassadors to China to present tribute. A large navy was built, including four-masted ships displacing 1,500 tons. A standing army of 1 million troops (some estimate as many as 1.9 million ) was created. The Chinese armies conquered Vietnam for around 20 years, while the Chinese fleet sailed the China seas and the Indian Ocean, cruising as far as the east coast of Africa. The Chinese gained influence in eastern Moghulistan. Several maritime Asian nations sent envoys with tribute for the Chinese emperor. Domestically, the Grand Canal was expanded and proved to be a stimulus to domestic trade. Over 100,000 tons of iron per year were produced. Many books were printed using movable type. The imperial palace in Beijing's Forbidden City reached its current splendor. It was also during these centuries that the potential of south China came to be fully exploited. New crops were widely cultivated and industries such as those producing porcelain and textiles flourished.
In 1449, Esen Tayisi led an Oirat Mongol invasion of northern China which culminated in the capture of the Zhengtong Emperor at Tumu.
In 1542, the Mongol leader Altan Khan began to harass China along the northern border, reaching the suburbs of Beijing in 1550. The empire also had to deal with Japanese pirates attacking the southeastern coastline; General Qi Jiguang was instrumental in defeating these pirates. The deadliest earthquake of all times, the Shaanxi earthquake of 1556 that killed approximately 830,000 people, occurred during the Jiajing Emperor's reign.
During the Ming dynasty the last construction on the Great Wall was undertaken to protect China from foreign invasions. While the Great Wall had been built in earlier times, most of what is seen today was either built or repaired by the Ming. The brick and granite work was enlarged, the watch towers were redesigned, and cannons were placed along its length.
Qing dynasty (AD 1644–1911).
The Qing dynasty (1644–1911) was the last imperial dynasty in China. Founded by the Manchus, it was the second non-Han Chinese dynasty to rule all over Chinese territory. The Manchus were formerly known as "Jurchen", residing in the northeastern part of the Ming territory outside the Great Wall. They emerged as the major threat to the late Ming dynasty after Nurhaci united all Jurchen tribes and established an independent state. However, the Ming dynasty would be overthrown by Li Zicheng's peasants rebellion, with Beijing captured in 1644 and the last Ming Emperor Chongzhen committing suicide. The Manchu allied with the Ming dynasty general Wu Sangui to seize Beijing, which was made the capital of the Qing dynasty, and then proceeded to subdue the remaining Ming's resistance in the south. The decades of Manchu conquest caused enormous loss of lives and the economic scale of China shrank drastically. In total, the Manchu conquest of China (1618–1683) cost as many as 25 million lives. Nevertheless, the Manchus adopted the Confucian norms of traditional Chinese government in their rule and were considered a Chinese dynasty.
The Manchus enforced a 'queue order,' forcing the Han Chinese to adopt the Manchu queue hairstyle and Manchu-style clothing. The traditional Han clothing, or "Hanfu", was also replaced by Manchu-style clothing "Qipao" (bannermen dress and "Tangzhuang"). The Kangxi Emperor ordered the creation of Kangxi Dictionary, the most complete dictionary of Chinese characters ever put together at the time. The Qing dynasty set up the "Eight Banners" system that provided the basic framework for the Qing military organization. The bannermen were prohibited from participating in trade and manual labour unless they petitioned to be removed from banner status. They were considered a form of nobility and were given preferential treatment in terms of annual pensions, land and allotments of cloth.
Over the next half-century, all areas previously under the Ming dynasty were consolidated under the Qing. Xinjiang, Tibet, and Mongolia were also formally incorporated into Chinese territory. Between 1673 and 1681, the Emperor Kangxi suppressed the Revolt of the Three Feudatories, an uprising of three generals in Southern China who had been denied hereditary rule to large fiefdoms granted by the previous emperor. In 1683, the Qing staged an amphibious assault on southern Taiwan, bringing down the rebel Kingdom of Tungning, which was founded by the Ming loyalist Koxinga in 1662 after the fall of the Southern Ming, and had served as a base for continued Ming resistance in Southern China.
By the end of Qianlong Emperor's long reign, the Qing Empire was at its zenith. China ruled more than one-third of the world's population, and had the largest economy in the world. By area of extent, it was one of the largest empires ever in history.
In the 19th century, the empire was internally stagnated and externally threatened by imperialism. The defeat by the British Empire in the First Opium War (1840) led to the Treaty of Nanking (1842), under which Hong Kong was ceded and opium import was legitimized. Subsequent military defeats and unequal treaties with other imperial powers would continue even after the fall of the Qing dynasty.
Internally, the Taiping Rebellion (1851–1864), a quasi-Christian religious movement led by the "Heavenly King" Hong Xiuquan, raided roughly a third of Chinese territory for over a decade until they were finally crushed in the Third Battle of Nanking in 1864. Arguably one of the largest wars in the 19th century in terms of troop involvement, there was massive loss of life, with a death toll of about 20 million. A string of rebellions followed, which included the Punti–Hakka Clan Wars, Nien Rebellion, Muslim Rebellion, and Panthay Rebellion. Although all rebellions were eventually put down at enormous cost and with many casualties, the central imperial authority was seriously weakened. The Banner system the Manchus had relied upon so long proved a total failure, as the Banner forces were unable to suppress the rebels. The government called upon local officials in the provinces who raised “New Armies,” which did prove a success in crushing the rebellion. China never rebuilt a strong central army, but the local officials often became warlords, who used military power to effectively rule independently in their provinces.
In response to calamities within the empire and threats from imperialism, the Self-Strengthening Movement was an institutional reform in the second half of the 1800s. The aim was to modernize the empire, with prime emphasis on strengthening the military. However, the reform was undermined by corrupt officials, cynicism, and quarrels within the imperial family. As a result, the "Beiyang Fleet" were soundly defeated in the First Sino-Japanese War (1894–1895). Guangxu Emperor and the reformists then launched a more comprehensive reform effort, the Hundred Days' Reform (1898), but it was shortly overturned by the conservatives under Empress Dowager Cixi in a military coup.
At the turn of the 20th century, a conservative anti-foreign movement, the Boxer Rebellion, violently revolted against foreign influence in Northern China. The group proceeded to attack Chinese Christians and missionaries. The Empress Dowager, probably seeking to ensure her continual grip on power, sided with the Boxers as they advanced on Beijing. In response, a relief expedition of the Eight-Nation Alliance invaded China to rescue the besieged foreign missions. Consisting of British, Japanese, Russian, Italian, German, French, US, and Austrian troops, the alliance defeated the Boxers and demanded further concessions from the Qing government.
The early 1900s saw increasing civil disorder, despite reform talk by Cixi and the Qing government. Slavery in China was abolished in 1910. The Xinhai Revolution in 1911 overthrew the Qing's imperial rule.
Modern China.
Historians agree that the fall of the Qing dynasty demarcated the modern era in Chinese history. Scholars, however, are studying the reasons for that fall in the previous 130 years. Keith Schoppa, the editor of "The Columbia Guide to Modern Chinese History" argues, "A date around 1780 as the beginning of modern China is thus closer to what we know today as historical 'reality."' It also allows us to have a better baseline to understand the precipitous decline of the Chinese polity in the nineteenth and twentieth centuries."
Republic of China (1912–1949).
Frustrated by the Qing court's resistance to reform and by China's weakness, young officials, military officers, and students began to advocate the overthrow of the Qing dynasty and the creation of a republic. They were inspired by the revolutionary ideas of Sun Yat-sen. A revolutionary military uprising, the Wuchang Uprising, began on 10 October 1911, in Wuhan. The provisional government of the Republic of China was formed in Nanjing on 12 March 1912.
Sun Yat-sen was declared President, but Sun was forced to turn power over to Yuan Shikai, who commanded the New Army and was Prime Minister under the Qing government, as part of the agreement to let the last Qing monarch abdicate (a decision Sun would later regret). Over the next few years, Yuan proceeded to abolish the national and provincial assemblies, and declared himself emperor in late 1915. Yuan's imperial ambitions were fiercely opposed by his subordinates; faced with the prospect of rebellion, he abdicated in March 1916, and died in June of that year.
Yuan's death in 1916 left a power vacuum in China; the republican government was all but shattered. This ushered in the Warlord Era, during which much of the country was ruled by shifting coalitions of competing provincial military leaders.
In 1919, the May Fourth Movement began as a response to the terms imposed on China by the Treaty of Versailles ending World War I, but quickly became a nationwide protest movement about the domestic situation in China. The protests were a moral success as the cabinet fell and China refused to sign the Treaty of Versailles, which had awarded German holdings to Japan. The New Culture Movement stimulated by the May Fourth Movement waxed strong throughout the 1920s and 1930s. According to Ebrey:
The discrediting of liberal Western philosophy amongst leftist Chinese intellectuals led to more radical lines of thought inspired by the Russian Revolution, and supported by agents of the Comintern sent to China by Moscow. This created the seeds for the irreconcilable conflict between the left and right in China that would dominate Chinese history for the rest of the century.
In the 1920s, Sun Yat-sen established a revolutionary base in south China, and set out to unite the fragmented nation. With assistance from the Soviet Union (themselves fresh from a socialist uprising), he entered into an alliance with the fledgling Communist Party of China. After Sun's death from cancer in 1925, one of his protégés, Chiang Kai-shek, seized control of the "Kuomintang" (Nationalist Party or KMT) and succeeded in bringing most of south and central China under its rule in a military campaign known as the Northern Expedition (1926–1927). Having defeated the warlords in south and central China by military force, Chiang was able to secure the nominal allegiance of the warlords in the North. In 1927, Chiang turned on the CPC and relentlessly chased the CPC armies and its leaders from their bases in southern and eastern China. In 1934, driven from their mountain bases such as the Chinese Soviet Republic, the CPC forces embarked on the Long March across China's most desolate terrain to the northwest, where they established a guerrilla base at Yan'an in Shaanxi Province. During the Long March, the communists reorganized under a new leader, Mao Zedong (Mao Tse-tung).
The bitter struggle between the KMT and the CPC continued, openly or clandestinely, through the 14-year long Japanese occupation of various parts of the country (1931–1945). The two Chinese parties nominally formed a united front to oppose the Japanese in 1937, during the Sino-Japanese War (1937–1945), which became a part of World War II. Japanese forces committed numerous war atrocities against the civilian population, including biological warfare (see Unit 731) and the Three Alls Policy ("Sankō Sakusen"), the three alls being: ""Kill All, Burn All and Loot All"".
Following the defeat of Japan in 1945, the war between the Nationalist government forces and the CPC resumed, after failed attempts at reconciliation and a negotiated settlement. By 1949, the CPC had established control over most of the country "(see Chinese Civil War)". Westad says the Communists won the Civil War because they made fewer military mistakes than Chiang, and because in his search for a powerful centralized government, Chiang antagonized too many interest groups in China. Furthermore, his party was weakened in the war against Japanese. Meanwhile the Communists told different groups, such as peasants, exactly what they wanted to hear, and cloaked themselves in the cover of Chinese Nationalism. When the Nationalist government forces was defeated by CPC forces in mainland China in 1949, the Nationalist government retreated to Taiwan with its forces, along with Chiang and most of the KMT leadership and a large number of their supporters; the Nationalist government had taken effective control of Taiwan at the end of WWII as part of the overall Japanese surrender, when Japanese troops in Taiwan surrendered to Republic of China troops.
People's Republic of China (1949–present).
Major combat in the Chinese Civil War ended in 1949 with Kuomintang (KMT) pulling out of the mainland, with the government relocating to Taipei and maintaining control only over a few islands. The Communist Party of China was left in control of mainland China. On 1 October 1949, Mao Zedong proclaimed the People's Republic of China. "Communist China" and "Red China" were two common names for the PRC.
The PRC was shaped by a series of campaigns and five-year plans. The economic and social plan known as the Great Leap Forward resulted in an estimated 45 million deaths. Mao's government carried out mass executions of landowners, instituted collectivisation and implemented the Laogai camp system. In 1966, Mao and his allies launched the Cultural Revolution, which would last until Mao's death a decade later. The Cultural Revolution, motivated by power struggles within the Party and a fear of the Soviet Union, led to a major upheaval in Chinese society.
In 1972, at the peak of the Sino-Soviet split, Mao and Zhou Enlai met Richard Nixon in Beijing to establish relations with the United States. In the same year, the PRC was admitted to the United Nations in place of the Republic of China for China's membership of the United Nations, and permanent membership of the Security Council.
A power struggle followed Mao's death in 1976. The Gang of Four were arrested and blamed for the excesses of the Cultural Revolution, marking the end of a turbulent political era in China. Deng Xiaoping outmaneuvered Mao's anointed successor chairman Hua Guofeng, and gradually emerged as the "de facto" leader over the next few years.
Deng Xiaoping was the Paramount Leader of China from 1978 to 1992, although he never became the head of the party or state, and his influence within the Party led the country to significant economic reforms. The Communist Party subsequently loosened governmental control over citizens' personal lives and the communes were disbanded with many peasants receiving multiple land leases, which greatly increased incentives and agricultural production. This turn of events marked China's transition from a planned economy to a mixed economy with an increasingly open market environment, a system termed by some as "market socialism", and officially by the Communist Party of China as "Socialism with Chinese characteristics". The PRC adopted its current constitution on 4 December 1982.
In 1989, the death of former general secretary Hu Yaobang helped to spark the Tiananmen Square protests of 1989, during which students and others campaigned for several months, speaking out against corruption and in favour of greater political reform, including democratic rights and freedom of speech. However, they were eventually put down on 4 June when PLA troops and vehicles entered and forcibly cleared the square, resulting in numerous casualties. This event was widely reported and brought worldwide condemnation and sanctions against the government. The "Tank Man" incident in particular became famous.
CPC general secretary and PRC President Jiang Zemin and PRC Premier Zhu Rongji, both former mayors of Shanghai, led post-Tiananmen PRC in the 1990s. Under Jiang and Zhu's ten years of administration, the PRC's economic performance pulled an estimated 150 million peasants out of poverty and sustained an average annual gross domestic product growth rate of 11.2%. The country formally joined the World Trade Organization in 2001.
Although the PRC needs economic growth to spur its development, the government has begun to worry that rapid economic growth has negatively impacted the country's resources and environment. Another concern is that certain sectors of society are not sufficiently benefiting from the PRC's economic development; one example of this is the wide gap between urban and rural areas. As a result, under former CPC general secretary and President Hu Jintao and Premier Wen Jiabao, the PRC has initiated policies to address these issues of equitable distribution of resources, but the outcome remains to be seen. More than 40 million farmers have been displaced from their land, usually for economic development, contributing to 87,000 demonstrations and riots across China in 2005. For much of the PRC's population, living standards have seen extremely large improvements and freedom continues to expand, but political controls remain tight and rural areas poor.

</doc>
<doc id="5762" url="http://en.wikipedia.org/wiki?curid=5762" title="Civil engineering">
Civil engineering

Civil engineering is a professional engineering discipline that deals with the design, construction, and maintenance of the physical and naturally built environment, including works like roads, bridges, canals, dams, and buildings. Civil engineering is the second-oldest engineering discipline after military engineering, and it is defined to distinguish non-military engineering from military engineering. It is traditionally broken into several sub-disciplines including architectural engineering, environmental engineering, geotechnical engineering, geophysics, geodesy, control engineering, structural engineering, transportation engineering, earth science, atmospheric sciences, forensic engineering, municipal or urban engineering, water resources engineering, materials engineering, offshore engineering, quantity surveying, coastal engineering, surveying, and construction engineering. Civil engineering takes place in the public sector from municipal through to national governments, and in the private sector from individual homeowners through to international companies.
History of the civil engineering profession.
Engineering has been an aspect of life since the beginnings of human existence. The earliest practice of civil engineering may have commenced between 4000 and 2000 BC in Ancient Egypt and Mesopotamia (Ancient Iraq) when humans started to abandon a nomadic existence, creating a need for the construction of shelter. During this time, transportation became increasingly important leading to the development of the wheel and sailing.
Until modern times there was no clear distinction between civil engineering and architecture, and the term engineer and architect were mainly geographical variations referring to the same occupation, and often used interchangeably. The construction of pyramids in Egypt (circa 2700–2500 BC) were some of the first instances of large structure constructions. Other ancient historic civil engineering constructions include the Qanat water management system (the oldest is older than 3000 years and longer than 71 km,) the Parthenon by Iktinos in Ancient Greece (447–438 BC), the Appian Way by Roman engineers (c. 312 BC), the Great Wall of China by General Meng T'ien under orders from Ch'in Emperor Shih Huang Ti (c. 220 BC) and the stupas constructed in ancient Sri Lanka like the Jetavanaramaya and the extensive irrigation works in Anuradhapura. The Romans developed civil structures throughout their empire, including especially aqueducts, insulae, harbors, bridges, dams and roads.
In the 18th century, the term civil engineering was coined to incorporate all things civilian as opposed to military engineering. The first self-proclaimed civil engineer was John Smeaton, who constructed the Eddystone Lighthouse. In 1771 Smeaton and some of his colleagues formed the Smeatonian Society of Civil Engineers, a group of leaders of the profession who met informally over dinner. Though there was evidence of some technical meetings, it was little more than a social society.
In 1818 the Institution of Civil Engineers was founded in London, and in 1820 the eminent engineer Thomas Telford became its first president. The institution received a Royal Charter in 1828, formally recognising civil engineering as a profession. Its charter defined civil engineering as:
The first private college to teach Civil Engineering in the United States was Norwich University, founded in 1819 by Captain Alden Partridge. The first degree in Civil Engineering in the United States was awarded by Rensselaer Polytechnic Institute in 1835. The first such degree to be awarded to a woman was granted by Cornell University to Nora Stanton Blatch in 1905.
History of civil engineering.
Civil engineering is the application of physical and scientific principles for solving the problems of society, and its history is intricately linked to advances in understanding of physics and mathematics throughout history. Because civil engineering is a wide ranging profession, including several separate specialized sub-disciplines, its history is linked to knowledge of structures, materials science, geography, geology, soils, hydrology, environment, mechanics and other fields.
Throughout ancient and medieval history most architectural design and construction was carried out by artisans, such as stonemasons and carpenters, rising to the role of master builder. Knowledge was retained in guilds and seldom supplanted by advances. Structures, roads and infrastructure that existed were repetitive, and increases in scale were incremental.
One of the earliest examples of a scientific approach to physical and mathematical problems applicable to civil engineering is the work of Archimedes in the 3rd century BC, including Archimedes Principle, which underpins our understanding of buoyancy, and practical solutions such as Archimedes' screw. Brahmagupta, an Indian mathematician, used arithmetic in the 7th century AD, based on Hindu-Arabic numerals, for excavation (volume) computations.
The civil engineer.
Education and licensure.
"Civil engineers" typically possess an academic degree in civil engineering. The length of study is three to five years, and the completed degree is designated as a bachelor of engineering, or a bachelor of science. The curriculum generally includes classes in physics, mathematics, project management, design and specific topics in civil engineering. After taking basic courses in most sub-disciplines of civil engineering, they move onto specialize in one or more sub-disciplines at advanced levels. While an undergraduate degree (BEng/BSc) normally provides successful students with industry-accredited qualification, some academic institutions offer post-graduate degrees (MEng/MSc), which allow students to further specialize in their particular area of interest.
In most countries, a bachelor's degree in engineering represents the first step towards professional certification, and a professional body certifies the degree program. After completing a certified degree program, the engineer must satisfy a range of requirements (including work experience and exam requirements) before being certified. Once certified, the engineer is designated as a professional engineer (in the United States, Canada and South Africa), a chartered engineer (in most Commonwealth countries), a chartered professional engineer (in Australia and New Zealand), or a European engineer (in most countries of the European Union). There are international agreements between relevant professional bodies to allow engineers to practice across national borders.
The benefits of certification vary depending upon location. For example, in the United States and Canada, "only a licensed professional engineer may prepare, sign and seal, and submit engineering plans and drawings to a public authority for approval, or seal engineering work for public and private clients." This requirement is enforced under provincial law such as the Engineers Act in Quebec.
No such legislation has been enacted in other countries including the United Kingdom. In Australia, state licensing of engineers is limited to the state of Queensland. Almost all certifying bodies maintain a code of ethics which all members must abide by.
Engineers must obey contract law in their contractual relationships with other parties. In cases where an engineer's work fails, he may be subject to the law of tort of negligence, and in extreme cases, criminal charges. An engineer's work must also comply with numerous other rules and regulations such as building codes and environmental law.
Sub-disciplines.
In general, civil engineering is concerned with the overall interface of human created fixed projects with the greater world. General civil engineers work closely with surveyors and specialized civil engineers to design grading, drainage, pavement, water supply, sewer service, electric and communications supply, and land divisions. General engineers spend much time visiting project sites, developing community consensus, and preparing construction plans. General civil engineering is also referred to as site engineering, a branch of civil engineering that primarily focuses on converting a tract of land from one usage to another. Civil engineers apply the principles of geotechnical engineering, structural engineering, environmental engineering, transportation engineering and construction engineering to residential, commercial, industrial and public works projects of all sizes and levels of construction.
Materials science and engineering.
"Materials science" is closely related to civil engineering. Material engineering studies fundamental characteristics of materials, and deals with ceramics such as concrete and mix asphalt concrete, strong metals such as aluminum and steel, and polymers including polymethylmethacrylate (PMMA) and carbon fibers.
Materials engineering also involves protection and prevention (paints and finishes). Alloying combines two types of metals to produce another metal with desired properties. It incorporates elements of applied physics and chemistry. With recent media attention on nanoscience and nanotechnology, materials science has been at the forefront of academic research. It is also an important part of forensic engineering and failure analysis.
Coastal engineering.
"Coastal engineering" is concerned with managing coastal areas. In some jurisdictions, the terms sea defense and coastal protection mean defense against flooding and erosion, respectively. The term coastal defense is the more traditional term, but coastal management has become more popular as the field has expanded to techniques that allow erosion to claim land.
Construction engineering.
"Construction engineering" involves planning and execution, transportation of materials, site development based on hydraulic, environmental, structural and geotechnical engineering. As construction firms tend to have higher business risk than other types of civil engineering firms do, construction engineers often engage in more business-like transactions, for example, drafting and reviewing contracts, evaluating logistical operations, and monitoring prices of supplies.
Earthquake engineering.
"Earthquake engineering" involves designing structures to withstand hazardous earthquake exposures. Earthquake engineering is a sub-discipline of structural engineering. The main objectives of earthquake engineering are to understand interaction of structures on the shaky ground; foresee the consequences of possible earthquakes; and design, construct and maintain structures to perform at earthquake in compliance with building codes.
Environmental engineering.
"Environmental engineering" is the contemporary term for sanitary engineering, though sanitary engineering traditionally had not included much of the hazardous waste management and environmental remediation work covered by environmental engineering. Public health engineering and environmental health engineering are other terms being used.
Environmental engineering deals with treatment of chemical, biological, or thermal wastes, purification of water and air, and remediation of contaminated sites after waste disposal or accidental contamination. Among the topics covered by environmental engineering are pollutant transport, water purification, waste water treatment, air pollution, solid waste treatment, and hazardous waste management. Environmental engineers administer pollution reduction, green engineering, and industrial ecology. Environmental engineers also compile information on environmental consequences of proposed actions.
Geotechnical engineering.
"Geotechnical engineering" studies rock and soil supporting civil engineering systems. Knowledge from the field of geology, materials science, mechanics, and hydraulics is applied to safely and economically design foundations, retaining walls, and other structures. Environmental efforts to protect groundwater and safely maintain landfills have spawned a new area of research called geoenvironmental engineering.
Identification of soil properties presents challenges to geotechnical engineers. Boundary conditions are often well defined in other branches of civil engineering, but unlike steel or concrete, the material properties and behavior of soil are difficult to predict due to its variability and limitation on investigation. Furthermore, soil exhibits nonlinear (stress-dependent) strength, stiffness, and dilatancy (volume change associated with application of shear stress), making studying soil mechanics all the more difficult.
Water resources engineering.
"Water resources engineering" is concerned with the collection and management of water (as a natural resource). As a discipline it therefore combines hydrology, environmental science, meteorology, geology, conservation, and resource management. This area of civil engineering relates to the prediction and management of both the quality and the quantity of water in both underground (aquifers) and above ground (lakes, rivers, and streams) resources. Water resource engineers analyze and model very small to very large areas of the earth to predict the amount and content of water as it flows into, through, or out of a facility. Although the actual design of the facility may be left to other engineers.
Hydraulic engineering is concerned with the flow and conveyance of fluids, principally water. This area of civil engineering is intimately related to the design of pipelines, water supply network, drainage facilities (including bridges, dams, channels, culverts, levees, storm sewers), and canals. Hydraulic engineers design these facilities using the concepts of fluid pressure, fluid statics, fluid dynamics, and hydraulics, among others.
Structural engineering.
"Structural engineering" is concerned with the structural design and structural analysis of buildings, bridges, towers, flyovers (overpasses), tunnels, off shore structures like oil and gas fields in the sea, aerostructure and other structures. This involves identifying the loads which act upon a structure and the forces and stresses which arise within that structure due to those loads, and then designing the structure to successfully support and resist those loads. The loads can be self weight of the structures, other dead load, live loads, moving (wheel) load, wind load, earthquake load, load from temperature change etc. The structural engineer must design structures to be safe for their users and to successfully fulfill the function they are designed for (to be "serviceable"). Due to the nature of some loading conditions, sub-disciplines within structural engineering have emerged, including wind engineering and earthquake engineering.
Design considerations will include strength, stiffness, and stability of the structure when subjected to loads which may be static, such as furniture or self-weight, or dynamic, such as wind, seismic, crowd or vehicle loads, or transitory, such as temporary construction loads or impact. Other considerations include cost, constructability, safety, aesthetics and sustainability.
Surveying.
"Surveying" is the process by which a surveyor measures certain dimensions that generally occur on the surface of the Earth. Surveying equipment, such as levels and theodolites, are used for accurate measurement of angular deviation, horizontal, vertical and slope distances. With computerisation, electronic distance measurement (EDM), total stations, GPS surveying and laser scanning have supplemented (and to a large extent supplanted) the traditional optical instruments. This information is crucial to convert the data into a graphical representation of the Earth's surface, in the form of a map. This information is then used by civil engineers, contractors and even realtors to design from, build on, and trade, respectively. Elements of a building or structure must be correctly sized and positioned in relation to each other and to site boundaries and adjacent structures. Although surveying is a distinct profession with separate qualifications and licensing arrangements, civil engineers are trained in the basics of surveying and mapping, as well as geographic information systems. Surveyors may also lay out the routes of railways, tramway tracks, highways, roads, pipelines and streets as well as position other infrastructures, such as harbors, before construction.
In the United States, Canada, the United Kingdom and most Commonwealth countries land surveying is considered to be a distinct profession. Land surveyors are not considered to be engineers, and have their own professional associations and licencing requirements. The services of a licenced land surveyor are generally required for boundary surveys (to establish the boundaries of a parcel using its legal description) and subdivision plans (a plot or map based on a survey of a parcel of land, with boundary lines drawn inside the larger parcel to indicate the creation of new boundary lines and roads), both of which are generally referred to as cadastral surveying.
Construction surveying is generally performed by specialised technicians. Unlike land surveyors, the resulting plan does not have legal status. Construction surveyors perform the following tasks:
Transportation engineering.
"Transportation engineering" is concerned with moving people and goods efficiently, safely, and in a manner conducive to a vibrant community. This involves specifying, designing, constructing, and maintaining transportation infrastructure which includes streets, canals, highways, rail systems, airports, ports, and mass transit. It includes areas such as transportation design, transportation planning, traffic engineering, some aspects of urban engineering, queueing theory, pavement engineering, Intelligent Transportation System (ITS), and infrastructure management.
Municipal or urban engineering.
"Municipal engineering" is concerned with municipal infrastructure. This involves specifying, designing, constructing, and maintaining streets, sidewalks, water supply networks, sewers, street lighting, municipal solid waste management and disposal, storage depots for various bulk materials used for maintenance and public works (salt, sand, etc.), public parks and bicycle paths. In the case of underground utility networks, it may also include the civil portion (conduits and access chambers) of the local distribution networks of electrical and telecommunications services. It can also include the optimizing of waste collection and bus service networks. Some of these disciplines overlap with other civil engineering specialties, however municipal engineering focuses on the coordination of these infrastructure networks and services, as they are often built simultaneously, and managed by the same municipal authority.
Forensic engineering.
"Forensic engineering" is the investigation of materials, products, structures or components that fail or do not operate or function as intended, causing personal injury or damage to property. The consequences of failure are dealt with by the law of product liability. The field also deals with retracing processes and procedures leading to accidents in operation of vehicles or machinery. The subject is applied most commonly in civil law cases, although it may be of use in criminal law cases. Generally the purpose of a Forensic engineering investigation is to locate cause or causes of failure with a view to improve performance or life of a component, or to assist a court in determining the facts of an accident. It can also involve investigation of intellectual property claims, especially patents.
Control engineering.
"Control engineering" (or "control systems engineering") is the branch of civil engineering discipline that applies control theory to design systems with desired behaviors. The practice uses sensors to measure the output performance of the device being controlled (often a vehicle) and those measurements can be used to give feedback to the input actuators that can make corrections toward desired performance. When a device is designed to perform without the need of human inputs for correction it is called automatic control (such as cruise control for regulating a car's speed). Multidisciplinary in nature, control systems engineering activities focus on implementation of control systems mainly derived by mathematical modeling of systems of a diverse range.

</doc>
<doc id="5763" url="http://en.wikipedia.org/wiki?curid=5763" title="Cantonese (disambiguation)">
Cantonese (disambiguation)

Cantonese is a language spoken primarily in south China.
Cantonese may also refer to:

</doc>
<doc id="5765" url="http://en.wikipedia.org/wiki?curid=5765" title="Çatalhöyük">
Çatalhöyük

Çatalhöyük (; also "Çatal Höyük" and "Çatal Hüyük"; "çatal" is Turkish for "fork", "höyük" for "mound") was a very large Neolithic and Chalcolithic proto-city settlement in southern Anatolia, which existed from approximately 7500 BC to 5700 BC, and flourished around 7000BC. It is the largest and best-preserved Neolithic site found to date. In July 2012, it was inscribed as a UNESCO World Heritage Site.
Çatalhöyük is located overlooking the Konya Plain, southeast of the present-day city of Konya (ancient Iconium) in Turkey, approximately 140 km (87 mi) from the twin-coned volcano of Mount Hasan. The eastern settlement forms a mound which would have risen about 20 m (66 ft) above the plain at the time of the latest Neolithic occupation. There is also a smaller settlement mound to the west and a Byzantine settlement a few hundred meters to the east. The prehistoric mound settlements were abandoned before the Bronze Age. A channel of the Çarşamba river once flowed between the two mounds, and the settlement was built on alluvial clay which may have been favourable for early agriculture.
Archaeology.
The site was first excavated by James Mellaart in 1958. He later led a team which excavated there for four seasons between 1961 and 1965. These excavations revealed this section of Anatolia as a centre of advanced culture in the Neolithic period.
Mellaart was banned from Turkey for his involvement in the Dorak affair in which he published drawings of supposedly important Bronze Age artifacts that later went missing.
After this scandal, the site lay idle until 1993, when investigations began under the leadership of Ian Hodder then at the University of Cambridge. These investigations are among the most ambitious excavation projects currently in progress according to, among others, Colin Renfrew. In addition to extensive use of archaeological science, psychological and artistic interpretations of the symbolism of the wall paintings have been employed. Hodder, a former student of Mellaart, chose the site as the first "real world" test of his then-controversial theory of post-processual archaeology.
Culture.
Çatalhöyük was composed entirely of domestic buildings, with no obvious public buildings. While some of the larger ones have rather ornate murals, these rooms' purpose remains unclear.
The population of the eastern mound has been estimated at up to 10,000 people, but population likely varied over the community’s history. An average population of between 5,000 to 8,000 is a reasonable estimate. The inhabitants lived in mud-brick houses that were crammed together in an agglutinative manner. No footpaths or streets were used between the dwellings, which were clustered in a honeycomb-like maze. Most were accessed by holes in the ceiling, with doors reached by ladders and stairs. The rooftops were effectively streets. The ceiling openings also served as the only source of ventilation, allowing smoke from the houses' open hearths and ovens to escape. 
Houses had plaster interiors characterized by squared-off timber ladders or steep stairs. These were usually on the south wall of the room, as were cooking hearths and ovens. Each main room served for cooking and daily activities. The main rooms contained raised platforms that may have been used for a range of domestic activities. All interior walls and platforms were plastered to a smooth finish. Ancillary rooms were used as storage, and were accessed through low openings from main rooms.
All rooms were kept scrupulously clean. Archaeologists identified very little rubbish in the buildings, finding middens outside the ruins, with sewage and food waste, as well as significant amounts of wood ash. In good weather, many daily activities may also have taken place on the rooftops, which may have formed a plaza. In later periods, large communal ovens appear to have been built on these rooftops. Over time, houses were renewed by partial demolition and rebuilding on a foundation of rubble, which was how the mound was gradually built up. As many as eighteen levels of settlement have been uncovered.
The people of Çatalhöyük buried their dead within the village. Human remains have been found in pits beneath the floors and, especially, beneath hearths, the platforms within the main rooms, and under beds. Bodies were tightly flexed before burial and were often placed in baskets or wrapped in reed mats. Disarticulated bones in some graves suggest that bodies may have been exposed in the open air for a time before the bones were gathered and buried. In some cases, graves were disturbed and the individual’s head removed from the skeleton. These heads may have been used in rituals, as some were found in other areas of the community. Some skulls were plastered and painted with ochre to recreate faces, a custom more characteristic of Neolithic sites in Syria and at Neolithic Jericho than at sites closer by.
Vivid murals and figurines are found throughout the settlement, on interior and exterior walls. Distinctive clay figurines of women, notably the Seated Woman of Çatalhöyük have been found in the upper levels of the site. Although no identifiable temples have been found, the graves, murals, and figurines suggest that the people of Çatalhöyük had a religion rich in symbols. Rooms with concentrations of these items may have been shrines or public meeting areas. Predominant images include men with erect phalluses, hunting scenes, red images of the now extinct aurochs (wild cattle) and stags, and vultures swooping down on headless figures. Relief figures are carved on walls, such as of lionesses facing one another.
Heads of animals, especially of cattle, were mounted on walls. A painting of the village, with the twin mountain peaks of Hasan Dağ in the background, is frequently cited as the world's oldest map and the first landscape painting. However, some archaeologists question this interpretation. Stephanie Meece, for example, argues that it is more likely a painting of a leopard skin instead of a volcano, and a decorative geometric design instead of a map.
Çatalhöyük had no apparent social classes, as no houses with distinctive features (belonging to royalty or religious hierarchy, for example) have been found so far. The most recent investigations also reveal little social distinction based on gender, with men and women receiving equivalent nutrition and seeming to have equal social status, as typically found in Paleolithic cultures.
In upper levels of the site, it becomes apparent that the people of Çatalhöyük were gaining skills in agriculture and the domestication of animals. Female figurines have been found within bins used for storage of cereals, such as wheat and barley, and the figurines are presumed to be of a deity protecting the grain. Peas were also grown, and almonds, pistachios, and fruit were harvested from trees in the surrounding hills. Sheep were domesticated and evidence suggests the beginning of cattle domestication as well. However, hunting continued to be a major source of food for the community. Pottery and obsidian tools appear to have been major industries; obsidian tools were probably both used and also traded for items such as Mediterranean sea shells and flint from Syria.
Religion.
A striking feature of Çatalhöyük are its female figurines. Mellaart, the original excavator, argued that these well-formed, carefully made figurines, carved and molded from marble, blue and brown limestone, schist, calcite, basalt, alabaster, and clay, represented a female deity. Although a male deity existed as well, “…statues of a female deity far outnumber those of the male deity, who moreover, does not appear to be represented at all after Level VI”. To date, eighteen levels have been identified. These artfully-hewn figurines were found primarily in areas Mellaart believed to be shrines. The stately goddess seated on a throne flanked by two female lions ("illustration") was found in a grain bin, which Mellaart suggests might have been a means of ensuring the harvest or protecting the food supply.
Whereas Mellaart excavated nearly two hundred buildings in four seasons, the current excavator, Ian Hodder, spent an entire season excavating one building alone. Hodder and his team, in 2004 and 2005, began to believe that the patterns suggested by Mellaart were false. They found one similar figurine, but the vast majority did not imitate the Mother Goddess style that Mellaart suggested. Instead of a Mother Goddess culture, Hodder points out that the site gives little indication of a matriarchy or patriarchy.
In an article in the "Turkish Daily News", Hodder is reported as denying that Çatalhöyük was a matriarchal society and quoted as saying "When we look at what they eat and drink and at their social statues, we see that men and women had the same social status. There was a balance of power. Another example is the skulls found. If one's social status was of high importance in Çatalhöyük, the body and head were separated after death. The number of female and male skulls found during the excavations is almost equal."
In a report in September 2009 on the discovery of around 2000 figurines Hodder is quoted as saying:
“Çatalhöyük was excavated in the 1960s in a methodical way, but not using the full range of natural science techniques that are available to us today. Sir James Mellaart who excavated the site in the 1960s came up with all sorts of ideas about the way the site was organised and how it was lived in and so on,” he said. “We’ve now started working there since the mid 1990s and come up with very different ideas about the site. One of the most obvious examples of that is that Çatalhöyük is perhaps best known for the idea of the mother goddess. But our work more recently has tended to show that in fact there is very little evidence of a mother goddess and very little evidence of some sort of female-based matriarchy. That’s just one of the many myths that the modern scientific work is undermining.”
Professor Lynn Meskell explained that while the original excavations had found only 200 figures, the new excavations had uncovered 2000 figurines of which most were animals, with less than 5% of the figurines women.
Estonian folklorist Uku Masing has suggested as early as in 1976, that Çatalhöyük was probably a hunting and gathering religion and the Mother Goddess figurine didn't represent a female deity. He implied that perhaps a longer period of time was needed in order to develop symbols for agricultural rites. His theory was developed in the paper "Some remarks on the mythology of the people of Catal Hüyük".
Archaeological project support.
The current archaeological investigations at Çatalhöyük are supported by the following institutions and organizations:

</doc>
