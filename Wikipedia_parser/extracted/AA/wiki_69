<doc id="5199" url="http://en.wikipedia.org/wiki?curid=5199" title="Canada–United States relations">
Canada–United States relations

Relations between Canada and the United States of America have spanned more than two centuries. This includes a shared British cultural heritage, and the eventual development of one of the most stable and mutually-beneficial international relationships in the modern world. Each is the other's chief economic partner and tourism and migration between the two nations has increased rapport. The most serious breach in the relationship was the War of 1812, which began with an American invasion of Canada, and a subsequent Canadian counterattack into U.S territory. In 1815 the war ended with a military stalemate reached. The border remained the same after the war and was demilitarized, but the British no longer aided Indian attacks on American territory. Apart from minors raids, it has remained peaceful. Fears of an American takeover played an important role in the formation of the Dominion of Canada (1867), and Canada's rejection of free trade (1911). Military collaboration was close during World War II and continued throughout the Cold War on both a bilateral basis through NORAD and through multilateral participation in NATO. A very high volume of trade and migration between the two, as well as a heavy overlapping of popular and elite culture, has generated closer ties, especially after the signing of the Canada–United States Free Trade Agreement in 1988.
Canada and the United States are currently the world's largest trading partners, share the world's longest border, and have significant interoperability within the defence sphere. Recent difficulties have included repeated trade disputes, environmental concerns, Canadian concern for the future of oil exports, and issues of illegal immigration and the threat of terrorism. Nevertheless, trade between the two countries has continued to expand in both absolute and relative terms for the last two hundred years, but especially following the 1988 FTA and the subsequent signing of the North American Free Trade Agreement (NAFTA) in 1994 which has since further merged the two economies.
The foreign policies of the neighbours have been closely aligned since the Cold War. However, Canada has disagreed with American policies regarding the Vietnam War, the status of Cuba, the Iraq War, Missile Defense, and the War on Terrorism. A serious diplomatic debate is whether the Northwest Passage is in international waters or under Canadian sovereignty.
There are close cultural ties between modern day Canada and the United States, advanced by such similarities as the language as both predominantly speak English.
Meanwhile co-operation on many fronts, such as the ease of the flow of goods, services, and people across borders are to be even more extended, as well as the establishment of joint border inspection agencies, relocation of U.S. food inspectors agents to Canadian plants and "vice versa", greater sharing of intelligence, and harmonizing regulations on everything from food to manufactured goods, thus further increasing the American-Canadian assemblage.
According to Gallup's annual public opinion polls, Canada has consistently been Americans' favorite nation, with 96% of Americans viewing Canada favorably in 2012. According to a 2013 BBC World Service Poll, 84% of Americans view their northern neighbor's influence positively, with only 5% expressing a negative view, the most favorable perception of Canada in the world. As of spring 2013, 64% of Canadians had a favorable view of the U.S. and 81% expressed confidence in Obama to do the right thing in international matters. According to the same poll, 30% viewed the U.S. negatively.
History.
Colonial wars.
Before the British conquest of French Canada in 1760, there had been a series of wars between the British and the French which were fought out in the colonies as well as in Europe and the high seas. In general, the British heavily relied on American colonial militia units, while the French heavily relied on their Indian allies. The Iroquois Indians were important allies of the British. Much of the fighting involved ambushes and small-scale warfare in the villages along the border between New England and Quebec. The New England colonies had a much larger population than Quebec, so major invasions came from south to north. The Indian allies, only loosely controlled by the French, repeatedly raided New England villages to kidnap women and children, and torture and kill the men. Those who survived were brought up as Francophone Catholics. The tension along the border was exacerbated by religion, the French Catholics and English Protestants had a deep mutual distrust. There was a naval dimension as well, involving privateers attacking enemy merchant ships.
England seized Quebec from 1629 to 1632, and Acadia in 1613 and again from 1654 to 1670; These territories were returned to France by the peace treaties. The major wars were (to use American names), King William's War (1689-1697); Queen Anne's War (1702-1713); King George's War (1744-1748), and the French and Indian War (1755-1763).
New England soldiers and sailors were critical to the successful British campaign to capture the French fortress of Louisbourg in 1745, and (after it had been returned by treaty) to capture it again in 1758.
Mingling of peoples.
From the 1750s to the 21st century, there has been extensive mingling of the Canadian and American populations, with large movements in both directions.
New England Yankees settled large parts of Nova Scotia before 1775, and were neutral during the American Revolution. At the end of the Revolution, about 75,000 Loyalists moved out of the new United States to Nova Scotia, New Brunswick, and the lands of Quebec, east and south of Montreal. From 1790 to 1812 many farmers moved from New York and New England into Ontario (mostly to Niagara, and the north shore of Lake Ontario). In the mid and late 19th century gold rushes attracted American prospectors, mostly to British Columbia (Cariboo, Fraser gold rushes) and later to the Yukon. In the early 20th century, the opening of land blocks in the Prairie Provinces attracted many farmers from the American Midwest. Many Mennonites immigrated from Pennsylvania and formed their own colonies. In the 1890s some Mormons went north to form communities in Alberta after the LDS Church rejected plural marriage. The 1960s saw the arrival of about 50,000 draft-dodgers who opposed the Vietnam War.
In the late 19th and early 20th centuries, about 900,000 French Canadians moved to the U.S., with 395,000 residents there in 1900. Two-thirds went to mill towns in New England, where they formed distinctive ethnic communities. By the late 20th century, they had abandoned the French language, but most kept the Catholic religion. About twice as many English Canadians came to the U.S., but they did not form distinctive ethnic settlements.
Canada was a way-station through which immigrants from other lands stopped for a while, ultimately heading to the U.S. In 1851–1951, 7.1 million people arrived in Canada (mostly from Continental Europe), and 6.6 million left Canada, most of them to the U.S.
American Revolution.
At the outset of the American Revolution, the American revolutionaries hoped the French Canadians in Quebec and the Colonists in Nova Scotia would join their rebellion and they were pre-approved for joining the United States in the Articles of Confederation. When Canada was invaded during the American Revolutionary War, thousands joined the American cause and formed regiments that fought during the war; however most remained neutral and some joined the British effort. Britain advised the French Canadians that the British Empire already enshrined their rights in the Quebec Act, which the American colonies had viewed as one of the Intolerable Acts. The American invasion was a fiasco and Britain tightened its grip on its northern possessions; in 1777, a major British invasion into New York led to the surrender of the entire British army at Saratoga, and led France to enter the war as an ally of the U.S. The French Canadians largely ignored France's appeals for solidarity. After the war Canada became a refuge for about 75,000 Loyalists who either wanted to leave the U.S., or were compelled by Patriot reprisals to do so.
Among the original Loyalists there were 3500 free blacks. Most went to Nova Scotia and in 1792, 1200 migrated to Sierra Leone. About 2000 black slaves were brought in by Loyalist owners; they remained slaves in Canada until the Empire abolished slavery in 1833. Before 1860, about 30,000-40,000 blacks entered Canada; many were already free and others were escaped slaves who came through the Underground Railroad.
War of 1812.
The Treaty of Paris (1783), which ended the war, called for British forces to vacate all their forts south of the Great Lakes border. Britain refused to do so, citing failure of the United States to provide financial restitution for Loyalists who had lost property in the war. The Jay Treaty in 1795 with Great Britain resolved that lingering issue and the British departed the forts. Thomas Jefferson saw the nearby British imperial presence as a threat to the United States, and so he opposed the Jay Treaty, and it became one of the major political issues in the United States at the time. Thousands of Americans immigrated to Upper Canada (Ontario) from 1785 to 1812 to obtain cheaper land and better tax rates prevalent in that province; despite expectations that they would be loyal to the U.S. if a war broke out, in the event they were largely non-political.
Tensions mounted again after 1805, erupting into the War of 1812, when the Americans declared war on Britain. The Americans were angered by British harassment of U.S. ships on the high seas and seizure ("Impressment") of 6,000 sailors from American ships, severe restrictions against neutral American trade with France, and British support for hostile Indian tribes in Ohio and territories the U.S. had gained in 1783. American "honor" was an implicit issue. The Americans were outgunned by more than 10 to 1 by the Royal Navy, but could call on an army much larger than the British garrison in Canada, and so a land invasion of Canada was proposed as the only feasible, and most advantegous means of attacking the British Empire. Americans on the western frontier also hoped an invasion would bring an end to British support of Native American resistance to the westward expansion of the United States, typified by Tecumseh's coalition of tribes. Americans may also have wanted to annex Canada.
Once war broke out, the American strategy was to seize Canada - perhaps as a means of forcing concessions from the British Empire, or perhaps in order to annex it. There was some hope that settlers in western Canada—most of them recent immigrants from the U.S.—would welcome the chance to overthrow their British rulers. However, the American invasions were defeated primarily by British regulars with support from Native Americans and Upper Canada (Ontario) militia. Aided by the powerful Royal Navy, a series of British raids on the American coast were highly successful, culminating with an attack on Washington that resulted in the British burning of the White House, Capitol, and other public buildings. Major British invasions of New York in 1814 and Louisiana in 1814–15 were fiascoes, with the British retreating from New York and decisively defeated at the Battle of New Orleans. At the end of the war, Britain's American Indian allies had largely been defeated, and the American's controlled a strip of Western Ontario centered on Fort Malden. However, Britain held much of Maine, and, with the support of their remaining American Indian allies, huge areas of the Old Northwest, including Wisconsin and much of Michigan and Illinois. With the surrender of Napoleon in 1814, Britain ended naval policies that angered Americans; with the defeat of the Indian tribes the threat to American expansion was ended. The upshot was both sides had asserted their honour, Canada was not annexed, and London and Washington had nothing more to fight over. The war was ended by the Treaty of Ghent, which took effect in February 1815. A series of postwar agreements further stabilized peaceful relations along the Canadian-US border. Canada reduced American immigration for fear of undue American influence, and built up the Anglican church as a counterweight to the largely American Methodist and Baptist churches.
In later years, Anglophone Canadians, especially in Ontario, viewed the War of 1812 as a heroic and successful resistance against invasion and as a victory that defined them as a people. The myth that the Canadian militia had defeated the invasion almost single-handed, known logically as the "militia myth", became highly prevalent after the war, having been propounded by John Strachan, Anglican Bishop of York. Meanwhile the United States celebrated victory in its "Second War of Independence," and war heroes such as Andrew Jackson and William Henry Harrison headed to the White House.
Conservative reaction.
In the aftermath of the War of 1812, pro-imperial conservatives led by Anglican Bishop John Strachan took control in Ontario ("Upper Canada"), and promoted the Anglican religion as opposed to the more republican Methodist and Baptist churches. A small interlocking elite, known as the Family Compact took full political control. Democracy, as practiced in the US, was ridiculed. The policies had the desired effect of deterring immigration from United States. Revolts in favor of democracy in Ontario and Quebec ("Lower Canada") in 1837 were suppressed; many of the leaders fled to the US. The American policy was to largely ignore the rebellions, and indeed ignore Canada generally in favor of westward expansion of the American Frontier.
Alabama claims.
At the end of the American Civil War in 1865, Americans were angry at British support for the Confederacy. One result was toleration of Fenian efforts to use the U.S. as a base to attack Canada. More serious was the demand for a huge payment to cover the damages caused, on the notion that British involvement had lengthened the war. Senator Charles Sumner, the chairman of the Senate Foreign Relations Committee, originally wanted to ask for $2 billion, or alternatively the ceding of all of Canada to the United States. When American Secretary of State William H. Seward negotiated the Alaska Purchase with Russia in 1867, he intended it as the first step in a comprehensive plan to gain control of the entire northwest Pacific Coast. Seward was a firm believer in Manifest Destiny, primarily for its commercial advantages to the U.S. Seward expected British Columbia to seek annexation to the U.S. and thought Britain might accept this in exchange for the "Alabama" claims. Soon other elements endorsed annexation, Their plan was to annex British Columbia, Red River Colony (Manitoba), and Nova Scotia, in exchange for the dropping the damage claims. The idea reached a peak in the spring and summer of 1870, with American expansionists, Canadian separatists, and British anti-imperialists seemingly combining forces. The plan was dropped for multiple reasons. London continued to stall, American commercial and financial groups pressed Washington for a quick settlement of the dispute on a cash basis, growing Canadian nationalist sentiment in British Columbia called for staying inside the British Empire, Congress became preoccupied with Reconstruction, and most Americans showed little interest in territorial expansion. The "Alabama Claims" dispute went to international arbitration. In one of the first major cases of arbitration, the tribunal in 1872 supported the American claims and ordered Britain to pay $15.5 million. Britain paid and the episode ended in peaceful relations.
Dominion of Canada.
Canada became a self-governing dominion in 1867 in internal affairs while Britain controlled diplomacy and defense policy. Prior to Confederation, there was an Oregon boundary dispute in which the Americans claimed the 54th degree latitude. That issue was resolved by splitting the disputed territory; the northern half became British Columbia, and the southern half the states of Washington and Oregon. Strained relations with America continued, however, due to a series of small-scale armed incursions named the Fenian raids by Irish-American Civil War veterans across the border from 1866 to 1871 in an attempt to trade Canada for Irish independence. The American government, angry at Canadian tolerance of Confederate raiders during the American Civil War, moved very slowly to disarm the Fenians. The British government, in charge of diplomatic relations, protested cautiously, as Anglo-American relations were tense. Much of the tension was relieved as the Fenians faded away and in 1872 by the settlement of the Alabama Claims, when Britain paid the U.S. $15.5 million for war losses caused by warships built in Britain and sold to the Confederacy.
Disputes over ocean boundaries on Georges Bank and over fishing, whaling, and sealing rights in the Pacific were settled by international arbitration, setting an important precedent.
Emigration to and from the United States.
After 1850, the pace of industrialization and urbanization was much faster in the United States, drawing a wide range of immigrants from the North. By 1870, 1/6 of all the people born in Canada had moved to the United States, with the highest concentrations in New England, which was the destination of emigrants from Quebec and the Maritimes; people from Ontario moved into nearby Michigan. It was common for people to move back and forth across the border, such as seasonal lumberjacks, entrepreneurs looking for larger markets, and families looking for jobs in the textile mills that paid much higher wages than in Quebec.
The southward migration slacked off after 1890, as Canadian industry began a growth spurt. By then, the American frontier was closing, and hundreds of thousands of farmers looking for fresh land moved from the United States north into the Prairie Provinces. The net result of the flows were that in 1901 there were 128,000 American-born residents in Canada (3.5% of the Canadian population) and 1.18 million Canadian-born residents in the United States (1.6% of the U.S. population).
Alaska boundary.
A long-standing controversy was the Alaska boundary dispute, settled in favor of the United States in 1903. At issue was the exact boundary between Alaska and Canada, specifically whether Canada would have a port near the present American town of Haines that would give an all-Canadian route to the rich new Yukon goldfields. The dispute was settled by arbitration, and the British delegate voted with the Americans—to the astonishment and disgust of Canadians who suddenly realized that Britain considered its relations with the United States paramount compared to those with Canada.
1907 saw a minor controversy over USS "Nashville" sailing into the Great Lakes via Canada without Canadian permission. To head off future embarrassments, in 1909 the two sides signed the International Boundary Waters Treaty and the International Joint Commission was established to manage the Great Lakes.
Reciprocal trade with U.S..
Anti-Americanism reached a shrill peak in 1911 in Canada. The Liberal government in 1911 negotiated a Reciprocity treaty with the U.S. that would lower trade barriers. Canadian manufacturing interests were alarmed that free trade allow the bigger and more efficient American factories to take their markets. The Conservatives made it a central campaign issue in the 1911 election, warning that it would be a "sell out" to the United States with economic annexation a special danger. Conservative slogan was "No truck or trade with the Yankees", as they appealed to Canadian nationalism and nostalgia for the British Empire to win a major victory.
Canadian autonomy.
Canada demanded and received permission from London to send its own delegation to the Versailles Peace Talks in 1919, with the proviso that it sign the treaty under the British Empire. Canada subsequently took responsibility for its own foreign and military affairs in the 1920s. Its first ambassador to the United States, Vincent Massey, was named in 1927. The United States first ambassador to Canada was William Phillips. Canada became an active member of the British Commonwealth, the League of Nations, and the World Court, none of which included the U.S.
Relations with the United States were cordial until 1930, when Canada vehemently protested the new Smoot–Hawley Tariff Act by which the U.S. raised tariffs (taxes) on products imported from Canada. Canada retaliated with higher tariffs of its own against American products, and moved toward more trade within the British Commonwealth. U.S.-Canadian trade fell 75% as the Great Depression dragged both countries down.
Down to the 1920s the war and naval departments of both nations designed hypothetical war game scenarios with the other as an enemy. These were primarily exercises; the departments were never told to get ready for a real war. In 1921, Canada developed Defence Scheme No. 1 for an attack on American cities and for forestalling invasion by the United States until Imperial reinforcements arrived. Through the later 1920s and 1930s, the United States Army War College developed a plan for a war with the British Empire waged largely on North American territory, in War Plan Red (interestingly, American war planners had no thoughts of returning captured British territory.)
Herbert Hoover meeting in 1927 with British Ambassador Sir Esme Howard agreed on the "absurdity of contemplating the possibility of war between the United States and the British Empire."
In 1938, as war clouds gathered in Europe, U.S. President Franklin Roosevelt gave a public speech at Queens University in Kingston, Ontario, declaring that the United States would not sit idly by if another power tried to dominate Canada. Diplomats saw it as a clear warning to Germany not to attack Canada.
World War II.
The two nations cooperated closely in World War II, as both nations saw new levels of prosperity and a determination to defeat the Axis powers. Prime Minister William Lyon Mackenzie King and President Franklin D. Roosevelt were determined not to repeat the mistakes of their predecessors. They met in August 1940 at Ogdensburg, issuing a declaration calling for close cooperation, and formed the Permanent Joint Board on Defense (PJBD).
King sought to raise Canada's international visibility by hosting the August 1943 Quadrant conference in Quebec on military and political strategy; he was a gracious host but was kept out of the important meetings by Winston Churchill and Roosevelt.
Canada allowed the construction of the Alaska Highway and participated in the building of the atomic bomb. 49,000 Americans joined the RCAF (Canadian) or RAF (British) air forces through the Clayton Knight Committee, which had Roosevelt's permission to recruit in the U.S. in 1940-42.
American attempts in the mid-1930s to integrate British Columbia into a united West Coast military command had aroused Canadian opposition. Fearing a Japanese invasion of Canada's vulnerable coast, American officials urged the creation of a united military command for an eastern Pacific Ocean theater of war. Canadian leaders feared American imperialism and the loss of autonomy more than a Japanese invasion. In 1941, Canadians successfully argued within the PJBD for mutual cooperation rather than unified command for the West Coast.
Newfoundland.
The United States built large military bases in Newfoundland, at the time, a British crown colony. The American involvement ended the depression and brought new prosperity; Newfoundland's business community sought closer ties with the United States as expressed by the Economic Union Party. Ottawa took notice and wanted Newfoundland to join Canada, which it did after hotly contested referenda. There was little demand in the United States for the acquisition of Newfoundland, so the United States did not protest the British decision not to allow an American option on the Newfoundland referendum.
Cold War.
Following co-operation in the two World Wars, Canada and the United States lost much of their previous animosity. As Britain's influence as a global imperial power declined, Canada and the United States became extremely close partners. Canada was a close ally of the United States during the Cold War.
Nixon Shock 1971.
The United States had become Canada's largest market, and after the war the Canadian economy became dependent on smooth trade flows with the United States so much that in 1971 when the United States enacted the "Nixon Shock" economic policies (including a 10% tariff on all imports) it put the Canadian government into a panic. This led in a large part to the articulation of Prime Minister Trudeau's "Third Option" policy of diversifying Canada's trade and downgrading the importance of Canada – United States relations. In a 1972 speech in Ottawa, Nixon declared the "special relationship" between Canada and the United States dead.
1980s.
Issues in Canada–U.S. relations in the 1980s included the kidnapping of Sidney Jaffe from Canada by an American bail bondsman, U.S. testing of cruise missiles on Canadian soil, and the signing of the Canada–United States Free Trade Agreement.
Anti-Americanism.
Since the arrival of the Loyalists as refugees from the American Revolution in the 1780s, historians have identified a constant theme of Canadian fear of the United States and of "Americanization" or a cultural takeover. In the War of 1812, for example, the enthusiastic response by French militia to defend Lower Canada reflected, according to Heidler and Heidler (2004), "the fear of Americanization." Scholars have traced this attitude over time in Ontario and Quebec.
Canadian intellectuals who wrote about the U.S. in the first half of the 20th century identified America as the world center of modernity, and deplored it. Imperialists (who admired the British Empire) explained that Canadians had narrowly escaped American conquest with its rejection of tradition, its worship of "progress" and technology, and its mass culture; they explained that Canada was much better because of its commitment to orderly government and societal harmony. There were a few ardent defenders of the nation to the south, notably liberal and socialist intellectuals such as F. R. Scott and Jean-Charles Harvey (1891-1967).
Looking at television, Collins (1990) finds that it is in English Canada that fear of cultural Americanization is most powerful, for there the attractions of the U.S. are strongest. Meren (2009) argues that after 1945, the emergence of Quebec nationalism and the desire to preserve French-Canadian cultural heritage led to growing anxiety regarding American cultural imperialism and Americanization. In 2006 surveys showed that 60 percent of Quebecers had a fear of Americanization, while other surveys showed they preferred their current situation to that of the Americans in the realms of health care, quality of life as seniors, environmental quality, poverty, educational system, racism and standard of living. While agreeing that job opportunities are greater in America, 89 percent disagreed with the notion that they would rather be in the United States, and they were more likely to feel closer to English Canadians than to Americans. However, there is evidence that the elites and Quebec are much less fearful of Americanization, and much more open to economic integration than the general public.
The history has been traced in detail by a leading Canadian historian J.L. Granatstein in "Yankee Go Home: Canadians and Anti-Americanism" (1997). Current studies report the phenomenon persists. Two scholars report, "Anti-Americanism is alive and well in Canada today, strengthened by, among other things, disputes related to NAFTA, American involvement in the Middle East, and the ever-increasing Americanization of Canadian culture." Jamie Glazov writes, "More than anything else, Diefenbaker became the tragic victim of Canadian anti-Americanism, a sentiment the prime minister had fully embraced by 1962. was unable to imagine himself (or his foreign policy) without enemies." Historian J. M. Bumsted says, "In its most extreme form, Canadian suspicion of the United States has led to outbreaks of overt anti-Americanism, usually spilling over against Americans resident in Canada." John R. Wennersten writes, "But at the heart of Canadian anti-Americanism lies a cultural bitterness that takes an American expatriate unawares. Canadians fear the American media's influence on their culture and talk critically about how Americans are exporting a culture of violence in its television programming and movies." However Kim Nossal points out that the Canadian variety is much milder than anti-Americanism in some other countries. By contrast Americans show very little knowledge or interest one way or the other regarding Canadian affairs. Canadian historian Frank Underhill, quoting Canadian playwright Merrill Denison summed it up: "Americans are benevolently ignorant about Canada, whereas Canadians are malevolently informed about the United States."
According to Paul Pirie, a Canadian analyst and historian, the American Revolution was in itself essentially a "failure", and should thus be renounced by the American people, citing a variety of reasons, ranging from higher incarceration rates and less life satisfaction in the United States in comparison to other English-speaking nations, particularly Canada.
Relations between political executives.
The executive of each country is represented differently. In the United States, the president is both head of state and head of government, and his "administration" is the executive. In Canada the prime minister is head of government only, and his or her "government" or "ministry" directs the executive.
Mulroney and Reagan.
Relations between Brian Mulroney and Ronald Reagan were famously close. This relationship resulted in negotiations on a potential free trade agreement, and a treaty of acid rain causing emissions, both major policy goals of Mulroney, that would be finalized under the presidency of George H. W. Bush.
Chrétien and Clinton.
Although Jean Chrétien was wary to appearing too close to the president, personally, he and Bill Clinton were known to be golfing partners. Their governments had many small trade quarrels over magazines, softwood lumber, and so on, but on the whole were quite friendly. Both leaders had run on reforming or abolishing NAFTA, but the agreement went ahead with the addition of environmental and labor side agreements. Crucially, the Clinton administration lent rhetorical support to Canadian unity during the 1995 referendum in Quebec on independence from Canada.
Bush and Chrétien.
Relations between Chrétien and George W. Bush were strained throughout their overlapping times in office. Jean Chrétien publicly mused that U.S. foreign policy might be part of the "root causes" of terrorism shortly after the September 11 attacks. Some Americans did not appreciate his "smug moralism", and Chrétien's public refusal to support the 2003 Iraq war was met with chagrin in the United States, especially among conservatives.
Bush and Harper.
Stephen Harper and George W. Bush were thought to share warm personal relations and also close ties between their administrations. Because Bush was so unpopular in Canada, however, this was rarely emphasized by the Harper government.
Shortly after being congratulated by Bush for his victory in February 2006, Harper rebuked U.S. ambassador to Canada David Wilkins for criticizing the Conservatives' plans to assert Canada's sovereignty over the Arctic Ocean waters with military force.
Harper and Obama.
President Barack Obama's first international trip was to Canada on February 19, 2009. Aside from Canadian lobbying against "Buy American" provisions in the U.S. stimulus package, relations between the two administrations have been smooth.
They have also held friendly bets on hockey games during the Winter Olympic season. In the 2010 Winter Olympics hosted by Canada in Vancouver, Canada defeated the US in both gold medal matches, allowing Stephen Harper to receive a case of Molson Canadian beer from Barack Obama, in reverse, if Canada lost, Harper would provide a case of Yuengling beer to Obama. During the 2014 Winter Olympics, alongside US Secretary of State John Kerry & Minister of Foreign Affairs John Baird, Stephen Harper was given a case of Samuel Adams beer by Obama for the Canadian gold medal victory over the US in women's hockey, and the semi-final victory over the US in men's hockey.
Canada-United States Regulatory Cooperation Council (RCC) (2011).
On February 4, 2011, Harper and Obama issued a "Declaration on a Shared Vision for Perimeter Security and Economic Competitiveness" and announced the creation of the Canada-United States Regulatory Cooperation Council (RCC) "to increase regulatory transparency and coordination between the two countries."
Health Canada and the United States Food and Drug Administration (FDA) under the RCC mandate, undertook the "first of its kind" initiative by selecting "as its first area of alignment common cold indications for certain over-the-counter antihistamine ingredients (GC 2013-01-10)."
Critics of the plan have compared Regulatory Cooperation Council (RCC) to the SPP, without Mexico. On Wednesday, December 7, Harper flew to Washington to meet with Obama and sign an agreement to implement the joint action plans that had been developed since the initial meeting in February. The plans called on both countries to spend more on border infrastructure, share more information on people who cross the border, and acknowledge more of each other's safety and security inspection on third-country traffic. An editorial in "The Globe and Mail" praised the agreement for giving Canada the ability to track whether failed refugee claimants have left Canada via the U.S. and for eliminating "duplicated baggage screenings on connecting flights". The agreement is not a legally-binding treaty, and relies on the political will and ability of the executives of both governments to implement the terms of the agreement. These types of executive agreements are routine—on both sides of the Canada-U.S. border.
Military and security.
The Canadian military, like forces of other NATO countries, fought alongside the United States in most major conflicts since World War II, including the Korean War, the Gulf War, the Kosovo War, and most recently the war in Afghanistan. The main exceptions to this were the Canadian government's opposition to the Vietnam War and the Iraq War, which caused some brief diplomatic tensions. Despite these issues, military relations have remained close.
American defense arrangements with Canada are more extensive than with any other country. The Permanent Joint Board of Defense, established in 1940, provides policy-level consultation on bilateral defense matters. The United States and Canada share North Atlantic Treaty Organization (NATO) mutual security commitments. In addition, American and Canadian military forces have cooperated since 1958 on continental air defense within the framework of the North American Aerospace Defense Command (NORAD). Canadian forces have provided indirect support for the American invasion of Iraq that began in 2003. Moreover, interoperability with the American armed forces has been a guiding principle of Canadian military force structuring and doctrine since the end of the Cold War. Canadian navy frigates, for instance, integrate seamlessly into American carrier battle groups.
In commemoration of the 200th Anniversary of the War of 1812 ambassadors from Canada and the US, and naval officers from both countries gathered at the Pritzker Military Library on August 17, 2012, for a panel discussion on Canada-US relations with emphasis on national security-related matters. Also as part of the commemoration, the navies of both countries sailed together throughout the Great Lakes region.
War in Afghanistan.
Canada's elite JTF2 unit joined American special forces in Afghanistan shortly after the al-Qaida attacks on September 11, 2001. Canadian forces joined the multinational coalition in Operation Anaconda in January 2002. On April 18, 2002, an American pilot bombed Canadian forces involved in a training exercise, killing four and wounding eight Canadians. A joint American-Canadian inquiry determined the cause of the incident to be pilot error, in which the pilot interpreted ground fire as an attack; the pilot ignored orders that he felt were "second-guessing" his field tactical decision. Canadian forces assumed a six-month command rotation of the International Security Assistance Force in 2003; in 2005, Canadians assumed operational command of the multi-national Brigade in Kandahar, with 2,300 troops, and supervises the Provincial Reconstruction Team in Kandahar, where al-Qaida forces are most active. Canada has also deployed naval forces in the Persian Gulf since 1991 in support of the UN Gulf Multinational Interdiction Force.
The Canadian Embassy in Washington, DC maintains a public relations web site named CanadianAlly.com, which is intended "to give American citizens a better sense of the scope of Canada's role in North American and Global Security and the War on Terror".
The New Democratic Party and some recent Liberal leadership candidates have expressed opposition to Canada's expanded role in the Afghan conflict on the ground that it is inconsistent with Canada's historic role (since the Second World War) of peacekeeping operations.
2003 Invasion of Iraq.
According to contemporary polls, 71% of Canadians were opposed to the 2003 invasion of Iraq. Many Canadians, and the former Liberal Cabinet headed by Paul Martin (as well as many Americans such as Bill Clinton and Barack Obama), made a policy distinction between conflicts in Afghanistan and Iraq, unlike the Bush Doctrine, which linked these together in a "Global war on terror".
Trade.
Canada and the United States have the world's largest trading relationship, with huge quantities of goods and people flowing across the border each year. Since the 1987 Canada–United States Free Trade Agreement, there have been no tariffs on most goods passed between the two countries.
In the course of the softwood lumber dispute, the U.S. has placed tariffs on Canadian softwood lumber because of what it argues is an unfair Canadian government subsidy, a claim which Canada disputes. The dispute has cycled through several agreements and arbitration cases. Other notable disputes include the Canadian Wheat Board, and Canadian cultural "restrictions" on magazines and television (See CRTC, CBC, and National Film Board of Canada). Canadians have been criticized about such things as the ban on beef since a case of Mad Cow disease was discovered in 2003 in cows from the United States (and a few subsequent cases) and the high American agricultural subsidies. Concerns in Canada also run high over aspects of the North American Free Trade Agreement (NAFTA) such as Chapter 11.
One ongoing and complex trade issue involves the importation of cheaper prescription drugs from Canada to the United States. American drug companies—often supporters of political campaigns—have come out against the practice.
Environmental issues.
A principal instrument of this cooperation is the International Joint Commission (IJC), established as part of the Boundary Waters Treaty of 1909 to resolve differences and promote international cooperation on boundary waters. The Great Lakes Water Quality Agreement of 1972 is another historic example of joint cooperation in controlling trans-border water pollution. However, there have been some disputes. Most recently, the Devil's Lake Outlet, a project instituted by North Dakota, has angered Manitobans who fear that their water may soon become polluted as a result of this project.
Beginning in 1986 the Canadian government of Brian Mulroney began pressing the Reagan administration for an "Acid Rain Treaty" in order to do something about U.S. industrial air pollution causing acid rain in Canada. The Reagan administration was hesitant, and questioned the science behind Mulroney's claims. However, Mulroney was able to prevail. The product was the signing and ratification of the Air Quality Agreement of 1991 by the first Bush administration. Under that treaty, the two governments consult semi-annually on trans-border air pollution, which has demonstrably reduced acid rain, and they have since signed an annex to the treaty dealing with ground level ozone in 2000. Despite this, trans-border air pollution remains an issue, particularly in the Great Lakes-St. Lawrence watershed during the summer. The main source of this trans-border pollution results from coal-fired power stations, most of them located in the Midwestern United States.
As part of the negotiations to create NAFTA, Canada and the U.S. signed, along with Mexico, the North American Agreement On Environmental Cooperation which created the Commission for Environmental Cooperation which monitors environmental issues across the continent, publishing the North American Environmental Atlas as one aspect of its monitoring duties.
Currently neither of the countries' governments support the Kyoto Protocol, which set out time scheduled curbing of greenhouse gas emissions. Unlike the United States, Canada has ratified the agreement. Yet after ratification, due to internal political conflict within Canada, the Canadian government does not enforce the Kyoto Protocol, and has received criticism from environmental groups and from other governments for its climate change positions. In January 2011, the Canadian minister of the environment, Peter Kent, explicitly stated that the policy of his government with regards to greenhouse gas emissions reductions is to wait for the United States to act first, and then try to harmonize with that action - a position that has been condemned by environmentalists and Canadian nationalists, and as well as scientists and government think-tanks.
Illicit drugs.
In 2003 the American government became concerned when members of the Canadian government announced plans to decriminalize marijuana. David Murray, an assistant to U.S. Drug Czar John P. Walters, said in a CBC interview that, "We would have to respond. We would be forced to respond." However the election of the Conservative Party in early 2006 halted the liberalization of marijuana laws for the foreseeable future.
A 2007 joint report by American and Canadian officials on cross-border drug smuggling indicated that, despite their best efforts, "drug trafficking still occurs in significant quantities in both directions across the border. The principal illicit substances smuggled across our shared border are MDMA ("Ecstasy"), cocaine, and marijuana." - The report indicated that Canada was a major producer of "Ecstasy" and marijuana for the U.S. market, while the U.S. was a transit country for cocaine entering Canada.
Diplomacy.
Views of presidents and prime ministers.
Presidents and prime ministers typically make formal or informal statements that indicate the diplomatic policy of their administration. Diplomats and journalists at the time—and historians since—dissect the nuances and tone to detect the warmth or coolness of the relationship.
Territorial disputes.
These include maritime boundary disputes:
Territorial land disputes:
and disputes over the international status of the:
Arctic disputes.
A long-simmering dispute between Canada and the U.S. involves the issue of Canadian sovereignty over the Northwest Passage (the sea passages in the Arctic). Canada’s assertion that the Northwest Passage represents internal (territorial) waters has been challenged by other countries, especially the U.S., which argue that these waters constitute an international strait (international waters). Canadians were alarmed when Americans drove the reinforced oil tanker through the Northwest Passage in 1969, followed by the icebreaker Polar Sea in 1985, which actually resulted in a minor diplomatic incident. In 1970, the Canadian parliament enacted the Arctic Waters Pollution Prevention Act, which asserts Canadian regulatory control over pollution within a 100-mile zone. In response, the United States in 1970 stated, "We cannot accept the assertion of a Canadian claim that the Arctic waters are internal waters of Canada…. Such acceptance would jeopardize the freedom of navigation essential for United States naval activities worldwide." A compromise of sorts was reached in 1988, by an agreement on "Arctic Cooperation," which pledges that voyages of American icebreakers "will be undertaken with the consent of the Government of Canada." However the agreement did not alter either country's basic legal position. Paul Cellucci, the American ambassador to Canada, in 2005 suggested to Washington that it should recognize the straits as belonging to Canada. His advice was rejected and Harper took opposite positions. The U.S. opposes Harper's proposed plan to deploy military icebreakers in the Arctic to detect interlopers and assert Canadian sovereignty over those waters.
Common memberships.
Canada and the United States both hold membership in a number of multinational organizations such as:
Diplomatic missions.
Canadian missions in the United States.
Canada's chief diplomatic mission to the United States is the Canadian Embassy in Washington, D.C.. It is further supported by many Consulates located through United States of America.
The Canadian Government supports Consulates in several major U.S. cities including: Anchorage, Atlanta‡, Boston‡, Buffalo‡, Chicago‡, Dallas‡, Denver‡, Detroit‡, Houston, Los Angeles‡, Miami‡, Minneapolis‡, New York City‡, Philadelphia, Phoenix, Raleigh, Sacramento, San Diego, San Francisco/Silicon Valley‡, San Juan, and Seattle.‡
There is also a trade office located in Palo Alto.
American missions in Canada.
The United States's chief diplomatic mission to Canada is the United States Embassy in Ottawa. It is further supported by many consulates located through Canada.
The American government supports consulates in several major Canadian cities/regions including:
Calgary, Halifax, Northwest Territories‡, Nunavut‡, Montreal, Quebec City, Southwestern Ontario‡, Toronto, Vancouver, Winnipeg, and Yukon‡.

</doc>
<doc id="5211" url="http://en.wikipedia.org/wiki?curid=5211" title="Christianity">
Christianity

Christianity (from the Ancient Greek word Χριστός, "Christos", a translation of the Hebrew מָשִׁיחַ, "Māšîăḥ", meaning "the anointed one", together with the Latin suffixes "-ian" and "-itas") is a monotheistic religion based on the life and oral teachings of Jesus as presented in the New Testament. Christianity is the world's largest religion, with approximately 2.2 billion adherents, known as Christians. Most Christians believe that Jesus is the Son of God, fully divine and fully human, and the saviour of humanity whose coming was prophesied in the Old Testament. Consequently, Christians refer to Jesus as Christ or the Messiah.
The foundations of Christian theology are expressed in ecumenical creeds. These professions of faith state that Jesus suffered, died, was buried, and was resurrected from the dead in order to grant eternal life to those who believe in him and trust in him for the remission of their sins. The creeds further maintain that Jesus bodily ascended into heaven, where he reigns with God the Father. Most Christian denominations teach that Jesus will return to judge everybody, living and dead, and to grant eternal life to his followers. He is considered the model of a virtuous life. His ministry, crucifixion, and resurrection are often referred to as the "gospel", meaning "good news" (a loan translation of the ). The term "gospel" also refers to written accounts of Jesus's life and teaching, four of which the Gospels of Matthew, Mark, Luke, and John are considered canonical and included in Christian Bibles.
Christianity is an Abrahamic religion that began as a Jewish sect in the mid-1st century. Originating in the Levant region of the Middle East, it quickly spread to Syria, Mesopotamia, Asia Minor, and Egypt. It grew in size and influence over a few centuries, and by the end of the 4th century had become the official state church of the Roman Empire, replacing other forms of religion practiced under Roman rule. During the Middle Ages, most of the remainder of Europe was Christianized, and adherents were gained in the Middle East, North Africa, Ethiopia, and parts of India. Following the Age of Discovery, Christianity spread to the Americas, Australasia, sub-Saharan Africa, and the rest of the world through missionary work and colonization. Christianity has played a prominent role in the shaping of Western civilization.
Worldwide, the three largest groups of Christianity are the Catholic Church, the Eastern Orthodox Church, and the various denominations of Protestantism. The Roman Catholic and Eastern Orthodox patriarchates split from one another in the schism of the 11th century, and Protestantism came into existence during the Reformation of the 16th century, splitting from the Roman Catholic Church.
Beliefs.
Christians share a certain set of beliefs that they hold as essential to their faith, though there are many important differences of interpretation and opinion of the Bible on which Christianity is based.
Creeds.
Concise doctrinal statements or confessions of religious beliefs are known as creeds (from Latin "credo", meaning "I believe"). They began as baptismal formulae and were later expanded during the Christological controversies of the 4th and 5th centuries to become statements of faith.
Many evangelical Protestants reject creeds as definitive statements of faith, even while agreeing with some or all of the substance of the creeds. The Baptists have been non-creedal "in that they have not sought to establish binding authoritative confessions of faith on one another." Also rejecting creeds are groups with roots in the Restoration Movement, such as the Christian Church (Disciples of Christ), the Evangelical Christian Church in Canada and the Churches of Christ.
The Apostles' Creed remains the most popular statement of the articles of Christian faith which are generally acceptable to most Christian denominations that are creedal. It is widely used by a number of Christian denominations for both liturgical and catechetical purposes, most visibly by liturgical Churches of Western Christian tradition, including the Latin Church of the Catholic Church, Lutheranism, Anglicanism, and Western Orthodoxy. It is also used by Presbyterians, Methodists, and Congregationalists. This particular creed was developed between the 2nd and 9th centuries. Its central doctrines are those of the Trinity and God the Creator. Each of the doctrines found in this creed can be traced to statements current in the apostolic period. The creed was apparently used as a summary of Christian doctrine for baptismal candidates in the churches of Rome.
Its main points include:
The Nicene Creed, largely a response to Arianism, was formulated at the Councils of Nicaea and Constantinople in 325 and 381 respectively and ratified as the universal creed of Christendom by the First Council of Ephesus in 431.
The Chalcedonian Definition, or Creed of Chalcedon, developed at the Council of Chalcedon in 451, though rejected by the Oriental Orthodox Churches, taught Christ "to be acknowledged in two natures, inconfusedly, unchangeably, indivisibly, inseparably": one divine and one human, and that both natures, while perfect in themselves, are nevertheless also perfectly united into one person.
The Athanasian Creed, received in the Western Church as having the same status as the Nicene and Chalcedonian, says: "We worship one God in Trinity, and Trinity in Unity; neither confounding the Persons nor dividing the Substance."
Most Christians (Roman Catholics, Eastern Orthodox, Oriental Orthodox and Protestants alike) accept the use of creeds, and subscribe to at least one of the creeds mentioned above.
Ten Commandments.
The Ten Commandments are a set of biblical principles relating to ethics and worship which play a fundamental role in Judaism and most forms of Christianity. They include instructions to worship only God and to keep the Sabbath, and prohibitions against idolatry, blasphemy, murder, theft, and adultery. Different groups follow slightly different traditions for interpreting and numbering them. According to the synoptic gospels, Christ generalised the law into two underlying principles; The first is "Hear, O Israel: The Lord our God, the Lord is one; and you shall love the Lord your God with all your heart, and with all your soul, and with all your mind, and with all your strength." While the second is "You shall love your neighbor as yourself."
These are quotes from and . Barnes' Notes on the New Testament comments on these verses saying: "These comprehend the substance of what Moses in the law, and what the prophets have spoken. What they have said has been to endeavour to win men to the love of God and each other. Love to God and man comprehends the whole religion; and to produce this has been the design of Moses, the prophets, the Saviour, and the apostles."
Jesus Christ.
The central tenet of Christianity is the belief in Jesus as the Son of God and the Messiah (Christ). Christians believe that Jesus, as the Messiah, was anointed by God as savior of humanity, and hold that Jesus' coming was the fulfillment of messianic prophecies of the Old Testament. The Christian concept of the Messiah differs significantly from the contemporary Jewish concept. The core Christian belief is that through belief in and acceptance of the death and resurrection of Jesus, sinful humans can be reconciled to God and thereby are offered salvation and the promise of eternal life.
While there have been many theological disputes over the nature of Jesus over the earliest centuries of Christian history, Christians generally believe that Jesus is God incarnate and "true God and true man" (or both fully divine and fully human). Jesus, having become fully human, suffered the pains and temptations of a mortal man, but did not sin. As fully God, he rose to life again. According to the Bible, "God raised him from the dead", he ascended to heaven, is "seated at the right hand of the Father" and will ultimately return to fulfill the rest of Messianic prophecy such as the Resurrection of the dead, the Last Judgment and final establishment of the Kingdom of God.
According to the canonical gospels of Matthew and Luke, Jesus was conceived by the Holy Spirit and born from the Virgin Mary. Little of Jesus' childhood is recorded in the canonical Gospels, however infancy Gospels were popular in antiquity. In comparison, his adulthood, especially the week before his death, is well documented in the Gospels contained within the New Testament, because that part of his life was believed to be most important. The Biblical accounts of Jesus' ministry include: his baptism, miracles, preaching, teaching, and deeds.
Death and resurrection.
Christians consider the resurrection of Jesus to be the cornerstone of their faith (see 1 Corinthians 15) and the most important event in history. Among Christian beliefs, the death and resurrection of Jesus are two core events on which much of Christian doctrine and theology is based. According to the New Testament Jesus was crucified, died a physical death, was buried within a tomb, and rose from the dead three days later. 
The New Testament mentions several resurrection appearances of Jesus on different occasions to his twelve apostles and disciples, including "more than five hundred brethren at once", before Jesus' Ascension to heaven. Jesus' death and resurrection are commemorated by Christians in all worship services, with special emphasis during Holy Week which includes Good Friday and Easter Sunday.
The death and resurrection of Jesus are usually considered the most important events in Christian theology, partly because they demonstrate that Jesus has power over life and death and therefore has the authority and power to give people eternal life.
Christian churches accept and teach the New Testament account of the resurrection of Jesus with very few exceptions. Some modern scholars use the belief of Jesus' followers in the resurrection as a point of departure for establishing the continuity of the historical Jesus and the proclamation of the early church. Some liberal Christians do not accept a literal bodily resurrection, seeing the story as richly symbolic and spiritually nourishing myth. Arguments over death and resurrection claims occur at many religious debates and interfaith dialogues. Paul the Apostle, an early Christian convert and missionary, wrote, "If Christ was not raised, then all our preaching is useless, and your trust in God is useless." 
Salvation.
Paul the Apostle, like Jews and Roman pagans of his time, believed that sacrifice can bring about new kinship ties, purity, and eternal life. For Paul the necessary sacrifice was the death of Jesus: Gentiles who are "Christ's" are, like Israel, descendants of Abraham and "heirs according to the promise". The God who raised Jesus from the dead would also give new life to the "mortal bodies" of Gentile Christians, who had become with Israel the "children of God" and were therefore no longer "in the flesh". 
Modern Christian churches tend to be much more concerned with how humanity can be saved from a universal condition of sin and death than the question of how both Jews and Gentiles can be in God's family. According to both Catholic and Protestant doctrine, salvation comes by Jesus' substitutionary death and resurrection. The Catholic Church teaches that salvation does not occur without faithfulness on the part of Christians; converts must live in accordance with principles of love and ordinarily must be baptized. Martin Luther taught that baptism was necessary for salvation, but modern Lutherans and other Protestants tend to teach that salvation is a gift that comes to an individual by God's grace, sometimes defined as "unmerited favor", even apart from baptism.
Christians differ in their views on the extent to which individuals' salvation is pre-ordained by God. Reformed theology places distinctive emphasis on grace by teaching that individuals are completely incapable of self-redemption, but that sanctifying grace is irresistible. In contrast Catholics, Orthodox Christians and Arminian Protestants believe that the exercise of free will is necessary to have faith in Jesus.
Trinity.
"Trinity" refers to the teaching that the one God comprises three distinct, eternally co-existing persons; the "Father", the "Son" (incarnate in Jesus Christ), and the "Holy Spirit". Together, these three persons are sometimes called the Godhead, although there is no single term in use in Scripture to denote the unified Godhead. In the words of the Athanasian Creed, an early statement of Christian belief, "the Father is God, the Son is God, and the Holy Spirit is God, and yet there are not three Gods but one God". They are distinct from another: the Father has no source, the Son is begotten of the Father, and the Spirit proceeds from the Father. Though distinct, the three persons cannot be divided from one another in being or in operation.
The Trinity is an essential doctrine of mainstream Christianity. "Father, Son and Holy Spirit" represents both the immanence and transcendence of God. God is believed to be infinite and God's presence may be perceived through the actions of Jesus Christ and the Holy Spirit.
According to this doctrine, God is not divided in the sense that each person has a third of the whole; rather, each person is considered to be fully God (see Perichoresis). The distinction lies in their relations, the Father being unbegotten; the Son being begotten of the Father; and the Holy Spirit proceeding from the Father and (in Western Christian theology) from the Son. Regardless of this apparent difference, the three 'persons' are each eternal and omnipotent.
The word "trias", from which "trinity" is derived, is first seen in the works of Theophilus of Antioch. He wrote of "the Trinity of God (the Father), His Word (the Son) and His Wisdom (Holy Spirit)". The term may have been in use before this time. Afterwards it appears in Tertullian. In the following century the word was in general use. It is found in many passages of Origen.
Trinitarians.
"Trinitarianism" denotes those Christians who believe in the concept of the "Trinity". Almost all Christian denominations and Churches hold Trinitarian beliefs. Although the words "Trinity" and "Triune" do not appear in the Bible, theologians beginning in the 3rd century developed the term and concept to facilitate comprehension of the New Testament teachings of God as Father, God as Jesus the Son, and God as the Holy Spirit. Since that time, Christian theologians have been careful to emphasize that Trinity does not imply three gods, nor that each member of the Trinity is one-third of an infinite God; Trinity is defined as one God in three Persons.
Nontrinitarians.
"Nontrinitarianism" refers to theology that rejects the doctrine of the Trinity. Various nontrinitarian views, such as adoptionism or modalism, existed in early Christianity, leading to the disputes about Christology. Nontrinitarianism later appeared again in the Gnosticism of the Cathars in the 11th through 13th centuries, and by groups with Unitarian theology in the Protestant Reformation of the 16th century, and in the Age of Enlightenment of the 18th century, and in some groups arising during the Second Great Awakening of the 19th century.
Scriptures.
Christianity, like other religions, has adherents whose beliefs and biblical interpretations vary. Christianity regards the biblical canon, the Old Testament and the New Testament, as the inspired word of God. The traditional view of inspiration is that God worked through human authors so that what they produced was what God wished to communicate. The Greek word referring to inspiration in is "Theopneustos", which literally means "God-breathed".
Some believe that divine inspiration makes our present Bibles inerrant. Others claim inerrancy for the Bible in its original manuscripts, although none of those are extant. Still others maintain that only a particular translation is inerrant, such as the King James Version. Another view closely related is Biblical infallibility or limited inerrancy, which affirms that the Bible is free of error as a guide to salvation, but may include errors on matters such as history, geography or science.
The books of the Bible accepted among the Orthodox, Catholic and Protestant churches vary somewhat, with Jews accepting only the Hebrew Bible as canonical; there is however substantial overlap. These variations are a reflection of the range of traditions, and of the councils that have convened on the subject. Every version of the Old Testament always includes the books of the Tanakh, the canon of the Hebrew Bible. The Catholic and Orthodox canons, in addition to the Tanakh, also include the Deuterocanonical Books as part of the Old Testament. These books appear in the Septuagint, but are regarded by Protestants to be apocryphal. However, they are considered to be important historical documents which help to inform the understanding of words, grammar and syntax used in the historical period of their conception. Some versions of the Bible include a separate Apocrypha section between the Old Testament and the New Testament. The New Testament, originally written in Koine Greek, contains 27 books which are agreed upon by all churches.
Modern scholarship has raised many issues with the Bible. While the Authorized King James Version is held to by many because of its striking English prose, in fact it was translated from the Erasmus Greek Bible which in turn "was based on a single 12th Century manuscript that is one of the worst manuscripts we have available to us". Much scholarship in the past several hundred years has gone into comparing different manuscripts in order to reconstruct the original text. Another issue is that several books are considered to be forgeries. The injunction that women "be silent and submissive" in 1 Timothy 12 is thought by many to be a forgery by a follower of Paul, a similar phrase in 1 Corinthians 14, which is thought to be by Paul, appears in different places in different manuscripts and is thought to originally be a margin note by a copyist. Other verses in 1 Corinthians, such as 1 Corinthians 11:2-16 where women are instructed to wear a covering over their hair "when they pray or prophesies", contradict this verse.
A final issue with the Bible is the way in which books were selected for inclusion in the New Testament. Other Gospels have now been recovered, such as those found near Nag Hammadi in 1945, and while some of these texts are quite different from what Christians have been used to, it should be understood that some of this newly recovered Gospel material is quite possibly contemporaneous with, or even earlier than, the New Testament Gospels. The core of the Gospel of Thomas, in particular, may date from as early as 50 AD, and if so would provide an insight into the earliest gospel texts that underlie the canonical Gospels, texts that are mentioned in Luke 1:1-2. The Gospel of Thomas contains much that is familiar from the canonical Gospels – verse 113, for example (“The Father’s Kingdom is spread out upon the earth, but people do not see it”), is reminiscent of Luke 17:20-21 – and the Gospel of John, with a terminology and approach that is suggestive of what was later termed "Gnosticism", has recently been seen as a possible response to the Gospel of Thomas, a text that is commonly labelled "proto-Gnostic". Scholarship, then, is currently exploring the relationship in the Early Church between mystical speculation and experience on the one hand and the search for church order on the other, by analyzing new-found texts, by subjecting canonical texts to further scrutiny, and by an examination of the passage of New Testament texts to canonical status.
Catholic and Orthodox interpretations.
In antiquity, two schools of exegesis developed in Alexandria and Antioch. Alexandrine interpretation, exemplified by Origen, tended to read Scripture allegorically, while Antiochene interpretation adhered to the literal sense, holding that other meanings (called "theoria") could only be accepted if based on the literal meaning.
Catholic theology distinguishes two senses of scripture: the literal and the spiritual.
The "literal" sense of understanding scripture is the meaning conveyed by the words of Scripture. The "spiritual" sense is further subdivided into:
Regarding exegesis, following the rules of sound interpretation, Catholic theology holds:
Protestant interpretation.
Some Protestant interpreters make use of typology.
Eschatology.
The end of things, whether the end of an individual life, the end of the age, or the end of the world, broadly speaking is Christian eschatology; the study of the destiny of humans as it is revealed in the Bible. The major issues in Christian eschatology are the Tribulation, death and the afterlife, the Rapture, the Second Coming of Jesus, Resurrection of the Dead, Heaven and Hell, Millennialism, the Last Judgment, the end of the world, and the New Heavens and New Earth.
Christians believe that the second coming of Christ will occur at the end of time after a period of severe persecution (the Great Tribulation). All who have died will be resurrected bodily from the dead for the Last Judgment. Jesus will fully establish the Kingdom of God in fulfillment of scriptural prophecies.
Death and afterlife.
Most Christians believe that human beings experience divine judgment and are rewarded either with eternal life or eternal damnation. This includes the general judgement at the resurrection of the dead as well as the belief (held by Roman Catholics, Orthodox and most Protestants) in a judgment particular to the individual soul upon physical death.
In Roman Catholicism, those who die in a state of grace, i.e., without any mortal sin separating them from God, but are still imperfectly purified from the effects of sin, undergo purification through the intermediate state of purgatory to achieve the holiness necessary for entrance into God's presence. Those who have attained this goal are called "saints" (Latin "sanctus", "holy").
Some Christian groups, such as Seventh-day Adventists, hold to mortalism, the belief that the human soul is not naturally immortal, and is unconscious during the intermediate state between bodily death and resurrection. These Christians also hold to Annihilationism, the belief that subsequent to the final judgement, the wicked will cease to exist rather than suffer everlasting torment. Jehovah's Witnesses hold to a similar view.
Worship.
Justin Martyr described 2nd-century Christian liturgy in his "First Apology" (c. 150) to Emperor Antoninus Pius, and his description remains relevant to the basic structure of Christian liturgical worship:
Thus, as Justin described, Christians assemble for communal worship on Sunday, the day of the resurrection, though other liturgical practices often occur outside this setting. Scripture readings are drawn from the Old and New Testaments, but especially the gospel accounts. Often these are arranged on an annual cycle, using a book called a lectionary. Instruction is given based on these readings, called a sermon, or homily. There are a variety of congregational prayers, including thanksgiving, confession, and intercession, which occur throughout the service and take a variety of forms including recited, responsive, silent, or sung. The Lord's Prayer, or Our Father, is regularly prayed.
Some groups depart from this traditional liturgical structure. A division is often made between "High" church services, characterized by greater solemnity and ritual, and "Low" services, but even within these two categories there is great diversity in forms of worship. Seventh-day Adventists meet on Saturday, while others do not meet on a weekly basis. Charismatic or Pentecostal congregations may spontaneously feel led by the Holy Spirit to action rather than follow a formal order of service, including spontaneous prayer. Quakers sit quietly until moved by the Holy Spirit to speak.
Some Evangelical services resemble concerts with rock and pop music, dancing, and use of multimedia. For groups which do not recognize a priesthood distinct from ordinary believers the services are generally led by a minister, preacher, or pastor. Still others may lack any formal leaders, either in principle or by local necessity. Some churches use only a cappella music, either on principle (for example, many Churches of Christ object to the use of instruments in worship) or by tradition (as in Orthodoxy).
Nearly all forms of churchmanship celebrate the Eucharist (Holy Communion), which consists of a consecrated meal. It is reenacted in accordance with Jesus' instruction at the Last Supper that his followers do in remembrance of him as when he gave his disciples bread, saying, "This is my body", and gave them wine saying, "This is my blood". Some Christian denominations practice closed communion. They offer communion to those who are already united in that denomination or sometimes individual church. Catholics restrict participation to their members who are not in a state of mortal sin. Most other churches practice open communion since they view communion as a means to unity, rather than an end, and invite all believing Christians to participate.
Worship can be varied for special events like baptisms or weddings in the service or significant feast days. In the early church, Christians and those yet to complete initiation would separate for the Eucharistic part of the worship. In many churches today, adults and children will separate for all or some of the service to receive age-appropriate teaching. Such children's worship is often called Sunday school or Sabbath school (Sunday schools are often held before rather than during services).
Sacraments.
In Christian belief and practice, a "sacrament" is a rite, instituted by Christ, that mediates grace, constituting a sacred mystery. The term is derived from the Latin word "sacramentum", which was used to translate the Greek word for "mystery". Views concerning both what rites are sacramental, and what it means for an act to be a sacrament vary among Christian denominations and traditions.
The most conventional functional definition of a sacrament is that it is an outward sign, instituted by Christ, that conveys an inward, spiritual grace through Christ. The two most widely accepted sacraments are Baptism and the Eucharist (or Holy Communion), however, the majority of Christians also recognize five additional sacraments: Confirmation (Chrismation in the Orthodox tradition), Holy Orders, Confession, Anointing of the Sick, and Matrimony.
Taken together, these are the Seven Sacraments as recognised by churches in the High church tradition—notably Roman Catholic, Eastern Orthodox, Oriental Orthodox, Independent Catholic, Old Catholic most Anglicans, and some Lutherans. Most other denominations and traditions typically affirm only Baptism and Eucharist as sacraments, while some Protestant groups, such as the Quakers, reject sacramental theology. Most Protestant Christian denominations who believe these rites do not communicate grace prefer to call them "ordinances".
Liturgical calendar.
Roman Catholics, Anglicans, Eastern Christians, and traditional Protestant communities frame worship around a liturgical calendar. This includes holy days, such as solemnities which commemorate an event in the life of Jesus or the saints, periods of fasting such as Lent, and other pious events such as memoria or lesser festivals commemorating saints. Christian groups that do not follow a liturgical tradition often retain certain celebrations, such as Christmas, Easter and Pentecost. A few churches make no use of a liturgical calendar.
Symbols.
Christianity has not generally practised aniconism, or the avoidance or prohibition of types of images, even if the early Jewish Christians sects, as well as some modern denominations, preferred to some extent not to use figures in their symbols, by invoking the Decalogue's prohibition of idolatry.
The cross, which is today one of the most widely recognised symbols in the world, was used as a Christian symbol from the earliest times. Tertullian, in his book "De Corona", tells how it was already a tradition for Christians to trace repeatedly on their foreheads the sign of the cross. Although the cross was known to the early Christians, the crucifix did not appear in use until the 5th century.
Among the symbols employed by the primitive Christians, that of the fish or Ichthys seems to have ranked first in importance. From monumental sources such as tombs it is known that the symbolic fish was familiar to Christians from the earliest times. The fish was depicted as a Christian symbol in the first decades of the 2nd century. Its popularity among Christians was due principally, it would seem, to the famous acrostic consisting of the initial letters of five Greek words forming the word for fish (Ichthys), which words briefly but clearly described the character of Christ and the claim to worship of believers: "Iesous Christos Theou Yios Soter" (Ίησοῦς Χριστός, Θεοῦ Υἱός, Σωτήρ), meaning, "Jesus Christ, Son of God, Saviour".
Other major Christian symbols include the chi-rho monogram, the dove (symbolic of the Holy Spirit), the sacrificial lamb (symbolic of Christ's sacrifice), the vine (symbolising the necessary connectedness of the Christian with Christ) and many others. These all derive from writings found in the New Testament.
Baptism.
Baptism is the ritual act, with the use of water, by which a person is admitted to membership of the Church. Beliefs on baptism vary among denominations. Differences occur firstly, on whether the act has any spiritual significance, some churches hold to the doctrine of Baptismal Regeneration, which affirms that baptism creates or strengthens a person's faith, and is intimately linked to salvation, this view is held by Catholic and Eastern Orthodox churches as well as Lutherans and Anglicans, while others simply acknowledge it as a purely symbolic act, an external public declaration of the inward change which has taken place in the person. Secondly, there are differences of opinion on the methodology of the act. These methods being: Baptism by Immersion; if immersion is total, Baptism by Submersion; and Baptism by Affusion (pouring) and Baptism by Aspersion (sprinkling). Those who hold the first view may also adhere to the tradition of Infant Baptism.
Prayer.
Jesus' teaching on prayer in the Sermon on the Mount displays a distinct lack of interest in the external aspects of prayer. A concern with the techniques of prayer is condemned as 'pagan', and instead a simple trust in God's fatherly goodness is encouraged. Elsewhere in the New Testament this same freedom of access to God is also emphasized. This confident position should be understood in light of Christian belief in the unique relationship between the believer and Christ through the indwelling of the Holy Spirit.
In subsequent Christian traditions, certain physical gestures are emphasized, including medieval gestures such as genuflection or making the sign of the cross. Kneeling, bowing and prostrations (see also poklon) are often practiced in more traditional branches of Christianity. Frequently in Western Christianity the hands are placed palms together and forward as in the feudal commendation ceremony. At other times the older orans posture may be used, with palms up and elbows in.
"Intercessory prayer" is prayer offered for the benefit of other people. There are many intercessory prayers recorded in the Bible, including prayers of the Apostle Peter on behalf of sick persons and by prophets of the Old Testament in favor of other people. In the New Testament book of James no distinction is made between the intercessory prayer offered by ordinary believers and the prominent Old Testament prophet Elijah. The effectiveness of prayer in Christianity derives from the power of God rather than the status of the one praying.
The ancient church, in both Eastern Christianity and Western Christianity, developed a tradition of asking for the intercession of (deceased) saints, and this remains the practice of most Eastern Orthodox, Oriental Orthodox, Roman Catholic, and some Anglican churches. Churches of the Protestant Reformation however rejected prayer to the saints, largely on the basis of the sole mediatorship of Christ. The reformer Huldrych Zwingli admitted that he had offered prayers to the saints until his reading of the Bible convinced him that this was idolatrous.
According to the Catechism of the Catholic Church: "Prayer is the raising of one's mind and heart to God or the requesting of good things from God." The Book of Common Prayer in the Anglican tradition is a guide which provides a set order for church services, containing set prayers, scripture readings, and hymns or sung Psalms.
History.
Early Church and Christological Councils.
Christianity began as a Jewish sect in the Levant of the middle east in the mid-1st century. It had a number of influences, including other religions. Christian ideas such as "angels, the end of the world, a final judgment, the resurrection, and heaven and hell received form and substance from ... Zoroastrian beliefs". Its earliest development took place under the leadership of the Twelve Apostles, particularly Saint Peter and Paul the Apostle, followed by the early bishops, whom Christians consider the successors of the Apostles.
According to the scriptures, Christians were from the beginning subject to persecution by some Jewish religious authorities, who disagreed with the apostles' teachings (See Split of early Christianity and Judaism). This involved punishments, including death, for Christians such as Stephen and James, son of Zebedee. Larger-scale persecutions followed at the hands of the authorities of the Roman Empire, first in the year 64, when Emperor Nero blamed them for the Great Fire of Rome. According to Church tradition, it was under Nero's persecution that early Church leaders Peter and Paul of Tarsus were each martyred in Rome.
Further widespread persecutions of the Church occurred under nine subsequent Roman emperors, most intensely under Decius and Diocletian. From the year 150, Christian teachers began to produce theological and apologetic works aimed at defending the faith. These authors are known as the Church Fathers, and study of them is called Patristics. Notable early Fathers include Ignatius of Antioch, Polycarp, Justin Martyr, Irenaeus, Tertullian, Clement of Alexandria, and Origen. However, Armenia is considered the first nation to accept Christianity in 301 AD.
End of Roman persecution under Emperor Constantine (313 AD).
State persecution ceased in the 4th century, when Constantine I issued an edict of toleration in 313. On 27 February 380, Emperor Theodosius I enacted a law establishing Nicene Christianity as the state church of the Roman Empire. From at least the 4th century, Christianity has played a prominent role in the shaping of Western civilization.
Constantine was also instrumental in the convocation of the First Council of Nicaea in 325, which sought to address the Arian heresy and formulated the Nicene Creed, which is still used by the Catholic Church, Eastern Orthodoxy, Anglican Communion, and many Protestant churches. Nicaea was the first of a series of Ecumenical (worldwide) Councils which formally defined critical elements of the theology of the Church, notably concerning Christology. The Assyrian Church of the East did not accept the third and following Ecumenical Councils, and are still separate today.
The presence of Christianity in Africa began in the middle of the 1st century in Egypt, and by the end of the 2nd century in the region around Carthage. Mark the Evangelist started the Orthodox Church of Alexandria in about 43 AD. Important Africans who influenced the early development of Christianity includes Tertullian, Clement of Alexandria, Origen of Alexandria, Cyprian, Athanasius and Augustine of Hippo. The later rise of Islam in North Africa reduced the size and numbers of Christian congregations, leaving only the Coptic Church in Egypt, the Ethiopian Orthodox Tewahedo Church in the Horn of Africa, and the Nubian Church in the Sudan (Nobatia, Makuria, and Alodia).
In terms of prosperity and cultural life, the Byzantine Empire was one of the peaks in Christian history and Orthodox civilization, and Constantinople remained the leading city of the Christian world in size, wealth, and culture. There was a renewed interest in classical Greek philosophy, as well as an increase in literary output in vernacular Greek. Byzantine art and literature held a pre-eminent place in Europe, and the cultural impact of Byzantine art on the west during this period was enormous and of long lasting significance.
Early Middle Ages.
With the decline and fall of the Roman Empire in the west, the papacy became a political player, first visible in Pope Leo's diplomatic dealings with Huns and Vandals. The church also entered into a long period of missionary activity and expansion among the various tribes. Whilst arianists instituted the death penalty for practicing pagans (see Massacre of Verden as example), Catholicism also spread among the Germanic peoples, the Celtic and Slavic peoples, the Hungarians, and the Baltic peoples. Christianity has been an important part of the shaping of Western civilization, at least since the 4th century.
Around 500, St. Benedict set out his Monastic Rule, establishing a system of regulations for the foundation and running of monasteries. Monasticism became a powerful force throughout Europe, and gave rise to many early centers of learning, most famously in Ireland, Scotland and Gaul, contributing to the Carolingian Renaissance of the 9th century.
In the 7th century Muslims conquered Syria (including Jerusalem), North Africa and Spain. Part of the Muslims' success was due to the exhaustion of the Byzantine empire in its decades long conflict with Persia. Beginning in the 8th century, with the rise of Carolingian leaders, the papacy began to find greater political support in the Frankish Kingdom.
The Middle Ages brought about major changes within the church. Pope Gregory the Great dramatically reformed ecclesiastical structure and administration. In the early 8th century, iconoclasm became a divisive issue, when it was sponsored by the Byzantine emperors. The Second Ecumenical Council of Nicaea (787) finally pronounced in favor of icons. In the early 10th century, Western Christian monasticism was further rejuvenated through the leadership of the great Benedictine monastery of Cluny.
Hebraism, like Hellenism, has been an all-important factor in the development of Western Civilization; Judaism, as the precursor of Christianity, has indirectly had had much to do with shaping the ideals and morality of western nations since the Christian era.
High and Late Middle Ages.
In the west, from the 11th century onward, older cathedral schools developed into universities (see University of Oxford, University of Paris, and University of Bologna.) The traditional medieval universities — evolved from Catholic and Protestant church schools — then established specialized academic structures for properly educating greater numbers of students as professionals. Prof. Walter Rüegg, editor of "A History of the University in Europe", reports that universities then only trained students to become clerics, lawyers, civil servants, and physicians.
Originally teaching only theology, universities steadily added subjects including medicine, philosophy and law, becoming the direct ancestors of modern institutions of learning.
The university is generally regarded as an institution that has its origin in the Medieval Christian setting. Prior to the establishment of universities, European higher education took place for hundreds of years in Christian cathedral schools or monastic schools ("Scholae monasticae"), in which monks and nuns taught classes; evidence of these immediate forerunners of the later university at many places dates back to the 6th century AD.
Accompanying the rise of the "new towns" throughout Europe, mendicant orders were founded, bringing the consecrated religious life out of the monastery and into the new urban setting. The two principal mendicant movements were the Franciscans and the Dominicans founded by St. Francis and St. Dominic respectively. Both orders made significant contributions to the development of the great universities of Europe. Another new order were the Cistercians, whose large isolated monasteries spearheaded the settlement of former wilderness areas. In this period church building and ecclesiastical architecture reached new heights, culminating in the orders of Romanesque and Gothic architecture and the building of the great European cathedrals.
From 1095 under the pontificate of Urban II, the Crusades were launched. These were a series of military campaigns in the Holy Land and elsewhere, initiated in response to pleas from the Byzantine Emperor Alexios I for aid against Turkish expansion. The Crusades ultimately failed to stifle Islamic aggression and even contributed to Christian enmity with the sacking of Constantinople during the Fourth Crusade.
Over a period stretching from the 7th to the 13th century, the Christian Church underwent gradual alienation, resulting in a schism dividing it into a so-called Latin or Western Christian branch, the Roman Catholic Church, and an Eastern, largely Greek, branch, the Orthodox Church. These two churches disagree on a number of administrative, liturgical, and doctrinal issues, most notably papal primacy of jurisdiction. The Second Council of Lyon (1274) and the Council of Florence (1439) attempted to reunite the churches, but in both cases the Eastern Orthodox refused to implement the decisions and the two principal churches remain in schism to the present day. However, the Roman Catholic Church has achieved union with various smaller eastern churches.
Beginning around 1184, following the crusade against the Cathar heresy, various institutions, broadly referred to as the Inquisition, were established with the aim of suppressing heresy and securing religious and doctrinal unity within Christianity through conversion and prosecution.
Protestant Reformation and Counter-Reformation.
The 15th-century Renaissance brought about a renewed interest in ancient and classical learning. Another major schism, the Reformation, resulted in the splintering of the Western Christendom into several Christian denominations. Martin Luther in 1517 protested against the sale of indulgences and soon moved on to deny several key points of Roman Catholic doctrine.
Other reformers like Zwingli, Calvin, Knox and Simons further criticized Roman Catholic teaching and worship. These challenges developed into the movement called Protestantism, which repudiated the primacy of the pope, the role of tradition, the seven sacraments, and other doctrines and practices. The Reformation in England began in 1534, when King Henry VIII had himself declared head of the Church of England. Beginning in 1536, the monasteries throughout England, Wales and Ireland were dissolved.
Partly in response to the Protestant Reformation, the Roman Catholic Church engaged in a substantial process of reform and renewal, known as the Counter-Reformation or Catholic Reform. The Council of Trent clarified and reasserted Roman Catholic doctrine. During the following centuries, competition between Roman Catholicism and Protestantism became deeply entangled with political struggles among European states.
Meanwhile, the discovery of America by Christopher Columbus in 1492 brought about a new wave of missionary activity. Partly from missionary zeal, but under the impetus of colonial expansion by the European powers, Christianity spread to the Americas, Oceania, East Asia, and sub-Saharan Africa.
Throughout Europe, the divides caused by the Reformation led to outbreaks of religious violence and the establishment of separate state churches in Europe. Lutheranism spread into northern, central and eastern parts of present-day Germany, Livonia and Scandinavia. Anglicanism was established in England in 1534. Calvinism and other Protestant denominations were introduced in Scotland, the Netherlands, Hungary, Switzerland and France. Ultimately, these differences led to the outbreak of conflicts in which religion played a key factor. The Thirty Years' War, the English Civil War, and the French Wars of Religion are prominent examples. These events intensified the Christian debate on persecution and toleration.
Post-Enlightenment.
In the era known as the Great Divergence, when in the West the Age of Enlightenment and the Scientific revolution brought about great societal changes, Christianity was confronted with various forms of skepticism and with certain modern political ideologies such as versions of socialism and liberalism. Events ranged from mere anti-clericalism to violent outbursts against Christianity such as the Dechristianisation during the French Revolution, the Spanish Civil War, and general hostility of Marxist movements, especially the Russian Revolution.
Especially pressing in Europe was the formation of nation states after the Napoleonic era. In all European countries, different Christian denominations found themselves in competition, to greater or lesser extents, with each other and with the state. Variables are the relative sizes of the denominations and the religious, political, and ideological orientation of the state. Urs Altermatt of the University of Fribourg, looking specifically at Catholicisms in Europe, identifies four models for the European nations. In traditionally Catholic countries such as Belgium, Spain, and to some extent Austria, religious and national communities are more or less identical. Cultural symbiosis and separation are found in Poland, Ireland, and Switzerland, all countries with competing denominations. Competition is found in Germany, the Netherlands, and again Switzerland, all countries with minority Catholic populations who to a greater or lesser extent did identify with the nation. Finally, separation between religion (again, specifically Catholicism) and the state is found to a great degree in France and Italy, countries where the state actively opposed itself to the authority of the Catholic Church.
The combined factors of the formation of nation states and ultramontanism, especially in Germany and the Netherlands but also in England (to a much lesser extent), often forced Catholic churches, organizations, and believers to choose between the national demands of the state and the authority of the Church, specifically the papacy. This conflict came to a head in the First Vatican Council, and in Germany would lead directly to the Kulturkampf, where liberals and Protestants under the leadership of Bismarck managed to severely restrict Catholic expression and organization.
Christian commitment in Europe dropped as modernity and secularism came into their own in Europe, particularly in the Czech Republic and Estonia, while religious commitments in America have been generally high in comparison to Europe. The late 20th century has shown the shift of Christian adherence to the Third World and southern hemisphere in general, with the western civilization no longer the chief standard bearer of Christianity.
Some Europeans (including diaspora), Indigenous peoples of the Americas, and natives of other continents have revived their respective peoples' historical folk religions. Approximately 7.1 to 10% of Arabs are Christians most prevalent in Egypt, Syria and Lebanon.
Demographics.
With around 2.2 billion adherents, split into 3 main branches of Catholic, Protestant and Orthodox, Christianity is the world's largest religion. The Christian share of the world's population has stood at around 33% for the last hundred years, which says that one in three persons on earth are Christians. This masks a major shift in the demographics of Christianity; large increases in the developing world (around 23,000 per day) have been accompanied by substantial declines in the developed world, mainly in Europe and North America (around 7,600 per day).
Christianity is still the predominant religion in Europe, the Americas and Southern Africa. In Asia, it is the dominant religion in Georgia, Armenia, East Timor and the Philippines. However, it is declining in many areas including the Northern and Western United States, Oceania (Australia and New Zealand), northern Europe (including Great Britain, Scandinavia and other places), France, Germany, the Canadian provinces of Ontario, British Columbia, and Quebec, and parts of Asia (especially the Middle East, South Korea, Taiwan, and Macau).
The Christian population is not decreasing in Brazil, the Southern United States and the province of Alberta, Canada, but the percentage is decreasing. In countries such as Australia and New Zealand, the Christian population are declining in both numbers and percentage. 
Despite the declining numbers in the Western World, Christianity remains the dominant religion in the Western World, where 70% are Christians, According to a 2011 survey found that 76.2% of Europeans described themselves as Christians, and about 86.0% of the Americas population consider themselves Christians, (90% in Latin America and 77.4% in North America). And about 73.36% in Oceania still practice Christianity.
However, there are many charismatic movements that have become well established over large parts of the world, especially Africa, Latin America and Asia. A leading Saudi Arabian Muslim leader Sheikh Ahmad al Qatanni reported on Aljazeera that every day 16,000 African Muslims convert to Christianity. He claimed that Islam was losing 6 million African Muslims a year to becoming Christians, including Muslims in Algeria, France, Iran, India, Morocco, Russia, and Turkey, and Central Asia. It is also reported that Christianity is popular among people of different backgrounds in India (mostly Hindus), and Malaysia, Mongolia, Nigeria, Vietnam, Singapore, Indonesia, China, Japan, and South Korea.
In most countries in the developed world, church attendance among people who continue to identify themselves as Christians has been falling over the last few decades. Some sources view this simply as part of a drift away from traditional membership institutions, while others link it to signs of a decline in belief in the importance of religion in general.
Christianity, in one form or another, is the sole state religion of the following nations: Argentina (Roman Catholic), Costa Rica (Roman Catholic), Denmark (Evangelical Lutheran), El Salvador (Roman Catholic), England (Anglican), Finland (Evangelical Lutheran & Orthodox), Georgia (Georgian Orthodox), Greece (Greek Orthodox), Iceland (Evangelical Lutheran), Liechtenstein (Roman Catholic), Malta (Roman Catholic), Monaco (Roman Catholic), and Vatican City (Roman Catholic).
There are numerous other countries, such as Cyprus, which although do not have an established church, still give official recognition to a specific Christian denomination.
Major denominations.
The three primary divisions of Christianity are Catholicism, Eastern Orthodoxy, and Protestantism. There are other Christian groups that do not fit neatly into one of these primary categories. The Nicene Creed is "accepted as authoritative by the Roman Catholic, Eastern Orthodox, Anglican, and major Protestant churches."
There is a diversity of doctrines and practices among groups calling themselves Christian. These groups are sometimes classified under denominations, though for theological reasons many groups reject this classification system. A broader distinction that is sometimes drawn is between Eastern Christianity and Western Christianity, which has its origins in the East–West Schism (Great Schism) of the 11th century.
Most Protestant traditions branch out from the Reformed tradition in some way. In addition to the Lutheran and Reformed branches of the Reformation, there is Anglicanism after the English Reformation. The Anabaptist tradition was largely ostracized by the other Protestant parties at the time, but has achieved a measure of affirmation in more recent history.
As well as these modern divisions, there were many diverse Christian communities with wildly different Christologies, eschatologies, soteriologies, and cosmologies that existed alongside the "Early Church" which is itself a projected concept to indicate which communities were "proto-orthodox", in that their views would become dominate. In many ways, the first three centuries of Christianity was significantly more diverse than the modern Church.
Catholic.
The Catholic Church comprises those particular Churches, headed by bishops, in communion with the Pope, the Bishop of Rome, as its highest authority in matters of faith, morality and Church governance. Like the Eastern Orthodox, the Roman Catholic Church through Apostolic succession traces its origins to the Christian community founded by Jesus Christ. Catholics maintain that the "one, holy, catholic and apostolic church" founded by Jesus subsists fully in the Roman Catholic Church, but also acknowledges other Christian churches and communities and works towards reconciliation among all Christians. The Catholic faith is detailed in the "Catechism of the Catholic Church".
The 2,834 sees are grouped into 23 particular rites, the largest being the Latin Church, each with distinct traditions regarding the liturgy and the administering the sacraments. With more than 1.1 billion baptized members, the Catholic Church is the largest church representing over half of all Christians and one sixth of the world's population.
Various smaller communities, such as the Old Catholic and Independent Catholic Churches, include the word "Catholic" in their title, and share much in common with Roman Catholicism but are no longer in communion with the See of Rome.
Orthodox.
Eastern Orthodoxy comprises those churches in communion with the Patriarchal Sees of the East, such as the Ecumenical Patriarch of Constantinople. Like the Roman Catholic Church, the Eastern Orthodox Church also traces its heritage to the foundation of Christianity through Apostolic succession and has an episcopal structure, though the autonomy of the individual, mostly national churches is emphasized. A number of conflicts with Western Christianity over questions of doctrine and authority culminated in the Great Schism. Eastern Orthodoxy is the second largest single denomination in Christianity, with an estimated 225–300 million adherents.
The Oriental Orthodox Churches (also called "Old Oriental Churches") are those eastern churches that recognize the first three ecumenical councils—Nicaea, Constantinople and Ephesus—but reject the dogmatic definitions of the Council of Chalcedon and instead espouse a Miaphysite christology. The Oriental Orthodox communion comprises six groups: Syriac Orthodox, Coptic Orthodox, Ethiopian Orthodox, Eritrean Orthodox, Malankara Orthodox Syrian Church (India) and Armenian Apostolic churches. These six churches, while being in communion with each other are completely independent hierarchically. These churches are generally not in communion with Eastern Orthodox Churches with whom they are in dialogue for erecting a communion.
Protestant.
In the 16th century, Martin Luther, Huldrych Zwingli, and John Calvin inaugurated what has come to be called Protestantism. Luther's primary theological heirs are known as Lutherans. Zwingli and Calvin's heirs are far broader denominationally, and are broadly referred to as the Reformed Tradition.
The oldest Protestant groups separated from the Catholic Church in the 16th century Protestant Reformation, followed in many cases by further divisions. For example, the Methodist Church grew out of Anglican minister John Wesley's evangelical and revival movement in the Anglican Church. Several Pentecostal and non-denominational Churches, which emphasize the cleansing power of the Holy Spirit, in turn grew out of the Methodist Church. Because Methodists, Pentecostals, and other evangelicals stress "accepting Jesus as your personal Lord and Savior", which comes from Wesley's emphasis of the New Birth, they often refer to themselves as being born-again.
Estimates of the total number of Protestants are very uncertain, partly because of the difficulty in determining which denominations should be placed in this category, but it seems clear that Protestantism is the second largest major group of Christians after Catholicism in number of followers (although the Orthodox Church is larger than any single Protestant denomination). Often that number is put at 800 million, corresponding to nearly 40% of world's Christians. The majority of Protestants are members of just a handful of denominational families, i.e. Adventists, Anglicans, Baptists, Calvinists (Presbyterians & Reformed), Congregationalists, Lutherans, Methodists and Pentecostals.
A special grouping are the Anglican churches descended from the Church of England and organised in the Anglican Communion. Some Anglican churches consider themselves both Protestant and Catholic. Some Anglicans consider their church a branch of the "One Holy Catholic Church" alongside of the Roman Catholic and Eastern Orthodox Churches, a concept rejected by the Roman Catholic Church and some Eastern Orthodox.
Some groups of individuals who hold basic Protestant tenets identify themselves simply as "Christians" or "born-again Christians". They typically distance themselves from the confessionalism and/or creedalism of other Christian communities by calling themselves "non-denominational". Often founded by individual pastors, they have little affiliation with historic denominations.
Restorationists and others.
The Second Great Awakening, a period of religious revival that occurred in the United States during the early 1800s, saw the development of a number of unrelated churches. They generally saw themselves as restoring the original church of Jesus Christ rather than reforming one of the existing churches. A common belief held by Restorationists was that the other divisions of Christianity had introduced doctrinal defects into Christianity, which was known as the Great Apostasy.
Some of the churches originating during this period are historically connected to early 19th-century camp meetings in the Midwest and Upstate New York. American Millennialism and Adventism, which arose from Evangelical Protestantism, influenced the Jehovah's Witnesses movement and, as a reaction specifically to William Miller, the Seventh-day Adventists. Others, including the Christian Church (Disciples of Christ), Evangelical Christian Church in Canada, Churches of Christ, and the Christian churches and churches of Christ, have their roots in the contemporaneous Stone-Campbell Restoration Movement, which was centered in Kentucky and Tennessee. Other groups originating in this time period include the Christadelphians and Latter Day Saint movement. While the churches originating in the Second Great Awakening have some superficial similarities, their doctrine and practices vary significantly.
Esoteric Christians regard Christianity as a mystery religion, and profess the existence and possession of certain esoteric doctrines or practices, hidden from the public but accessible only to a narrow circle of "enlightened", "initiated", or highly educated people. Some of the esoteric Christian institutions include the Rosicrucian Fellowship, the Anthroposophical Society and the Martinism.
Messianic Judaism (or Messianic Movement) is the name of a Christian movement comprising a number of streams, whose members may identify themselves as Jewish. It blends evangelical theology with elements of religious Jewish practice and terminology. Messianic Judaism affirms the messiahship and divinity of "Yeshua" (the Hebrew name of Jesus), whilst also adhering to aspects of Jewish dietary law and custom.
Christian culture.
Western culture, throughout most of its history, has been nearly equivalent to Christian culture, and many of the population of the Western hemisphere could broadly be described as cultural Christians. Though Western culture contained several polytheistic religions during its early years under the Greek and Roman empires, as the centralized Roman power waned, the dominance of the Catholic Church was the only consistent force in Europe. Until the Age of Enlightenment, Christian culture guided the course of philosophy, literature, art, music and science. Christian disciplines of the respective arts have subsequently developed into Christian philosophy, Christian art, Christian music, Christian literature etc.
Christianity had a significant impact on education and science and medicine as the church created the bases of the Western system of education, and was the sponsor of founding universities in the Western world as the university is generally regarded as an institution that has its origin in the Medieval Christian setting. Many clerics throughout history have made significant contributions to science and Jesuits in particular have made numerous significant contributions to the development of science. The Civilizing influence of Christianity includes social welfare, founding hospitals, economics (as the Protestant work ethic), politics architecture, literature and family life.
Christians have made a myriad contributions in a broad and diverse range of fields, including the sciences, arts, politics, literatures and business. According to "100 Years of Nobel Prizes" a review of Nobel prizes award between 1901 and 2000 reveals that (65.4%) of Nobel Prizes Laureates, have identified Christianity in its various forms as their religious preference.
"Postchristianity" is the term for the decline of Christianity, particularly in Europe, Canada, Australia and to a minor degree the Southern Cone, in the 20th and 21st centuries, considered in terms of postmodernism. It refers to the loss of Christianity's monopoly on values and world view in historically Christian societies.
Cultural Christians are secular people with a Christian heritage who may not believe in the religious claims of Christianity, but who retain an affinity for the popular culture, art, music, and so on related to it. Another frequent application of the term is to distinguish political groups in areas of mixed religious backgrounds.
Ecumenism.
Most Christian groups and individual denominations have long expressed ideals of being reconciled with each other, and in the 20th century, Christian ecumenism advanced in two ways. One way was greater cooperation between groups, such as the Edinburgh Missionary Conference of Protestants in 1910, the Justice, Peace and Creation Commission of the World Council of Churches founded in 1948 by Protestant and Orthodox churches, and similar national councils like the National Council of Churches in Australia which includes Roman Catholics.
The other way was institutional union with new United and uniting churches. Congregationalist, Methodist, and Presbyterian churches united in 1925 to form the United Church of Canada, and in 1977 to form the Uniting Church in Australia. The Church of South India was formed in 1947 by the union of Anglican, Baptist, Methodist, Congregationalist, and Presbyterian churches.
Steps towards reconciliation on a global level were taken in 1965 by the Roman Catholic and Orthodox churches mutually revoking the excommunications that marked their Great Schism in 1054; the Anglican Roman Catholic International Commission (ARCIC) working towards full communion between those churches since 1970; and the Lutheran and Roman Catholic churches signing the Joint Declaration on the Doctrine of Justification in 1999 to address conflicts at the root of the Protestant Reformation. In 2006, the World Methodist Council, representing all Methodist denominations, adopted the declaration.
Another example of ecumenism is the invention of and growing usage of the Christian Flag, which was designed to represent all of Christendom. The flag has a white field, with a red Latin cross inside a blue canton.
Criticism and apologetics.
Criticism of Christianity and Christians goes back to the Apostolic age, with the New Testament recording friction between the followers of Jesus and the Pharisees and scribes (e.g. Mark 7:1-23 and Matthew 15:1-20). In the 2nd century, Christianity was criticized by the Jews on various grounds, e.g. that the prophecies of the Hebrew Bible could not have been fulfilled by Jesus, given that he did not have a successful life. By the 3rd century, criticism of Christianity had mounted, partly as a defense against it, and the 15-volume "Adversus Christianos" by Porphyry was written as a comprehensive attack on Christianity, in part building on the pre-Christian concepts of Plotinus.
By the 12th century, the Mishneh Torah (i.e., Rabbi Moses Maimonides) was criticizing Christianity on the grounds of idol worship, in that Christians attributed divinity to Jesus who had a physical body. In the 19th century, Nietzsche began to write a series of attacks on the "unnatural" teachings of Christianity (e.g. avoidance of temptations), and continued anti-Christian attacks to the end of his life. In the 20th century, the philosopher Bertrand Russell expressed his criticism of Christianity in "Why I Am Not a Christian", formulating his rejection of Christianity in the setting of logical arguments.
Criticism of Christianity continues to date, e.g. Jewish and Muslim theologians criticize the doctrine of the Trinity held by most Christians, stating that this doctrine in effect assumes that there are three Gods, running against the basic tenet of monotheism. New Testament scholar Robert M. Price has outlined the possibility that some Bible stories are based partly on myth in "The Christ Myth Theory and its problems".
Christian apologetics aims to present a rational basis for Christianity. The word "apologetic" comes from the Greek word "apologeomai", meaning "in defense of". Christian apologetics has taken many forms over the centuries, starting with Paul the Apostle. The philosopher Thomas Aquinas presented five arguments for God's existence in the "Summa Theologica", while his "Summa contra Gentiles" was a major apologetic work.

</doc>
<doc id="5213" url="http://en.wikipedia.org/wiki?curid=5213" title="Computing">
Computing

Computing is any goal-oriented activity requiring, benefiting from, or creating algorithmic processes - e.g. through computers. Computing includes designing, developing and building hardware and software systems; processing, structuring, and managing various kinds of information; doing scientific research on and with computers; making computer systems behave intelligently; and creating and using communications and entertainment media. The field of computing includes computer engineering, software engineering, computer science, information systems, and information technology.
Definitions.
The ACM "Computing Curricula 2005" defined "computing" as follows:
"In a general way, we can define computing to mean any goal-oriented activity requiring, benefiting from, or creating computers. Thus, computing includes designing and building hardware and software systems for a wide range of purposes; processing, structuring, and managing various kinds of information; doing scientific studies using computers; making computer systems behave intelligently; creating and using communications and entertainment media; finding and gathering information relevant to any particular purpose, and so on. The list is virtually endless, and the possibilities are vast."
and it defines five sub-disciplines of the "computing" field: Computer Science, Computer Engineering, Information Systems, Information Technology, and Software Engineering.
However, "Computing Curricula 2005" also recognizes that the meaning of "computing" depends on the context:
"Computing also has other meanings that are more specific, based on the context in which the term is used. For example, an information systems specialist will view computing somewhat differently from a software engineer. Regardless of the context, doing computing well can be complicated and difficult. Because society needs people to do computing well, we must think of computing not only as a profession but also as a discipline."
The term "computing" has sometimes been narrowly defined, as in a 1989 ACM report on "Computing as a Discipline":
"The discipline of computing is the systematic study of algorithmic
processes that describe and transform information: their theory, analysis, design, efficiency, implementation, and application. The fundamental question underlying all computing is "What can be (efficiently) automated?"
The term "computing" is also synonymous with counting and calculating. In earlier times, it was used in reference to mechanical computing machines.
History of computing.
The history of computing is longer than the history of computing hardware and modern computing technology and includes the history of methods intended for pen and paper or for chalk and slate, with or without the aid of tables.
Computing is intimately tied to the representation of numbers. But long before abstractions like "the number" arose, there were mathematical concepts to serve the purposes of civilization. These concepts include one-to-one correspondence (the basis of counting), comparison to a standard (used for measurement), and the "3-4-5" right triangle (a device for assuring a "right angle").
The earliest known tool for use in computation was the abacus, and it was thought to have been invented in Babylon circa 2400 BC. Its original style of usage was by lines drawn in sand with pebbles. Abaci, of a more modern design, are still used as calculation tools today. This was the first known computer and most advanced system of calculation known to date - preceding Greek methods by 2,000 years.
Computer.
A computer is a machine that manipulates data according to a set of instructions called a computer program. The program has an executable form that the computer can use directly to execute the instructions. The same program in its human-readable source code form, enables a programmer to study and develop the algorithm. Because the instructions can be carried out in different types of computers, a single set of source instructions converts to machine instructions according to the central processing unit type.
The execution process carries out the instructions in a computer program. Instructions express the computations performed by the computer. They trigger sequences of simple actions on the executing machine. Those actions produce effects according to the semantics of the instructions.
Computer software and hardware.
Computer software or just "software", is a collection of computer programs and related data that provides the instructions for telling a computer what to do and how to do it. Software refers to one or more computer programs and data held in the storage of the computer for some purposes. In other words, software is a set of "programs, procedures, algorithms" and its "documentation" concerned with the operation of a data processing system. Program software performs the function of the program it implements, either by directly providing instructions to the computer hardware or by serving as input to another piece of software. The term was coined to contrast with the old term hardware (meaning physical devices). In contrast to hardware, software "cannot be touched". Software is also sometimes used in a more narrow sense, meaning application software only.
Application software.
Application software, also known as an "application" or an "app", is computer software designed to help the user to perform specific tasks. Examples include enterprise software, accounting software, office suites, graphics software and media players. Many application programs deal principally with documents. Apps may be bundled with the computer and its system software, or may be published separately. Some users are satisfied with the bundled apps and need never install one.
Application software is contrasted with system software and middleware, which manage and integrate a computer's capabilities, but typically do not directly apply them in the performance of tasks that benefit the user. The system software serves the application, which in turn serves the user.
Application software applies the power of a particular computing platform or system software to a particular purpose. Some apps such as Microsoft Office are available in versions for several different platforms; others have narrower requirements and are thus called, for example, a Geography application for Windows or an Android application for education or Linux gaming. Sometimes a new and popular application arises that only runs on one platform, increasing the desirability of that platform. This is called a killer application.
System software.
System software, or systems software, is computer software designed to operate and control the computer hardware and to provide a platform for running application software. System software includes operating systems, utility software, device drivers, window systems, and firmware. Frequently development tools such as compilers, linkers, and debuggers are classified as system software.
Computer network.
A computer network, often simply referred to as a network, is a collection of hardware components and computers interconnected by communication channels that allow sharing of resources and information. Where at least one process in one device is able to send/receive data to/from at least one process residing in a remote device, then the two devices are said to be in a network.
Networks may be classified according to a wide variety of characteristics such as the medium used to transport the data, communications protocol used, scale, topology, and organizational scope.
Communications protocols define the rules and data formats for exchanging information in a computer network, and provide the basis for network programming. Well-known communications protocols are Ethernet, a hardware and Link Layer standard that is ubiquitous in local area networks, and the Internet Protocol Suite, which defines a set of protocols for internetworking, i.e. for data communication between multiple networks, as well as host-to-host data transfer, and application-specific data transmission formats.
Computer networking is sometimes considered a sub-discipline of electrical engineering, telecommunications, computer science, information technology or computer engineering, since it relies upon the theoretical and practical application of these disciplines.
Internet.
The Internet is a global system of interconnected computer networks that use the standard Internet protocol suite (TCP/IP) to serve billions of users ' that consists of millions of private, public, academic, business, and government networks, of local to global scope, that are linked by a broad array of electronic, wireless and optical networking technologies. The Internet carries an extensive range of information resources and services, such as the inter-linked hypertext documents of the World Wide Web (WWW) and the infrastructure to support email.
Computer user.
, who uses a computer or network service. A user often has a user account and is identified by a username (also user name), screen name (also screenname), nickname (also nick), or handle, which derives from the identical Citizen's Band radio term.
In hacker-related terminology, users are divided into "lusers" and "power users".
In projects where the system actor is another system or a software agent, there may be no end-user. In that case, the end-users for the system is indirect end-users.
End-user.
The term end-user refers to the ultimate operator of a piece of software, but it is also a concept in software engineering, referring to an abstraction of that group of end-users of computers (i.e. the expected user or target-user). The term is used to distinguish those who only operate the software from the developer of the system, who knows a programming language and uses it to create new functions for end-users.
Computer programming.
Computer programming in general is the process of writing, testing, debugging, and maintaining the source code and documentation of computer programs. This source code is written in a programming language, which is an artificial language often more restrictive or demanding than natural languages, but easily translated by the computer. The purpose of programming is to invoke the desired behavior (customization) from the machine. The process of writing high quality source code requires knowledge of both the application's domain "and" the computer science domain. The highest-quality software is thus developed by a team of various domain experts, each person a specialist in some area of development. But the term "programmer" may apply to a range of program quality, from hacker to open source contributor to professional. And a single programmer could do most or all of the computer programming needed to generate the proof of concept to launch a new "killer" application.
Computer programmer.
A programmer, computer programmer, or coder is a person who writes computer software. The term "computer programmer" can refer to a specialist in one area of computer programming or to a generalist who writes code for many kinds of software. One who practices or professes a formal approach to programming may also be known as a programmer analyst. A programmer's primary computer language (C, C++, Java, Lisp, Python, Smalltalk, etc.) is often prefixed to the above titles, and those who work in a web environment often prefix their titles with "web". The term "programmer" can be used to refer to a software developer, software engineer, computer scientist, or software analyst. However, members of these professions typically possess other software engineering skills, beyond programming; for this reason, the term "programmer" is sometimes considered an insulting or derogatory oversimplification of these other professions.
Computer industry.
The computer industry is made up of all of the businesses involved in developing computer software, designing computer hardware and computer networking infrastructures, the manufacture of computer components and the provision of information technology services including system administration and maintenance.
Software industry.
The software industry includes businesses engaged in development, maintenance and publication of software. The industry also includes software services, such as training, documentation, and consulting.
Sub-disciplines of computing.
Computer engineering.
Computer engineering is a discipline that integrates several fields of Electrical engineering and computer science required to develop computer hardware and software. Computer engineers usually have training in electronic engineering (or electrical engineering), software design, and hardware-software integration instead of only software engineering or electronic engineering. Computer engineers are involved in many hardware and software aspects of computing, from the design of individual microprocessors, personal computers, and supercomputers, to circuit design. This field of engineering not only focuses on how computer systems themselves work, but also how they integrate into the larger picture.
Software engineering.
Software engineering (SE) is the application of a systematic, disciplined, quantifiable approach to the design, development, operation, and maintenance of software, and the study of these approaches; that is, the application of engineering to software.
In layman's terms, it is the act of using insights to conceive, model and scale a solution to a problem. The first reference to the term is the 1968 NATO Software Engineering Conference and was meant to provoke thought regarding the perceived "software crisis" at the time. "Software development", a much used and more generic term, does not necessarily subsume the engineering paradigm. The generally accepted concepts of Software Engineering as an engineering discipline have been specified in the Guide to the Software Engineering Body of Knowledge (SWEBOK). The SWEBOK has become an internationally accepted standard ISO/IEC TR 19759:2005.
Computer science.
Computer science or computing science (abbreviated CS or Comp Sci) is the scientific and practical approach to computation and its applications. A computer scientist specializes in the theory of computation and the design of computational systems.
Its subfields can be divided into practical techniques for its implementation and application in computer systems and purely theoretical areas. Some, such as computational complexity theory, which studies fundamental properties of computational problems, are highly abstract, while others, such as computer graphics, emphasize real-world applications. Still others focus on the challenges in implementing computations. For example, programming language theory studies approaches to description of computations, while the study of computer programming itself investigates various aspects of the use of programming languages and complex systems, and human-computer interaction focuses on the challenges in making computers and computations useful, usable, and universally accessible to humans.
Information systems.
"Information systems (IS)" is the study of complementary networks of hardware and software (see information technology) that people and organizations use to collect, filter, process, create, and distribute data. 
The study bridges business and computer science using the theoretical foundations of information and computation to study various business models and related algorithmic processes within a computer science discipline. Computer Information System(s) (CIS) is a field studying computers and algorithmic processes, including their principles, their software and hardware designs, their applications, and their impact on society while IS emphasizes functionality over design.
Information technology.
Information technology (IT) is the application of computers and telecommunications equipment to store, retrieve, transmit and manipulate data, often in the context of a business or other enterprise. The term is commonly used as a synonym for computers and computer networks, but it also encompasses other information distribution technologies such as television and telephones. Several industries are associated with information technology, such as computer hardware, software, electronics, semiconductors, internet, telecom equipment, e-commerce and computer services.
Systems administration.
A system administrator, IT systems administrator, systems administrator, or sysadmin is a person employed to maintain and operate a computer system and/or network. The duties of a system administrator are wide-ranging, and vary widely from one organization to another. Sysadmins are usually charged with installing, supporting and maintaining servers or other computer systems, and planning for and responding to service outages and other problems. Other duties may include scripting or light programming, project management for systems-related projects, supervising or training computer operators, and being the consultant for computer problems beyond the knowledge of technical support staff.

</doc>
<doc id="5215" url="http://en.wikipedia.org/wiki?curid=5215" title="Casino">
Casino

In modern English, a casino is a facility which houses and accommodates certain types of gambling activities. The industry that deals in casinos is called the gaming industry. Casinos are most commonly built near or combined with hotels, restaurants, retail shopping, cruise ships or other tourist attractions. There is much debate over whether or not the social and economic consequences of casino gambling outweigh the initial revenue that may be generated. In the United States, many states are grappling with high unemployment and budget deficits and are now turning to legalizing casinos, often in places that are not tourist destinations. Some casinos are also known for hosting live entertainment events, such as stand-up comedy, concerts, and sporting events.
Etymology and usage.
The term "Casino" is of Italian origin, the root word being "Casa" (house) and originally meant a small country villa, summerhouse or pavilion. The word changed to refer to a building built for pleasure, usually on the grounds of a larger Italian villa or palazzo. Such buildings were used to host civic town functions – including dancing, music listening, and gambling.
There are examples of such casinos at Villa Giulia and Villa Farnese. In modern day Italian, this term designates a bordello (also called "casa chiusa", literally "closed house"), while the gambling house is spelled "casinò" with an accent.
During the 19th century, the term "casino" came to include other public buildings where pleasurable activities, including gambling, and sports took place. An example of this type of building is the Newport Casino in Newport, Rhode Island.
Not all casinos were used for gaming. The Copenhagen Casino was a theatre, known for the use made of its hall for mass public meetings during the 1848 Revolution which made Denmark a constitutional monarchy. Until 1937 it was a well-known Danish theatre. The Hanko Casino located in Hanko, Finland - one of that town's most conspicuous landmarks - was never used for gambling. Rather, it was a banquet hall for the Russian nobility which frequented this spa resort in the late 19th century, and is presently used as a restaurant. The Catalina Casino, a famous landmark overlooking Avalon Harbor on Santa Catalina Island, California, has never been used for traditional games of chance, which were already outlawed in California by the time it was built.
In military and non-military usage in Spanish and German, a "casino" or "kasino" is an officers' mess; curiously, in Italian - the source-language of the word - a "casino" is either a brothel, a mess, or a noisy environment, while a gaming house is called a "casinò". A confusing linguistic false friend for translators.
History of gambling houses.
The precise origin of gambling is unknown. It is generally believed that gambling in some form or another has been seen in almost every society in history. From the Ancient Greeks and Romans to Napoleon's France and Elizabethan England, much of history is filled with stories of entertainment based on games of chance.
The first known European gambling house, not called a casino although meeting the modern definition, was the Ridotto, established in Venice, Italy in 1638 to provide controlled gambling during the carnival season. It was closed in 1770 as the city government perceived it to impoverish the local gentry.
In American history, early gambling establishments were known as saloons. The creation and importance of saloons was greatly influenced by four major cities; New Orleans, St. Louis, Chicago and San Francisco. It was in the saloons that travelers could find people to talk to, drink with, and often gamble with. During the early 20th century in America, gambling became outlawed and banned by state legislation and social reformers of the time. However, in 1931, gambling was legalized throughout the state of Nevada. America's first legalized casinos were set up in those places. In 1978 New Jersey allowed gambling in Atlantic City, now America's second largest gambling city.
Gambling in casinos.
Most jurisdictions worldwide limit gambling to persons over the age of license (16 to 21 years of age in most countries which permit the operation of casinos).
Customers gamble by playing games of chance, in some cases with an element of skill, such as craps, roulette, baccarat, blackjack, and video poker. Most games played have mathematically-determined odds that ensure the house has at all times an overall advantage over the players. This can be expressed more precisely by the notion of expected value, which is uniformly negative (from the player's perspective). This advantage is called the "house edge". In games such as poker where players play against each other, the house takes a commission called the rake. Casinos sometimes give out complimentary items to gamblers.
"Payout" is the percentage of funds ("winnings") returned by players.
Casinos in the USA say that a player staking money won from the casino is "playing with house money".
Video Lottery Machines (slot machines) have become one of the most popular form of gambling in casinos. investigative reports have started calling into question whether the modern-day slot-machine is addictive.
Design.
Casino design - regarded as a psychological exercise - is an intricate process that involves optimising floor plan, décor and atmospherics to encourage consumer gambling. 
Factors influencing consumer gambling tendencies include sound, odour and lighting. Natasha Dow Schüll, an anthropologist at the Massachusetts Institute of Technology, highlights the audio directors at Silicon Gaming’s decision to make its slot machines resonate in, “the universally pleasant tone of C, sampling existing casino soundscapes to create a sound that would please but not clash”.
Dr Alan Hirsch, founder of the Smell & Taste Treatment and Research Foundation in Chicago, studied the impact of certain scents on gamblers, discerning that a pleasant albeit unidentifiable odour released by Las Vegas slots machines generated approximately 50% more in daily revenue. He suggested that the scent acted as an aphrodisiac, facilitating a more aggressive form of gambling.
Casino designer Roger Thomas is credited with implementing a successful, disruptive design for the Las Vegas Wynn Resorts’ casinos in 2008. He broke casino design convention by introducing natural sunlight and flora to appeal to a female demographic. Thomas inserted skylights and antique clocks, defying the commonplace notion that a casino should be a timeless space.
Markets.
The following lists major casino markets in the world with casino revenue of over $1 billion USD as published in PricewaterhouseCoopers's report on 
the outlook for the global casino market:
By company.
According to Bloomberg, accumulated revenue of biggest casino operator companies worldwide amounted almost 55 billion US dollars as per 2011. SJM Holdings ltd. was the leading company in this field and earned 9.7 billion in 2011, followed by Las Vegas Sands Corp. (7.4 bn). The third biggest casino operator company (based on revenue) was Caesars Entertainment with revenue of 6.2 bn US dollar.
Significant sites.
While there are casinos in many places, a few places have become well-known specifically for gambling. Perhaps the place almost defined by its casino is Monte Carlo, but other places are known as gambling centers.
Monte Carlo, Monaco.
Monte Carlo has a famous casino popular with well-off visitors and is a tourist attraction in its own right. A song and a film named "The Man Who Broke the Bank at Monte Carlo" need no explanation—they clearly refer to the casino.
"Monte Carlo"'s Casino has also been depicted in many books including Ben Mezrich's Busting Vegas, where a group of Massachusetts Institute of Technology students beat the casino out of nearly $1 000 000. This book is based on real people and events; however, many of those events are contested by main character Semyon Dukach.
The casino has made Monte Carlo so well known for games of chance that mathematical methods for solving various problems using many quasi-random numbers—numbers with the statistical distribution of numbers generated by chance—are formally known as Monte Carlo methods. Monte Carlo was part of the plot in a few James Bond novels and films.
Macau.
The former Portuguese colony of Macau, a special administrative region of China since 1999, is a popular destination for visitors who wish to gamble. This started in Portuguese times, when Macau was popular with visitors from nearby British Hong Kong where gambling was more closely regulated. The Venetian Macao is the largest casino in the world.
United States.
There are almost 900 casinos now in the United States, with that number steadily growing as more states seek to legalize casinos. 38 states now have some form of casino gambling. Relatively small places such as Las Vegas are best known for gambling; larger cities such as Chicago are not defined by their casinos in spite of the large turnover.
The Las Vegas Valley has the largest concentration of casinos in the United States. Based on revenue, Atlantic City, New Jersey ranks second, and the Chicago region third.
Top American casino markets by revenue (2009 annual revenues):
The Nevada Gaming Control Board divides Clark County, which is coextensive with the Las Vegas metropolitan area, into seven regions for reporting purposes.
Indian gaming has been responsible for a rise in the number of casinos outside of Las Vegas and Atlantic City.
Security.
Given the large amounts of currency handled within a casino, both patrons and staff may be tempted to cheat and steal, in collusion or independently; most casinos have security measures to prevent this. Security cameras located throughout the casino are the most basic measure.
Modern casino security is usually divided between a physical security force and a specialized surveillance department. The physical security force usually patrols the casino and responds to calls for assistance and reports of suspicious or definite criminal activity. A specialized surveillance department operates the casino's closed circuit television system, known in the industry as the eye in the sky. Both of these specialized casino security departments work very closely with each other to ensure the safety of both guests and the casino's assets, and have been quite successful in preventing crime. Some casinos also have catwalks in the ceiling above the casino floor, which allow surveillance personnel to look directly down, through one way glass, on the activities at the tables and slot machines.
When it opened in 1989, The Mirage was the first casino to use cameras full-time on all table games.
In addition to cameras and other technological measures, casinos also enforce security through rules of conduct and behavior; for example, players at card games are required to keep the cards they are holding in their hands visible at all times.
Business practices.
Over the past few decades, casinos have developed many different marketing techniques for attracting and maintaining loyal patrons. Many casinos use a loyalty rewards program used to track players' spending habits and target their patrons more effectively, by sending mailings with free slot play and other promotions.
Crime.
One area of controversy surrounding casinos is their relationship to crime rates. Economic studies that show a positive relationship between casinos and crime usually fail to consider the visiting population at risk when they calculate the crime rate in casino areas. Such studies thus count the crimes committed by visitors, but do not count visitors in the population measure, and this overstates the crime rates in casino areas. Part of the reason this methodology is used, despite it leading to an overstatement of crime rates is that reliable data on tourist count are often not available.
In a 2004 report by the US Department of Justice, researchers interviewed people who had been arrested in Las Vegas and Des Moines and found that the percentage of problem or pathological gamblers among the arrestees was three to five times higher than in the general population. According to some police reports, incidences of reported crime often double and triple in communities within three years of a casino opening.

</doc>
<doc id="5216" url="http://en.wikipedia.org/wiki?curid=5216" title="Khmer language">
Khmer language

Khmer (; , ; or more formally, , ), or Cambodian, is the language of the Khmer people and the official language of Cambodia. With approximately 16 million speakers, it is the second most widely spoken Austroasiatic language (after Vietnamese). Khmer has been considerably influenced by Sanskrit and Pali, especially in the royal and religious registers, through the vehicles of Hinduism and Buddhism. It is also the earliest recorded and earliest written language of the Mon–Khmer family, predating Mon and by a significant margin Vietnamese. The Khmer language has influenced, and also been influenced by, Thai, Lao, Vietnamese, Chinese and Cham, all of which, due to geographical proximity and long-term cultural contact, form a sprachbund in peninsular Southeast Asia.
Khmer is primarily an analytic, isolating language. There are no inflections, conjugations or case endings. Instead, particles and auxiliary words are used to indicate grammatical relationships. General word order is subject–verb–object. Many words conform to the typical Mon–Khmer pattern of a "main" syllable preceded by a minor syllable.
The Khmer language is written with an abugida known in Khmer as អក្សរខ្មែរ (), "Khmer script". Khmer differs from neighboring languages such as Thai, Burmese, Lao and Vietnamese in that it is not a tonal language.
Classification.
Khmer is a member of the Austroasiatic language family, the most archaic family in an area that stretches from the Malay Peninsula through Southeast Asia to East India. Austroasiatic, which also includes Mon, Vietnamese and Munda, has been studied since 1856 and was first proposed as a language family in 1907. Despite the amount of research, there is still doubt about the internal relationship of the languages of Austroasiatic. Most classifications place Khmer in the eastern branch of a Mon-Khmer sub-grouping. In these classification schemes Khmer's closest genetic relatives are the Bahnaric and Pearic languages. More recent classifications doubt the validity of the Mon-Khmer sub-grouping and place the Khmer language as its own branch of Austroasiatic equidistant from the other 12 branches of the family.
Geographic Distribution and Dialects.
Khmer is spoken by some 13 million people in Cambodia, where it is the official language. It is also a second language for most of the minority groups and indigenous hill tribes there. Additionally there are a million speakers of Khmer native to southern Vietnam (1999) and 1.4 million in northeast Thailand (2006).
Khmer dialects, although mutually intelligible, are sometimes quite marked. Notable variations are found in speakers from Phnom Penh (which is the capital city), the rural Battambang area, the areas of Northeast Thailand adjacent to Cambodia such as Surin province, the Cardamom Mountains, and in southern Vietnam. The dialects form a continuum running roughly north to south. Standard Cambodian Khmer is mutually intelligible with the others but a Khmer Krom speaker from Vietnam, for instance, may have great difficulty communicating with a Khmer native to Sisaket Province in Thailand.
The following is a classification scheme showing the development of the modern Khmer dialects.
Standard Khmer, or Central Khmer, the language as taught in schools and used by the media is based on the Battambang dialect spoken throughout the plains of the northwest and central provinces.
Northern Khmer (called "Khmer Surin" in Khmer) refers to the dialects spoken by many in several border provinces of present-day Northeast Thailand. After the fall of the Khmer Empire in the early 15th century, the Dongrek Mountains served as a natural border leaving the Khmer north of the mountains under the sphere of influence of the Kingdom of Lan Xang. The conquests of Cambodia by Naresuan the Great for Ayutthaya furthered their political and economic isolation from Cambodia proper, leading to a dialect that developed relatively independently from the midpoint of the Middle Khmer period. This has resulted in a distinct accent influenced by the surrounding tonal languages, Lao and Thai, lexical differences, and phonemic differences in both vowels and distribution of consonants. Additionally, syllable-final , which has become silent in other dialects of Khmer, is still pronounced in Northern Khmer. Some linguists classify Northern Khmer as a separate, but closely related language rather than a dialect.
Western Khmer, also called Cardamom Khmer or Chanthaburi Khmer, spoken by a very small, isolated population in the Cardamom mountain range extending from western Cambodia into eastern Central Thailand, although little studied, is unique in that it maintains a definite system of vocal register that has all but disappeared in other dialects of modern Khmer.
Phnom Penh Khmer is spoken in the capital and surrounding areas. This dialect is characterized by merging or complete elision of syllables, considered by speakers from other regions to be a "relaxed" pronunciation. For instance, "Phnom Penh" will sometimes be shortened to "m'Penh". Another characteristic of Phnom Penh speech is observed in words with an "r" either as an initial consonant or as the second member of a consonant cluster (as in the English word "bread"). The "r", trilled or flapped in other dialects, is either pronounced as an uvular trill or not pronounced at all. This alters the quality of any preceding consonant causing a harder, more emphasized pronunciation. Another unique result is that the syllable is spoken with a low-rising or "dipping" tone much like the "hỏi" tone in Vietnamese. For example, some people pronounce (meaning "fish") as , the "r" is dropped and the vowel begins by dipping much lower in tone than standard speech and then rises, effectively doubling its length. Another example is the word ("study, learn"). It is pronounced , with the "uvular r" and the same intonation described above.
Khmer Krom or Southern Khmer is spoken by the indigenous Khmer population of the Mekong Delta, formerly controlled by the Khmer Empire but part of Vietnam since 1698. Khmers are persecuted by the Vietnamese government for using their native language and, since the 1950s, have been forced to take Vietnamese names. Consequently very little research has been published regarding this dialect. It generally has been influenced by Vietnamese for three centuries and accordingly displays a pronounced accent, tendency toward monosyllablic words and lexical differences from the standard.
Historical periods.
Linguistic study of the Khmer language divides its history into four periods one of which, the Old Khmer period, is subdivided into pre-Angkorian and Angkorian. Pre-Angkorian Khmer, the language after its divergence from Proto-Mon–Khmer until the ninth century, is only known from words and phrases in Sanskrit texts of the era. Old Khmer (or Angkorian Khmer) is the language as it was spoken in the Khmer Empire from the 9th century until the weakening of the empire sometime in the 13th century. Old Khmer is attested by many primary sources and has been studied in depth by a few scholars, most notably Saveros Pou, Phillip Jenner and Heinz-Jürgen Pinnow. Following the end of the Khmer Empire the language lost the standardizing influence of being the language of government and accordingly underwent a turbulent period of change in morphology, phonology and lexicon. The language of this transition period, from about the 14th to 18th centuries, is referred to as Middle Khmer and saw borrowing from Thai, Lao and, to a lesser extent, Vietnamese. The changes during this period are so profound that the rules of Modern Khmer can not be applied to correctly understand Old Khmer. The language became recognizable as Modern Khmer, spoken from the 19th century till today.
The following table shows the conventionally accepted historical stages of Khmer.
Just as modern Khmer was emerging from the transitional period represented by Middle Khmer, Cambodia fell under the influence of French colonialism. In 1887 Cambodia was fully integrated into French Indochina which brought in a French-speaking aristocracy. This led to French becoming the language of higher education and the intellectual class. Many native scholars in the early 20th century, led by a monk named Chuon Nath, resisted the French influence on their language and championed Khmerization, using Khmer roots (and Pali and Sanskrit) to coin new words for modern ideas, instead of French. Nath cultivated modern Khmer-language identity and culture, overseeing the translation of the entire Pali Buddhist canon into Khmer and creating the modern Khmer language dictionary that is still in use today, thereby ensuring that Khmer would survive, and indeed flourish, during the French colonial period.
Phonology.
The phonological system described here is the inventory of sounds of the spoken language, not how they are written in the Khmer alphabet.
Consonants.
Khmer is frequently described as having aspirated stops. However, these may be analyzed as consonant clusters, , as infixes can occur between the stop and the aspiration "(phem, phem)," or as non-distinctive phonetic detail in other consonant clusters, such as the "khm" in "Khmer." and are occasional allophones of the implosives.
In addition, the consonants , , and may occasionally occur in recent loan words in the speech of Cambodians familiar with French and other languages. These non-native sounds are not represented in the Khmer script, although combinations of letters otherwise unpronounceable are used to represent them when necessary. In the speech of those who are not bilingual, these sounds are approximated with natively occurring phonemes:
Vowel nuclei.
Various researchers have proposed slightly different analyses of the vowels. This may be in part because political centralization has not yet been achieved, so standard Khmer does not prevail throughout Cambodia. Additionally, the Cambodian Civil War resulted in massive internal population upheaval. As such, many speakers of even the same community may have different phonological inventories. Two proposals follow. The first is Huffman's analysis of Standard Khmer, and the second is Wayland's analysis of Battambang Khmer, the dialect upon which the standard is based.
The precise number and the phonetic value of vowel nuclei vary from dialect to dialect. Short and long vowels of equal quality are distinguished solely by duration.
Syllable structure.
Khmer words are predominantly either monosyllabic or sesquisyllabic, with stress falling on the final syllable. There are two possible clusters of three consonants at the beginning of syllables, , and 85 possible two-consonant clusters:
Syllables begin with one of these consonants or consonant clusters, followed by one of the vowel nuclei. The aspiration in some clusters is allophonic. When the vowel nucleus is short, there has to be a final consonant. can exist in a syllable coda, while and approach and respectively. The stops have no audible release when occurring as syllable finals.
The most common word structure in Khmer is a full syllable as described above, which may be preceded by an unstressed, "minor" syllable that has a consonant-vowel structure of CV-, CrV-, CVN- or CrVN- (N is any nasal in the Khmer inventory). The vowel in these preceding syllables is usually reduced in conversation to , however in careful or formal speech and in television and radio, they are always clearly articulated.
Words with three or more syllables exist, particularly those pertaining to science, the arts, and religion. However, these words are loanwords, usually derived from Pali, Sanskrit, or more recently, French.
Suprasegmental features.
Phonation and tone.
Khmer once had a phonation distinction in its vowels, which was indicated in writing by choosing between two sets of letters for the preceding consonant according to the historical source of the phonation. However, phonation has been lost in all but the most archaic dialect of Khmer (Western Khmer). For example, Old Khmer distinguished voiced and unvoiced pairs as in vs . The vowels after voiced consonants became breathy voiced and diphthongized: . When consonant voicing was lost, the distinction was maintained by the vowel: , and later the phonation disappeared as well: .
Although most Cambodian dialects are not tonal, colloquial Phnom Penh dialect has developed a tonal contrast (level versus peaking tone) to compensate for the elision of .
Prosody.
Stress.
Stress in Khmer is non-phonemic (does not distinguish different meanings) and thus is considered to depend entirely on syllable structure. Owing in part to the sesquisyllabic nature of Khmer, syllabic stress is also highly predictable. In native disyllabic words, the first syllable is always a minor syllable and the second syllable is stressed. Loan words and reduplications also tend to follow this pattern:
In words of more than two syllables, primary stress is always on the final syllable with secondary stress on every second syllable. Thus, in a three-syllable word, the first syllable exhibits secondary stress, while primary stress is on the third syllable:
Compound words of three syllables, however, follow the stress of the constituent words:
In a four-syllable word, the second syllable carries secondary stress and primary stress is on the fourth syllable:
Words of five or more syllables are exceedingly rare in everyday conversation but do occur in academic, governmental, and religious contexts. These words are all derived from Sanskrit or Pali roots but follow Khmer pronunciation and stress patterns:
Intonation.
As Khmer is primarily an analytic pro-drop language, intonation (non-phonemic pitch variation throughout clauses) often conveys semantic context. The intonation pattern of a typical Khmer declarative phrase is a steady rise throughout followed by an abrupt drop on the last syllable. 
Other intonation contours signify a different type of phrase such as the "full doubt" interrogative, similar to "yes-no" questions in English or the exclamatory phrase. Full doubt interrogatives remain fairly even in tone throughout but rise sharply towards the end.
Exclamatory phrases follow the typical steadily rising pattern, but rise sharply on the last syllable instead of falling.
Grammar.
Khmer is generally a subject–verb–object (SVO) language with prepositions. Although primarily an analytic language, lexical derivation by means of prefixes and infixes occurs but is not always productive in the modern language.
Adjectives and Adverbs.
Adjectives, demonstratives and numerals follow the noun they modify. Adverbs likewise follow the verb. Morphologically, adjectives and adverbs are not distinguished, with many words often serving either function. Similar to other languages of the region, intensity can be expressed by reduplication.
As Khmer sentences rarely use a copula, adjectives are also employed as verbs. Comparatives are formed by the use of "ciəng": "A X "ciəng" B" (A is more X than B). The most common way to express the idea of superlatives is the construction "A X "ciəng kee"" (A is X-est of all).
Nouns.
The noun has no grammatical gender or singular/plural distinction and is uninflected. Technically there are no articles, but indefiniteness is often expressed by the word for "one" following the noun. Plurality can be marked by postnominal particles, numerals, or reduplicating the adjective, which, although similar to intensification, is usually not ambiguous due to context.
Classifying particles for use between numerals and nouns exist although are not always obligatory as in, for example, Thai. Pronouns are subject to a complicated system of social register, the choice of pronoun depending on the perceived relationships between speaker, audience and referent (see Social registers below). Kinship terms, nicknames and proper names are often used as pronouns (including for the first person) among intimates. Frequently, subject pronouns are dropped in colloquial conversation.
Verbs.
As is typical of most East Asian languages, the verb does not inflect at all; tense and aspect can be shown by particles and adverbs or understood by context. Most commonly, time words such as "yesterday", "earlier", "tomorrow", indicate tense when not inferrable from context. There is no participle form. The gerund is formed by using : "A V" (A is in the process of V). Serial verb construction is quite common. Negation is achieved by putting before them and at the end of the sentence or clause. In normal speech verbs can also be negated without the need for an ending particle by putting before them.
Numerals.
The numbers are:
Social registers.
Khmer employs a system of registers in which the speaker must always be conscious of the social status of the person spoken to. The different registers, which include those used for common speech, polite speech, speaking to or about royals and speaking to or about monks, employ alternate verbs, names of body parts and pronouns. This results in what appears to foreigners as separate languages and, in fact, isolated villagers often are unsure how to speak with royals and royals raised completely within the court do not feel comfortable speaking the common register. As an example, the word for "to eat" used between intimates or in reference to animals is . Used in polite reference to commoners, it's . When used of those of higher social status, it's or . For monks the word is and for royals, . Another result is that the pronominal system is complex and full of honorific variations, just a few of which are shown in the table below.
Writing system.
Khmer is written with the Khmer script, an abugida developed from the Pallava script of India before the 7th century when the first known inscription appeared. Written left-to-right with vowel signs that can be placed after, before, above or below the consonant they follow, the Khmer script is similar in appearance and usage to Thai and Lao, both of which were based on the Khmer system. The Khmer script is also distantly related to the Mon script, the ancestor of the modern Burmese script. Khmer numerals, which were inherited from Indian numerals, are used more widely than Hindu-Arabic numerals. Within Cambodia, literacy in the Khmer alphabet is estimated at 77.6%.
Consonant symbols in Khmer are divided into two groups, or series. The first series carries the inherent vowel while the second series carries the inherent vowel . The Khmer names of the series, ("voiceless") and ("voiced"), respectively, indicate that the second series consonants were used to represent the voiced phonemes of Old Khmer. As the voicing of stops was lost, however, the contrast shifted to the phonation of the attached vowels which, in turn, evolved into a simple difference of vowel quality, often by diphthongization. This process has resulted in the Khmer alphabet having two symbols for most consonant phonemes and each vowel symbol having two possible readings, depending on the series of the initial consonant:

</doc>
<doc id="5218" url="http://en.wikipedia.org/wiki?curid=5218" title="Central processing unit">
Central processing unit

A central processing unit (CPU) (formerly also referred to as a central processor unit) is the hardware within a computer that carries out the instructions of a computer program by performing the basic arithmetical, logical, and input/output operations of the system. The term has been in use in the computer industry at least since the early 1960s. The form, design, and implementation of CPUs have changed over the course of their history, but their fundamental operation remains much the same.
A computer can have more than one CPU; this is called multiprocessing. All modern CPUs are microprocessors, meaning contained on a single chip. Some integrated circuits (ICs) can contain multiple CPUs on a single chip; those ICs are called multi-core processors. An IC containing a CPU can also contain peripheral devices, and other components of a computer system; this is called a system on a chip (SoC).
Two typical components of a CPU are the arithmetic logic unit (ALU), which performs arithmetic and logical operations, and the control unit (CU), which extracts instructions from memory and decodes and executes them, calling on the ALU when necessary.
Not all computational systems rely on a central processing unit. An array processor or vector processor has multiple parallel computing elements, with no one unit considered the "center". In the distributed computing model, problems are solved by a distributed interconnected set of processors.
History.
Computers such as the ENIAC had to be physically rewired to perform different tasks, which caused these machines to be called "fixed-program computers". Since the term "CPU" is generally defined as a device for software (computer program) execution, the earliest devices that could rightly be called CPUs came with the advent of the stored-program computer.
The idea of a stored-program computer was already present in the design of J. Presper Eckert and John William Mauchly's ENIAC, but was initially omitted so that it could be finished sooner. On June 30, 1945, before ENIAC was made, mathematician John von Neumann distributed the paper entitled "First Draft of a Report on the EDVAC". It was the outline of a stored-program computer that would eventually be completed in August 1949. EDVAC was designed to perform a certain number of instructions (or operations) of various types. Significantly, the programs written for EDVAC were to be stored in high-speed computer memory rather than specified by the physical wiring of the computer. This overcame a severe limitation of ENIAC, which was the considerable time and effort required to reconfigure the computer to perform a new task. With von Neumann's design, the program, or software, that EDVAC ran could be changed simply by changing the contents of the memory. EDVAC, however, was not the first stored-program computer; the Manchester Small-Scale Experimental Machine, a small prototype stored-program computer, ran its first program on 21 June 1948 and the Manchester Mark 1 ran its first program during the night of 16–17 June 1949.
Early CPUs were custom-designed as a part of a larger, sometimes one-of-a-kind, computer. However, this method of designing custom CPUs for a particular application has largely given way to the development of mass-produced processors that are made for many purposes. This standardization began in the era of discrete transistor mainframes and minicomputers and has rapidly accelerated with the popularization of the integrated circuit (IC). The IC has allowed increasingly complex CPUs to be designed and manufactured to tolerances on the order of nanometers. Both the miniaturization and standardization of CPUs have increased the presence of digital devices in modern life far beyond the limited application of dedicated computing machines. Modern microprocessors appear in everything from automobiles to cell phones and children's toys.
While von Neumann is most often credited with the design of the stored-program computer because of his design of EDVAC, others before him, such as Konrad Zuse, had suggested and implemented similar ideas. The so-called Harvard architecture of the Harvard Mark I, which was completed before EDVAC, also utilized a stored-program design using punched paper tape rather than electronic memory. The key difference between the von Neumann and Harvard architectures is that the latter separates the storage and treatment of CPU instructions and data, while the former uses the same memory space for both. Most modern CPUs are primarily von Neumann in design, but CPUs with the Harvard architecture are seen as well, especially in embedded applications; for instance, the Atmel AVR microcontrollers are Harvard architecture processors.
Relays and vacuum tubes (thermionic valves) were commonly used as switching elements; a useful computer requires thousands or tens of thousands of switching devices. The overall speed of a system is dependent on the speed of the switches. Tube computers like EDVAC tended to average eight hours between failures, whereas relay computers like the (slower, but earlier) Harvard Mark I failed very rarely. In the end, tube-based CPUs became dominant because the significant speed advantages afforded generally outweighed the reliability problems. Most of these early synchronous CPUs ran at low clock rates compared to modern microelectronic designs (see below for a discussion of clock rate). Clock signal frequencies ranging from 100 kHz to 4 MHz were very common at this time, limited largely by the speed of the switching devices they were built with.
Transistor and integrated circuit CPUs.
The design complexity of CPUs increased as various technologies facilitated building smaller and more reliable electronic devices. The first such improvement came with the advent of the transistor. Transistorized CPUs during the 1950s and 1960s no longer had to be built out of bulky, unreliable, and fragile switching elements like vacuum tubes and electrical relays. With this improvement more complex and reliable CPUs were built onto one or several printed circuit boards containing discrete (individual) components.
During this period, a method of manufacturing many interconnected transistors in a compact space was developed. The integrated circuit (IC) allowed a large number of transistors to be manufactured on a single semiconductor-based die, or "chip". At first only very basic non-specialized digital circuits such as NOR gates were miniaturized into ICs. CPUs based upon these "building block" ICs are generally referred to as "small-scale integration" (SSI) devices. SSI ICs, such as the ones used in the Apollo guidance computer, usually contained up to a few score transistors. To build an entire CPU out of SSI ICs required thousands of individual chips, but still consumed much less space and power than earlier discrete transistor designs. As microelectronic technology advanced, an increasing number of transistors were placed on ICs, thus decreasing the quantity of individual ICs needed for a complete CPU. MSI and LSI (medium- and large-scale integration) ICs increased transistor counts to hundreds, and then thousands.
In 1964 IBM introduced its System/360 computer architecture which was used in a series of computers that could run the same programs with different speed and performance. This was significant at a time when most electronic computers were incompatible with one another, even those made by the same manufacturer. To facilitate this improvement, IBM utilized the concept of a microprogram (often called "microcode"), which still sees widespread usage in modern CPUs. The System/360 architecture was so popular that it dominated the mainframe computer market for decades and left a legacy that is still continued by similar modern computers like the IBM zSeries. In the same year (1964), Digital Equipment Corporation (DEC) introduced another influential computer aimed at the scientific and research markets, the PDP-8. DEC would later introduce the extremely popular PDP-11 line that originally was built with SSI ICs but was eventually implemented with LSI components once these became practical. In stark contrast with its SSI and MSI predecessors, the first LSI implementation of the PDP-11 contained a CPU composed of only four LSI integrated circuits.
Transistor-based computers had several distinct advantages over their predecessors. Aside from facilitating increased reliability and lower power consumption, transistors also allowed CPUs to operate at much higher speeds because of the short switching time of a transistor in comparison to a tube or relay. Thanks to both the increased reliability as well as the dramatically increased speed of the switching elements (which were almost exclusively transistors by this time), CPU clock rates in the tens of megahertz were obtained during this period. Additionally while discrete transistor and IC CPUs were in heavy usage, new high-performance designs like SIMD (Single Instruction Multiple Data) vector processors began to appear. These early experimental designs later gave rise to the era of specialized supercomputers like those made by Cray Inc.
Microprocessors.
In the 1970s the fundamental inventions by Federico Faggin (Silicon Gate MOS ICs with self-aligned gates along with his new random logic design methodology) changed the design and implementation of CPUs forever. Since the introduction of the first commercially available microprocessor (the Intel 4004) in 1970, and the first widely used microprocessor (the Intel 8080) in 1974, this class of CPUs has almost completely overtaken all other central processing unit implementation methods. Mainframe and minicomputer manufacturers of the time launched proprietary IC development programs to upgrade their older computer architectures, and eventually produced instruction set compatible microprocessors that were backward-compatible with their older hardware and software. Combined with the advent and eventual success of the ubiquitous personal computer, the term "CPU" is now applied almost exclusively to microprocessors. Several CPUs (denoted 'cores') can be combined in a single processing chip.
Previous generations of CPUs were implemented as discrete components and numerous small integrated circuits (ICs) on one or more circuit boards. Microprocessors, on the other hand, are CPUs manufactured on a very small number of ICs; usually just one. The overall smaller CPU size as a result of being implemented on a single die means faster switching time because of physical factors like decreased gate parasitic capacitance. This has allowed synchronous microprocessors to have clock rates ranging from tens of megahertz to several gigahertz. Additionally, as the ability to construct exceedingly small transistors on an IC has increased, the complexity and number of transistors in a single CPU has increased many fold. This widely observed trend is described by Moore's law, which has proven to be a fairly accurate predictor of the growth of CPU (and other IC) complexity.
While the complexity, size, construction, and general form of CPUs have changed enormously since 1950, it is notable that the basic design and function has not changed much at all. Almost all common CPUs today can be very accurately described as von Neumann stored-program machines. As the aforementioned Moore's law continues to hold true, concerns have arisen about the limits of integrated circuit transistor technology. Extreme miniaturization of electronic gates is causing the effects of phenomena like electromigration and subthreshold leakage to become much more significant. These newer concerns are among the many factors causing researchers to investigate new methods of computing such as the quantum computer, as well as to expand the usage of parallelism and other methods that extend the usefulness of the classical von Neumann model.
Operation.
The fundamental operation of most CPUs, regardless of the physical form they take, is to execute a sequence of stored instructions called a program. The instructions are kept in some kind of computer memory. There are four steps that nearly all CPUs use in their operation: fetch, decode, execute, and writeback.
The first step, fetch, involves retrieving an instruction (which is represented by a number or sequence of numbers) from program memory. Its location (address) in program memory is determined by a program counter (PC), which stores a number that identifies the address of the next instruction to be fetched. After an instruction is fetched, the PC is incremented by the length of the instruction word in terms of memory units. Often, the instruction to be fetched must be retrieved from relatively slow memory, causing the CPU to stall while waiting for the instruction to be returned. This issue is largely addressed in modern processors by caches and pipeline architectures (see below).
The instruction that the CPU fetches from memory is used to determine what the CPU is to do. In the decode step, the instruction is broken up into parts that have significance to other portions of the CPU. The way in which the numerical instruction value is interpreted is defined by the CPU's instruction set architecture (ISA). Often, one group of numbers in the instruction, called the opcode, indicates which operation to perform. The remaining parts of the number usually provide information required for that instruction, such as operands for an addition operation. Such operands may be given as a constant value (called an immediate value), or as a place to locate a value: a register or a memory address, as determined by some addressing mode. In older designs the portions of the CPU responsible for instruction decoding were unchangeable hardware devices. However, in more abstract and complicated CPUs and ISAs, a microprogram is often used to assist in translating instructions into various configuration signals for the CPU. This microprogram is sometimes rewritable so that it can be modified to change the way the CPU decodes instructions even after it has been manufactured.
After the fetch and decode steps, the execute step is performed. During this step, various portions of the CPU are connected so they can perform the desired operation. If, for instance, an addition operation was requested, the arithmetic logic unit (ALU) will be connected to a set of inputs and a set of outputs. The inputs provide the numbers to be added, and the outputs will contain the final sum. The ALU contains the circuitry to perform simple arithmetic and logical operations on the inputs (like addition and bitwise operations). If the addition operation produces a result too large for the CPU to handle, an arithmetic overflow flag in a flags register may also be set.
The final step, writeback, simply "writes back" the results of the execute step to some form of memory. Very often the results are written to some internal CPU register for quick access by subsequent instructions. In other cases results may be written to slower, but cheaper and larger, main memory. Some types of instructions manipulate the program counter rather than directly produce result data. These are generally called "jumps" and facilitate behavior like loops, conditional program execution (through the use of a conditional jump), and functions in programs. Many instructions will also change the state of digits in a "flags" register. These flags can be used to influence how a program behaves, since they often indicate the outcome of various operations. For example, one type of "compare" instruction considers two values and sets a number in the flags register according to which one is greater. This flag could then be used by a later jump instruction to determine program flow.
After the execution of the instruction and writeback of the resulting data, the entire process repeats, with the next instruction cycle normally fetching the next-in-sequence instruction because of the incremented value in the program counter. If the completed instruction was a jump, the program counter will be modified to contain the address of the instruction that was jumped to, and program execution continues normally. In more complex CPUs than the one described here, multiple instructions can be fetched, decoded, and executed simultaneously. This section describes what is generally referred to as the "classic RISC pipeline", which in fact is quite common among the simple CPUs used in many electronic devices (often called microcontroller). It largely ignores the important role of CPU cache, and therefore the access stage of the pipeline.
Design and implementation.
The basic concept of a CPU is as follows:
Hardwired into a CPU's design is a list of basic operations it can perform, called an instruction set. Such operations may include adding or subtracting two numbers, comparing numbers, or jumping to a different part of a program. Each of these basic operations is represented by a particular sequence of bits; this sequence is called the opcode for that particular operation. Sending a particular opcode to a CPU will cause it to perform the operation represented by that opcode. To execute an instruction in a computer program, the CPU uses the opcode for that instruction as well as its arguments (for instance the two numbers to be added, in the case of an addition operation). A computer program is therefore a sequence of instructions, with each instruction including an opcode and that operation's arguments.
The actual mathematical operation for each instruction is performed by a subunit of the CPU known as the arithmetic logic unit or ALU. In addition to using its ALU to perform operations, a CPU is also responsible for reading the next instruction from memory, reading data specified in arguments from memory, and writing results to memory.
In many CPU designs, an instruction set will clearly differentiate between operations that load data from memory, and those that perform math. In this case the data loaded from memory is stored in registers, and a mathematical operation takes no arguments but simply performs the math on the data in the registers and writes it to a new register, whose value a separate operation may then write to memory.
Control unit.
The control unit of the CPU contains circuitry that uses electrical signals to direct the entire computer system to carry out stored program instructions. The control unit does not execute program instructions; rather, it directs other parts of the system to do so. The control unit must communicate with both the arithmetic/logic unit and memory.
Integer range.
The way a CPU represents numbers is a design choice that affects the most basic ways in which the device functions. Some early digital computers used an electrical model of the common decimal (base ten) numeral system to represent numbers internally. A few other computers have used more exotic numeral systems like ternary (base three). Nearly all modern CPUs represent numbers in binary form, with each digit being represented by some two-valued physical quantity such as a "high" or "low" voltage.
Related to number representation is the size and precision of numbers that a CPU can represent. In the case of a binary CPU, a "bit" refers to one significant place in the numbers a CPU deals with. The number of bits (or numeral places) a CPU uses to represent numbers is often called "word size", "bit width", "data path width", or "integer precision" when dealing with strictly integer numbers (as opposed to floating point). This number differs between architectures, and often within different parts of the very same CPU. For example, an 8-bit CPU deals with a range of numbers that can be represented by eight binary digits (each digit having two possible values), that is, 28 or 256 discrete numbers. In effect, integer size sets a hardware limit on the range of integers the software run by the CPU can utilize.
Integer range can also affect the number of locations in memory the CPU can address (locate). For example, if a binary CPU uses 32 bits to represent a memory address, and each memory address represents one octet (8 bits), the maximum quantity of memory that CPU can address is 232 octets, or 4 GiB. This is a very simple view of CPU address space, and many designs use more complex addressing methods like paging to locate more memory than their integer range would allow with a flat address space.
Higher levels of integer range require more structures to deal with the additional digits, and therefore more complexity, size, power usage, and general expense. It is not at all uncommon, therefore, to see 4- or 8-bit microcontrollers used in modern applications, even though CPUs with much higher range (such as 16, 32, 64, even 128-bit) are available. The simpler microcontrollers are usually cheaper, use less power, and therefore generate less heat, all of which can be major design considerations for electronic devices. However, in higher-end applications, the benefits afforded by the extra range (most often the additional address space) are more significant and often affect design choices. To gain some of the advantages afforded by both lower and higher bit lengths, many CPUs are designed with different bit widths for different portions of the device. For example, the IBM System/370 used a CPU that was primarily 32 bit, but it used 128-bit precision inside its floating point units to facilitate greater accuracy and range in floating point numbers. Many later CPU designs use similar mixed bit width, especially when the processor is meant for general-purpose usage where a reasonable balance of integer and floating point capability is required.
Clock rate.
The clock rate is the speed at which a microprocessor executes instructions. Every computer contains an internal clock that regulates the rate at which instructions are executed and synchronizes all the various computer components. The faster the clock, the more instructions the CPU can execute per second.
Most CPUs, and indeed most sequential logic devices, are synchronous in nature. That is, they are designed with, and operate under, assumptions about a synchronization signal. This signal, known as a clock signal, usually takes the form of a periodic square wave. By calculating the maximum time that electrical signals can move in various branches of a CPU's many circuits, the designers can select an appropriate period for the clock signal.
This period must be longer than the amount of time it takes for a signal to move, or propagate, in the worst-case scenario. In setting the clock period to a value well above the worst-case propagation delay, it is possible to design the entire CPU and the way it moves data around the "edges" of the rising and falling clock signal. This has the advantage of simplifying the CPU significantly, both from a design perspective and a component-count perspective. However, it also carries the disadvantage that the entire CPU must wait on its slowest elements, even though some portions of it are much faster. This limitation has largely been compensated for by various methods of increasing CPU parallelism (see below).
However, architectural improvements alone do not solve all of the drawbacks of globally synchronous CPUs. For example, a clock signal is subject to the delays of any other electrical signal. Higher clock rates in increasingly complex CPUs make it more difficult to keep the clock signal in phase (synchronized) throughout the entire unit. This has led many modern CPUs to require multiple identical clock signals to be provided to avoid delaying a single signal significantly enough to cause the CPU to malfunction. Another major issue as clock rates increase dramatically is the amount of heat that is dissipated by the CPU. The constantly changing clock causes many components to switch regardless of whether they are being used at that time. In general, a component that is switching uses more energy than an element in a static state. Therefore, as clock rate increases, so does energy consumption, causing the CPU to require more heat dissipation in the form of CPU cooling solutions.
One method of dealing with the switching of unneeded components is called clock gating, which involves turning off the clock signal to unneeded components (effectively disabling them). However, this is often regarded as difficult to implement and therefore does not see common usage outside of very low-power designs. One notable late CPU design that uses extensive clock gating to reduce the power requirements of the videogame console is that of the IBM PowerPC-based Xbox 360. Another method of addressing some of the problems with a global clock signal is the removal of the clock signal altogether. While removing the global clock signal makes the design process considerably more complex in many ways, asynchronous (or clockless) designs carry marked advantages in power consumption and heat dissipation in comparison with similar synchronous designs. While somewhat uncommon, entire asynchronous CPUs have been built without utilizing a global clock signal. Two notable examples of this are the ARM compliant AMULET and the MIPS R3000 compatible MiniMIPS. Rather than totally removing the clock signal, some CPU designs allow certain portions of the device to be asynchronous, such as using asynchronous ALUs in conjunction with superscalar pipelining to achieve some arithmetic performance gains. While it is not altogether clear whether totally asynchronous designs can perform at a comparable or better level than their synchronous counterparts, it is evident that they do at least excel in simpler math operations. This, combined with their excellent power consumption and heat dissipation properties, makes them very suitable for embedded computers.
Parallelism.
The description of the basic operation of a CPU offered in the previous section describes the simplest form that a CPU can take. This type of CPU, usually referred to as "subscalar", operates on and executes one instruction on one or two pieces of data at a time.
This process gives rise to an inherent inefficiency in subscalar CPUs. Since only one instruction is executed at a time, the entire CPU must wait for that instruction to complete before proceeding to the next instruction. As a result, the subscalar CPU gets "hung up" on instructions which take more than one clock cycle to complete execution. Even adding a second execution unit (see below) does not improve performance much; rather than one pathway being hung up, now two pathways are hung up and the number of unused transistors is increased. This design, wherein the CPU's execution resources can operate on only one instruction at a time, can only possibly reach "scalar" performance (one instruction per clock). However, the performance is nearly always subscalar (less than one instruction per cycle).
Attempts to achieve scalar and better performance have resulted in a variety of design methodologies that cause the CPU to behave less linearly and more in parallel. When referring to parallelism in CPUs, two terms are generally used to classify these design techniques. Instruction level parallelism (ILP) seeks to increase the rate at which instructions are executed within a CPU (that is, to increase the utilization of on-die execution resources), and thread level parallelism (TLP) purposes to increase the number of threads (effectively individual programs) that a CPU can execute simultaneously. Each methodology differs both in the ways in which they are implemented, as well as the relative effectiveness they afford in increasing the CPU's performance for an application.
Instruction level parallelism.
One of the simplest methods used to accomplish increased parallelism is to begin the first steps of instruction fetching and decoding before the prior instruction finishes executing. This is the simplest form of a technique known as instruction pipelining, and is utilized in almost all modern general-purpose CPUs. Pipelining allows more than one instruction to be executed at any given time by breaking down the execution pathway into discrete stages. This separation can be compared to an assembly line, in which an instruction is made more complete at each stage until it exits the execution pipeline and is retired.
Pipelining does, however, introduce the possibility for a situation where the result of the previous operation is needed to complete the next operation; a condition often termed data dependency conflict. To cope with this, additional care must be taken to check for these sorts of conditions and delay a portion of the instruction pipeline if this occurs. Naturally, accomplishing this requires additional circuitry, so pipelined processors are more complex than subscalar ones (though not very significantly so). A pipelined processor can become very nearly scalar, inhibited only by pipeline stalls (an instruction spending more than one clock cycle in a stage).
Further improvement upon the idea of instruction pipelining led to the development of a method that decreases the idle time of CPU components even further. Designs that are said to be "superscalar" include a long instruction pipeline and multiple identical execution units. In a superscalar pipeline, multiple instructions are read and passed to a dispatcher, which decides whether or not the instructions can be executed in parallel (simultaneously). If so they are dispatched to available execution units, resulting in the ability for several instructions to be executed simultaneously. In general, the more instructions a superscalar CPU is able to dispatch simultaneously to waiting execution units, the more instructions will be completed in a given cycle.
Most of the difficulty in the design of a superscalar CPU architecture lies in creating an effective dispatcher. The dispatcher needs to be able to quickly and correctly determine whether instructions can be executed in parallel, as well as dispatch them in such a way as to keep as many execution units busy as possible. This requires that the instruction pipeline is filled as often as possible and gives rise to the need in superscalar architectures for significant amounts of CPU cache. It also makes hazard-avoiding techniques like branch prediction, speculative execution, and out-of-order execution crucial to maintaining high levels of performance. By attempting to predict which branch (or path) a conditional instruction will take, the CPU can minimize the number of times that the entire pipeline must wait until a conditional instruction is completed. Speculative execution often provides modest performance increases by executing portions of code that may not be needed after a conditional operation completes. Out-of-order execution somewhat rearranges the order in which instructions are executed to reduce delays due to data dependencies. Also in case of Single Instructions Multiple Data — a case when a lot of data from the same type has to be processed, modern processors can disable parts of the pipeline so that when a single instruction is executed many times, the CPU skips the fetch and decode phases and thus greatly increases performance on certain occasions, especially in highly monotonous program engines such as video creation software and photo processing.
In the case where a portion of the CPU is superscalar and part is not, the part which is not suffers a performance penalty due to scheduling stalls. The Intel P5 Pentium had two superscalar ALUs which could accept one instruction per clock each, but its FPU could not accept one instruction per clock. Thus the P5 was integer superscalar but not floating point superscalar. Intel's successor to the P5 architecture, P6, added superscalar capabilities to its floating point features, and therefore afforded a significant increase in floating point instruction performance.
Both simple pipelining and superscalar design increase a CPU's ILP by allowing a single processor to complete execution of instructions at rates surpassing one instruction per cycle (IPC). Most modern CPU designs are at least somewhat superscalar, and nearly all general purpose CPUs designed in the last decade are superscalar. In later years some of the emphasis in designing high-ILP computers has been moved out of the CPU's hardware and into its software interface, or ISA. The strategy of the very long instruction word (VLIW) causes some ILP to become implied directly by the software, reducing the amount of work the CPU must perform to boost ILP and thereby reducing the design's complexity.
Thread-level parallelism.
Another strategy of achieving performance is to execute multiple programs or threads in parallel.
This area of research is known as parallel computing. In Flynn's taxonomy, this strategy is known as Multiple Instructions-Multiple Data or MIMD.
One technology used for this purpose was multiprocessing (MP). The initial flavor of this technology is known as symmetric multiprocessing (SMP), where a small number of CPUs share a coherent view of their memory system. In this scheme, each CPU has additional hardware to maintain a constantly up-to-date view of memory. By avoiding stale views of memory, the CPUs can cooperate on the same program and programs can migrate from one CPU to another. To increase the number of cooperating CPUs beyond a handful, schemes such as non-uniform memory access (NUMA) and directory-based coherence protocols were introduced in the 1990s. SMP systems are limited to a small number of CPUs while NUMA systems have been built with thousands of processors. Initially, multiprocessing was built using multiple discrete CPUs and boards to implement the interconnect between the processors. When the processors and their interconnect are all implemented on a single silicon chip, the technology is known as a multi-core processor.
It was later recognized that finer-grain parallelism existed with a single program. A single program might have several threads (or functions) that could be executed separately or in parallel. Some of the earliest examples of this technology implemented input/output processing such as direct memory access as a separate thread from the computation thread. A more general approach to this technology was introduced in the 1970s when systems were designed to run multiple computation threads in parallel. This technology is known as multi-threading (MT). This approach is considered more cost-effective than multiprocessing, as only a small number of components within a CPU is replicated to support MT as opposed to the entire CPU in the case of MP. In MT, the execution units and the memory system including the caches are shared among multiple threads. The downside of MT is that the hardware support for multithreading is more visible to software than that of MP and thus supervisor software like operating systems have to undergo larger changes to support MT. One type of MT that was implemented is known as block multithreading, where one thread is executed until it is stalled waiting for data to return from external memory. In this scheme, the CPU would then quickly switch to another thread which is ready to run, the switch often done in one CPU clock cycle, such as the UltraSPARC Technology. Another type of MT is known as simultaneous multithreading, where instructions of multiple threads are executed in parallel within one CPU clock cycle.
For several decades from the 1970s to early 2000s, the focus in designing high performance general purpose CPUs was largely on achieving high ILP through technologies such as pipelining, caches, superscalar execution, out-of-order execution, etc. This trend culminated in large, power-hungry CPUs such as the Intel Pentium 4. By the early 2000s, CPU designers were thwarted from achieving higher performance from ILP techniques due to the growing disparity between CPU operating frequencies and main memory operating frequencies as well as escalating CPU power dissipation owing to more esoteric ILP techniques.
CPU designers then borrowed ideas from commercial computing markets such as transaction processing, where the aggregate performance of multiple programs, also known as throughput computing, was more important than the performance of a single thread or program.
This reversal of emphasis is evidenced by the proliferation of dual and multiple core CMP (chip-level multiprocessing) designs and notably, Intel's newer designs resembling its less superscalar P6 architecture. Late designs in several processor families exhibit CMP, including the x86-64 Opteron and Athlon 64 X2, the SPARC UltraSPARC T1, IBM POWER4 and POWER5, as well as several video game console CPUs like the Xbox 360's triple-core PowerPC design, and the PS3's 7-core Cell microprocessor.
Data parallelism.
A less common but increasingly important paradigm of CPUs (and indeed, computing in general) deals with data parallelism. The processors discussed earlier are all referred to as some type of scalar device. As the name implies, vector processors deal with multiple pieces of data in the context of one instruction. This contrasts with scalar processors, which deal with one piece of data for every instruction. Using Flynn's taxonomy, these two schemes of dealing with data are generally referred to as SIMD (single instruction, multiple data) and SISD (single instruction, single data), respectively. The great utility in creating CPUs that deal with vectors of data lies in optimizing tasks that tend to require the same operation (for example, a sum or a dot product) to be performed on a large set of data. Some classic examples of these types of tasks are multimedia applications (images, video, and sound), as well as many types of scientific and engineering tasks. Whereas a scalar CPU must complete the entire process of fetching, decoding, and executing each instruction and value in a set of data, a vector CPU can perform a single operation on a comparatively large set of data with one instruction. Of course, this is only possible when the application tends to require many steps which apply one operation to a large set of data.
Most early vector CPUs, such as the Cray-1, were associated almost exclusively with scientific research and cryptography applications. However, as multimedia has largely shifted to digital media, the need for some form of SIMD in general-purpose CPUs has become significant. Shortly after inclusion of floating point execution units started to become commonplace in general-purpose processors, specifications for and implementations of SIMD execution units also began to appear for general-purpose CPUs. Some of these early SIMD specifications like HP's Multimedia Acceleration eXtensions (MAX) and Intel's MMX were integer-only. This proved to be a significant impediment for some software developers, since many of the applications that benefit from SIMD primarily deal with floating point numbers. Progressively, these early designs were refined and remade into some of the common, modern SIMD specifications, which are usually associated with one ISA. Some notable modern examples are Intel's SSE and the PowerPC-related AltiVec (also known as VMX).
Performance.
The "performance" or "speed" of a processor depends on, among many other factors, the clock rate (generally given in multiples of hertz) and the instructions per clock (IPC), which together are the factors for the instructions per second (IPS) that the CPU can perform.
Many reported IPS values have represented "peak" execution rates on artificial instruction sequences with few branches, whereas realistic workloads consist of a mix of instructions and applications, some of which take longer to execute than others. The performance of the memory hierarchy also greatly affects processor performance, an issue barely considered in MIPS calculations. Because of these problems, various standardized tests, often called "benchmarks" for this purpose—such as SPECint – have been developed to attempt to measure the real effective performance in commonly used applications.
Processing performance of computers is increased by using multi-core processors, which essentially is plugging two or more individual processors (called "cores" in this sense) into one integrated circuit. Ideally, a dual core processor would be nearly twice as powerful as a single core processor. In practice, however, the performance gain is far less, only about 50%, due to imperfect software algorithms and implementation. Increasing the number of cores in a processor (i.e. dual-core, quad-core, etc.) increases the workload that can be handled. This means that the processor can now handle numerous asynchronous events, interrupts, etc. which can take a toll on the CPU (Central Processing Unit) when overwhelmed. These cores can be thought of as different floors in a processing plant, with each floor handling a different task. Sometimes, these cores will handle the same tasks as cores adjacent to them if a single core is not enough to handle the information.

</doc>
<doc id="5221" url="http://en.wikipedia.org/wiki?curid=5221" title="Carnivora">
Carnivora

Carnivora ( or ; from Latin "carō" (stem "carn-") "flesh", + "vorāre" "to devour") is a diverse order that includes over 280 species of placental mammals. Its members are formally referred to as carnivorans, whereas the word "carnivore" (often popularly applied to members of this group) can refer to any meat-eating organism. Carnivorans are the most diverse in size of any mammalian order, ranging from the least weasel ("Mustela nivalis"), at as little as and , to the polar bear ("Ursus maritimus"), which can weigh up to , to the southern elephant seal ("Mirounga leonina"), whose adult males weigh up to and measure up to in length.
The first carnivoran was a carnivore, and nearly all carnivorans today primarily eat meat. Some, such as cats and pinnipeds, depend entirely on meat for their nutrition. Others, such as raccoons and bears, depending on the local habitat, are more omnivorous: the giant panda is almost exclusively a herbivore, but will take fish, eggs and insects, while the polar bear's harsh habitat forces it to subsist mainly on prey. Carnivorans have teeth, claws, and binocular vision adapted for catching and eating other animals. Many hunt in packs and are social animals, giving them an advantage over larger prey.
Carnivorans apparently evolved in North America out of members of the family Miacidae (miacids) about 42 million years ago. They soon split into cat-like and dog-like forms (Feliformia and Caniformia). Their molecular phylogeny shows the extant Carnivora are a monophyletic group, the crown-group of the Carnivoramorpha.
Distinguishing features.
Most carnivorans are terrestrial; they usually have strong, sharp claws, with never fewer than four toes on each foot, and well-developed, prominent canine teeth, cheek teeth (premolars, and molars) that generally have cutting edges. The last premolar of the upper jaw and first molar of the lower are termed the carnassials or sectorial teeth. These blade-like teeth occlude (close) with a scissor-like action for shearing and shredding meat. Carnassials are most highly developed in the Felidae and the least developed in the Ursidae. Carnivorans have six incisors and two conical canines in each jaw. The only two exceptions to this are the sea otter ("Enhydra lutris"), which has four incisors in the lower jaw, and the sloth bear ("Melursus ursinus"), which has four incisors in the upper jaw. The number of molars and premolars is variable between carnivoran species, but all teeth are deeply rooted and are diphyodont. Incisors are retained by carnivorans and the third incisor is commonly large and sharp (canine-like). Carnivorans have either four or five digits on each foot, with the first digit on the forepaws, also known as the dew claw, being vestigial in most species and absent in some.
The Canoidea superfamily (or Caniformia suborder) – Canidae (wolves, dogs and foxes), Mephitidae (skunks and stink badgers), Mustelidae (weasels, badgers, and otters), Procyonidae (raccoons), Ursidae (bears), Otariidae (eared seals), Odobenidae (walrus), and Phocidae (earless seals) (the last three families formerly classified in the suborder Pinnipedia) and the extinct family Amphicyonidae (bear-dogs) – are characterized by having nonchambered or partially chambered auditory bullae, nonretractable claws, and a well-developed baculum. Most species are rather simply colored, lacking the flashy spotted or rosetted coats like many species of felids and viverrids have. This is because Canoidea tend to range in the temperate and subarctic biomes, although Mustelidae and Procyonidae have a few tropical species. Most are terrestrial, although a few species, like procyonids, are arboreal. All families except the Canidae and a few species of Mustelidae are plantigrade. Diet is varied and most tend to be omnivorous to some degree, and thus the carnassial teeth are less specialized. Canoidea have more premolars and molars in an elongated skull.
The Feloidea superfamily (or Feliformia suborder)– Felidae (cats), Prionodontidae (Asiatic linsangs), Herpestidae (mongooses), Hyaenidae (hyenas), Viverridae (civets), and Eupleridae (Malagasy carnivorans), as well as the extinct family Nimravidae (paleofelids) – often have spotted, rosetted or striped coats, and tend to be more brilliantly colored than their Canoidean counterparts. This is because these species tend to range in tropical habitats, although a few species do inhabit temperate and subarctic habitats. Many are arboreal or semiarboreal, and the majority are digitigrade. Diet tends to be more strictly carnivorous, especially in the Felidae family. They have fewer teeth and shorter skulls, with much more specialized carnassials meant for shearing meat. Felidae claws are retractile, or rarely, semiretractile. The terminal phalanx, with the claw attached, folds back in the forefoot into a sheath by the outer side of the middle phalanx of the digit, and is retained in this position when at rest by a strong elastic ligament. In the hindfoot, the terminal joint or phalanx is retracted on to the top, and not the side of the middle phalanx. Deep flexor muscles straighten the terminal phalanges, so the claws protrude from their sheaths, and the soft "velvety" paw becomes suddenly converted into a formidable weapon. The habitual retraction of the claws preserves their points from wear.
The Pinnipedia superfamily (walruses, seals, and sea lions), now considered to be part of Caniformia, are medium to large (to 6.5 m) aquatic mammals. Being homeothermic (warm-blooded) marine mammals, pinnipeds need a low surface area to body mass ratio. Otherwise, they would suffer from excessive heat loss due to water's high capacity for heat conduction. The body is usually insulated with a thick layer of fat called blubber and typically covered with hair. The digits are not separate, but connected by a thick web that forms flippers for swimming; thus, the forelimbs and hindlimbs are transformed into paddles. This enables them to dive at extreme depths (600 m for the Weddell seal). They can remain underwater for long periods of time, sometimes an hour or more, but most dives are usually short. The facial region of skull is relatively small, with pinnae very small or lacking, and the vibrissae are well developed. The molariform teeth are mostly homodont and the canines are well developed. The tail is very short or absent, the ears are small or absent as well, and the external genitalia are hidden in slits or depressions in the body.
Skull structure.
Members of Carnivora have a characteristic skull shape with relatively large brains encased in a heavy skull. The skull has a highly developed zygomatic arch just behind the maxilla (common to all mammals and their cynodont forebears), and they have ossified external auditory bullae. Feloidea have a two-chambered auditory bulla. In addition to allowing extra room for the passage of muscles to work the lower jaw, the zygomatic arch also allows for differentiation of separate muscle groups to be involved in biting and chewing. Masseters attach from the dentary (specifically, the masseteric fossa) to the zygomatic arch and onto the maxilla in front of the arch, providing crushing force. The temporalis attaches from the dentary (specifically, the coronoid process) to the side of the braincase, providing torque about the axis of jaw articulation. In comparing the skulls of carnivores and herbivores, it can be seen that the shearing force of the temporalis is somewhat more important to carnivores, which have more room on the braincase (this is not unrelated to carnivoran intelligence) and commonly develop a sagittal crest (running from posterior to anterior on the skull), providing yet additional room for temporalis attachment. Carnivoran jaws can only move on a vertical axis, in an up-and-down motion, and cannot move from side-to-side. The jaw joint in carnivores tends to lie within the plane of tooth occlusion, an arrangement that further emphasizes shearing (as in a pair of scissors). In herbivores, the crushing force of the masseters is relatively more important than is shearing. The jaw joint is generally well above the plane of tooth occlusion, allowing extra room for masseteric attachment on the dentary and causing the rotation of the lower jaw to be translated into straight-ahead crushing force between the teeth of the upper and lower jaws.
Physiology.
Carnivora have a simple stomach adapted to digest primarily meat, as compared to the elaborate digestive systems of herbivorous animals, which are necessary to break down tough, complex plant fibers. The caecum is either absent or short and simple, and the colon is not sacculated or much wider than the small intestine. Most species of Carnivora are, to some degree, omnivorous, except the Felidae and Pinnipedia, which are obligate carnivores. Most have highly developed senses, especially vision and hearing, and often a highly acute sense of smell in many species, such as in the Canoidea. They are excellent runners: some are long-distance runners, but more commonly are sprinters. Even bears and raccoons, although seemingly slow and clumsy, are capable of remarkable bursts of speed.
Diet specializations.
Carnivorans include carnivores, omnivores, and even a few primarily herbivorous species, such as the giant panda and the binturong. Important teeth for carnivorans are the large, slightly recurved canines, used to dispatch prey, and the carnassial complex, used to rend meat from bone and slice it into digestible pieces. Dogs have molar teeth behind the carnassials for crushing bones, but cats have only a greatly reduced, functionless molar behind the carnassial in the upper jaw. Cats will strip bones clean but will not crush them to get the marrow inside. Omnivores, such as bears and raccoons, have developed blunt, molar-like carnassials. Carnassials are a key adaptation for terrestrial vertebrate predation; all other placental orders are primarily herbivores, insectivores, or aquatic.
Reproductive system.
Carnivorans tend to produce a single litter annually, but some produce multiple litters a year, and larger carnivorans, like bears, have gaps of 2–3 yr between litters. The average gestation period lies between 50 and 115 days, although the ursids and mustelids have delayed implantation, thus extending the gestation period six to 9 months beyond the normal period. Litter sizes are usually small, ranging from one to 13 young, which are born with underdeveloped eyes and ears. In most species, the mother has exclusive or at least primary care of the offspring. Many species of carnivorans are solitary, but a few are gregarious.
Phylogeny.
Carnivorans evolved from members of the paraphyletic family Miacidae (miacids). The transition from Miacidae to Carnivora was a general trend in the middle and late Eocene, with taxa from both North America and Eurasia involved. The divergence of carnivorans from other miacids, as well as the divergence of the two clades within Carnivora, Caniformia and Feliformia, is now inferred to have happened in the middle Eocene, about 42 million years ago (mya). Traditionally, the extinct family Viverravidae (viverravids) had been thought to be the earliest carnivorans, with fossil records first appearing in the Paleocene of North America about 60 mya, but recently described evidence from cranial morphology now places them outside the order Carnivora.
The Miacidae are not a monophyletic group, but a paraphyletic array of stem taxa. Today, Carnivora is restricted to the crown group, Carnivora and miacoids are grouped in the clade Carnivoramorpha, and the miacoids are regarded as basal carnivoramorphs. Based on dental features and braincase sizes, it is now known that Carnivora must have evolved from a form even more primitive than Creodonta, and thus these two orders may not even be sister groups. The Carnivora, Creodonta, Pholidota, and a few other extinct orders are informally grouped together in the clade Ferae. Older classification schemes divided the order into two suborders: Fissipedia (which included the families of primarily land Carnivora) and Pinnipedia (which included the true seals, eared seals, and walrus). However, it is now recognized that the Fissipedia is a paraphyletic group and that the pinnipeds were not the sister group to the fissipeds but rather had arisen from among them.
Carnivora are generally divided into the suborders Feliformia (cat-like) and Caniformia (dog-like), the latter of which includes the pinnipeds. The pinnipeds are part of a clade, known as the Arctoidea, which also includes the Ursidae (bears) and the superfamily Musteloidea. The Musteloidea in turn consists of the Mustelidae (mustelids: weasels), Procyonidae (procyonids: raccoons), Mephitidae (skunks) and "Ailurus" (red panda). The oldest caniforms are the "Miacis" species "Miacis cognitus", the Amphicyonidae (bear-dogs) such as "Daphoenus", and "Hesperocyon" (of the family Canidae, subfamily Hesperocyoninae). Hesperocyonine canids first appeared in North America, and the earliest species is currently dated at 39.74 mya, but they were not represented in Europe until well into the Miocene, and not into Asia and Africa until the Pliocene. "Miacis" and Amphicyonidae were the first of the caniforms to split from the others and are sometimes considered to be sister groups to Ursidae, but the exact closeness of Amphicyonidae and Ursidae, as well as Arctoidae to Ursidae, is still uncertain. The Canidae (wolves, coyotes, jackals, foxes and dogs) are generally considered to be the sister group to Arctoidea. The Ursidae first occur in North America in the Late Eocene (ca. 38 mya) as the very small and graceful "Parictis" that had a skull only 7 cm long. Like the canids, this family does not appear in Eurasia and Africa until the Miocene. The other caniform families Amphicyonidae, Mustelidae and Procyonidae occur in both the Old World and the New World by the Late Eocene and Early Oligocene.
The ancestor of all Feliformia evolved from the Caniformia-Feliformia split. "Nandinia", the African palm civet, seems to be the most primitive of all the feliforms and the very first to split from the others. The Asiatic linsangs of the genus "Prionodon" (traditionally placed in the Viverridae) form a family of their own, as some recent studies indicate that "Prionodon" is actually the closest living relative to the cats. The Nimravidae are sometimes seen as the most basal of all feliforms and the first to split from the others, but there is a possibility that Nimravidae might not even belong within the order, and therefore its position as a clade within Carnivora is currently unstable. Other studies indicate that the barbourofelids form a separate family, which is closely related to the true felids instead of being related to the nimravids. Recognizable nimravid fossils date from the late Eocene (37 mya), from the Chadronian White River Carnivora Formation at Flagstaff Rim, Wyoming. Nimravid diversity appears to have peaked about 28 mya. The hypercarnivorous (strictly meat-eating) nimravid feliforms were extinct in North America after 26 mya and felids did not arrive in North America until the early middle Miocene (16 mya).
It has been suggested that canids evolved hypercarnivorous morphologies because feliforms were absent during this period (the "cat-gap", 26-18.5 mya), however recent data do not support this hypothesis. Hypercarnivore feliforms (felids and nimravids) occupied an area that canids did not and where felids, nimravids, and hypercarnivorous creodonts are found. Hypercarnivorous canids were present before the disappearance of the nimravids, and all went extinct before the appearance of felids. Following the extinction of nimravids, only three taxa originated, two of which were relatively small in body size. Disparity increased during the "cat-gap" even with the extinction of the hypercarnivorous extremes. This was due to the extinction of morphological intermediates, and because carnivorans began to occupy hypocarnivorous (nonmeat-specialist) morphospace for the first time in North America. Procyonids did not arrive in North America until the early Miocene, and "modern" ursids (e.g., Ursinae), did not arrive until the late Miocene. Extinct lineages of Ursidae were present in North America from the late Eocene through the Miocene and Amphicyonid (bear-dogs) were present during this period as well, but occupied a morphospace generally shared with canids and not in close proximity to ursids. A large question remains as to why there was a progressive decline in hypercarnivorous carnivoramorphans during the late Oligocene/early Miocene. During this period all hypercarnivorous forms disappeared from the fossil record, including hypercarnivorous feliforms, canids, and mustelids. One possible explanation is climate change. Earth was gradually cooling after the late Paleocene, and over a period spanning the Eocene/Oligocene boundary, a dramatic climatic cooling event occurred.
A recent study has finally resolved the exact position of "Ailurus": the red panda is neither a procyonid nor an ursid, but forms a monotypic family, with the other musteloids as its closest living relatives. The same study also showed that the mustelids are not a primitive family, as was once thought. Their small body size is a secondary trait—the primitive body form of the arctoids was large, not small. Recent molecular studies also suggest that the endemic Carnivora of Madagascar, including three genera usually classed with the civets and four genera of mongooses classed with the Herpestidae, are all descended from a single ancestor. They form a single sister taxon to the Herpestidae. The hyenas are also closely related to this clade.
Classification.
The most common modern classification scheme divides the Carnivora into fifteen living and a number of extinct families, as follows:

</doc>
<doc id="5222" url="http://en.wikipedia.org/wiki?curid=5222" title="Colombia">
Colombia

Colombia ( , or ), officially the Republic of Colombia ( ), is a country situated in the northwest corner of South America, bordered to the northwest by Panama; to the east by Venezuela and Brazil; to the south by Ecuador and Peru; and it shares maritime limits with Costa Rica, Nicaragua, Honduras, Jamaica, Dominican Republic and Haiti. It is a unitary, constitutional republic comprising thirty-two departments.
The territory of what is now Colombia was originally inhabited by indigenous peoples including the Muisca, Quimbaya, and Tairona. The Spanish arrived in 1499 and initiated a period of conquest and colonization ultimately creating the Viceroyalty of New Granada, with its capital at Bogotá. Independence from Spain was won in 1819, but by 1830 "Gran Colombia" had collapsed with the secession of Venezuela and Ecuador. What is now Colombia and Panama emerged as the Republic of New Granada. The new nation experimented with federalism as the Granadine Confederation (1858), and then the United States of Colombia (1863), before the Republic of Colombia was finally declared in 1886. Panama seceded in 1903.
Since the 1960s, the country has suffered from an asymmetric low-intensity armed conflict. The conflict escalated in the 1990s, but since 2000 the conflict has decreased considerably.
Colombia is ethnically diverse, the descendants of the original native inhabitants, Spanish colonists, Africans originally brought to the country as slaves, and 20th-century immigrants from Europe and the Middle East have produced a diverse cultural heritage. This has also been influenced by Colombia's varied geography, and the imposing landscape of the country has resulted in the development of very strong regional identities. The majority of the urban centres are located in the highlands of the Andes mountains, but Colombian territory also encompasses Amazon rainforest, tropical grassland and both Caribbean and Pacific coastlines.
Ecologically, Colombia is one of the world's 17 megadiverse countries, and is considered the most megadiverse per square kilometer.
Colombia is a middle power with the third largest economy in South America, and is part of the CIVETS group of six leading emerging markets. 
Its principal industries include oil, mining, chemicals, health related products, food processing, agricultural products, textile and fabrics, garments, forest products, machinery, electronics, military products, metal products, home and office material, construction equipment and materials, banking, financial services, software, IT services and the automotive industry.
Etymology.
The name "Colombia" is derived from the last name of Christopher Columbus (, ). It was conceived by the Venezuelan revolutionary Francisco de Miranda as a reference to all the New World, but especially to those under the Spanish and Portuguese rule. The name was later adopted by the Republic of Colombia of 1819, formed out of the territories of the old Viceroyalty of New Granada (modern-day Colombia, Panama, Venezuela, Ecuador, and northwest Brazil).
In 1835, when Venezuela and Ecuador parted ways, the Cundinamarca region that remained became a new country – the Republic of New Granada. In 1858 New Granada officially changed its name to the Granadine Confederation, then in 1863 the United States of Colombia, before finally adopting its present name – the Republic of Colombia – in 1886.
To refer to the country, the Colombian government uses the terms "Colombia" and "República de Colombia".
History.
Pre-Columbian era.
Due to its geographical location, the present territory of Colombia was a corridor of population migration from Mesoamerica and the Andes to the Caribbean and Amazon. The oldest archaeological finds are from the Pubenza and El Totumo sites in the Magdalena Valley 100 km southwest of Bogotá, dating from about 14,400 BCE. These sites correspond to the Paleoindian period (18,000-8000 BCE). In Puerto Hormiga and other sites, traces from the Archaic Period (~8000-2000 BCE) have been found. Vestiges indicate that there was also early occupation in regions like El Abra and Tequendama in Cundinamarca. The oldest pottery discovered in the Americas, found at San Jacinto, dates to 5000 - 4000 BCE.
By 10,500 BCE, the territory of what is now Colombia was inhabited by aboriginal people. Nomadic hunter-gatherer tribes existed near present-day Bogotá (at El Abra and Tequendama sites) which traded with one another and with cultures living in the Magdalena River Valley. Between 5000 and 1000 BCE, hunter-gatherer tribes transitioned to agrarian societies; fixed settlements were established, and pottery appeared. Beginning in the 1st millennium BCE, groups of Amerindians including the Muisca, Quimbaya, and Tairona developed the political system of "cacicazgos" with a pyramidal structure of power headed by caciques. The Muiscas inhabited mainly the area of what is now the Departments of Boyacá and Cundinamarca high plateau ("Altiplano Cundiboyacense"). They farmed maize, potato, quinoa and cotton, and traded worked gold, emeralds, blankets, ceramic handicrafts, coca and salt with neighboring nations. The Taironas inhabited northern Colombia in the isolated Andes mountain range of Sierra Nevada de Santa Marta. The Quimbayas inhabited regions of the Cauca River Valley between the Occidental and Central cordilleras.
Spanish rule.
Alonso de Ojeda (who had sailed with Columbus) reached the Guajira Peninsula in 1499. Spanish explorers, led by Rodrigo de Bastidas, made the first exploration of the Caribbean littoral in 1500. Christopher Columbus navigated near the Caribbean in 1502. In 1508, Vasco Núñez de Balboa accompanied an expedition to the territory through the region of Gulf of Urabá and they founded the town of Santa María la Antigua del Darién in 1510, the first stable settlement on the continent. 
Santa Marta was founded in 1525, and Cartagena in 1533. Spanish conquistador Gonzalo Jiménez de Quesada led an expedition to the interior in April, 1536, and christened the districts through which he passed "New Kingdom of Granada". In August, 1538, he founded provisionally its capital near the Muisca cacicazgo of Bacatá, and named it "Santa Fe". The name soon acquired a suffix and was called Santa Fe de Bogotá. Two other notable journeys by early conquistadors to the interior took place in the same period. Sebastián de Belalcázar, conqueror of Quito, traveled north and founded Cali, in 1536, and Popayán, in 1537; from 1536-1539, German conquistador Nikolaus Federmann crossed the Llanos Orientales and went over the Cordillera Oriental in a search for El Dorado, the "city of gold". The legend and the gold would play a pivotal role in luring the Spanish and other Europeans to New Granada during the 16th and 17th centuries.
In 1542, the region of New Granada, along with all other Spanish possessions in South America, became part of the Viceroyalty of Peru, with its capital at Lima. In 1547, New Granada became the Captaincy-General of New Granada within the viceroyalty.
In 1549, the Royal Audiencia was created by a royal decree, New Granada was ruled by the Royal Audience of Santa Fe de Bogotá, which at that time comprised the provinces of Santa Marta, Rio de San Juan, Popayán, Guayana and Cartagena. But important decisions were taken to the colony from Spain by the Council of the Indies.
Indigenous peoples in New Granada, experienced a reduction in population due to conquest by the Spanish as well as European-carried diseases such as smallpox, to which they had no immunity. With the risk that the land was deserted, the Spanish Crown sold properties to the governors, conquerors and their descendants creating large farms and possession of mines. In the 16th century, Europeans began to bring slaves from Africa. To protect and exploit the indigenous peoples, several forms of land ownership and regulation were established: "resguardos", "encomiendas" and "haciendas". Repopulation was achieved by allowing colonization by farmers and their families who came from Spain.
In 1717 the Viceroyalty of New Granada was originally created, and then it was temporarily removed, to finally be reestablished in 1739. The Viceroyalty had Santa Fé de Bogotá as its capital. This Viceroyalty included some other provinces of northwestern South America which had previously been under the jurisdiction of the Viceroyalties of New Spain or Peru and correspond mainly to today's Venezuela, Ecuador and Panama. So, Bogotá became one of the principal administrative centers of the Spanish possessions in the New World, along with Lima and Mexico City, though it remained somewhat backward compared to those two cities in several economic and logistical ways.
The 18th-century priest, botanist and mathematician José Celestino Mutis was delegated by Viceroy Antonio Caballero y Góngora to conduct an inventory of the nature of the New Granada. Started in 1783, this became known as the Royal Botanical Expedition to New Granada which classified plants, wildlife and founded the first astronomical observatory in the city of Santa Fe de Bogotá. In July 1801 the Prussian scientist Alexander von Humboldt reached Santa Fe de Bogotá where he met with Mutis. In addition, historical figures in the process of independence in New Granada emerged from the expedition as the astronomer Francisco José de Caldas, the scientist Francisco Antonio Zea, the zoologist Jorge Tadeo Lozano and the painter Salvador Rizo.
Independence.
Since the beginning of the periods of conquest and colonization, there were several rebel movements under Spanish rule, most of them were either crushed or remained too weak to change the overall situation. The last one which sought outright independence from Spain sprang up around 1810, following the independence of St. Domingue (present-day Haiti) in 1804, which provided a non-negligible degree of support to the eventual leaders of this rebellion: Simón Bolívar and Francisco de Paula Santander.
A movement initiated by Antonio Nariño, who opposed Spanish centralism and led the opposition against the viceroyalty, led to the independence of Cartagena in November 1811, and the formation of two independent governments which fought a civil war – a period known as La Patria Boba. The following year Nariño proclaimed the United Provinces of New Granada, headed by Camilo Torres Tenorio. Despite the successes of the rebellion, the emergence of two distinct ideological currents among the liberators (federalism and centralism) gave rise to an internal clash which contributed to the reconquest of territory by the Spanish. The viceroyalty was restored under the command of Juan de Samano, whose regime punished those who participated in the uprisings. The retribution stoked renewed rebellion, which, combined with a weakened Spain, made possible a successful rebellion led by the Venezuelan-born Simón Bolívar, who finally proclaimed independence in 1819. The pro-Spanish resistance was finally defeated in 1822 in the present territory of Colombia and in 1823 in Venezuela.
The territory of the Viceroyalty of New Granada became the Republic of Colombia organized as a union of Ecuador, Colombia and Venezuela (Panama was then an integral part of Colombia). The Congress of Cúcuta in 1821 adopted a constitution for the new Republic. Simón Bolívar became the first President of Colombia, and Francisco de Paula Santander was made Vice President. However, the new republic was unstable and ended with the rupture of Venezuela in 1829, followed by Ecuador in 1830.
Colombia was the first constitutional government in South America, and the Liberal and Conservative parties, founded in 1848 and 1849 respectively, are two of the oldest surviving political parties in the Americas. Slavery was abolished in Colombia in 1851.
Internal political and territorial divisions led to the secession of Venezuela and Quito (today's Ecuador) in 1830. The so-called "Department of Cundinamarca" adopted the name "Nueva Granada", which it kept until 1856 when it became the "Confederación Granadina" (Granadine Confederation). After a two-year civil war in 1863, the "United States of Colombia" was created, lasting until 1886, when the country finally became known as the Republic of Colombia. Internal divisions remained between the bipartisan political forces, occasionally igniting very bloody civil wars, the most significant being the Thousand Days' War (1899–1902).
20th century.
The United States of America's intentions to influence the area (especially the Panama Canal construction and control) led to the separation of the Department of Panama in 1903 and the establishment of it as a nation. The United States paid Colombia $25,000,000 in 1921, seven years after completion of the canal, for redress of President Roosevelt's role in the creation of Panama, and Colombia recognized Panama under the terms of the Thomson–Urrutia Treaty.
Colombia was engulfed in the Year-Long War with Peru over a territorial dispute involving the Amazonas department and its capital Leticia.
Soon after, Colombia achieved some degree of political stability, which was interrupted by a bloody conflict that took place between the late 1940s and the early 1950s, a period known as "La Violencia" ("The Violence"). Its cause was mainly mounting tensions between the two leading political parties, which subsequently ignited after the assassination of the Liberal presidential candidate Jorge Eliécer Gaitán on 9 April 1948. The ensuing riots in Bogotá, known as El Bogotazo, spread throughout the country and claimed the lives of at least 180,000 Colombians.
Colombia entered the Korean War when Laureano Gómez was elected as President. It was the only Latin American country to join the war in a direct military role as an ally of the United States. Particularly important was the heroic resistance of the Colombian troops at Old Baldy.
From 1953 to 1964 the violence between the two political parties decreased first when Gustavo Rojas deposed the President of Colombia in a coup d'état and negotiated with the guerrillas, and then under the military junta of General Gabriel París Gordillo.
After Rojas' deposition, the Colombian Conservative Party and Colombian Liberal Party agreed to create the "National Front", a coalition which would jointly govern the country. Under the deal, the presidency would alternate between conservatives and liberals every 4 years for 16 years; the two parties would have parity in all other elective offices. The National Front ended "La Violencia", and National Front administrations attempted to institute far-reaching social and economic reforms in cooperation with the Alliance for Progress. In the end, the contradictions between each successive Liberal and Conservative administration made the results decidedly mixed. Despite the progress in certain sectors, many social and political problems continued, and guerrilla groups were formally created such as the FARC, ELN, EPL, MAQL and M-19 to fight the government and political apparatus.
Since the 1960s, the country has suffered from an asymmetric low-intensity armed conflict between the government forces, left-wing guerrilla groups and right-wing paramilitaries. The conflict escalated in the 1990s. The conflict in Colombia takes place mainly in remote rural areas or marginalized sectors of very difficult access.
The United States has been heavily involved in the conflict since its beginnings, when in the early 1960s the U.S. government encouraged the Colombian military to attack leftist militias in rural Colombia. This was part of the U.S. fight against communism.
On 4 July 1991, a new Constitution was promulgated. The changes generated by the new constitution are viewed as positive by Colombian society.
Recent history.
During the presidency of Álvaro Uribe, the government applied more military pressure on the FARC and other outlawed groups. After the offensive, many security indicators improved. Since 2002 the violence decreased significantly, with some paramilitary groups demobilizing as part of a controversial peace process and the guerrillas lost control of much of the territory they had once dominated. Colombia achieved a great decrease in cocaine production, leading White House drug czar R. Gil Kerlikowske to announce that Colombia is no longer the world's biggest producer of cocaine.
In February 2008, millions of Colombians demonstrated against the FARC. 26,648 FARC and ELN fighters have decided to demobilize since 2002. During these years the military forces of the Republic of Colombia managed to be strengthened.
The Peace process in Colombia, 2012 refers to the dialogue between the Colombian government and guerrilla of FARC-EP with the aim to find a political solution to the armed conflict. The Colombian government and rebel groups meet in Cuba. As of May 2014, the talks have represented breakthroughs. The Government also began a process of assistance and reparation for victims of conflict.
Colombia shows modest progress in the struggle to defend human rights, as expressed by HRW. In terms of international relations, Colombia has moved from a period of tense animosity with Venezuela, towards a prosperous outlook to further enhance integration. Colombia has also won a seat on the Security Council of the UN.
Today Colombia is the third largest oil producer in South America and it is estimated that by 2012, Colombia will be producing a million barrels a day.
In 2013, the National Administrative Department of Statistics (DANE) reported that 30.6% of the population were living below the poverty line, of which 9.1% in "extreme poverty". 820,000 people have been lifted out of poverty. The Government has also been developing a process of financial inclusion within the country's most vulnerable population.
Recent economic growth has led to a considerable increase of new millionaires, including the new entrepreneurs Colombians with net worth exceeding US$1 billion.
Geography.
The geography of Colombia is characterized by its six main natural regions that present their own unique characteristics, from the Andes mountain range region shared with Ecuador and Venezuela; the Pacific coastal region shared with Panama and Ecuador; the Caribbean coastal region shared with Venezuela and Panama; the "Llanos" (plains) shared with Venezuela; the Amazon Rainforest region shared with Venezuela, Brazil, Peru and Ecuador; to the insular area, comprising islands in both the Atlantic and Pacific oceans.
Colombia is bordered to the northwest by Panama; to the east by Venezuela and Brazil; to the south by Ecuador and Peru; it established its maritime boundaries with neighboring countries through seven agreements on the Caribbean Sea and three on the Pacific Ocean. It lies between latitudes 12°N and 4°S, and longitudes 67° and 79°W.
Part of the Ring of Fire, a region of the world subject to earthquakes and volcanic eruptions, Colombia is dominated by the Andes (which contain the majority of the country's urban centres). Beyond the Colombian Massif (in the south-western departments of Cauca and Nariño) these are divided into three branches known as "cordilleras" (mountain ranges): the Cordillera Occidental, running adjacent to the Pacific coast and including the city of Cali; the Cordillera Central, running between the Cauca and Magdalena river valleys (to the west and east respectively) and including the cities of Medellín, Manizales, Pereira and Armenia; and the Cordillera Oriental, extending north east to the Guajira Peninsula and including Bogotá, Bucaramanga and Cúcuta.
Peaks in the Cordillera Occidental exceed , and in the Cordillera Central and Cordillera Oriental they reach . At , Bogotá is the highest city of its size in the world.
East of the Andes lies the savanna of the "Llanos", part of the Orinoco River basin, and, in the far south east, the jungle of the Amazon rainforest. Together these lowlands comprise over half Colombia's territory, but they contain less than 3% of the population. To the north the Caribbean coast, home to 20% of the population and the location of the major port cities of Barranquilla and Cartagena, generally consists of low-lying plains, but it also contains the Sierra Nevada de Santa Marta mountain range, which includes the country's tallest peaks (Pico Cristóbal Colón and Pico Simón Bolívar), and the La Guajira Desert. By contrast the narrow and discontinuous Pacific coastal lowlands, backed by the Serranía de Baudó mountains, are sparsely populated and covered in dense vegetation. The principal Pacific port is Buenaventura.
The main rivers of Colombia are Magdalena, Cauca, Guaviare, Atrato, Meta, Putumayo and Caquetá. Colombia has four main drainage systems: the Pacific drain, the Caribbean drain, the Orinoco Basin and the Amazon Basin. The Orinoco and Amazon Rivers mark limits with Colombia to Venezuela and Peru respectively.
Protected areas and the "National Park System" cover an area of about and account for 11.46% of the Colombian territory. Compared to neighboring countries, rates of deforestation in Colombia are still relatively low. Colombia is the sixth country in the world by magnitude of total renewable freshwater supply, and still has large reserves of freshwater.
Climate.
Colombians customarily describe their country in terms of the climatic zones. Below in elevation is the tierra caliente (hot land), where temperatures are above . About 82.5% of the country's total area lies in the tierra caliente.
The majority of the population can be found in the tierra templada (temperate land, between ), where temperatures vary between and the tierra fría (cold land, ).
In the tierra fría mean temperatures range between . Beyond the tierra fría lie the alpine conditions of the forested zone and then the treeless grasslands of the páramos. Above , where temperatures are below freezing, is the tierra helada, a zone of permanent snow and ice.
Biodiversity.
Colombia is one of the megadiverse countries in biodiversity, ranking first in bird species. As for plants, the country has between 40,000 and 45,000 plant species, equivalent to 10 or 20% of total global species, this is even more remarkable given that Colombia is considered a country of intermediate size. Colombia is the second most biodiverse country in the world, lagging only after Brazil which is approximately 7 times bigger.
Colombia is the country in the planet more characterized by a high biodiversity, with the highest rate of species by area unit worldwide and it has the largest number of endemisms (species that are not found naturally anywhere else) of any country. About 10% of the species of the Earth live in Colombia, including over 1,900 species of bird, more than in Europe and North America combined, Colombia has 10% of the world’s mammals species, 14% of the amphibian species and 18% of the bird species of the world.
Colombia has about 2,000 species of marine fish and is the second most diverse country in freshwater fish. Colombia is the country with more endemic species of butterflies, number 1 in terms of orchid species and approximately 7,000 species of beetles. Colombia is second in the number of amphibian species and is the third most diverse country in reptiles and palms. There are about 2,900 species of mollusks and according to estimates there are about 300,000 species of invertebrates in the country. In Colombia there are 32 terrestrial biomes and 314 types of ecosystems.
Government and politics.
The government of Colombia takes place within the framework of a presidential representative democratic republic as established in the Constitution of 1991. In accordance with the principle of separation of powers, government is divided into three branches: the executive branch, the legislative branch and the judicial branch.
As the head of the executive branch, the President of Colombia serves as both head of state and head of government, followed by the Vice President and the Council of Ministers. The president is elected by popular vote to serve four-year terms and is limited to a maximum of two such terms (increased from one in 2005). At the provincial level executive power is vested in department governors, municipal mayors and local administrators for smaller administrative subdivisions, such as "corregimientos" or "comunas". All regional elections are held one year and five months after the presidential election.
The legislative branch of government is represented nationally by the Congress, a bicameral institution comprising a 166-seat Chamber of Representatives and a 102-seat Senate. The Senate is elected nationally and the Chamber of Representatives is elected in electoral districts. Members of both houses are elected to serve four-year terms two months before the president, also by popular vote. 
The judicial branch is headed by the Supreme Court, consisting of 23 judges divided into three chambers (Penal, Civil and Agrarian, and Labour). The judicial branch also includes the Council of State, which has special responsibility for administrative law and also provides legal advice to the executive, the Constitutional Court, responsible for assuring the integrity of the Colombian constitution, and the Superior Council of Judicature, responsible for auditing the judicial branch. Colombia operates a system of civil law, which since 2005 has been applied through an adversarial system.
Despite a number of controversies, the democratic security policy has ensured that former President Uribe remained popular among Colombian people, with his approval rating peaking at 76%, according to a poll in 2009. However, having served two terms, he was constitutionally barred from seeking re-election in 2010. In the run-off elections on 20 June 2010 the former Minister of defense Juan Manuel Santos won with 69% of the vote against the second most popular candidate, Antanas Mockus. A second round was required since no candidate received over the 50% winning threshold of votes. Santos won nearly 51% of the vote in second-round elections on 15 June 2014, beating right-wing rival Óscar Iván Zuluaga, who won 45%. His term as Colombia's president runs for four years beginning 7 August 2014.
Foreign affairs.
The foreign affairs of Colombia are headed by the President, as head of state, and managed by the Minister of Foreign Affairs. Colombia has diplomatic missions in all continents.
Colombia was one of the 4 founding members of the Pacific Alliance, which is a political, economic and co-operative integration mechanism that promotes the free circulation of goods, services, capital and persons between the members, as well as a common stock exchange and joint embassies in several countries. Colombia is also a member of the United Nations, the Organization of American States, the Organization of Ibero-American States, the Union of South American Nations and the Andean Community of Nations.
Military.
The executive branch of government is responsible for managing the defense of Colombia, with the President commander-in-chief of the armed forces. The Ministry of Defence exercises day-to-day control of the military and the Colombian National Police. Colombia has 449,415 active military personnel. And in 2012 3.3% of the country's GDP went towards military expenditure, placing it 18th in the world. Colombia's armed forces are the largest in Latin America, and it is the second largest spender on its military after Brazil.
The Colombian military is divided into three branches: the National Army of Colombia; the Colombian Air Force; and the Colombian Navy. The National Police functions as a gendarmerie, operating independently from the military as the law enforcement agency for the entire country. Each of these operates with their own intelligence apparatus separate from the national intelligence agency (ANIC, in Spanish).
The National Army is formed by divisions, brigades, special brigades and special units; the Colombian Navy by the Naval Infantry, the Naval Force of the Caribbean, the Naval Force of the Pacific, the Naval Force of the South, the Naval Force of the East, Colombia Coast Guards, Naval Aviation and the Specific Command of San Andres y Providencia; and the Air Force by 15 air units. The National Police has a presence in all municipalities.
Administrative divisions.
Colombia is divided into 32 departments and one capital district, which is treated as a department (Bogotá also serves as the capital of the department of Cundinamarca). Departments are subdivided into municipalities, each of which is assigned a municipal seat, and municipalities are in turn subdivided into "corregimientos" in rural areas and into "comunas" in urban areas. Each department has a local government with a governor and assembly directly elected to four-year terms, and each municipality is headed by a mayor and council. There is a popularly elected local administrative board in each of the "corregimientos" or "comunas".
In addition to the capital four other cities have been designated districts (in effect special municipalities), on the basis of special distinguishing features. These are Barranquilla, Cartagena, Santa Marta and Buenaventura. Some departments have local administrative subdivisions, where towns have a large concentration of population and municipalities are near each other (for example in Antioquia and Cundinamarca). Where departments have a low population (for example Amazonas, Vaupés and Vichada), special administrative divisions are employed, such as "department "corregimientos"", which are a hybrid of a municipality and a "corregimiento".
Click on a department on the map below to go to its article.
Economy.
Historically an agrarian economy, Colombia urbanised rapidly in the 20th century, by the end of which just 22.7% of the workforce were employed in agriculture, generating just 11.5% of GDP; 18.7% of the workforce were employed in industry and 58.5% in services, responsible for 36% and 52.5% of GDP respectively.
Colombia's market economy grew steadily in the latter part of the 20th century, with gross domestic product (GDP) increasing at an average rate of over 4% per year between 1970 and 1998. The country suffered a recession in 1999 (the first full year of negative growth since the Great Depression), and the recovery from that recession was long and painful. However, in recent years growth has been impressive, reaching 6.9% in 2007, one of the highest rates of growth in Latin America. According to International Monetary Fund estimates, in 2012 Colombia's GDP (PPP) was US$500 billion (28th in the world and third in South America).
Government spending has stabilized at about 29 percent of total domestic output. Public debt remains at around 33 percent of GDP. A strong fiscal climate was reaffirmed by a boost in bond ratings. Inflation has remained relatively low in recent years, standing at 1.94% in December 2013. The national unemployment rate fell to 8.4 percent in December 2013, although the informality is the biggest problem facing the labour market (the income of formal workers climbed 24.8% in 5 years while labor incomes of informal workers rose only 9%). Colombia has Free trade Zone (FTZ), such as Zona Franca del Pacifico, located in the Valle del Cauca, one of the most striking areas for foreign investment.
Colombia is rich in natural resources, and its main exports include mineral fuels, oils, distillation products, precious stones, forest products, pulp and paper, coffee, meat, cereals and vegetable oils, cotton, oilseed, sugars and sugar confectionery, fruit and other agricultural products, food processing, processed fish products, beverages, machinery, electronics, military products, aircraft, ships, motor vehicles, metal products, ferro-alloys, home and office material, chemicals and health related products, petrochemicals, agrochemicals, inorganic salts and acids, perfumery and cosmetics, medicaments, plastics, animal fibers, textile and fabrics, clothing and footwear, leather, construction equipment and materials, cement, software, among others.
Colombia is also known as an important global source of emeralds, while over 70% of cut flowers imported by the United States are Colombian. Non-traditional exports have boosted the growth of Colombian foreign sales as well as the diversification of destinations of export thanks to new free trade agreements. Principal trading partners are the United States, India, China, the European Union and some Latin American countries.
The electricity production in Colombia comes mainly from renewable energy sources. 67.7% is obtained from the hydroelectric generation.
The financial sector has grown favorably due to good liquidity in the economy, the growth of credit and in general to the positive performance of the Colombian economy. The Colombian Stock Exchange through the Latin American Integrated Market (MILA) offers a regional market to trade equities.
Tourism in Colombia is an important sector in the country's economy. Foreign tourist visits were predicted to have risen from 0.6 million in 2007 to 2.2 million in 2013.
Science and technology.
Colombia has more than 5,500 research groups in science and technology. iNNpulsa, a government body that promotes entrepreneurship and innovation in the country, provides grants to startups, in addition to other services it and institutions like Apps.co provide. Co-working spaces have arisen to serve as communities for startups large and small. Organizations such as the Corporation for biological research for the support of young people interested in scientific work has been successfully developed in Colombia. The International Center for Tropical Agriculture based in Colombia investigates the increasing challenge of global warming and food security.
Important inventions related to the medicine have been made in Colombia, such as the first external artificial pacemaker with internal electrodes, invented by the electronics engineer Jorge Reynolds Pombo, invention of great importance for those who suffer from heart failure. Also were invented in Colombia one of the most important techniques for the correction of refractive errors of vision known as LASIK and the Hakim valve for the treatment of Hydrocephalus, among others. Colombia has begun to innovate in military technology for its army and other armies of the world; especially in the design and creation of personal ballistic protection products, military robots, simulators and radar.
Some leading Colombian scientists are Joseph M. Tohme, researcher recognized for his work on the genetic diversity of food, Manuel Elkin Patarroyo who is known for his groundbreaking work on synthetic vaccines for malaria, Francisco Lopera who discovered the "Paisa Mutation" or a type of early-onset Alzheimer's, Rodolfo Llinás known for his study of the intrinsic neurons properties and the theory of a syndrome that had changed the way of understanding the functioning of the brain, Jairo Quiroga Puello recognized for his studies on the characterization of synthetic substances which can be used to fight fungus, tumors, tuberculosis and even some viruses and Ángela Restrepo who established accurate diagnoses and treatments to combat the effects of a disease caused by the "Paracoccidioides brasiliensis", among other scientists.
Infrastructure.
Transportation in Colombia is regulated within the functions of the Ministry of Transport and entities such as the National Roads Institute (INVÍAS) responsible for the Highways in Colombia (13 000 km), the Aerocivil, responsible for civil aviation and airports, the General Maritime Directorate (Dimar) has the responsibility of coordinating maritime traffic control along with the Colombian Navy, among others and under the supervision of the Superintendency of Ports and Transport. There will be of divided expressways in Colombia by 2014.
It is expected that the Antonio Nariño Airport of the Pasto and the El Caraño Airport of Quibdó in Chocó become international aerodrome.
China and Colombia have discussed a Panama Canal rival, a 'Dry Canal' rail link between the Pacific and a new city near Cartagena.
Demographics.
With an estimated 46 million people in 2008, Colombia is the third-most populous country in Latin America, after Brazil and Mexico. It is also home to the third-largest number of Spanish speakers in the world after Mexico and the United States. At the beginning of the 20th century, Colombia's population was approximately 4 million. The population increased at a rate of 1.9% between 1975 and 2005, predicted to drop to 1.2% over the next decade. Colombia is projected to have a population of 50.7 million by 2015. These trends are reflected in the country's age profile. In 2005 over 30% of the population was under 15 years old, compared to just 5.1% aged 65 and over.
The population is concentrated in the Andean highlands and along the Caribbean coast. The nine eastern lowland departments, comprising about 54% of Colombia's area, have less than 3% of the population and a density of less than one person per square kilometer (two persons per square mile). Traditionally a rural society, movement to urban areas was very heavy in the mid-20th century, and Colombia is now one of the most urbanized countries in Latin America. The urban population increased from 31% of the total in 1938 to 60% in 1975, and by 2005 the figure stood at 72.7%. The population of Bogotá alone has increased from just over 300,000 in 1938 to approximately 8 million today. In total seventy one cities now have populations of 100,000 or more (2013). As of 2012 Colombia has the world's largest populations of internally displaced persons (IDPs), estimated to be up to 4.9 million people.
The life expectancy is 79 years in 2012 and infant mortality is 15 per thousand in 2012. In 2012, 93.6% of adults and 98.2% of youth are literate and the government spends about 4.4% of its GDP in education.
Colombia is ranked third in the world in the Happy Planet Index.
Languages.
More than 99.2% of Colombians speak the Spanish language, also called Castilian; 65 Amerindian languages, 2 Creole languages and the Romani language are also spoken in the country. English has official status in the San Andrés, Providencia and Santa Catalina Islands.
The overwhelming majority of Colombians speak Spanish (see also Colombian Spanish), but in total 101 languages are listed for Colombia in the Ethnologue database. The specific number of spoken languages varies slightly since some authors consider as different languages what others consider are varieties or dialects of the same language, the best estimates recorded that 70 languages are spoken in the country today. Most of these belong to the Chibchan, Arawak, Cariban, language families. There are currently about 850,000 speakers of native languages.
Ethnic groups.
The descendants of the original native inhabitants, Spanish colonists, African people originally brought to the country as slaves and 20th-century immigrants from Europe and the Middle East have produced a diverse cultural heritage in Colombia. The demographic distribution reflects a pattern is influenced by colonial history. Whites tend to live mainly in urban centers, like Bogotá or Medellín, and the burgeoning highland cities. The populations of the major cities also include mestizos. Mestizo "campesinos" (people living in rural areas) also live in the Andean highlands where some Spanish conquerors mixed with the women of Amerindian chiefdoms. Mestizos include artisans and small tradesmen that have played a major part in the urban expansion of recent decades.
According to the 2005 census, 49% of Colombia's population is Mestizo or of mixed European and Amerindian ancestry. Approximately 37% is of European ancestry (predominantly Spanish, and a part of Italian, French, and German). About 10.6% is of African ancestry. Indigenous Amerindians comprise 3.4% of the population. The 2005 census reported that the "non-ethnic population", consisting of whites and mestizos (those of mixed white European and Amerindian ancestry), constituted 86% of the national population.
Many of the Indigenous peoples experienced a reduction in population during the Spanish rule and many others were absorbed into the mestizo population, but the remainder currently represents over eighty distinct cultures. Reserves ("resguardos") established for indigenous peoples occupy (27% of the country's total) and are inhabited by more than 800,000 people. Some of the largest indigenous groups are the Wayuu, the Paez, the Pastos, the Emberá and the Zenú. The departments of La Guajira, Cauca, Nariño, Córdoba and Sucre have the largest indigenous populations.
The Organización Nacional Indígena de Colombia (ONIC), founded at the first National Indigenous Congress in 1982, is an organization representing the indigenous peoples of Colombia. In 1991, Colombia signed and ratified the current international law concerning indigenous peoples, Indigenous and Tribal Peoples Convention, 1989.
Black Africans were brought as slaves, mostly to the coastal lowlands, beginning early in the 16th century and continuing into the 19th century. Large Afro-Colombian communities are found today on the Caribbean and Pacific coasts. The population of the department of Chocó, running along the northern portion of Colombia's Pacific coast, is over 80% black. British and Jamaicans migrated mainly to the islands of San Andres and Providencia. A number of other Europeans and North Americans migrated to the country in the late 19th and early 20th centuries, including people from the former USSR during and after the Second World War.
Many immigrant communities have settled on the Caribbean coast, in particular recent immigrants from the Middle East. Barranquilla (the largest city of the Colombian Caribbean) and other Caribbean cities have the largest populations of Lebanese, Palestinian, and other Arabs. There are also important communities of Chinese, Japanese, Romanis and Jews. There is a major migration trend of Venezuelans, due to the political and economic situation in Venezuela.
Religion.
The National Administrative Department of Statistics (DANE) does not collect religious statistics, and accurate reports are difficult to obtain. However, based on various studies and a survey, about 90% of the population adheres to Christianity, the majority of which (70.9%) are Roman Catholic. 16.7% of Colombians adhere to Protestantism or Evangelicalism, 4.7% are Atheists and Agnostics, 3.5% claim to believe in God, but they don't believe in religion. 1.8% of Colombians adhere to Jehovah's Witnesses and Adventism and under 1% to Islam, Judaism, Buddhism, Mormonism, Hinduism, Indigenous religions, Hare Krishna movement, Rastafari movement, Orthodox Catholic Church, and spiritual studies. The remaining persons responded they did not know or did not respond to the survey. However, 35.9% of Colombians reported that they did not practice their faith actively.
While Colombia remains a mostly Roman Catholic country by baptism numbers, the 1991 Colombian constitution guarantees freedom and equality of religion. Today there is more openness to a great diversity of beliefs.
Culture.
Colombia lies at the crossroads of Latin America and the broader American continent, and as such has been hit by a wide range of cultural influences. Native American, Spanish and other European, African, American, Caribbean, and Middle Eastern influences, as well as other Latin American cultural influences, are all present in Colombia's modern culture. Urban migration, industrialization, globalization, and other political, social and economic changes have also left an impression.
Many national symbols, both objects and themes, have arisen from Colombia's diverse cultural traditions and aim to represent what Colombia, and the Colombian people, have in common. Cultural expressions in Colombia are promoted by the government through the Ministry of Culture.
Literature.
Colombian literature dates back to pre-Columbian era; a notable example of the period is the epic poem known as the "Legend of Yurupary". In Spanish colonial times notable writers include Hernando Domínguez Camargo and his epic poem to San Ignacio de Loyola, Juan Rodríguez Freyle ("The Sheep") and the nun Francisca Josefa de Castillo, representative of mysticism.
Post-independence literature linked to Romanticism highlighted Antonio Nariño, José Fernández Madrid, Camilo Torres Tenorio and Francisco Antonio Zea. In the second half of the nineteenth century and early twentieth century the literary genre known as "costumbrismo" became popular; great writers of this period were Tomás Carrasquilla, Jorge Isaacs and Rafael Pombo (the latter of whom wrote notable works of children's literature). Within that period, authors such as José Asunción Silva, José Eustasio Rivera, León de Greiff, Porfirio Barba-Jacob and José María Vargas Vila developed the modernist movement. In 1872, Colombia established the Colombian Academy of Language, the first Spanish language academy in the Americas. Candelario Obeso wrote the groundbreaking "Cantos Populares de mi Tierra" (1877), the first book of poetry by an Afro-Colombian author.
Between 1939 and 1940 seven books of poetry were published under the name "Stone and Sky" in the city of Bogotá that significantly impacted the country; they were edited by the poet Jorge Rojas. In the following decade, Gonzalo Arango founded the movement of "nothingness" in response to the violence of the time; he was influenced by nihilism, existentialism, and the thought of another great Colombian writer: Fernando González Ochoa. During the boom in Latin American literature, successful writers emerged, led by Nobel laureate Gabriel García Márquez and his magnum opus, "One Hundred Years of Solitude", Eduardo Caballero Calderón, Manuel Mejía Vallejo, and Álvaro Mutis, a writer who was awarded the Cervantes Prize and the Prince of Asturias Award for Letters. Other leading contemporary authors are Fernando Vallejo (Rómulo Gallegos Prize) and Germán Castro Caycedo, the best-selling writer in Colombia after García Márquez.
Visual arts.
Colombian art has over 3,000 years of history. Colombian artists have captured the country's changing political and cultural backdrop using a range of styles and mediums. There is archeological evidence of ceramics being produced earlier in Colombia than anywhere else in the Americas, dating as early as 3,000 BCE.
The earliest examples of gold craftsmanship have been attributed to the Tumaco people of the Pacific coast and date to around 325 BCE. Roughly between 200 BCE and 800 CE, the San Agustín culture, masters of stonecutting, entered its “classical period". They erected raised ceremonial centres, sarcophagi, and large stone monoliths depicting anthropomorphic and zoomorphhic forms out of stone.
Colombian art has followed the trends of the time, so during the 16th to 18th centuries, Spanish Catholicism had a huge influence on Colombian art, and the popular Baroque style was replaced with Rococo when the Bourbons ascended to the Spanish crown. More recently, Colombian artists Pedro Nel Gómez and Santiago Martínez Delgado started the Colombian Murial Movement in the 1940s, featuring the neoclassical features of Art Deco.
Since the 1950s, the Colombian art started to have a distinctive point of view, reinventing traditional elements under the concepts of the 20th century. Examples of this are the Greiff portraits by Ignacio Gomez Jaramillo, showing what the Colombian art could do with the new techniques applied to typical Colombian themes. Carlos Correa, with his paradigmathic “Naturaleza muerta en silencio” (silent dead nature), combines geometrical abstraction and cubism. Alejandro Obregón is often considered as the father of modern Colombian painting, and one of the most influential artist in this period, due to his originality, the painting of Colombian landscapes with symbolic and expressionist use of animals, (specially the andean condor). Fernando Botero and Omar Rayo are probably the most widely known Colombian artists in the international scene.
The Colombian sculpture from the sixteenth to 18th centuries was mostly devoted to religious depictions of ecclesiastic art, strongly influenced by the Spanish schools of sacred sculpture. During the early period of the Colombian republic, the national artists were focused in the production of sculptural portraits of politicians and public figures, in a plain neoclassicist trend. During the 20th century, the Colombian sculpture began to develop a bold and innovative work with the aim of reaching a better understanding of national sensitivity.
Photography in Colombia began with the arrival in the country of the Daguerreotype that was brought by the Baron Gros in 1841. The Piloto public library has Latin America’s largest archive of negatives, containing 1.7 million antique photographs covering Colombia 1848 until 2005.
Popular culture.
In general, Colombian music blends European-influenced guitar and song structure with large gaita flutes and percussion instruments from the indigenous population, while its percussion structure and dance forms come from Africa. Colombian music reflects a wealth of musical influences that have given birth to a dynamic musical environment. Some of the most popular music genres that have marked the Colombian music with special relevance are the cumbia, vallenato, joropo, salsa, bambuco, rock, pop and the classical music. Shakira and Juanes are two of the most well-known Colombian musicians internationally. Colombian music is promoted mainly by the support of the largest record labels, independent companies and the Government of Colombia, through the Ministry of Culture.
Colombian architecture is mainly derived of adapting European styles to local conditions, and Spanish influence, especially Andalusian, can be easily seen. The Teatro Colón in Bogotá is a lavish example of Colombian architecture from the Republican period, and the Archbishopric Cathedral also in the capital, was made in the neoclassic style in 1792, by Colombian architect Domingo de Petrés. Rogelio Salmona, whose works are noted for their use of red brick and natural shapes, is a widely renowned Colombian architect.
Theater was introduced in Colombia during the Spanish colonization in 1550 through zarzuela companies. Colombian theater is supported by the Ministry of Culture and a number of private and state owned organizations. The Ibero-American Theater Festival of Bogotá is the cultural event of the highest importance in Colombia and one of the biggest theater festivals in the world. Other important theater events are: The Festival of Puppet The Fanfare (Medellín), The Manizales Theater Festival, The Caribbean Theatre Festival (Santa Marta) and The Art Festival of Popular Culture "Cultural Invasion" (Bogotá).
Some important national circulation newspapers are El Tiempo and El Espectador. Television in Colombia has two privately owned TV networks and three state-owned TV networks with national coverage, as well as six regional TV networks and dozens of local TV stations. Private channels, RCN and Caracol are the highest-rated. The regional channels and regional newspapers cover a department or more and its content is made in these particular areas.
Although the Colombian cinema is young as an industry, more recently the film industry was growing with support from the Film Act passed in 2003.
Cuisine.
Colombia's varied cuisine is influenced by its diverse fauna and flora as well as the cultural traditions of the ethnic groups. Colombian dishes and ingredients vary widely by region. Some of the most common ingredients are: cereals such as rice and maize; tubers such as potato and cassava; assorted legumes; meats, including beef, chicken, pork and goat; fish; and seafood. Colombia cuisine also features a variety of tropical fruits such as cape gooseberry, feijoa, arazá, dragon fruit, mangostino, granadilla, papaya, guava, mora (blackberry), lulo, soursop and passionfruit.
Among the most representative appetizers and soups are patacones (fried green plantains), sancocho de gallina (chicken soup with root vegetables) and ajiaco (potato and corn soup). Representative snacks and breads are pandebono, arepas (corn cakes), aborrajados (fried sweet plantains with cheese), torta de choclo, empanadas and almojábanas. Representative main courses are bandeja paisa, lechona tolimense, mamona, tamales and fish dishes (such as arroz de lisa), especially in coastal regions where suero, costeño cheese and carimañolas are also eaten. Representative side dishes are papas criollas al horno (roasted Andean potatoes), papas chorreadas (potatoes with cheese) and arroz con coco (coconut rice). Organic food is a current trend in big cities, although in general across the country the fruits and veggies are very natural and fresh.
Representative desserts are buñuelos, natillas, torta Maria Luisa, bocadillo made of guayaba (guava jelly), cocadas (coconut balls), casquitos de guayaba (candied guava peels), torta de natas, obleas, flan de arequipe, roscón, milhoja, and the tres leches cake (a sponge cake soaked in milk, covered in whipped cream, then served with condensed milk). Typical sauces (salsas) are hogao (tomato and onion sauce) and Colombian-style ají.
Some representative beverages are coffee (Tinto), champús, cholado, lulada, avena colombiana, sugarcane juice, aguapanela, aguardiente, hot chocolate and fresh fruit juices (often made with sugar and water or milk).
Sports.
Tejo is Colombia’s national sport and is a team sport that involves launching projectiles to hit a target. But of all sports in Colombia, football is the most popular. Colombia was the champion of the 2001 Copa América, in which they set a new record of being undefeated, conceding no goals and winning each match. Interestingly, Colombia was the first team to win FIFA best mover in 1993 where the achievement was first introduced and the second team to win it twice with the second being in 2013.
Colombia is a mecca for roller skaters. The national team is a perennial powerhouse at the World Roller Speed Skating Championships. Colombia has traditionally been very good in cycling and a large number of Colombian cyclists have triumphed in major competitions of cycling.
Baseball, another sport rooted in the Caribbean Coast, Colombia was world amateur champion in 1947 and 1965.
Baseball is popular in the Caribbean. Mainly in the cities, Cartagena, Barranquilla and Santa Marta. Of those cities have come good players like: Orlando Cabrera, Edgar Rentería who was champion of the World Series in 1997 and 2010, and others who have played in Major League Baseball.
Boxing is one of the sports that more world champions has produced for Colombia.
Motorsports also occupies an important place in the sporting preferences of Colombians; Juan Pablo Montoya is a race car driver known for winning 7 Formula One events. Colombia also has excelled in sports such as taekwondo, shooting sport, wrestling, judo, bowling, athletics and has a long tradition in weightlifting.
Health.
Life expectancy at birth in 2000 was 74 years; the life expectancy increased to 79 years by 2012. Health standards in Colombia have improved very much since the 1980s, healthcare reforms have led to the massive improvements in the healthcare systems of the country. Although this new system has widened population coverage by the social and health security system from 21% (pre-1993) to 96% in 2012, health disparities persist, with the poor continuing to suffer less attention in their medical procedures.
Through health tourism, many people from over the world travel from their places of residence to other countries in search of medical treatment and the attractions in the countries visited. Colombia is projected as one of Latin America’s main destinations in terms of health tourism due to the quality of its health care professionals, a good number of institutions devoted to health, and an immense inventory of natural and architectural sites. Cities such as Bogotá, Cali and Medellín are the most visited in cardiology procedures, neurology, dental treatments, stem cell therapy, ENT, ophthalmology and joint replacements among others for the medical services of high quality.
A study conducted by America Economia magazine ranked 18 Colombian health care institutions among the top 45 in Latin America, amounting to 42 percent of the total.
Education.
The educational experience of many Colombian children begins with attendance at a preschool academy until age five ("Educación preescolar"). Basic education ("Educación básica") is compulsory by law. It has two stages: Primary basic education ("Educación básica primaria") which goes from first to fifth grade – children from six to ten years old, and Secondary basic education ("Educación básica secundaria"), which goes from sixth to ninth grade. Basic education is followed by Middle vocational education ("Educación media vocacional") that comprises the tenth and eleventh grades. It may have different vocational training modalities or specialties (academic, technical, business, and so on.) according to the curriculum adopted by each school.
After the successful completion of all the basic and middle education years, a high-school diploma is awarded. The high-school graduate is known as a "bachiller", because secondary basic school and middle education are traditionally considered together as a unit called "bachillerato" (sixth to eleventh grade). Students in their final year of middle education take the ICFES test (now renamed Saber 11) in order to gain access to higher education ("Educación superior"). This higher education includes undergraduate professional studies, technical, technological and intermediate professional education, and post-graduate studies. Technical professional institutions of Higher Education are also opened to students holder of a qualification in Arts and Business. This qualification is usually awarded by the SENA after a two years curriculum.
"Bachilleres" (high-school graduates) may enter into a professional undergraduate career program offered by a university; these programs last up to five years (or less for technical, technological and intermediate professional education, and post-graduate studies), even as much to six to seven years for some careers, such as medicine. In Colombia, there is not an institution such as college; students go directly into a career program at a university or any other educational institution to obtain a professional, technical or technological title. Once graduated from the university, people are granted a (professional, technical or technological) diploma and licensed (if required) to practice the career they have chosen. For some professional career programs, students are required to take the Saber-Pro test, in their final year of undergraduate academic education.
Public spending on education as a proportion of gross domestic product in 2012 was 4.4%. This represented 15.8% of total government expenditure. In 2012, the primary and secondary gross enrolment ratios stood at 106.9% and 92.8% respectively. School-life expectancy was 13.2 years. A total of 93.6% of the population aged 15 and older were recorded as literate, including 98.2% of those aged 15–24.

</doc>
<doc id="5224" url="http://en.wikipedia.org/wiki?curid=5224" title="Citizen Kane">
Citizen Kane

Citizen Kane is a 1941 American drama film directed, co-written, produced by, and starring Orson Welles. The picture was Welles' first feature film. The film was nominated for Academy Awards in nine categories; it won an Academy Award for Best Writing (Original Screenplay) by Herman Mankiewicz and Welles. Considered by many critics, filmmakers, and fans to be the greatest film ever made, "Citizen Kane" was voted the greatest film of all time in five consecutive "Sight & Sound"s polls of critics, until it was displaced by "Vertigo" in the 2012 poll. It topped the American Film Institute's 100 Years ... 100 Movies list in 1998, as well as AFI's 2007 update. "Citizen Kane" is particularly praised for its cinematography, music, and narrative structure, which were innovative for its time.
The story is a "film à clef" that examines the life and legacy of Charles Foster Kane, played by Welles, a character based in part upon the American newspaper magnate William Randolph Hearst, Chicago tycoons Samuel Insull and Harold McCormick, and aspects of Welles' own life. Upon its release, Hearst prohibited mention of the film in any of his newspapers. Kane's career in the publishing world is born of idealistic social service, but gradually evolves into a ruthless pursuit of power. Narrated principally through flashbacks, the story is told through the research of a newsreel reporter seeking to solve the mystery of the newspaper magnate's dying word: "Rosebud".
After his success in the theatre with his Mercury Players, and his controversial 1938 radio broadcast of "The War of the Worlds" on "The Mercury Theatre on the Air", Welles was courted by Hollywood. He signed a contract with RKO Pictures in 1939. Unusual for an untried director, he was given the freedom to develop his own story, to use his own cast and crew, and to have final cut privilege. Following two abortive attempts to get a project off the ground, he wrote the screenplay for "Citizen Kane", collaborating on the effort with Herman Mankiewicz. Principal photography took place in 1940 and the film received its American release in 1941.
While a critical success, "Citizen Kane" failed to recoup its costs at the box office. The film faded from view after its release but was subsequently returned to the fore when it was praised by such French critics as Jean-Paul Sartre and André Bazin and given an American revival in 1956.
The film was released on Blu-ray Disc on September 13, 2011, for a special 70th anniversary edition.
Plot.
Charles Foster Kane, an enormously wealthy newspaper publisher, has been living alone in Florida on his vast palatial estate, Xanadu, for the last years of his life, with a "No Trespassing" sign on the gate. On his deathbed, he holds a snow globe and utters the single word, "Rosebud", before dying; the globe slips from his hand and smashes on the floor. Kane's death becomes sensational news around the world. Newsreel reporter Jerry Thompson becomes intrigued, and decides to learn all he can about Kane's private life to discover the meaning of "Rosebud".
The reporter interviews the great man's friends and associates, and Kane's story unfolds as a series of flashbacks. Thompson approaches Kane's second wife, Susan Alexander, now an alcoholic who runs her own nightclub, but she refuses to tell him anything and demands that he leave. Thompson then goes to the private archive of the late Walter Parks Thatcher, a banker who served as Kane's guardian during his childhood and adolescence. Through Thatcher's written memoirs, Thompson learns about Kane's childhood. Thompson then interviews Kane's personal business manager, Mr. Bernstein; his estranged best friend, Jedediah Leland; Susan, for a second time, successfully this time; and, finally, his butler, Raymond, at the Xanadu estate.
These flashbacks reveal that Kane's childhood was spent in poverty in Colorado (his parents ran a boarding house), until "the world's third largest gold mine" was discovered on the seemingly worthless property his mother had acquired. His mother, Mary, sends him away to the East to live with Thatcher, so that he may be properly educated. After gaining full control over his trust fund at the age of 25, Kane enters the newspaper business and embarks on a career of yellow journalism. He takes control of the newspaper, the "New York Inquirer", and hires the best journalists available. He then rises to power by successfully manipulating public opinion regarding the Spanish American War, marrying the niece of a President of the United States, and campaigning for the office of Governor of New York.
Kane's marriage disintegrates over the years, and he begins an affair with Susan Alexander, a singer. Both his wife and his political opponent discover the affair and this brings an abrupt end to both his marriage and his political aspirations. Kane marries Susan, and forces her into a humiliating operatic career for which she has neither the talent nor the ambition. Kane finally allows her to abandon her singing career after she attempts suicide. After years spent in boredom and isolation on the Xanadu estate, constantly under his domination, Susan ultimately leaves Kane.
Kane spends his last years building his vast estate and lives alone, interacting only with his staff. The butler recounts that Kane had said "Rosebud" after Susan left him, right after seeing and pocketing a snow globe.
Back at Xanadu, Kane's vast number of belongings are catalogued: priceless works of art are intermingled with worthless pieces of modern furniture. Thompson finds that he is unable to solve the mystery and concludes that the meaning of "Rosebud" will forever remain an enigma. He theorizes that "Mr. Kane was a man who got everything he wanted, and then lost it. Maybe Rosebud was something he couldn't get, or something he lost." As the film ends, the camera reveals that Rosebud was the name of a sled from Kane's childhood–an allusion to the only time in his life that he was truly happy. The sled, thought to be junk, is burned in a basement furnace by Xanadu's departing staff.
Cast and characters.
The cast of "Citizen Kane" is listed at the American Film Institute Catalog of Feature Films.
The film's closing credits read, "Most of the principal actors are new to motion pictures. The Mercury Theatre is proud to introduce them." Welles, along with his partner John Houseman, had assembled them into a group known as the Mercury Players to perform his productions in the Mercury Theatre in 1937. After accepting his Hollywood contract in 1939, Welles worked between Los Angeles and New York where the Mercury Theatre continued their weekly radio broadcasts for "The Campbell Playhouse". Welles had wanted all the Mercury Players to debut in his first film, but the cancellation of "The Heart of Darkness" project in December 1939 created a financial crisis for the group and some of the actors worked elsewhere. This caused friction between Welles and Houseman, and their partnership ended.
RKO executives were dismayed that so many of the major roles went to unknowns, but Welles's contract left them with no say in the matter. The film features debuts from William Alland, Agnes Moorehead, Everett Sloane, Ruth Warrick and Welles himself.
Production.
Development.
Welles's notoriety following "The War of the Worlds" broadcast earned him Hollywood's interest, and RKO studio head George J. Schaefer's unusual contract. Welles made a deal with Schaefer on July 21, 1939, to produce, direct, write, and act in two feature films. The studio had to approve the story and the budget if it exceeded $500,000. Welles was allowed to develop the story without interference, cast his own actors and crew members, and have the privilege of final cut – unheard of at the time for a first-time director. He had spent the first five months of his RKO contract trying to get several projects going with no success. The "Hollywood Reporter" said, "They are laying bets over on the RKO lot that the Orson Welles deal will end up without Orson ever doing a picture there." First, Welles tried to adapt "Heart of Darkness", but there was concern over the idea of depicting it entirely with point of view shots. Welles considered adapting Cecil Day-Lewis' novel "The Smiler With The Knife", but realized that to challenge himself with a new medium, he had to write an original story.
Screenwriter Herman J. Mankiewicz, recuperating from a car accident, was in-between jobs. He had originally been hired by Welles to work on "The Campbell Playhouse" radio program and was available to work on the screenplay for Welles's film. The writer had only received two screenplay credits between 1935 and when he began work on "Citizen Kane", and he needed the job. There is dispute amongst historians regarding whose idea it was to use William Randolph Hearst as the basis for Charles Foster Kane. Welles claimed it was his idea while film critic Pauline Kael (in her 1971 essay "Raising Kane") and Welles's former business partner, John Houseman, claim that it was Mankiewicz's idea. For some time, Mankiewicz had wanted to write a screenplay about a public figure – perhaps a gangster – whose story would be told by the people that knew him.
Mankiewicz had already written an unperformed play about John Dillinger entitled "The Tree Will Grow". Welles liked the idea of multiple viewpoints but was not interested in playing Dillinger. Mankiewicz and Welles talked about picking someone else to use as a model. They hit on the idea of using Hearst as their central character. Mankiewicz had frequented Hearst's parties until his alcoholism got him barred. The writer resented this and became obsessed with Hearst and Marion Davies. Hearst had great influence and the power to retaliate within Hollywood, so Welles had Mankiewicz work on the script outside of the city. Because of the writer's drinking problem, Houseman went along to provide assistance and make sure that he stayed focused. Welles also sought inspiration from Howard Hughes and Samuel Insull (who built an opera house for his wife). Although Mankiewicz and Houseman got on well with Welles, they incorporated some of his traits into Kane, such as his temper.
During production, "Citizen Kane" was referred to as "RKO 281". Most of the filming took place between June 29, 1940, and October 23, 1940, in what is now Stage 19 on the Paramount lot in Hollywood. There was some location filming at Balboa Park in San Diego and the San Diego Zoo, and still photographs of Oheka Castle in Huntington, New York, were used in the opening montage, representing Kane's Xanadu estate. Welles prevented studio executives of RKO from visiting the set. He understood their desire to control projects, and he knew they were expecting him to do an exciting film that would correspond to his "The War of the Worlds" radio broadcast. Welles's RKO contract had given him complete control over the production of the film when he signed on with the studio, something that he never again was allowed to exercise when making motion pictures. According to an RKO cost sheet from May 1942, the film cost $839,727 compared to an estimated budget of $723,800.
Screenplay.
Development.
Mankiewicz as co-writer.
Robert Carringer, author of "The Making of Citizen Kane" (1985), described the early stages of the screenplay:
Welles's first step toward the realization of "Citizen Kane" was to seek the assistance of a screenwriting professional. Fortunately, help was near at hand. . . . When Welles moved to Hollywood, it happened that a veteran screenwriter, Herman Mankiewicz, was recuperating from an automobile accident and between jobs ... Mankiewicz was an expatriate from Broadway who had been writing for films for almost fifteen years.
Mankiewicz met newspaper magnate William Randolph Hearst as a result of his friendship with Charles Lederer, another Hollywood screenwriter, who was a close nephew of Marion Davies, Hearst's mistress. Pauline Kael wrote that "Mankiewicz found himself on story-swapping terms with the power behind it all, Hearst himself ... through his friendship with Charles Lederer." Mankiewicz eventually saw Hearst as "... a finagling, calculating, Machiavellian figure," noted Kael, "and he and Lederer often wrote and had printed parodies of Hearst newspapers ..."
Mankiewicz, according to film author Harlan Lebo, was also "… one of Hollywood's most notorious personalities." Mankiewicz was the older brother of producer-director Joseph Mankiewicz, was a former writer for "The New Yorker" and "The New York Times" before moving to Hollywood in 1926. By the time Welles contacted him he had "... established himself as a brilliant wit, a writer of extraordinary talent, a warm friend to many of the screen world's brightest artists ... [he produced dialogue of the highest caliber."
"Herman Mankiewicz was a legendary figure in Hollywood," wrote Welles's associate John Houseman:
The son of a respected New Jersey schoolteacher, one of a brilliant class at Columbia, he had fought the war as a Marine, worked for the "World" and the "Times", collaborated on two unsuccessful plays with two otherwise infallibly successful playwrights, George Kaufman and Marc Connelly, come to California for six weeks to work on a silent film for Lon Chaney and stayed for sixteen years as one of the highest paid and most troublesome men in the business. His behavior, public and private, was a scandal. A neurotic drinker and a compulsive gambler, he was also one of the most intelligent, informed, witty, humane and charming men I have ever known.
Speaking with Peter Bogdanovich in February 1969, Orson Welles said, "Nobody was "more" miserable, "more" bitter, and "funnier" than Mank ... a perfect monument of self-destruction. But, you know, when the bitterness wasn't focused straight at you, he was the best company in the world." When Bogdanovich asked how important Mankiewicz was to the "Citizen Kane" script, Welles responded, "Mankiewicz's contribution? It was enormous."
Welles had engaged Mankiewicz to do script work on the stalled film project "The Smiler with a Knife". Despite a violent quarrel with Houseman in December 1939, after which Houseman had resigned from the Mercury, Welles arranged a lunch at New York's 21 Club with his former partner, and proposed that he work with Mankiewicz on a new project — "… little more than a notion, but an exciting one …," Houseman wrote:
Mankiewicz was notoriously unreliable: I asked Orson why he didn't take over the idea and write it himself. He said he didn't want to do that. Besides, Mank had asked for me to work with him. In the name of our former association Orson urged me to fly out, talk to Mankiewicz and, if I shared his enthusiasm, stay and work with him as his collaborator and editor till the script was done. It was an absurd venture, and that night Orson and I flew back to California together.
In February 1940 Mankiewicz was put on the Mercury payroll to work on a script with Houseman, a screenplay initially called "Orson Welles #1", then "American", and finally, "Citizen Kane". Writing took place from late February or March through early May 1940.
After finishing the script for "Citizen Kane", Mankiewicz gave a copy to Lederer, which Kael regarded as foolish:
He was so proud of his script that he lent a copy to Charles Lederer. In some crazily naive way, Mankiewicz seems to have imagined that Lederer would be pleased by how good it was. But Lederer, apparently, was deeply upset and took the script to his aunt and Hearst. It went from them to Hearst's lawyers … It was probably as a result of Mankiewicz's idiotic indiscretion that the various forces were set in motion that resulted in the cancellation of the premiere at the Radio City Music Hall the commercial failure of "Citizen Kane".
Lederer, however, told director Peter Bogdanovich that Kael was wrong in her conclusion, and that she never bothered to check with him about the facts. Lederer said he did not give Davies the script Mankiewicz loaned him: "I gave it "back" to him. He asked me if I thought Marion would be offended and I said I didn't think so."
Ideas and collaboration.
According to film historian Clinton Heylin, "... the idea of "Citizen Kane" was the original conception of Orson Welles, who in early 1940 first discussed the idea with John Houseman, who then suggested that both he and Welles leave for Los Angeles and discuss the idea with scriptwriter Herman Mankiewicz." He adds that Mankiewicz "... probably believed that Welles had little experience as an original scriptwriter ... may even have felt that "John Citizen USA", Welles's working title, was a project he could make his own."
When Houseman returned to California, he sat by the bedside of Mankiewicz — who was convalescing with a triple fracture of his left leg — and heard the basic outline of the story. "It was something he had been thinking about for years," Houseman wrote, "the idea of telling a man's private life (preferably one that suggested a recognizable American figure), immediately following his death, through the intimate and often incompatible testimony of those who had known him at different times and in different circumstances."
Welles himself had ideas that meshed with that concept, as he described in a 1969 interview in the book, "This is Orson Welles":
I'd been nursing an old notion – the idea of telling the same thing several times – and showing exactly the same thing from wholly different points of view. Basically, the idea "Rashomon" used later on. Mank liked it, so we started searching for the man it was going to be about. Some big American figure – couldn't be a politician, because you'd have to pinpoint him. Howard Hughes was the first idea. But we got pretty quickly to the press lords.
Hearst as story model.
According to film critic and author Pauline Kael, Mankiewicz "... was already caught up in the idea of a movie about Hearst ..." when he was still working at "The New York Times" in 1925. She learned from his family's babysitter, Marion Fisher, that she once typed as "... he dictated a screenplay, organized in flashbacks. She recalls that he had barely started on the dictation, which went on for several weeks, when she remarked that it seemed to be about William Randolph Hearst, and he said, 'You're a smart girl.'"
In Hollywood, Mankiewicz had frequented Hearst's parties until his alcoholism got him barred. Hearst was also a person known to Welles. "Once that was decided", wrote author Don Kilbourne, "Mankiewicz, Welles, and John Houseman, a cofounder of the Mercury Theatre, rented a place in the desert, and the task of creating "Citizen Kane" began." This "place in the desert" was on the historic Verde ranch on the Mojave River in Victorville. In later years, Houseman gave Mankiewicz "total" credit for "the creation of "Citizen Kane's" script" and credited Welles with "the visual presentation of the picture."
Mankiewicz was put under contract by Mercury Productions and was to receive no credit for his work as he was hired as a script doctor. According to his contract with RKO, Welles would be given sole screenplay credit, and had already written a rough script consisting of 300 pages of dialogue with occasional stage directions under the title of "John Citizen, USA".
In an interview with Huw Weldon on March 13, 1960, Orson Welles said: "Mr. Hearst was quite a bit like Kane, although Kane isn't really founded on Hearst in particular, many people sat for it so to speak".
Authorship.
One of the long standing debates of "Citizen Kane" has been the proper accreditation of the authorship of the screenplay, which the credits attribute to both Herman J. Mankiewicz and Orson Welles. Mankiewicz biographer Richard Meryman notes that the dispute had various causes, including the way the film was promoted. For instance, when RKO opened the film in New York City on May 1, 1941, followed by showings at theaters in other large cities, the publicity programs that were printed included photographs of Welles as "... the one-man band, directing, acting, and writing." In a letter to his father afterward, Mankiewicz wrote, "I'm particularly furious at the incredibly insolent description of how Orson wrote his masterpiece. The fact is that there isn't one single line in the picture that wasn't in writing – writing from and by me – before ever a camera turned."
Film historian Otto Friedrich wrote, "... it made unhappy to hear Welles quoted in Louella Parsons's column, before the question of screen credits was officially settled, as saying, 'So I wrote "Citizen Kane". ... Welles later claimed that he planned on a joint credit all along, but Mankiewicz claimed that Welles offered him a bonus of ten thousand dollars if he would let Welles take full credit."
Controversy over the authorship of the "Citizen Kane" screenplay was revived in 1971 by film critic Pauline Kael, whose essay, "Raising Kane," was printed in two installments in "The New Yorker" (February 20 and 27, 1971), and subsequently collected in "The Citizen Kane Book" (1971). According to Kael, Rita Alexander, Mankiewicz's personal secretary, stated that she "... took the dictation from Mankiewicz from the first paragraph to the last ... and later did the final rewriting and the cuts, and handled the script at the studio until after the film was shot. ... said Welles didn't write (or dictate) one line of the shooting script of "Citizen Kane."" She added that "Welles himself came to dinner once or twice ... she didn't meet him until after Mankiewicz had finished dictating the long first draft." However, Welles had his own secretary, Kathryn Trosper, who typed up Welles's suggestions and corrections, which were incorporated into the final script; Kael did not interview Trosper before producing her article.
Nevertheless, Kael maintained that Mankiewicz went to the Writers Guild and declared that he was the original author. According to Kael, "... he had ample proof of his authorship, and when he took his evidence to the Screen Writers Guild ... Welles was forced to split the credit and take second place in the listing." Charles Lederer, a screenwriter and a source for Kael's article, insisted that the credit never came to the Screen Writers Guild for arbitration.
Kael argued that Mankiewicz was the true author of the screenplay and therefore responsible for much of what made the film great. This angered many critics of the day, most notably critic-turned-filmmaker Peter Bogdanovich, a close friend of Welles who rebutted Kael's claims in an October 1972 article for "Esquire" titled "The "Kane" Mutiny." Other rebuttals included articles by Joseph McBride ("Film Heritage", Fall 1971) and Jonathan Rosenbaum ("Film Comment", Spring 1972 and Summer 1972), interviews with George Coulouris and Bernard Herrmann that appeared in "Sight & Sound" (Spring 1972), and remarks in Welles biographies by Barbara Leaming and Frank Brady. Rosenbaum also reviewed the controversy in his editor's notes to "This is Orson Welles" (1992).
"I happen to disagree with the premise of the whole book, because she tries to pretend that Welles is nothing and that a mediocre writer by the name of Mankiewicz was a hidden Voltaire," Bernard Herrmann said during a question-and-answer session following an October 1973 lecture at the George Eastman House Museum in Rochester, New York. "I'm not saying that Mankiewicz made no contribution. The titles clearly credit him. Orson says that he did make a valuable contribution. But really, without Orson, all of Mankiewicz's other pictures were nothing, before and after. With Orson, however, something happened to this wonderful man, but he could not have created "Citizen Kane"."
Robert L. Carringer likewise rebutted Kael's conclusions in an article titled "The Scripts of "Citizen Kane"" for the Winter 1978 edition of "Critical Enquiry". Carringer refers to early script drafts with Welles's incorporated handwritten contributions, and mentions the issues raised by Kael rested on the evidence of an early draft which was mostly written by Mankiewicz. However Carringer points out that subsequent drafts clarified Welles's contribution to the script:
Fortunately enough evidence to settle the matter has survived. A virtually complete set of script records for "Citizen Kane" has been preserved in the archives of RKO General Pictures in Hollywood, and these provide almost a day-to-day record of the history of the scripting ... The full evidence reveals that Welles's contribution to the "Citizen Kane" script was not only substantial but definitive.
Carringer notes that Mankiewicz' principal contribution was on the first two drafts of the screenplay, which he characterizes as being more like "rough gatherings" than actual drafts. Houseman accompanied Mankiewicz so as to ensure that the latter's drinking problem did not affect the screenplay. The early drafts established "... the plot logic and laid down the overall story contours, established the main characters, and provided numerous scenes and lines that would eventually appear in one form or another in the film." However he also noted that Kane in the early draft remained a caricature of Hearst rather than the fully developed character of the final film. The main quality missing in the early drafts but present in the final film is "... the stylistic wit and fluidity that is the most engaging trait of the film itself."
According to film critic David Thomson, "No one can now deny Herman Mankiewicz credit for the germ, shape, and pointed language of the screenplay, but no one who has seen the film as often as it deserves to be seen would dream that Welles is not its only begetter." Carringer considered that at least three scenes were solely Welles's work and, after weighing both sides of the argument, including sworn testimony from Mercury assistant Richard Baer, concluded, "We will probably never know for sure, but in any case Welles had at last found a subject with the right combination of monumentality, timeliness, and audacity." Harlan Lebo agrees, and adds, "... of far greater relevance is reaffirming the importance of the efforts that both men contributed to the creation of Hollywood's greatest motion picture."
Carringer notes that "Citizen Kane" was unusual in relation to his later films in that it was original material rather than adaptations of existing sources. He cites that Mankiewicz's main contribution was providing him with "... what any good first writer ought to be able to provide in such a case: a solid, durable story structure on which to build."
For his part, Welles stated the process of collaborating with Mankiewicz on the "Citizen Kane" screenplay in a letter to "The Times" (London), November 17, 1971:
The initial ideas for this film and its basic structure were the result of direct collaboration between us; after this we separated and there were two screenplays: one written by Mr. Mankiewicz, in Victorville, and the other, in Beverly Hills, by myself. ... The final version of the screenplay ... was drawn from both sources.
In his 1982 chronicle of the studio, "The RKO Story", scholar Richard B. Jewell concluded the following:
Besides producing, directing and playing the role of Kane, Welles deserved his co-authorship credit (with Herman J. Mankiewicz) on the screenplay. Film critic Pauline Kael argues otherwise in a 50,000 word essay on the subject, but her case against Welles is one-sided and unsupported by the facts.
Sources.
Charles Foster Kane.
Orson Welles never confirmed a principal source for the character of Charles Foster Kane. John Houseman, who edited and collaborated on the draft of the script written by Herman Mankiewicz, wrote that Kane is a synthesis of different personalities:
For the basic concept of Charles Foster Kane and for the main lines and significant events of his public life, Mankiewicz used as his model the figure of William Randolph Hearst. To this were added incidents and details invented or derived from other sources.
Houseman set out the obvious parallels, "... to which were grafted anecdotes from other giants of journalism, including Pulitzer, Northcliffe and Mank's first boss, Herbert Bayard Swope."
The film is commonly regarded as a fictionalized, unrelentingly hostile parody of William Randolph Hearst, in spite of Welles's statement that ""Citizen Kane" is the story of a wholly fictitious character." According to film historian Don Kilbourne, "... much of the information for "Citizen Kane" came from already-published material about Hearst ... some of Kane's speeches are almost verbatim copies of Hearst's. When Welles denied that the film was about the still-influential publisher, he did not convince many people."
Hearst biographer David Nasaw finds the film's depiction of Hearst unfair:
Welles' "Kane" is a cartoon-like caricature of a man who is hollowed out on the inside, forlorn, defeated, solitary because he cannot command the total obedience, loyalty, devotion, and love of those around him. Hearst, to the contrary, never regarded himself as a failure, never recognized defeat, never stopped loving Marion or his wife. He did not, at the end of his life, run away from the world to entomb himself in a vast, gloomy art-choked hermitage. Orson Welles may have been a great filmmaker, but he was neither a biographer nor a historian.
Arguing for the release of "Citizen Kane" before the RKO board, Welles pointed out the irony that it was Hearst himself who had brought so much attention to the film being about him, and that it was his own columnist, Louella Parsons, who was doing the most to publicize Kane's identification with Hearst. Public denials aside, Welles held the view that Hearst was a public figure and that the facts of a public figure's life were available for writers to reshape and restructure into works of fiction. Welles's legal advisor, Arnold Weissberger, put the issue in the form of a rhetorical question: "Will a man be allowed in effect to copyright the story of his life?"
In an interview in the 1992 book "This is Orson Welles", Welles said that he had excised one scene from Mankiewicz's first draft that had certainly been based on Hearst. "In the original script we had a scene based on a notorious thing Hearst had done, which I still cannot repeat for publication. And I cut it out because I thought it hurt the film and wasn't in keeping with Kane's character. If I'd kept it in, I would have had no trouble with Hearst. He wouldn't have dared admit it was him.
Pauline Kael wrote that a vestige of this abandoned subplot survives in a remark made by Susan Alexander to the reporter interviewing her: "Look, if you're smart, you'll get in touch with Raymond. He's the butler. You'll learn a lot from him. He knows where all the bodies are buried." Kael observed, "It's an odd, cryptic speech. In the first draft, Raymond "literally" knew where the bodies were buried: Mankiewicz had dished up a nasty version of the scandal sometimes referred to as the Strange Death of Thomas Ince." Referring to the suspicious 1924 death of the American film mogul after being a guest on Hearst's yacht, and noting that Kael's principal source was John Houseman, film scholar Jonathan Rosenbaum wrote that "it seems safe to conclude, even without her prodding, that some version of the story must have cropped up in Mankiewicz's first draft of the script, which Welles subsequently edited and added to."
One particular aspect of the character, Kane's profligate collecting of possessions, was directly taken from Hearst. "And it's very curious – a man who spends his entire life paying cash for objects he never looked at," Welles said. "He just acquired things, most of which were never opened, remained in boxes. It's really a quite accurate picture of Hearst to that extent." But Welles himself insisted that there were marked differences between his fictional creation and Hearst. He acknowledged that aspects of Kane were drawn from the lives of two business tycoons familiar from Welles's youth in Chicago — Samuel Insull and Harold Fowler McCormick.
A financier closely associated with Thomas Edison, Samuel Insull (1859–1938) was a man of humble origins who became the most powerful figure in the utilities field. He was married to a Broadway ingenue nearly 20 years his junior, and built the Chicago Civic Opera House. In 1925, after a 26-year absence, Gladys Wallis Insull returned to the stage in a charity revival of "The School for Scandal" that ran two weeks in Chicago. When the performance was repeated on Broadway in October 1925, Herman Mankiewicz — then the third-string theatre critic for "The New York Times" — was assigned to review the production. In an incident that became infamous, Mankiewicz returned to the press room drunk and wrote only the first sentence of a negative review before passing out on his typewriter. Mankiewicz resurrected the experience in writing the screenplay for "Citizen Kane", incorporating it into the narrative of Jedediah Leland.
In 1926 Insull took a six-year lease on Chicago's Studebaker Theatre and financed a repertory company in which his wife starred. Gladys Insull's nerves broke when her company failed to find success, and the lease expired at the same time Insull's $3 billion financial empire collapsed in the Depression. Like that of Charles Foster Kane, the life of Samuel Insull ended in bankruptcy and disgrace.
Like Kane, Harold McCormick was divorced by his aristocratic first wife, Edith Rockefeller, and lavishly promoted the opera career of his only modestly talented second wife, Ganna Walska.
According to composer David Raksin, Bernard Herrmann used to say that much of Kane's story was based on McCormick, but that there was also a good deal of Orson Welles himself in the flamboyant character. Welles lost his mother when he was nine years old and his father when he was 15. After this, he became the ward of Chicago's Dr. Maurice Bernstein. Bernstein is the last name of the only major character in "Citizen Kane" who receives a generally positive portrayal. Although Dr. Bernstein was nothing like the character in the film, Welles said, the use of the name "Bernstein" was a family joke. "I used to call people 'Bernstein' on the radio, all the time, too – just to make him laugh. ... Mank did all the best writing for Bernstein. I'd call that "the" most valuable thing he gave us."
Welles cited financier Basil Zaharoff as another inspiration for Kane. "I got the idea for the hidden-camera sequence in the Kane 'news digest' from a scene I did on "March of Time" in which Zaharoff, this great munitions-maker, was being moved around in his rose garden, just talking about the roses, in the last days before he died," Welles said. Robert L. Carringer reviewed the December 3, 1936, script of the radio obituary in which Welles played Zaharoff, and found other similarities. In the opening scene, Zaharoff's secretaries are burning masses of secret papers in the enormous fireplace of his castle. A succession of witnesses testify about the tycoon's ruthless practices. "Finally, Zaharoff himself appears — an old man nearing death, alone except for his servants in the gigantic palace in Monte Carlo that he had acquired for his longtime mistress. His dying wish is to be wheeled out 'in the sun by that rosebush.'"
Jedediah Leland.
In Hollywood in 1940, Orson Welles invited longtime friend and Mercury Theatre colleague Joseph Cotten to be part of a small group reading the script aloud for the first time. They got together around the pool at the Beverly Hills home of Herman Mankiewicz, Cotten wrote:
"I think I'll just listen," Welles said. "The title of this movie is "Citizen Kane", and I play guess who." He turned to me. "Why don't you think of yourself as Jedediah Leland? His name, by the way, is a combination of Jed Harris and your agent, Leland Hayward." "There all resemblance ceases," Herman reassured me. These afternoon garden readings continued, and as the Mercury actors began arriving, the story started to breathe.
"I regard Leland with enormous affection," Orson Welles said to filmmaker Peter Bogdanovich. He told Bogdanovich that the character of Jed Leland was based on drama critic Ashton Stevens, George Stevens's uncle and a close boyhood friend of Welles:
What I knew about Hearst came more from him than from my father – though my father did know him well ... But Ashton had taught Hearst to play the banjo, which is how he first got to be a drama critic, and, you know, Ashton was really one of the great ones. The last of the dandies – he worked for Hearst for some 50 years or so, and adored him. A gentleman ... very much like Jed.
Regarded as the dean of American drama critics, Ashton Stevens (1872–1951) began his journalism career in 1894 in San Francisco and started working for the Hearst newspapers three years later. In 1910 he moved to Chicago where he covered the theatre for 40 years and became a close friend of Orson Welles's guardian, Dr. Maurice Bernstein.
Screenwriter Herman Mankiewicz incorporated an incident from his own early career as a theatre critic for "The New York Times" into the narrative of Jed Leland. Mankiewicz was assigned to review the October 1925 opening of "The School for Scandal" — a production that marked the return of Gladys Wallis to the Broadway stage. A famous ingenue of the 1890s, Wallis had retired upon her marriage to Chicago utilities magnate Samuel Insull but was now, 26 years later, using her husband's fortune to form her own repertory company. After her opening-night performance in the role of Lady Teazle, drama critic Mankiewicz returned to the press room "... full of fury and too many drinks ...," wrote biographer Richard Meryman:
He was outraged by the spectacle of a 56-year-old millionairess playing a gleeful 18-year-old, the whole production bought for her like a trinket by a man Herman knew to be an unscrupulous manipulator. Herman began to write: "Miss Gladys Wallis, an aging, hopelessly incompetent amateur, opened last night in ..." Then Herman passed out, slumped over the top of his typewriter.
The unconscious Mankiewicz was discovered by his boss, George S. Kaufman, who composed a terse announcement that the "Times" review would appear the following day.
Mankiewicz resurrected the incident for "Citizen Kane". After Kane's second wife makes her opera debut, critic Jed Leland returns to the press room drunk. He passes out over the top of his typewriter after writing the first sentence of his review: "Miss Susan Alexander, a pretty but hopelessly incompetent amateur ..."
Susan Alexander.
The common assumption that the character of Susan Alexander was based on Marion Davies was a major reason William Randolph Hearst tried to destroy "Citizen Kane". In his foreword to Davies's autobiography, published posthumously in 1975, Orson Welles draws a sharp distinction between the real-life actress and his fictional creation:
That Susan was Kane's wife and Marion was Hearst's mistress is a difference more important than might be guessed in today's changed climate of opinion. The wife was a puppet and a prisoner; the mistress was never less than a princess. Hearst built more than one castle, and Marion was the hostess in all of them: they were pleasure domes indeed, and the Beautiful People of the day fought for invitations. Xanadu was a lonely fortress, and Susan was quite right to escape from it. The mistress was never one of Hearst's possessions: he was always her suitor, and she was the precious treasure of his heart for more than 30 years, until his last breath of life. Theirs is truly a love story. Love is not the subject of "Citizen Kane".
Welles cited Samuel Insull's building of the Chicago Opera House, and business tycoon Harold Fowler McCormick's lavish promotion of the opera career of his second wife, as direct influences on the screenplay. McCormick divorced Edith Rockefeller and married aspiring opera singer Ganna Walska as her fourth husband. He spent thousands of dollars on voice lessons for her and even arranged for Walska to take the lead in a production of "Zaza" at the Chicago Opera in 1920. Contemporaries said Walska had a terrible voice; "The New York Times" headlines of the day read, "Ganna Walska Fails as Butterfly: Voice Deserts Her Again When She Essays Role of Puccini's Heroine" (January 29, 1925), and "Mme. Walska Clings to Ambition to Sing" (July 14, 1927).
"According to her 1943 memoirs, "Always Room at the Top," Walska had tried every sort of fashionable mumbo jumbo to conquer her nerves and salvage her voice," reported "The New York Times" in 1996. "Nothing worked. During a performance of Giordano's "Fedora" in Havana she veered so persistently off key that the audience pelted her with rotten vegetables. It was an event that Orson Welles remembered when he began concocting the character of the newspaper publisher's second wife for "Citizen Kane"."
Charles Lederer, Marion Davies's nephew, read a draft of the script before filming began on "Citizen Kane". "The script I read didn't have any flavor of Marion and Hearst," Lederer said. "Robert McCormick was the man it was about." (Lederer confuses Walska's husband Harold F. McCormick with another member of the powerful Chicago family, one who may also have inspired Welles – crusading publisher Robert R. McCormick of the "Chicago Tribune".) Although there were things based on Marion Davies – jigsaw puzzles and drinking – Lederer noted that they were exaggerated in the film to help define the characterization of Susan Alexander.
"As for Marion," Orson Welles said, "she was an extraordinary woman – nothing like the character Dorothy Comingore played in the movie."
Film tycoon Jules Brulatour's second and third wives, Dorothy Gibson and Hope Hampton, both fleeting stars of the silent screen who later had marginal careers in opera, are also believed to have provided inspiration for the Susan Alexander character. The interview with Susan Alexander Kane in the Atlantic City nightclub was based on a contemporary interview with Evelyn Nesbit Thaw in the run-down club where she was performing.
Jim Gettys.
The character of political boss Jim Gettys (Ray Collins) is based on Charles F. Murphy, a leader in New York City's infamous Tammany Hall political machine. William Randolph Hearst and Murphy were political allies in 1902, when Hearst was elected to the U.S. House of Representatives, but the two fell out in 1905 when Hearst ran for mayor of New York. Hearst turned his muckraking newspapers on Tammany Hall in the person of Murphy, who was called "... the most hungry, selfish and extortionate boss Tammany has ever known." Murphy ordered that under no condition was Hearst to be elected. Hearst ballots were dumped into the East River, and new ballots were printed favoring his opponent. Hearst was defeated by some 3,000 votes and his newspapers bellowed against the election fraud. A historic cartoon of Murphy in convict stripes appeared November 10, 1905, three days after the vote. The caption read, "Look out, Murphy! It's a Short Lockstep from Delmonico's to Sing Sing ... Every honest voter in New York wants to see you in this costume."
In "Citizen Kane", Boss Jim Gettys (named Edward Rogers in the shooting script) admonishes Kane for printing a cartoon showing him in prison stripes:
If I owned a newspaper and if I didn't like the way somebody else was doing things – some politician, say – I'd fight them with everything I had. Only I wouldn't show him in a convict suit with stripes — so his children could see the picture in the paper. Or his mother.
As he pursues Gettys down the stairs, Kane threatens to send him to Sing Sing.
As an inside joke, Welles named Gettys after the father-in-law of Roger Hill, his headmaster at the Todd School and a lifelong friend.
"Rosebud".
In "This is Orson Welles", Welles credits the "Rosebud" device – the journalist's search for the enigmatic meaning of Kane's last word, the device that frames the film – to screenwriter Herman Mankiewicz. "Rosebud remained, because it was the only way we could find to get off, as they used to say in vaudeville," Welles said. "It manages to work, but I'm still not too keen about it, and I don't think that he was, either." The dialogue eventually reflects the screenwriters' desire to diminish the importance of the word's meaning; "We did everything we could to take the mickey out of it," Welles said.
As he began his first draft of the "Citizen Kane" screenplay in early 1940, Mankiewicz mentioned "Rosebud" to his secretary. When she asked, "Who is rosebud?" he replied, "It isn't a who, it's an it." The symbol of Mankiewicz's own damaged childhood was a treasured bicycle, stolen while he visited the public library and, in punishment, never replaced. "He mourned that all his life," wrote Pauline Kael, who believed Mankiewicz put the emotion of that boyhood loss into the loss that haunted Kane.
In his 2002 book "Hearst Over Hollywood", Louis Pizzitola reports one historian's statement that "Rosebud" was a nickname given to William Randolph Hearst's mother by portrait and landscape painter Orrin Peck. The Peck family were intimates of the Hearsts, and Orrin Peck was said to be nearer to Phoebe Apperson Hearst than her own son. Another theory of the origin of "Rosebud" is the similarity with the dying wish of Basil Zaharoff (who is one of the inspirations for the central character), to be wheeled "by the rosebush".
In 1989 author Gore Vidal stated that "Rosebud" was a nickname which Hearst had used for the clitoris of his mistress, Marion Davies. Vidal said that Davies had told this intimate detail to his close nephew, Charles Lederer, who had mentioned it to him years later. The claim was repeated in the 1996 documentary "The Battle Over Citizen Kane" and again in the 1999 dramatic film "RKO 281".
Film critic Roger Ebert said, "Some people have fallen in love with the story that Herman Mankiewicz, the co-author with Welles of the screenplay, happened to know that 'Rosebud' was William Randolph Hearst's pet name for an intimate part of Marion Davies' anatomy."
Welles biographer Frank Brady traces the story to the popular press in the late 1970s:
How Orson (or Mankiewicz) could have ever discovered this most private utterance is unexplained and why it took over 35 years for such a suggestive rationale to emerge, although the origins of everything to do with "Citizen Kane" had continually been placed under literary and cinematic microscopes for decades, is also unknown. If this highly unlikely story is even partially true ... Hearst may have become upset at the implied connotation, although any such connection seems to have been innocent on Welles's part. In any event, this bizarre explanation for the origin of one of the most famous words ever spoken on the screen has now made its way into serious studies of Welles and "Citizen Kane".
British film critic Philip French asked Welles's associate John Houseman, who worked on the "Citizen Kane" script with Mankiewicz, whether there was any truth to the story:
"Absolutely none," he said, pointing out that it was inconceivable that he would not have heard of something so provocative at the time, or that Welles could have kept such a secret for over 40 years.
In 1991, Edward Castle, a reporter for "The Las Vegas Sun", contended that Welles may have borrowed the name of Native American folklorist, educator and author Rosebud Yellow Robe for "Rosebud". Castle claimed to have found both of their signatures on the same sign-in sheets at CBS Radio studios in New York, where they both worked on different shows in the late 1930s. The word "Rosebud" appears, however, in the first draft script written by Herman Mankiewicz, not Welles.
"Rosebud is the emblem of the security, hope and innocence of childhood, which a man can spend his life seeking to regain," summarized Roger Ebert. "It is the green light at the end of Gatsby's pier; the leopard atop Kilimanjaro, seeking nobody knows what; the bone tossed into the air in ""."
Filmmaking innovations.
Orson Welles said that his preparation before making "Citizen Kane" was to watch John Ford's "Stagecoach" 40 times. 
A lot of people ought to study "Stagecoach". I wanted to learn how to make movies, and that's such a classically perfect one — don't you think so? ... As it turned out, the first day I ever walked onto a set was my first day as a director. I'd learned whatever I knew in the projection room — from Ford. After dinner every night for about a month, I'd run "Stagecoach", often with some different technician or department head from the studio, and ask questions. "How was this done?" "Why was this done?" It was like going to school.
Cinematography.
Film scholars and historians view "Citizen Kane" as Welles's attempt to create a new style of filmmaking by studying various forms of film making, and combining them all into one. However, in an interview in March 1960 with the BBC's Huw Wheldon, Welles stated that his love for cinema began only when he started the work on "Citizen Kane", and when asked where he got the confidence as a first-time director to direct a film so radically different from contemporary cinema, he responded, " ignorance ... sheer ignorance. There is no confidence to equal it. It's only when you know something about a profession that you are timid or careful."
The most innovative technical aspect of "Citizen Kane" is the extended use of deep focus. In nearly every scene in the film, the foreground, background and everything in between are all in sharp focus. This was done by cinematographer Gregg Toland through his experimentation with lenses and lighting. Toland described the achievement, made possible by the sensitivity of modern speed film, in an article for "Theatre Arts" magazine:
New developments in the science of motion picture photography are not abundant at this advanced stage of the game but periodically one is perfected to make this a greater art. Of these I am in an excellent position to discuss what is termed “Pan-focus”, as I have been active for two years in its development and used it for the first time in "Citizen Kane". Through its use, it is possible to photograph action from a range of eighteen inches from the camera lens to over two hundred feet away, with extreme foreground and background figures and action both recorded in sharp relief. Hitherto, the camera had to be focused either for a close or a distant shot, all efforts to encompass both at the same time resulting in one or the other being out of focus. This handicap necessitated the breaking up of a scene into long and short angles, with much consequent loss of realism. With pan-focus, the camera, like the human eye, sees an entire panorama at once, with everything clear and lifelike.
Any time deep focus was impossible – for example in the scene when Kane finishes a bad review of Alexander's opera while at the same time firing the person who started the review – an optical printer was used to make the whole screen appear in focus (visually layering one piece of film onto another). However, some apparently deep-focus shots were the result of in-camera effects, as in the famous scene where Kane breaks into Susan Alexander's room after her suicide attempt. In the background, Kane and another man break into the room, while simultaneously the medicine bottle and a glass with a spoon in it are in closeup in the foreground. The shot was an in-camera matte shot. The foreground was shot first, with the background dark. Then the background was lit, the foreground darkened, the film rewound, and the scene re-shot with the background action.
Another unorthodox method used in the film was the way low-angle shots were used to display a point of view facing upwards, thus allowing ceilings to be shown in the background of several scenes. Since films were primarily filmed on sound stages and not on location during the era of the Hollywood studio system, it was impossible to film at an angle that showed ceilings because the stages had none. In some instances, Welles's crew used muslin draped above the set to produce the illusion of a regular room with a ceiling. The boom microphones were hidden above the cloth, or in a trench dug into the floor, as in the scene where Kane meets Leland after his election loss.
Toland had approached Welles in 1940 to work on "Citizen Kane". Welles's reputation for experimentation in the theatre appealed to Toland and he found a sympathetic partner to "... test and prove several ideas generally being accepted as radical in Hollywood". Welles credited Toland on the same card as himself. "It's impossible to say how much I owe to Gregg. He was superb," Welles said.
Storytelling techniques.
"Citizen Kane" eschews the traditional linear, chronological narrative, and tells Kane's story entirely in flashback using different points of view, many of them from Kane's aged and forgetful associates, the cinematic equivalent of the unreliable narrator in literature. Welles also dispenses with the idea of a single storyteller and uses multiple narrators to recount Kane's life. The use of multiple narrators was unheard of in Hollywood films. Each narrator recounts a different part of Kane's life, with each story partly overlapping. The film depicts Kane as an enigma, a complicated man who, in the end, leaves viewers with more questions than answers as to his character, such as the newsreel footage where he is attacked for being both a communist and a fascist. The technique of using flashbacks had been used in earlier films such as "Wuthering Heights" in 1939 and "The Power and the Glory" in 1933, but no film was as immersed in this technique as "Citizen Kane". The use of the reporter Thompson acts as a surrogate for the audience, questioning Kane's associates and piecing together his life.
One of the narrative voices is the "News on the March" segment. Its stilted dialogue and portentous voiceover is a parody of "The March of Time" newsreel series which itself references an earlier newsreel which showed the 85-year-old arms czar Sir Basil Zaharoff getting wheeled to his train. Welles had earlier provided voiceovers for the "March of Time" radio show. "Citizen Kane" makes extensive use of stock footage to create the newsreel.
One of the story-telling techniques used in "Citizen Kane" was the use of montage to collapse time and space, using an episodic sequence on the same set while the characters changed costume and make-up between cuts so that the scene following each cut would look as if it took place in the same location, but at a time long after the previous cut. In the breakfast montage, Welles chronicles the breakdown of Kane's first marriage in five vignettes that condense 16 years of story time into two minutes of screen time. Welles said that the idea for the breakfast scene "... was stolen from "The Long Christmas Dinner" of Thornton Wilder ... a one-act play, which is a long Christmas dinner that takes you through something like 60 years of a family's life."
Special effects.
Welles also pioneered several visual effects in order to cheaply shoot things like crowd scenes and large interior spaces. For example, the scene where the camera in the opera house rises dramatically to the rafters to show the workmen showing a lack of appreciation for Susan Alexander Kane's performance, was shot by a camera craning upwards over the performance scene, then a curtain wipe to a miniature of the upper regions of the house, and then another curtain wipe matching it again with the scene of the workmen. Other scenes effectively employed miniatures to make the film look much more expensive than it truly was, such as various shots of Xanadu. A loud, full-screen closeup of a typewriter typing a single word (weak), magnifies the review for the "Chicago Inquirer".
Makeup.
The make-up artist Maurice Seiderman created the make-up for the film. RKO wanted the young Kane to look handsome and dashing, and Seiderman transformed the overweight Welles, beginning with his nose, which Welles always disliked. For the old Kane, Seiderman created a red plastic compound which he applied to Welles, allowing the wrinkles to move naturally. Kane's mustache was made of several hair tufts. Transforming Welles into the old Kane required six to seven hours, meaning he had to start at two in the morning to begin filming at nine. He would hold conferences while sitting in the make-up chair; sometimes working 16 hours a day. Even breaking a leg during filming could not stop him from directing around the clock, and he quickly returned to acting, using a steel leg brace.
Soundtrack.
"Before "Kane", nobody in Hollywood knew how to set music properly in movies," wrote filmmaker François Truffaut in a 1967 essay. ""Kane" was the first, in fact the only, great film that uses radio techniques."
Behind each scene, there is a resonance which gives it its color: the rain on the windows of the cabaret, "El Rancho," when the investigator goes to visit the down-and-out female singer who can only "work" Atlantic City; the echoes in the marble-lined Thatcher library; the overlapping voices whenever there are several characters. A lot of filmmakers know enough to follow Auguste Renoir's advice to fill the eyes with images at all costs, but only Orson Welles understood that the sound track had to be filled in the same way.
In addition to expanding on the potential of sound as a creator of moods and emotions, Welles pioneered a new aural technique, known as the "lightning-mix". Welles used this technique to link complex montage sequences via a series of related sounds or phrases. In offering a continuous sound track, Welles was able to join what would otherwise be extremely rough cuts together into a smooth narrative. For example, the audience witnesses Kane grow from a child into a young man in just two shots. As Kane's guardian hands him his sled, Kane begrudgingly wishes him a "Merry Christmas". Suddenly we are taken to a shot of his guardian fifteen years later, only to have the phrase completed for us: "and a Happy New Year". In this case, the continuity of the soundtrack, not the image, is what makes for a seamless narrative structure.
Welles also carried over techniques from radio not yet popular in films (though they would become staples). Using a number of voices, each saying a sentence or sometimes merely a fragment of a sentence, and splicing the dialogue together in quick succession, the result gave the impression of a whole town talking – and, equally important, what the town was talking about. Welles also favored the overlapping of dialogue, considering it more realistic than the stage and film tradition of characters not stepping on each other's sentences. He also pioneered the technique of putting the audio ahead of the visual in scene transitions (a J-cut); as a scene would come to a close, the audio would transition to the next scene before the visuals did.
Music.
In common with using personnel he had previously worked with in the Mercury Theatre, Welles recruited his close friend Bernard Herrmann to score "Citizen Kane". Herrmann was a longtime collaborator with Welles, providing music for almost all his radio broadcasts, including "The Fall of the City" (1937) and the "War of the Worlds" (1938) broadcast. The film was Herrmann's first motion picture score, and would be nominated for an Academy Award for Best Original Score, but would lose out to his own score for the film "All That Money Can Buy".
Herrmann's score for "Citizen Kane" was a watershed in film soundtrack composition and proved as influential as any of the film's other innovations, establishing him as an important voice in film soundtrack composition. The score eschewed the typical Hollywood practice of scoring a film with virtually non-stop music. Instead Herrmann used what he later described as '"radio scoring", musical cues which typically lasted between five and fifteen seconds to bridge the action or suggest a different emotional response. One of the most effective musical cues was the "Breakfast Montage." The scene begins with a graceful waltz theme and gets darker with each variation on that theme as the passage of time leads to the hardening of Kane's personality and the breakup of his marriage to Emily.
Herrmann realized that musicians slated to play his music were hired for individual unique sessions; there was no need to write for existing ensembles. This meant that he was free to score for unusual combinations of instruments, even instruments that are not commonly heard. In the opening sequence, for example, the tour of Kane's estate Xanadu, Herrmann introduces a recurring leitmotiv played by low woodwinds, including a quartet of alto flutes. Much of the music used in the newsreel was taken from other sources; examples include the "News on the March" music which was taken from RKO's music library, "Belgian March" by Anthony Collins, and accompanies the newsreel titles; and an excerpt from Alfred Newman's score for "Gunga Din" which is used as the background for the exploration of Xanadu. In the final sequence of the film, which shows the destruction of Rosebud in the fireplace of Kane's castle, Welles choreographed the scene while he had Herrmann's cue playing on the set.
For the operatic sequence which exposed Kane's protege Susan Alexander for the amateur she was, Herrmann composed a quasi-romantic scene, "Aria from Salammbô". There did exist two treatments of this 1862 novel by Gustave Flaubert: an opera by Ernest Reyer and an incomplete treatment by Modeste Mussorgsky. However, Herrmann made no reference to existing music. Herrmann put the aria in a key that would force the singer to strain to reach the high notes, culminating in a high D, well outside the range of Susan Alexander. Herrmann said he wanted to convey the impression of "... a terrified girl floundering in the quicksand of a powerful orchestra". On the soundtrack it was soprano Jean Forward who actually sang the vocal part for actress Dorothy Comingore.
In 1972 Herrmann said "I was fortunate to start my career with a film like "Citizen Kane", it's been a downhill run ever since!" Shortly before his death in 1985, Welles told director Henry Jaglom that the score was fifty percent responsible for the film's artistic success.
Herrmann was vocal in his criticism of Pauline Kael's claim that it was Mankiewicz, not Welles, who made the main thrust of the film, and also her assertions about the use of music in the film without consulting him:
Pauline Kael has written in "The Citizen Kane Book" (1971), that the production wanted to use Massenet's "Thais" but could not afford the fee. But Miss Kael never wrote or approached me to ask about the music. We could easily have afforded the fee. The point is that its lovely little strings would not have served the emotional purpose of the film.
Opera lovers are frequently amused by the parody of vocal coaching that appears in a singing lesson given to Susan Alexander by Signor Matiste. The character attempts to sing the famous cavatina "Una voce poco fa" from "Il barbiere di Siviglia" by Gioachino Rossini, but the lesson is interrupted when Alexander sings a high note flat.
At the beginning of Thompson's second interview of Susan Kane at her nightclub, the tune heard in the background is "In a Mizz", a 1939 jazz song by Charlie Barnet and Haven Johnson. "I kind of based the whole scene around that song," Orson Welles said. "The music is by Nat Cole — it's his trio." Later — beginning with the lyrics, "It can't be love" — "In a Mizz" is performed at the Everglades picnic, framing the fight in the tent between Susan and Kane. Musicians including bandleader Cee Pee Johnson (drums), Alton Redd (vocals), Raymond Tate (trumpet), Buddy Collette (alto sax) and Buddy Banks (tenor sax) are featured.
Reception.
Pre-release controversy.
Welles ran a closed set, limited access to dailies, and managed the publicity of "Kane", to ensure that its influence from Hearst's life was a secret. Publicity materials stated the film's inspiration was "Faust". RKO hoped to release the film in mid-February 1941. Writers for national magazines had early deadlines and so a rough cut was previewed for a select few on January 3, 1941. "Friday" magazine ran an article drawing point-by-point comparisons between Kane and Hearst and documented how Welles had led on Louella Parsons, Hollywood correspondent for Hearst papers, and made a fool of her in public. Reportedly, she was furious and demanded an immediate preview of the film. James G. Stewart, who was present at the screening, said that she walked out of the film. Soon after, Parsons called George Schaefer and threatened RKO with a lawsuit if they released "Kane". The next day, the front page headline in "Daily Variety" read, "HEARST BANS RKO FROM PAPERS." In two weeks, the ban was lifted for everything except "Kane."
"The Hollywood Reporter" ran a front-page story on January 13 that Hearst papers were about to run a series of editorials attacking Hollywood's practice of hiring refugees and immigrants for jobs that could be done by Americans. The goal was to put pressure on the other studios in order to force RKO to shelve "Kane". Soon afterwards, Schaefer was approached by Nicholas Schenck, head of MGM's parent company, with an offer on the behalf of Louis B. Mayer and other Hollywood executives to reimburse RKO if it would destroy the film. Once RKO's legal team reassured Schaefer, the studio announced on January 21 that "Kane" would be released as scheduled, and with one of the largest promotional campaigns in the studio's history. Schaefer brought Welles to New York City for a private screening of the film with the New York corporate heads of the studios and their lawyers. There was no objection to its release provided that certain changes, including the removal or softening of specific references that might offend Hearst, were made. Welles agreed, and editor Robert Wise (who became a celebrated film director in the 1950s and 60s) was brought in to cut the running time from two hours, two minutes, and 40 seconds to one hour, 59 minutes, and 16 seconds. That cut satisfied the corporate lawyers.
Hearst's response.
Hearing about the film enraged Hearst so much that he banned any advertising, reviewing, or mentioning of it in his papers, and had his journalists libel Welles. Following lobbying from Hearst, the head of Metro-Goldwyn-Mayer, Louis B. Mayer, acting on behalf of the whole film industry, made an offer to RKO Pictures of $805,000 to destroy all prints of the film and burn the negative. Welles used Hearst's opposition to "Citizen Kane" as a pretext for previewing the film in several opinion-making screenings in Los Angeles, lobbying for its artistic worth against the hostile campaign that Hearst was waging.
When George Schaefer of RKO rejected Hearst's offer to suppress the film, Hearst banned every newspaper and station in his media conglomerate from reviewing – or even mentioning – the film. He also had many movie theaters ban it, and many did not show it through fear of being socially exposed by his massive newspaper empire. The documentary "The Battle Over Citizen Kane" lays the blame for "Citizen Kane"s relative failure squarely at the feet of Hearst. The film did decent business at the box office; it went on to be the sixth highest grossing film in its year of release, a modest success its backers found acceptable. Nevertheless, the film's commercial performance fell short of its creators' expectations. In "The Chief: The Life of William Randolph Hearst", David Nasaw points out that Hearst's actions were not the only reason "Kane" failed, however: the innovations Welles made with narrative, as well as the dark message at the heart of the film (that the pursuit of success is ultimately futile) meant that a popular audience could not appreciate its merits.
In a pair of "Arena" documentaries about Welles's career produced and broadcast domestically by the BBC in 1982, Welles claimed that during opening week, a policeman approached him one night and told him: "Do not go to your hotel room tonight; Hearst has set up an undressed, underage girl to leap into your arms when you enter and a photographer to take pictures of you. Hearst is planning to publish it in all of his papers." Welles thanked the man and stayed out all night. However, it is not confirmed whether this was true. Welles also described how he accidentally bumped into Hearst in an elevator at the Fairmont Hotel when "Kane" was opening in San Francisco. Welles's father had been friends with Hearst, so Welles tried to comfortably ask if Hearst would see the film. Hearst ignored him. "As he was getting off at his floor, I said 'Charles Foster Kane would have accepted.' No reply", recalled the director. "And Kane would have you know. That was his style."
Although Hearst tried to suppress the film and limit its success, his efforts backfired in the long run, for now almost every reference to Hearst's life and career includes a reference to the parallels in the film. The irony of Hearst's attempts is that the film is now inexorably connected to him. This connection is reinforced by W. A. Swanberg's extensive biography entitled "Citizen Hearst".
Release and contemporary responses.
"Citizen Kane" was to open at RKO's flagship theatre, Radio City Music Hall, but did not; a possible factor was Louella Parsons's threat that "The American Weekly" would run a defamatory story on the grandfather of major RKO stockholder Nelson Rockefeller. Other exhibitors feared retaliation and refused to handle the film. Schaefer lined up a few theaters but Welles grew impatient and threatened RKO with a lawsuit. Hearst papers refused to accept advertising for the film. "Kane" opened at the RKO Palace on Broadway in New York on May 1, 1941, in Chicago on May 6, and in Los Angeles on May 8. "Kane" did well in cities and larger towns but fared poorly in more remote areas. RKO still had problems getting exhibitors to show the film. For example, one chain controlling more than 500 theaters got Welles's film as part of a package but refused to play it, reportedly out of fear of Hearst. The Hearst newspapers's disruption of the film's release damaged its box office performance and, as a result, "Citizen Kane" lost $160,000 during its initial run.
The reviews for the film were overwhelmingly positive, although some reviewers were challenged by Welles's break with Hollywood traditions. Kate Cameron, in her review for the "New York Daily-News", said that "Kane" was "... one of the most interesting and technically superior films that has ever come out of a Hollywood studio". In his review for the "New World Telegram", William Boehnel said that the film was "... staggering and belongs at once among the greatest screen achievements". Otis Ferguson, in his review for "The New Republic", said that "Kane" was "... the boldest free-hand stroke in major screen production since Griffith and Bitzer were running wild to unshackle the camera". John O'Hara, in "Newsweek", called it "... the best picture he'd ever seen"
The day following the premiere of "Citizen Kane," "The New York Times" critic Bosley Crowther wrote that "... it comes close to being the most sensational film ever made in Hollywood."
Count on Mr. Welles: he doesn't do things by halves. ... Upon the screen he discovered an area large enough for his expansive whims to have free play. And the consequence is that he has made a picture of tremendous and overpowering scope, not in physical extent so much as in its rapid and graphic rotation of thoughts. Mr. Welles has put upon the screen a motion picture that really moves.
Critic James Agate was decidedly negative in an October 1941 review, countering the superlatives given "Citizen Kane" by critics C. A. Lejeune and Dilys Powell. "Now imagine my horror, which includes self-distrust, at seeing no more in this film than the well-intentioned, muddled, amateurish thing one expects from high-brows. (Mr. Orson Welles's height of brow is enormous.) ... I thought the photography quite good, but nothing to write to Moscow about, the acting middling, and the whole thing a little dull."
Agate continued his review two weeks later:
"Citizen Kane" has entirely ousted the war as conversation fodder. Waiters ask me what I think of it, and the post is full of it. ... You know now that all the vulgar beef, beer and tobacco barons are vulgar because when they were about seven years of age somebody came and took away their skates. That is one explanation of this alleged world-shaking masterpiece, "Citizen Kane". Another point of view is that "Citizen Kane" is so great a masterpiece that it doesn't need explaining. ... In the meantime I continue to steer a middle course. I regard "Citizen Kane" as a quite good film which tries to run the psychological essay in harness with your detective thriller, and doesn't quite succeed.
In a 1941 review, Jorge Luis Borges called "Citizen Kane" a "metaphysical detective story", in that "... subject (both psychological and allegorical) is the investigation of a man's inner self, through the works he has wrought, the words he has spoken, the many lives he has ruined ..." Borges noted that "Overwhelmingly, endlessly, Orson Welles shows fragments of the life of the man, Charles Foster Kane, and invites us to combine them and reconstruct him." As well, "Forms of multiplicity and incongruity abound in the film: the first scenes record the treasures amassed by Kane; in one of the last, a poor woman, luxuriant and suffering, plays with an enormous jigsaw puzzle on the floor of a palace that is also a museum." Borges points out, "At the end we realize that the fragments are not governed by a secret unity: the detested Charles Foster Kane is a simulacrum, a chaos of appearances."
Accolades.
14th Academy Awards  – 1941.
"Citizen Kane", with nine nominations, was the sixteenth film to get more than six Academy Award nominations. It was nominated for the following awards:
It was widely thought the film would win most of the awards it was nominated for, but it only won the Best Writing (Original Screenplay) Oscar.
Film editor Robert Wise recalled each time "Citizen Kane"s name was called out as a nominee, the crowd booed. Most of Hollywood did not want the film to see the light of day, considering the threats that William Randolph Hearst had made if it did. According to "Variety", bloc voting against Welles by screen extras denied him Best Picture and Actor awards. British film critic Barry Norman attributed this to Hearst's wrath.
Other awards.
The National Board of Review of Motion Pictures gave 1941 "Best Acting" awards to Orson Welles and George Coulouris, and the film itself "Best Picture." That same year, "The New York Times" named it one of the Ten Best Films of the year, and the New York Film Critics Circle Award for "Best Picture" also went to "Citizen Kane".
Subsequent re-evaluation and recognition.
By 1942 "Citizen Kane" had run its course theatrically and, apart from a few showings at big city arthouse cinemas, it largely vanished from America until 1956. In that period, "Kane"'s and Welles' reputation fell among American critics. In 1949 critic Richard Griffith in his overview of cinema, "The Film Till Now", dismissed "Kane" as "... tinpot if not crackpot Freud."
Due to World War II, "Citizen Kane" was little seen in Europe. It was not until 1946 that it was shown in France, where it gained considerable acclaim, particularly from film critics such as André Bazin and from "Cahiers du cinéma" writers, including future film directors François Truffaut and Jean-Luc Godard. In his 1950 essay "The Evolution of Cinema", Bazin placed "Citizen Kane" centre stage as a work which ushered in a new period in cinema.
In the United States, it was neglected and forgotten until its revival on television in the mid-1950s. Three key events in 1956 led to its re-evaluation in the United States: first, RKO was one of the first studios to sell its library to television, and early that year "Citizen Kane" started to appear on television; second, the film was re-released theatrically to coincide with Welles's return to the New York stage, where he played "King Lear;" and third, American film critic Andrew Sarris wrote "Citizen Kane: The American Baroque" for "Film Culture", and described it as "the great American film." During Expo 58, a poll of over 100 film historians named "Kane" one of the top ten greatest films ever made (the group gave first-place honors to "The Battleship Potemkin"). When a group of young film directors announced their vote for the top six, they were booed for not including the film.
In the decades since, its critical status as one of the greatest films ever made has grown, with numerous essays and books on it including Peter Cowie's "The Cinema of Orson Welles", Ronald Gottesman's "Focus on Citizen Kane", a collection of significant reviews and background pieces, and most notably Kael's essay, "Raising Kane", which promoted the value of the film to a much wider audience than it had reached before. Despite its criticism of Welles, it further popularized the notion of "Citizen Kane" as the great American film. The rise of art house and film society circuits also aided in the film's rediscovery.
The British magazine "Sight & Sound" has produced a Top Ten list surveying film critics every decade since 1952, and is regarded as one of the most respected barometers of critical taste. "Citizen Kane" was a runner up to the top 10 in its 1952 poll but was voted as the greatest film ever made in its 1962 poll, retaining the top spot in every subsequent poll until 2012, when "Vertigo" displaced it.
The film has also ranked number one in the following film "best of" lists: Editorial Jaguar, FIAF Centenary List, France Critics Top 10, Cahiers du cinéma 100 films pour une cinémathèque idéale, Kinovedcheskie Zapiski, Russia Top 10, Romanian Critics Top 10, Time Out Magazine Greatest Films, and Village Voice 100 Greatest Films. Roger Ebert called "Citizen Kane" the greatest film ever made: "But people don't always ask about the greatest film. They ask, 'What's your favorite movie?' Again, I always answer with "Citizen Kane"."
In 1989, the United States Library of Congress deemed the film "culturally, historically, or aesthetically significant" and selected it for preservation in the National Film Registry. "Citizen Kane" was one of the first 25 movies designated as national treasures. The legislation creating the registry was enacted in 1988, growing out of the debate over a movie director's right to block film colorization.
On February 18, 1999, the United States Postal Service honored "Citizen Kane" by including it in its Celebrate the Century series. The film was honored again February 25, 2003, in a series of U.S. postage stamps marking the 75th anniversary of the Academy of Motion Picture Arts and Sciences. Art director Perry Ferguson represents the behind-the-scenes craftsmen of filmmaking in the series; he is depicted completing a sketch for "Citizen Kane".
"Citizen Kane" was ranked number one in the American Film Institute's polls of film industry artists and leaders in 1998 and 2007. "Rosebud" was chosen the 17th most memorable movie quotation in a 2005 AFI poll. The film's score was one of 250 nominees for the top 25 film scores in American cinema in another 2005 AFI poll.
The film currently has a 100% rating at Rotten Tomatoes, based on 66 reviews by approved critics.
Legacy.
Despite the critical success of "Citizen Kane" it nevertheless marked a decline in Welles's fortunes. In the book "Whatever Happened to Orson Welles?", Joseph McBride argues that the problems in making "Citizen Kane" caused lasting damage to his career. The damage started with RKO violating its contract with him by taking his next film, "The Magnificent Ambersons", away from him and adding a happy ending against his will. Hollywood's treatment of Welles and his work ultimately led to his self-imposed exile in Europe for much of the rest of his career, where he found a more sympathetic audience.
The documentary "The Battle Over Citizen Kane" posits that Welles's own life story resembled that of Kane far more than Hearst's: an overreaching wunderkind who ended up mournful and lonely in his old age. "Citizen Kane"'s editor, Robert Wise, summarized: "Well, I thought often afterwards, only in recent years when I saw the film again two or three years ago when they had the fiftieth anniversary, and I suddenly thought to myself, well, Orson was doing an autobiographical film and didn't realize it, because it's rather much the same, you know. You start here, and you have a big rise and tremendous prominence and fame and success and whatnot, and then tail off and tail off and tail off. And at least the arc of the two lives were very much the same ..."
Peter Bogdanovich, who was friends with Welles in his later years, disagreed with this on his own commentary on the "Citizen Kane" DVD, saying that Kane was nothing like Welles. Kane, he said, "... had none of the qualities of an artist, Orson had all the qualities of an artist." Bogdanovich also noted that Welles was never bitter "... about all the bad things that happened to him ...," and was a man who enjoyed life in his final years.
In addition, critics have reassessed Welles' career after his death, saying that he wasn't a failed Hollywood filmmaker, but a successful independent filmmaker.
Film critic Kim Newman believed the film's influence was visible in the film noir that followed, as well as the 1942 Hepburn-Tracy film "Keeper of the Flame." Martin Scorsese ranks it as one of his favorite films of all time.
The film's structure influenced the biographical films "Lawrence of Arabia" and "" – which begin with the subject's death and show their life in flashbacks – as well as Welles's thriller "Mr. Arkadin".
The film, for its topic of mass media manipulation of public opinion, is also famous for having been frequently presented as the perfect example to demonstrate the power that media has on influencing the democratic process. This exemplary citation of the film lasted till the end of the 20th century, when the paradigm of mass media depicted in "Citizen Kane" needed to be updated to take into account more globalized and more internet-based media scenarios. Since the film was based on William Randolph Hearst's actions in the late 19th and early 20th centuries, that model of media influence lasted for almost a century. Media mogul Rupert Murdoch is sometimes labeled as a latter-day "Citizen Kane".
Film memorabilia.
In June 1982, Steven Spielberg spent $60,500 to buy a Rosebud sled, one of three balsa sleds used in the closing scenes and the only one that was not burned. Spielberg had paid homage to "Citizen Kane" in the final shot of the government warehouse in his 1981 film, "Raiders of the Lost Ark". Spielberg commented, "Rosebud will go over my typewriter to remind me that quality in movies comes first."
After the Spielberg purchase, news outlets began reporting the claim of Arthur Bauer, a retired helicopter pilot in New York, that he owned another Rosebud, the hardwood sled used in Buddy Swan's scenes as the young Charles Foster Kane at the beginning of "Citizen Kane". "I'm sure it could be true," Welles said when asked for comment. In early 1942, Bauer was a 12-year-old student in Brooklyn and a member of his school's film club. He entered and won an RKO Pictures publicity contest and selected Rosebud as his prize. In 1996, Bauer's estate offered the painted pine sled at auction through Christie's. Bauer's son told CBS News that his mother had once wanted to paint the sled and use it as a plant stand; "Instead, my dad said, 'No, just save it and put it in the closet.'" On December 15, 1996, the hardwood sled was sold to an anonymous bidder in Los Angeles for $233,500.
In December 2007, Welles's personal copy of the last revised draft of "Citizen Kane" before the shooting script was sold at Sotheby's in New York for $97,000. Welles's Oscar for best original screenplay was offered for sale at the same auction, but failed to reach its estimate of $800,000 to $1.2 million. The Oscar, believed to have been lost by Welles, was rediscovered in 1994. Owned by the Dax Foundation, a Los Angeles based charity, it was sold at auction in 2011 by an anonymous seller to an anonymous buyer for $861,542.
A working draft script for "Citizen Kane" — with its original title, "American" — was sold at auction by Sotheby's, March 5–6, 2014. A second-draft script marked "Mr. Welles' working copy" in pencil on the manilla cover, it was expected to bring between $25,080 and $33,440; it sold for $164,692. The same item had been sold by Christie's in December 1991, together with a working script from "The Magnificent Ambersons", for $11,000.
A collection of 24 pages from a script of "Citizen Kane" was sold at auction April 26, 2014, for $15,000. A collection of approximately 235 stills and production photos sold for $7,812.The materials were among those found in boxes and trunks of Welles's personal possessions by his daughter Beatrice Welles.
Distribution rights.
In 1955, RKO sold the American television rights to its film library, including "Citizen Kane", to C&C Television Corp. Television rights to the pre-1956 RKO library were acquired by United Artists in 1960. RKO kept the non-broadcast television rights to its library and formed RKO Home Video in 1984. RKO, which had licensed the film to other home video companies, reissued the film in 1985 on VHS and Beta, while the laserdisc video rights to "Citizen Kane" were carried over to The Criterion Collection for its inaugural release in 1984. Turner Broadcasting System acquired broadcast television rights to the library when it acquired MGM/UA in 1986. Unable to sustain the debt load, Turner split up MGM/UA and kept the MGM film library, including American television rights to the RKO library. Turner acquired full worldwide rights to the RKO library in 1987. The RKO Home Video unit was reorganized into Turner Home Entertainment that year. For the film's 50th anniversary in 1991, Turner Entertainment utilized Paramount Pictures as its distributor for the film's re-release to theaters. In 1996, Time Warner acquired Turner and Warner Home Video absorbed Turner Home Entertainment. Today, Time Warner's Warner Bros. unit has distribution rights for "Citizen Kane".
Prints.
The composited camera negative of "Citizen Kane" was destroyed in a New Jersey film laboratory fire in the 1970s. Subsequent prints were ultimately derived from a master positive (a fine-grain preservation element) made in the 1940s and originally intended for use in overseas distribution. The soundtrack had not been lost.
Modern techniques were used to produce a pristine print for a 50th Anniversary theatrical revival reissue in 1991 (released by Paramount Pictures). The 2003 British DVD edition is taken from a master positive held by the British Film Institute. The current US DVD version (released by Warner Home Video) is taken from another digital restoration, supervised by Turner's company. The transfer to Region 1 DVD has been criticized by some film experts for being too bright. Also, in the scene in Bernstein's office (chapter 10), rain falling outside the window has been digitally erased, probably because it was thought to be excessive film grain. These alterations are not present in the UK Region 2, which is also considered to be more accurate in terms of contrast and brightness.
In 2003, Welles's daughter Beatrice sued Turner Entertainment and RKO Pictures, claiming the Welles estate is the legal copyright holder of the film. Her attorney said Orson Welles had left RKO with an exit deal terminating his contracts with the studio, meaning Welles still had an interest in the film, and his previous contract giving the studio the copyright of the film was null and void. Beatrice Welles also claimed, if the courts did not uphold her claim of copyright, RKO nevertheless owed the estate 20% of the profits, from a previous contract which has not been lived up to. On May 30, 2007, the appeals panel agreed Welles could proceed with the lawsuit against Turner Entertainment; the opinion partially overturning the 2004 decision by a lower court judge who had found in favor of Turner Entertainment on the issue of video rights.
Colorization controversy.
In the 1980s, "Citizen Kane" became a catalyst in the controversy over the colorization of black-and-white films. In November 1986, "New York Times" film critic Vincent Canby wrote, "It's something of an irony that at a time when concerned movie makers are trying to raise funds for the preservation of films originally shot in color (and which are now fading fast), other people have come along with grandiose plans to 'colorize' all black-and-white films, including, so help me, "Citizen Kane" and "The Magnificent Ambersons.""
One high-profile proponent of film colorization was Ted Turner, whose Turner Entertainment Co. acquired exclusive rights for the RKO library. A Turner Entertainment spokesperson initially stated that "Citizen Kane" would not be colorized: "We don't think it's appropriate. We think it's fine as it is". But at a July 1988 news conference called to unveil the newly colored "Casablanca", Ted Turner said, ""Citizen Kane?" I'm thinking of colorizing it."
In January 1989 the Associated Press reported that two companies were producing color tests of "Citizen Kane" for Turner Entertainment. Criticism increased with the AP's report that filmmaker Henry Jaglom remembered that shortly before his death Orson Welles had implored him to protect "Kane" from being colorized.
On February 14, 1989, Turner Entertainment president Roger Mayer announced that work to colorize "Citizen Kane" had been stopped:
Our attorneys looked at the contract between RKO Pictures Inc. and Orson Welles and his production company, Mercury Productions Inc., and, on the basis of their review, we have decided not to proceed with colorization of the movie. … While a court test might uphold our legal right to colorize the film, provisions of the contract could be read to prohibit colorization without permission of the Welles estate. We have completed restoration of a printing negative which now enables us to show first-rate black-and-white prints of this masterpiece.
"It was rather well known that Welles had very, very complete controls. His contract is quite unusual," Mayer said. "What we are saying is that when a director has final cut, it is the ultimate in creative control. The other contracts we have checked out are not like this at all."
One minute of the colorized test footage of "Citizen Kane" was included in a special "Arena" documentary, "The Complete Citizen Kane", produced by the BBC in 1991.
In December 1989, Turner Home Entertainment released a colorized version of "The Magnificent Ambersons" on VHS.
The colorization controversy was a factor in the passage of the National Film Preservation Act of 1988. The legislation created the National Film Registry that in September 1989 inducted its first 25 films, including "Citizen Kane" and "Casablanca". "One major reason for doing this is to require people like the broadcaster Ted Turner who've been adding color to some movies and reediting others for television to put notices on those versions saying that the movies have been altered," reported ABC News anchor Peter Jennings.
Home media.
Blu-ray.
On September 13, 2011, "Citizen Kane" was released on Blu-ray disc and DVD in a 70th anniversary box set called "The Ultimate Collectors Edition". "This, quite simply, is the Blu-ray release of the year," wrote the "San Francisco Chronicle". "Anyone who thinks Blu-ray doesn't make much of a difference to an older, B&W movie hasn't seen this reference-quality work by Warner Home Video." ""Citizen Kane" is a film whose visual scintillation is ageless, and you have never seen it look as good as it does here, in Warner's newly restored, 1080p, AVC/MPEG-4 transfer," wrote DVD Talk. Supplements include separate commentary tracks by Roger Ebert and Peter Bogdanovich, carried forward from Warner's 60th anniversary edition DVD release; two additional films, "The Battle Over Citizen Kane" and "RKO 281"; and packaging extras that include a hardcover booklet and a folio containing a reproduction of the original souvenir program, miniature lobby cards and other memorabilia.

</doc>
<doc id="5225" url="http://en.wikipedia.org/wiki?curid=5225" title="Code">
Code

In communications and information processing, code is system of rules to convert information—such as a letter, word, sound, image, or gesture—into another, sometimes shortened or secret, form or representation for communication through a channel or storage in a medium. An early example is the invention language, which enabled a person, through speech, to communicate what he or she saw, heard, felt, or thought to others. But speech limits the range of communication to the distance a voice can carry, and limits the audience to those present when the speech is uttered. The invention of writing, which converted spoken language into visual symbols, extended the range of communication across space and time.
The process of encoding converts information from a source into symbols for communication or storage. Decoding is the reverse process, converting code symbols back into a form that the recipient understands.
One reason for coding is to enable communication in places where ordinary plain language, spoken or written, is difficult or impossible. For example, semaphore, where the configuration of flags held by a signaller or the arms of a semaphore tower encodes parts of the message, typically individual letters and numbers. Another person standing a great distance away can interpret the flags and reproduce the words sent.
Theory.
In information theory and computer science, a code is usually considered as an algorithm which uniquely represents symbols from some source alphabet, by "encoded" strings, which may be in some other target alphabet. An extension of the code for representing sequences of symbols over the source alphabet is obtained by concatenating the encoded strings.
Before giving a mathematically precise definition, we give a brief example. The mapping 
is a code, whose source alphabet is the set formula_2 and whose target alphabet is the set formula_3. Using the extension of the code, the encoded string 0011001011 can be grouped into codewords as 0 011 0 01 011, and these in turn can be decoded to the sequence of source symbols "acabc".
Using terms from formal language theory, the precise mathematical definition of this concept is as follows: Let S and T be two finite sets, called the source and target alphabets, respectively. A code formula_4 is a total function mapping each symbol from S to a sequence of symbols over T, and the extension of formula_5 to a homomorphism of formula_6 into formula_7, which naturally maps each sequence of source symbols to a sequence of target symbols, is referred to as its extension.
Variable-length codes.
In this section we consider codes, which encode each source (clear text) character by a code word from some dictionary, and concatenation of such code words give us an encoded string.
Variable-length codes are especially useful when clear text characters have different probabilities; see also entropy encoding.
A "prefix code" is a code with the "prefix property": there is no valid code word in the system that is a prefix (start) of any other valid code word in the set. Huffman coding is the most known algorithm for deriving prefix codes. Prefix codes are widely referred to as "Huffman codes", even when the code was not produced by a Huffman algorithm.
Other examples of prefix codes are country calling codes, the country and publisher parts of ISBNs, and the Secondary Synchronization Codes used in the UMTS W-CDMA 3G Wireless Standard.
Kraft's inequality characterizes the sets of code word lengths that are possible in a prefix code. Virtually any uniquely decodable one-to-many code, not necessary a prefix one, must satisfy Kraft's inequality.
Error-correcting codes.
Codes may also be used to represent data in a way more resistant
to errors in transmission or storage. Such a "code" is
called an error-correcting code, and works by including carefully crafted redundancy with the stored (or transmitted) data. Examples include Hamming codes, Reed–Solomon, Reed–Muller, Walsh–Hadamard, Bose–Chaudhuri–Hochquenghem, Turbo, Golay, Goppa, low-density parity-check codes, and space–time codes.
Error detecting codes can be optimised to detect "burst errors", or "random errors".
Examples.
Codes in communication used for brevity.
A cable code replaces words (e.g., "ship" or "invoice") with shorter words, allowing the same information to be sent with fewer characters, more quickly, and most important, less expensively.
Codes can be used for brevity. When telegraph messages were the state of the art in rapid long distance communication, elaborate systems of commercial codes that encoded complete phrases into single words (commonly five-letter groups) were developed, so that telegraphers became conversant with such "words" as "BYOXO" ("Are you trying to weasel out of our deal?"), "LIOUY" ("Why do you not answer my question?"), "BMULD" ("You're a skunk!"), or "AYYLU" ("Not clearly coded, repeat more clearly."). Code words were chosen for various reasons: length, pronounceability, etc. Meanings were chosen to fit perceived needs: commercial negotiations, military terms for military codes, diplomatic terms for diplomatic codes, any and all of the preceding for espionage codes. Codebooks and codebook publishers proliferated, including one run as a front for the American Black Chamber run by Herbert Yardley between the First and Second World Wars. The purpose of most of these codes was to save on cable costs. The use of data coding for data compression predates the computer era; an early example is the telegraph Morse code where more-frequently used characters have shorter representations. Techniques such as Huffman coding are now used by computer-based algorithms to compress large data files into a more compact form for storage or transmission.
Bletchley Park.
Bletchley Park is an estate located in the town of Bletchley, in Milton Keynes, Buckinghamshire, England, and is run by the Bletchley Park Trust as a heritage attraction. During the Second World War, Bletchley Park was the site of the United Kingdom's main decryption establishment, the Government Code and Cypher School (GC&CS), where ciphers and codes of several Axis countries were decrypted, most importantly the ciphers generated by the German Enigma and Lorenz machines. The place was known as "B.P." to the people who worked there. For the many members of the Women's Royal Naval Service (Wrens) who worked at Bletchley Park, their posting was to HMS Pembroke V.
Character encodings.
Probably the most widely known data communications code so far (aka character representation) in use today is ASCII. In one or another (somewhat compatible) version, it is used by nearly all personal computers, terminals, printers, and other communication equipment. It represents 128 characters with seven-bit binary numbers—that is, as a string of seven 1s and 0s (bits). In ASCII a lowercase "a" is always 1100001, an uppercase "A" always 1000001, and so on. There are many other encodings, which represent each character by a byte (usually referred as code pages), integer code point (Unicode) or a byte sequence (UTF-8).
Genetic code.
Biological organisms contain genetic material that is used to control their function and development. This is DNA which contains units named genes that can produce proteins through a code (genetic code) in which a series of triplets (codons) of four possible nucleotides are translated into one of twenty possible amino acids. A sequence of codons results in a corresponding sequence of amino acids that form a protein.
Gödel code.
In mathematics, a Gödel code was the basis for the proof of Gödel's incompleteness theorem. Here, the idea was to map mathematical notation to a natural number (using a Gödel numbering).
Other.
There are codes using colors, like traffic lights, the color code employed to mark the nominal value of the electrical resistors or that of the trashcans devoted to specific types of garbage (paper, glass, biological, etc.)
In marketing, coupon codes can be used for a financial discount or rebate when purchasing a product from an internet retailer.
In military environments, specific sounds with the cornet are used for different uses: to mark some moments of the day, to command the infantry in the battlefield, etc.
Communication systems for sensory impairments, such as sign language for deaf people and braille for blind people, are based on movement or tactile codes.
Musical scores are the most common way to encode music.
Specific games, as chess, have their own code systems to record the matches (chess notation).
Cryptography.
In the history of cryptography, codes were once common for ensuring the confidentiality of communications, although ciphers are now used instead. See code (cryptography).
Secret codes intended to obscure the real messages, ranging from serious (mainly espionage in military, diplomatic, business, etc.) to trivial (romance, games) can be any kind of imaginative encoding: flowers, game cards, clothes, fans, hats, melodies, birds, etc., in which the sole requisite is the previous agreement of the meaning by both the sender and the receiver.
Other examples.
Other examples of encoding include:
Other examples of decoding include:
Codes and acronyms.
Acronyms and abbreviations can be considered codes, and in a sense all languages and writing systems are codes for human thought.
International Air Transport Association airport codes are three-letter codes used to designate airports and used for bag tags. Station codes are similarly used on railways, but are usually national, so the same code can be used for different stations if they are in different countries.
Occasionally a code word achieves an independent existence (and meaning) while the original equivalent phrase is forgotten or at least no longer has the precise meaning attributed to the code word. For example, '30' was widely used in journalism to mean "end of story", and has been used in other contexts to signify "the end".

</doc>
<doc id="5228" url="http://en.wikipedia.org/wiki?curid=5228" title="Cheirogaleidae">
Cheirogaleidae

The Cheirogaleidae are the family of strepsirrhine primates containing the various dwarf and mouse lemurs. Like all other lemurs, cheirogaleids live exclusively on the island of Madagascar.
Characteristics.
Cheirogaleids are smaller than the other lemurs and, in fact, they are the smallest primates. They have soft, long fur, colored grey-brown to reddish on top, with a generally brighter underbelly. Typically, they have small ears, large, close-set eyes, and long hind legs. Like all strepsirrhines, they have fine claws at the second toe of the hind legs. They grow to a size of only 13 to 28 cm, with a tail that is very long, sometimes up to one and a half times as long as the body. They weigh no more than 500 grams, with some species weighing as little as 60 grams.
Dwarf and mouse lemurs are nocturnal and arboreal. They are excellent climbers and can also jump far, using their long tails for balance. When on the ground (a rare occurrence), they move by hopping on their hind legs. They spend the day in tree hollows or leaf nests. Cheirogaleids are typically solitary, but sometimes live together in pairs. 
Their eyes possess a tapetum lucidum, a light-reflecting layer that improves their night vision. Some species, such as the lesser dwarf lemur, store fat at the hind legs and the base of the tail, and hibernate. Unlike lemurids, they have long upper incisors, although they do have the comb-like teeth typical of all strepsirhines. They have the dental formula: 
Cheirogaleids are omnivores, eating fruits, flowers and leaves (and sometimes nectar), as well as insects, spiders, and small vertebrates.
The females usually have three pairs of nipples. After a meager 60-day gestation, they will bear two to four (usually two or three) young. After five to six weeks, the young are weaned and become fully mature near the end of their first year or sometime in their second year, depending on the species. In human care, they can live for up to 15 years, although their life expectancy in the wild is probably significantly shorter.
Classification.
The five genera of cheirogaleids contain 34 species.

</doc>
<doc id="5229" url="http://en.wikipedia.org/wiki?curid=5229" title="Callitrichidae">
Callitrichidae

The Callitrichidae (also called Arctopitheci or Hapalidae) is a family of New World monkeys, including marmosets and tamarins. At times, this group of animals has been regarded as a subfamily, called Callitrichinae, of the family Cebidae.
This taxon was traditionally thought to be a primitive lineage, from which all the larger bodied platyrrhines evolved. However, some works argue that callitrichids are actually a dwarfed lineage.
Ancestral stem-callitrichids would likely have been "normal" sized ceboids that were dwarfed through evolutionary time. This may exemplify a rare example of insular dwarfing in a mainland context, with the "islands" being formed by biogeographic barriers during arid climatic periods when forest distribution became patchy, and/or by the extensive river networks in the Amazon Basin.
All callitrichids are arboreal. They are the smallest of the simian primates. They eat insects, fruit, and the sap or gum from trees; occasionally they will take small vertebrates. The marmosets rely quite heavily on exudates, with some species (e.g. "Callithrix jacchus" and "Cebuella pygmaea") considered obligate exudativores.
Callitrichids typically live in small, territorial groups of about five or six animals. Their social organization is unique among primates and is called a "cooperative polyandrous group". This communal breeding system involves groups of multiple males and females, but only one female is reproductively active. Females mate with more than one male and everyone shares the responsibility of carrying the offspring.
They are the only primate group that regularly produces twins, which constitute over 80% of births in species that have been studied. Unlike other male primates, male callitrichids generally provide as much parental care as females. Parental duties may include carrying, protecting, feeding, comforting, and even engaging in play behavior with offspring. In some cases, such as in the cotton-top tamarin ("Saguinus oedipus"), males, particularly those that are paternal, will even show a greater involvement in caregiving than females. The typical social structure seems to constitute a breeding group, with several of their previous offspring living in the group and providing significant help in rearing the young.

</doc>
<doc id="5230" url="http://en.wikipedia.org/wiki?curid=5230" title="Cebidae">
Cebidae

The Cebidae is one of the five families of New World monkeys now recognised. It includes the capuchin monkeys and squirrel monkeys. These species are found throughout tropical and subtropical South and Central America.
Characteristics.
Cebid monkeys are arboreal animals that only rarely travel on the ground. They are generally small monkeys, ranging in size up to that of the Brown Capuchin, with a body length of 33 to 56 cm, and a weight of 2.5 to 3.9 kilograms. They are somewhat variable in form and coloration, but all have the wide, flat, noses typical of New World Monkeys. They are different from marmosets as they have additional molar tooth and a prehensile tail. 
They are omnivorous, mostly eating fruit and insects, although the proportions of these foods vary greatly between species. They have the dental formula:
Females give birth to one or two young after a gestation period of between 130 and 170 days, depending on species. They are social animals, living in groups of between five and forty individuals, with the smaller species typically forming larger groups. They are generally diurnal in habit.
Classification.
Previously, New World monkeys were divided between Callitrichidae and this family. For a few recent years, marmosets, tamarins, and lion tamarins were placed as a subfamily (Callitrichinae) in Cebidae, while moving other genera from Cebidae into the families Aotidae, Pitheciidae and Atelidae. The most recent classification of New World monkeys again splits the callitrichids off, leaving only the capuchins and squirrel monkeys in this family.

</doc>
<doc id="5232" url="http://en.wikipedia.org/wiki?curid=5232" title="Chondrichthyes">
Chondrichthyes

Chondrichthyes (; from Greek χονδρ- "chondr-" 'cartilage', ἰχθύς "ichthys" 'fish') or cartilaginous fishes are jawed fish with paired fins, paired nares, scales, a heart with its chambers in series, and skeletons made of cartilage rather than bone. The class is divided into two subclasses: Elasmobranchii (sharks, rays and skates) and Holocephali (chimaeras, sometimes called ghost sharks, which are sometimes separated into their own class).
Within the infraphylum Gnathostomata, cartilaginous fishes are distinct from all other jawed vertebrates, the extant members of which all fall into Teleostomi.
Anatomy.
Skeleton.
The skeleton is cartilaginous. The notochord, which is present in the young, is gradually replaced by cartilage. Chondrichthyes also lack ribs, so if they leave water, the larger species' own body weight would crush their internal organs long before they would suffocate.
As they do not have bone marrow, red blood cells are produced in the spleen and the epigonal organ (special tissue around the gonads, which is also thought to play a role in the immune system). They are also produced in the Leydig's organ which is only found in cartilaginous fishes, although some do not possess it. The subclass Holocephali, which is a very specialized group, lacks both the Leydig's and epigonal organ.
Appendages.
Their tough skin is covered with dermal teeth (again with Holocephali as an exception as the teeth are lost in adults, only kept on the clasping organ seen on the front of the male's head), also called placoid scales (or "dermal denticles") making it feel like sandpaper. In most species, all dermal denticles are oriented in one direction, making the skin feel very smooth if rubbed in one direction and very rough if rubbed in the other. Another exception are the electric rays, which have a thick and flabby body, with soft, loose skin devoid of dermal denticles and thorns.
Originally the pectoral and pelvic girdles, which do not contain any dermal elements, did not connect. In later forms, each pair of fins became ventrally connected in the middle when scapulocoracoid and pubioischiadic bars evolved. In rays, the pectoral fins have connected to the head and are very flexible.
One of the primary characteristics present in most sharks is the heterocercal tail, which aids in locomotion.
Body covering.
Chondrichthyes have toothlike scales called dermal denticles or placoid scales. Denticles provide two functions, protection, and in most cases streamlining. Mucous glands exist in some species as well.
It is assumed that their oral teeth evolved from dermal denticles which migrated into the mouth, but it could be the other way around as the teleost bony fish "Denticeps clupeoides" has most of its head covered by dermal teeth (as does, probably, "Atherion elymus", another bony fish). This is most likely a secondary evolved characteristic which means there is not necessarily a connection between the teeth and the original dermal scales.
The old placoderms did not have teeth at all, but had sharp bony plates in their mouth. Thus, it is unknown which of the dermal or oral teeth evolved first. Neither is it sure how many times it has happened if it turns out to be the case. It has even been suggested that the original bony plates of all the vertebrates are gone and that the present scales are just modified teeth, even if both teeth and the body armor have a common origin a long time ago. However, there is no evidence of this at the moment.
Respiratory system.
All Chondrichthyes breathe through five to seven pairs of gills, depending on the species. In general, pelagic species must keep swimming to keep oxygenated water moving through their gills, whilst demersal species can actively pump water in through their spiracles and out through their gills. However, this is only a general rule and many species differ.
A spiracle is a small hole found behind each eye. These can be tiny and circular, such as found on the nurse shark ("Ginglymostoma cirratum"), to extended and slit-like, such as found on the wobbegongs (Orectolobidae). Many larger, pelagic species such as the mackerel sharks (Lamnidae) and the thresher sharks (Alopiidae) no longer possess them.
Immune system.
Like all other jawed vertebrates, members of Chondrichthyes have an adaptive immune system.
Reproduction.
Fertilization is internal. Development is usually live birth (ovoviviparous species) but can be through eggs (oviparous). Some rare species are viviparous. There is no parental care after birth; however, some Chondrichthyes do guard their eggs.
Classification.
The class Chondrichthyes has two subclasses: the subclass Elasmobranchii (sharks and rays) and the subclass Holocephali (chimaeras).
Evolution.
Unequivocal fossils of cartilaginous fishes first appeared in the fossil record by about 395 million years ago, during the middle Devonian. The radiation of elasmobranches in the chart on the right is divided into the taxa: Cladoselache, Eugeneodontiformes, Symmoriida, Xenacanthiformes, Ctenacanthiformes, Hybodontiformes, Galeomorphi, Squaliformes and Batoidea.
By the start of the Early Devonian 419 mya (million years ago), jawed fishes had divided into four distinct clades: the placoderms and spiny sharks, both of which are now extinct, and the cartilaginous and bony fishes, both of which are still extant. The modern bony fishes, class Osteichthyes, appeared in the late Silurian or early Devonian, about 416 million years ago. Spiny sharks are not classified as true sharks or as cartilaginous fishes, but as a distinct group, class Acanthodii. However, both the cartilaginous and bony fishes may have arisen from either the placoderms or the spiny sharks. Cartilaginous fishes first appeared about 395 Ma. The first abundant genus of shark, "Cladoselache", appeared in the oceans during the Devonian Period.
A Bayesian analysis of molecular data suggests that the Holocephali and Elasmoblanchii diverged in the Silurian () and that the sharks and rays/skates split in the Carboniferous ().

</doc>
<doc id="5233" url="http://en.wikipedia.org/wiki?curid=5233" title="Carl Linnaeus">
Carl Linnaeus

Carl Linnaeus (23 May 1707 – 10 January 1778), also known after his ennoblement as , was a Swedish botanist, physician, and zoologist, who laid the foundations for the modern biological naming scheme of binomial nomenclature. He is known as the father of modern taxonomy, and is also considered one of the fathers of modern ecology. Many of his writings were in Latin, and his name is rendered in Latin as (after 1761 Carolus a Linné).
Linnaeus was born in the countryside of Småland, in southern Sweden. He received most of his higher education at Uppsala University, and began giving lectures in botany there in 1730. He lived abroad between 1735 and 1738, where he studied and also published a first edition of his "" in the Netherlands. He then returned to Sweden, where he became professor of medicine and botany at Uppsala. In the 1740s, he was sent on several journeys through Sweden to find and classify plants and animals. In the 1750s and '60s, he continued to collect and classify animals, plants, and minerals, and published several volumes. At the time of his death, he was one of the most acclaimed scientists in Europe.
The Swiss philosopher Jean-Jacques Rousseau sent him the message: "Tell him I know no greater man on earth." The German writer Johann Wolfgang von Goethe wrote: "With the exception of Shakespeare and Spinoza, I know no one among the no longer living who has influenced me more strongly." Swedish author August Strindberg wrote: "Linnaeus was in reality a poet who happened to become a naturalist". Among other compliments, Linnaeus has been called "" (Prince of Botanists), "The Pliny of the North," and "The Second Adam".
In botany, the author abbreviation used to indicate Linnaeus as the authority for species' names is L. In older publications, sometimes the abbreviation "Linn." is found (for instance in: ). Linnaeus' remains comprise the type specimen for the species "Homo sapiens", following the International Code of Zoological Nomenclature, since the sole specimen he is known to have examined when writing the species description was himself.
Biography.
Early life.
Childhood.
Carl Linnaeus was born in the village of Råshult in Småland, Sweden, on 23 May 1707. He was the first child of Nils Ingemarsson Linnaeus and Christina Brodersonia. His father was the first in his ancestry to adopt a permanent surname. Before that, ancestors had used the patronymic naming system of Scandinavian countries: his father was named Ingemarsson after his father Ingemar Bengtsson. When Nils was admitted to the University of Lund, he had to take on a family name. He adopted the Latinate name Linnæus after a giant linden tree (or lime tree), "" in Swedish, that grew on the family homestead. This name was spelled with the æ ligature. When Carl was born, he was named Carl Linnæus, with his father's family name. The son also always spelled it with the æ ligature, both in handwritten documents and in publications. Carl's patronymic would have been Nilsson, as in Carl Nilsson Linnæus.
One of a long line of peasants and priests, Nils was an amateur botanist, a Lutheran minister, and the curate of the small village of Stenbrohult in Småland. Christina was the daughter of the rector of Stenbrohult, Samuel Brodersonius. She subsequently gave birth to three daughters and another son, Samuel (who would eventually succeed their father as rector of Stenbrohult and write a manual on beekeeping). A year after Linnaeus' birth, his grandfather Samuel Brodersonius died, and his father Nils became the rector of Stenbrohult. The family moved into the rectory from the curate's house.
Even in his early years, Linnaeus seemed to have a liking for plants, flowers in particular. Whenever he was upset, he was given a flower, which immediately calmed him. Nils spent much time in his garden and often showed flowers to Linnaeus and told him their names. Soon Linnaeus was given his own patch of earth where he could grow plants.
Early education.
Linnaeus' father began teaching him Latin, religion, and geography at an early age; one account says that due to family use of Latin for conversation, the boy learned Latin before he learned Swedish. When Linnaeus was seven, Nils decided to hire a tutor for him. The parents picked Johan Telander, a son of a local yeoman. Linnaeus did not like him, writing in his autobiography that Telander "was better calculated to extinguish a child's talents than develop them." Two years after his tutoring had begun, he was sent to the Lower Grammar School at Växjö in 1717. Linnaeus rarely studied, often going to the countryside to look for plants. He reached the last year of the Lower School when he was fifteen, which was taught by the headmaster, Daniel Lannerus, who was interested in botany. Lannerus noticed Linnaeus' interest in botany and gave him the run of his garden. He also introduced him to Johan Rothman, the state doctor of Småland and a teacher at Katedralskolan (a gymnasium) in Växjö. Also a botanist, Rothman broadened Linnaeus' interest in botany and helped him develop an interest in medicine. At the age of 17, Linnaeus had become well acquainted with the existing botanical literature. He remarks in his journal "read day and night, knowing like the back of my hand, Arvidh Månsson's Rydaholm Book of Herbs, Tillandz's Flora Åboensis, Palmberg's Serta Florea Suecana, Bromelii Chloros Gothica and Rudbeckii Hortus Upsaliensis..." 
Linnaeus entered the Växjö Katedralskola in 1724, where he studied mainly Greek, Hebrew, theology and mathematics, a curriculum designed for boys preparing for the priesthood. In the last year at the gymnasium, Linnaeus' father visited to ask the professors how his son's studies were progressing; to his dismay, most said that the boy would never become a scholar. Rothman believed otherwise, suggesting Linnaeus could have a future in medicine. The doctor offered to have Linnaeus live with his family in Växjö and to teach him physiology and botany. Nils accepted this offer.
University.
Lund.
Rothman showed Linnaeus that botany was a serious subject. He taught Linnaeus to classify plants according to Tournefort's system. Linnaeus was also taught about the sexual reproduction of plants, according to Sébastien Vaillant. In 1727, Linnaeus, age 21, enrolled in Lund University in Skåne. He was registered as "", the Latin form of his full name, which he also used later for his Latin publications.
Professor Kilian Stobæus, natural scientist, physician and historian, offered Linnaeus tutoring and lodging, as well as the use of his library, which included many books about botany. He also gave the student free admission to his lectures. In his spare time, Linnaeus explored the flora of Skåne, together with students sharing the same interests.
Uppsala.
In August 1728, Linnaeus decided to attend Uppsala University on the advice of Rothman, who believed it would be a better choice if Linnaeus wanted to study both medicine and botany. Rothman based this recommendation on the two professors who taught at the medical faculty at Uppsala: Olof Rudbeck the Younger and Lars Roberg. Although Rudbeck and Roberg had undoubtedly been good professors, by then they were older and not so interested in teaching. Rudbeck no longer gave public lectures, and had others stand in for him. The botany, zoology, pharmacology and anatomy lectures were not in their best state. In Uppsala, Linnaeus met a new benefactor, Olof Celsius, who was a professor of theology and an amateur botanist. He received Linnaeus into his home and allowed him use of his library, which was one of the richest botanical libraries in Sweden.
In 1729, Linnaeus wrote a thesis, ' on plant sexual reproduction. This attracted the attention of Rudbeck; in May 1730, he selected Linnaeus to give lectures at the University although the young man was only a second-year student. His lectures were popular, and Linnaeus often addressed an audience of 300 people. In June, Linnaeus moved from Celsius' house to Rudbeck's to become the tutor of the three youngest of his 24 children. His friendship with Celsius did not wane and they continued their botanical expeditions. Over that winter, Linnaeus began to doubt Tournefort's system of classification and decided to create one of his own. His plan was to divide the plants by the number of stamens and pistils. He began writing several books, which would later result in, for example, ' and '. He also produced a book on the plants grown in the Uppsala Botanical Garden, '.
Rudbeck's former assistant, Nils Rosén, returned to the University in March 1731 with a degree in medicine. Rosén started giving anatomy lectures and tried to take over Linnaeus' botany lectures, but Rudbeck prevented that. Until December, Rosén gave Linnaeus private tutoring in medicine. In December, Linnaeus had a "disagreement" with Rudbeck's wife and had to move out of his mentor's house; his relationship with Rudbeck did not appear to suffer. That Christmas, Linnaeus returned home to Stenbrohult to visit his parents for the first time in about three years. His mother had disapproved of his failing to become a priest, but she was pleased to learn he was teaching at the University.
Expedition to Lapland.
During a visit with his parents, Linnaeus told them about his plan to travel to Lapland; Rudbeck had made the journey in 1695, but the detailed results of his exploration were lost in a fire seven years afterwards. Linnaeus' hope was to find new plants, animals and possibly valuable minerals. He was also curious about the customs of the native Sami people, reindeer-herding nomads who wandered Scandinavia's vast tundras. In April 1732, Linnaeus was awarded a grant from the Royal Society of Sciences in Uppsala for his journey.
Linnaeus began his expedition from Uppsala in May; he travelled on foot and horse, bringing with him his journal, botanical and ornithological manuscripts and sheets of paper for pressing plants. Near Gävle he found great quantities of "Campanula serpyllifolia", later known as "Linnaea borealis", the twinflower that would become his favourite. He sometimes dismounted on the way to examine a flower or rock and was particularly interested in mosses and lichens, the latter a main part of the diet of the reindeer, a common and economically important animal in Lapland.
Linnaeus travelled clockwise around the coast of the Gulf of Bothnia, making major inland incursions from Umeå, Luleå and Tornio. He returned from his six-month long, over expedition in October, having gathered and observed many plants, birds and rocks. Although Lapland was a region with limited biodiversity, Linnaeus described about 100 previously unidentified plants. These became the basis of his book "".
In ' Linnaeus' ideas about nomenclature and classification were first used in a practical way, making this the first proto-modern Flora. The account covered 534 species, used the Linnaean classification system and included, for the described species, geographical distribution and taxonomic notes. It was Augustin Pyramus de Candolle who attributed Linnaeus with ' as the first example in the botanical genre of Flora writing. Botanical historian E. L. Greene described "" as "the most classic and delightful" of Linnaeus's works.
It was also during this expedition that Linnaeus had a flash of insight regarding the classification of mammals. Upon observing the lower jawbone of a horse at the side of a road he was traveling, Linnaeus remarked: "If I only knew how many teeth and of what kind every animal had, how many teats and where they were placed, I should perhaps be able to work out a perfectly natural system for the arrangement of all quadrupeds."
Dalarna.
In 1734, Linnaeus led a small group of students to Dalarna. Funded by the Governor of Dalarna, the expedition was to catalogue known natural resources and discover new ones, but also to gather intelligence on Norwegian mining activities at Røros.
European excursions.
Doctorate.
Back in Uppsala, Linnaeus' relations with Nils Rosén worsened, and thus he gladly accepted an invitation from the student Claes Sohlberg to spend the Christmas holiday in Falun with Sohlberg's family. Sohlberg's father was a mining inspector, and let Linnaeus visit the mines near Falun. Sohland's father suggested to Linnaeus he should bring Sohlberg to the Dutch Republic and continue to tutor him there for an annual salary. At that time, the Dutch Republic was one of the most revered places to study natural history and a common place for Swedes to take their doctoral degree; Linnaeus, who was interested in both of these, accepted.
In April 1735, Linnaeus and Sohlberg set out for the Netherlands, with Linnaeus to take a doctoral degree in medicine at the University of Harderwijk. On the way, they stopped in Hamburg, where they met the mayor, who proudly showed them a wonder of nature which he possessed: the taxidermied remains of a seven-headed hydra. Linnaeus quickly discovered it was a fake: jaws and clawed feet from weasels and skins from snakes had been glued together. The provenance of the hydra suggested to Linnaeus it had been manufactured by monks to represent the Beast of Revelation. As much as this may have upset the mayor, Linnaeus made his observations public and the mayor's dreams of selling the hydra for an enormous sum were ruined. Fearing his wrath, Linnaeus and Sohlberg had to leave Hamburg quickly.
When Linnaeus reached Harderwijk, he began working toward a degree immediately; at the time, Harderwijk was known for awarding "instant" degrees after as little as a week. First he handed in a thesis on the cause of malaria he had written in Sweden, which he then defended in a public debate. The next step was to take an oral examination and to diagnose a patient. After less than two weeks, he took his degree and became a doctor, at the age of 28. During the summer, Linnaeus met a friend from Uppsala, Peter Artedi. Before their departure from Uppsala, Artedi and Linnaeus had decided should one of them die, the survivor would finish the other's work. Ten weeks later, Artedi drowned in one of the canals of Amsterdam, and his unfinished manuscript on the classification of fish was left to Linnaeus to complete.
Publishing of "".
One of the first scientists Linnaeus met in the Netherlands was Johan Frederik Gronovius to whom Linnaeus showed one of the several manuscripts he had brought with him from Sweden. The manuscript described a new system for classifying plants. When Gronovius saw it, he was very impressed, and offered to help pay for the printing. With an additional monetary contribution by the Scottish doctor Isaac Lawson, the manuscript was published as "".
Linnaeus became acquainted with one of the most respected physicians and botanists in the Netherlands, Herman Boerhaave, who tried to convince Linnaeus to make a career there. Boerhaave offered him a journey to South Africa and America, but Linnaeus declined, stating he would not stand the heat. Instead, Boerhaave convinced Linnaeus that he should visit the botanist Johannes Burman. After his visit, Burman, impressed with his guest's knowledge, decided Linnaeus should stay with him during the winter. During his stay, Linnaeus helped Burman with his '. Burman also helped Linnaeus with the books on which he was working: ' and "".
George Clifford.
In August, during Linnaeus' stay with Burman, he met George Clifford III, a director of the Dutch East India Company and the owner of a rich botanical garden at the estate of Hartekamp in Heemstede. Clifford was very impressed with Linnaeus' ability to classify plants, and invited him to become his physician and superintendent of his garden. Linnaeus had already agreed to stay with Burman over the winter, and could thus not accept immediately. However, Clifford offered to compensate Burman by offering him a copy of Sir Hans Sloane's "Natural History of Jamaica", a rare book, if he let Linnaeus stay with him, and Burman accepted. On 24 September 1735, Linnaeus became the botanical curator and house physician at Hartekamp, free to buy any book or plant he wanted.
In July 1736, Linnaeus travelled to England, at Clifford's expense. He went to London to visit Sir Hans Sloane, a collector of natural history, and to see his cabinet, as well as to visit the Chelsea Physic Garden and its keeper, Philip Miller. He taught Miller about his new system of subdividing plants, as described in '. Miller was impressed, and from then on started to arrange the garden according to Linnaeus' system. Linnaeus also traveled to Oxford University to visit the botanist Johann Jacob Dillenius. He failed, however, to make Dillenius publicly accept his new classification system. He then returned to Hartekamp, bringing with him many specimens of rare plants. The next year, he published ', in which he described 935 genera of plants, and shortly thereafter he supplemented it with "", with another sixty ("sexaginta") genera.
His work at Hartekamp led to another book, "", a catalogue of the botanical holdings in the herbarium and botanical garden of Hartekamp. He wrote it in nine months (completed in July 1737), but it was not published until 1738. It contains the first use of the name "Nepenthes", which Linnaeus used to describe a genus of pitcher plants.
Linnaeus stayed with Clifford at Hartekamp until 18 October 1737 (new style), when he left the house to return to Sweden. Illness and the kindness of Dutch friends obliged him to stay some months longer in Holland. In May 1738, he set out for Sweden again. On the way home, he stayed in Paris for about a month, visiting botanists such as Antoine de Jussieu. After his return, Linnaeus never left Sweden again.
Return to Sweden.
When Linnaeus returned to Sweden on 28 June 1738, he went to Falun, where he entered into an engagement to Sara Elisabeth Moræa. Three months later, he moved to Stockholm to find employment as a physician, and thus to make it possible to support a family. Once again, Linnaeus found a patron; he became acquainted with Count Carl Gustav Tessin, who helped him get work as a physician at the Admiralty. During this time in Stockholm, Linnaeus helped found the Royal Swedish Academy of Science; he became the first Praeses in the academy by drawing of lots.
Because his finances had improved and were now sufficient to support a family, he received permission to marry his fiancée, Sara Elisabeth Moræa. Their wedding was held 26 June 1739. Seven months later, Sara gave birth to their first son, Carl. Two years later, a daughter, Elisabeth Christina, was born, and the subsequent year Sara gave birth to Sara Magdalena, who died when 15 days old. Sara and Linnaeus would later have four other children: Lovisa, Sara Christina, Johannes and Sophia.
In May 1741, Linnaeus was appointed Professor of Medicine at Uppsala University, first with responsibility for medicine-related matters. Soon, he changed place with the other Professor of Medicine, Nils Rosén, and thus was responsible for the Botanical Garden (which he would thoroughly reconstruct and expand), botany and natural history, instead. In October that same year, his wife and nine-year-old son followed him to live in Uppsala.
Further exploration of Sweden.
Öland and Gotland.
Ten days after he was appointed Professor, he undertook an expedition to the island provinces of Öland and Gotland with six students from the university, to look for plants useful in medicine. First, they travelled to Öland and stayed there until 21 June, when they sailed to Visby in Gotland. Linnaeus and the students stayed on Gotland for about a month, and then returned to Uppsala. During this expedition, they found 100 previously unrecorded plants. The observations from the expedition were later published in ', written in Swedish. Like ', it contained both zoological and botanical observations, as well as observations concerning the culture in Öland and Gotland.
During the summer of 1745, Linnaeus published two more books: ' and '. ' was a strictly botanical book, while ' was zoological. Anders Celsius had created the temperature scale named after him in 1742. Celsius' scale was inverted compared to today, the boiling point at 0 °C and freezing point at 100 °C. In 1745, Linnaeus inverted the scale to its present standard.
Västergötland.
In the summer of 1746, Linnaeus was once again commissioned by the Government to carry out an expedition, this time to the Swedish province of Västergötland. He set out from Uppsala on 12 June and returned on 11 August. On the expedition his primary companion was Erik Gustaf Lidbeck, a student who had accompanied him on his previous journey. Linnaeus described his findings from the expedition in the book "", published the next year. After returning from the journey the Government decided Linnaeus should take on another expedition to the southernmost province Scania. This journey was postponed, as Linnaeus felt too busy.
In 1747, Linnaeus was given the title archiater, or chief physician, by the Swedish king Adolf Frederick—a mark of great respect. The same year he was elected member of the Academy of Sciences in Berlin.
Scania.
In the spring of 1749, Linnaeus could finally journey to Scania, again commissioned by the Government. With him he brought his student, Olof Söderberg. On the way to Scania, he made his last visit to his brothers and sisters in Stenbrohult since his father had died the previous year. The expedition was similar to the previous journeys in most aspects, but this time he was also ordered to find the best place to grow walnut and Swedish whitebeam trees; these trees were used by the military to make rifles. The journey was successful, and Linnaeus' observations were published the next year in "".
Rector of Uppsala University.
In 1750, Linnaeus became rector of Uppsala University, starting a period where natural sciences were esteemed. Perhaps the most important contribution he made during his time at Uppsala was to teach; many of his students travelled to various places in the world to collect botanical samples. Linnaeus called the best of these students his "apostles". His lectures were normally very popular and were often held in the Botanical Garden. He tried to teach the students to think for themselves and not trust anybody, not even him. Even more popular than the lectures were the botanical excursions made every Saturday during summer, where Linnaeus and his students explored the flora and fauna in the vicinity of Uppsala.
Publishing of "".
Linnaeus published "" in 1751. The book contained a complete survey of the taxonomy system he had been using in his earlier works. It also contained information of how to keep a journal on travels and how to maintain a botanical garden.
Publishing of "".
Linnaeus published "", the work which is now internationally accepted as the starting point of modern botanical nomenclature, in 1753. The first volume was issued on 24 May, the second volume followed on 16 August of the same year. The book contained 1,200 pages and was published in two volumes; it described over 7,300 species. The same year the king dubbed him knight of the Order of the Polar Star, the first civilian in Sweden to become a knight in this order. He was then seldom seen not wearing the order.
Ennoblement.
Linnaeus felt Uppsala was too noisy and unhealthy, so he bought two farms in 1758: Hammarby and Sävja. The next year, he bought a neighbouring farm, Edeby. He spent the summers with his family at Hammarby; initially it only had a small one-storey house, but in 1762 a new, larger main building was added. In Hammarby, Linnaeus made a garden where he could grow plants that could not be grown in the Botanical Garden in Uppsala. He began constructing a museum on a hill behind Hammarby in 1766, where he moved his library and collection of plants. A fire that destroyed about one third of Uppsala and had threatened his residence there necessitated the move.
Since the initial release of ' in 1735, the book had been expanded and reprinted several times; the tenth edition was released in 1758. This edition established itself as the starting point for zoological nomenclature, the equivalent of '.
The Swedish king Adolf Frederick granted Linnaeus nobility in 1757, but he was not ennobled until 1761. With his ennoblement, he took the name Carl von Linné (Latinized as ""), 'Linné' being a shortened and gallicised version of 'Linnæus', and the German title 'von' signifying his ennoblement. The noble family's coat of arms prominently features a twinflower, one of Linnaeus' favourite plants; it was given the scientific name "Linnaea borealis" in his honour by Gronovius. The shield in the coat of arms is divided into thirds: red, black and green for the three kingdoms of nature (animal, mineral and vegetable) in Linnaean classification; in the center is an egg "to denote Nature, which is continued and perpetuated "in ovo"." At the bottom is a phrase in Latin, borrowed from the Aeneid, which reads "": we extend our fame by our deeds.
After his ennoblement, Linnaeus continued teaching and writing. His reputation had spread over the world, and he corresponded with many different people. For example, Catherine II of Russia sent him seeds from her country. He also corresponded with Giovanni Antonio Scopoli, "the Linnaeus of the Austrian Empire", who was a doctor and a botanist in Idrija, Duchy of Carniola (nowadays Slovenia). Scopoli communicated all of his research, findings, and descriptions (for example of the olm and the dormouse, two little animals hitherto unknown to Linnaeus). Linnaeus greatly respected him and showed great interest in his work. He named a solanaceous genus, "Scopolia", the source of scopolamine, after him. Because of a great distance, they didn't ever meet.
Last years.
Linnaeus was relieved of his duties in the Royal Swedish Academy of Science in 1763, but continued his work there as usual for more than ten years after. He stepped down as rector at Uppsala University in December 1772, mostly due to his declining health.
Linnaeus' last years were troubled by illness. He had suffered from a disease called the Uppsala fever in 1764, but survived thanks to the care of Rosén. He developed sciatica in 1773, and the next year, he had a stroke which partially paralysed him. He suffered a second stroke in 1776, losing the use of his right side and leaving him bereft of his memory; while still able to admire his own writings, he could not recognize himself as their author.
In December 1777, he had another stroke which greatly weakened him, and eventually led to his death on 10 January 1778 in Hammarby. Despite his desire to be buried in Hammarby, he was interred in Uppsala Cathedral on 22 January.
His library and collections were left to his widow Sara and their children. Joseph Banks, an English botanist, wanted to buy the collection, but his son Carl refused and moved the collection to Uppsala. However, in 1783 Carl died and Sara inherited the collection, having outlived both her husband and son. She tried to sell it to Banks, but he was no longer interested; instead an acquaintance of his agreed to buy the collection. The acquaintance was a 24-year-old medical student, James Edward Smith, who bought the whole collection: 14,000 plants, 3,198 insects, 1,564 shells, about 3,000 letters and 1,600 books. Smith founded the Linnean Society of London five years later.
The von Linné name ended with his son Carl, who never married. His other son, Johannes, had died aged 3. There are over two hundred descendants of Linnaeus through two of his daughters.
Apostles.
During Linnaeus' time as Professor and Rector of Uppsala University, he taught many devoted students, 17 of whom he called "apostles". They were the most promising, most committed students, and all of them made botanical expeditions to various places in the world, often with his help. The amount of this help varied; sometimes he used his influence as Rector to grant his apostles a scholarship or a place on an expedition. To most of the apostles he gave instructions of what to look for on their journeys. Abroad, the apostles collected and organised new plants, animals and minerals according to Linnaeus' system. Most of them also gave some of their collection to Linnaeus when their journey was finished. Thanks to these students, the Linnaean system of taxonomy spread through the world without Linnaeus ever having to travel outside Sweden after his return from Holland. The British botanist William T. Stearn notes without Linnaeus' new system, it would not have been possible for the apostles to collect and organise so many new specimens. Many of the apostles died during their expeditions.
Early expeditions.
Christopher Tärnström, the first apostle and a 43-year-old pastor with a wife and children, made his journey in 1746. He boarded a Swedish East India Company ship headed for China. Tärnström never reached his destination, dying of a tropical fever on Côn Sơn Island the same year. Tärnström's widow blamed Linnaeus for making her children fatherless, causing Linnaeus to prefer sending out younger, unmarried students after Tärnström. Six other apostles later died on their expeditions, including Pehr Forsskål and Pehr Löfling.
Two years after Tärnström's expedition, Finnish-born Pehr Kalm set out as the second apostle to North America. There he spent two-and-a-half years studying the flora and fauna of Pennsylvania, New York, New Jersey and Canada. Linnaeus was overjoyed when Kalm returned, bringing back with him many pressed flowers and seeds. At least 90 of the 700 North American species described in "Species Plantarum" had been brought back by Kalm.
Cook expeditions and Japan.
Daniel Solander was living in Linnaeus' house during his time as a student in Uppsala. Linnaeus was very fond of him, promising Solander his oldest daughter's hand in marriage. On Linnaeus' recommendation, Solander travelled to England in 1760, where he met the English botanist Joseph Banks. With Banks, Solander joined James Cook on his expedition to Oceania on the "Endeavour" in 1768–71. Solander was not the only apostle to journey with James Cook; Anders Sparrman followed on the "Resolution" in 1772–75 bound for, among other places, Oceania and South America. Sparrman made many other expeditions, one of them to South Africa.
Perhaps the most famous and successful apostle was Carl Peter Thunberg, who embarked on a nine-year expedition in 1770. He stayed in South Africa for three years, then travelled to Japan. All foreigners in Japan were forced to stay on the island of Dejima outside Nagasaki, so it was thus hard for Thunberg to study the flora. He did, however, manage to persuade some of the translators to bring him different plants, and he also found plants in the gardens of Dejima. He returned to Sweden in 1779, one year after Linnaeus' death.
Major publications.
"Systema Naturae".
The first edition of ' was printed in the Netherlands in 1735. It was a twelve-page work. By the time it reached its 10th edition in 1758, it classified 4,400 species of animals and 7,700 species of plants. In it, the unwieldy names mostly used at the time, such as "'", were supplemented with concise and now familiar "binomials", composed of the generic name, followed by a specific epithet – in the case given, "Physalis angulata". These binomials could serve as a label to refer to the species. Higher taxa were constructed and arranged in a simple and orderly manner. Although the system, now known as binomial nomenclature, was partially developed by the Bauhin brothers (see Gaspard Bauhin and Johann Bauhin) almost 200 years earlier, Linnaeus was the first to use it consistently throughout the work, including in monospecific genera, and may be said to have popularised it within the scientific community.
' (or, more fully, ') was first published in 1753, as a two-volume work. Its prime importance is perhaps that it is the primary starting point of plant nomenclature as it exists today.
"" was first published in 1737, delineating plant genera. Around 10 editions were published, not all of them by Linnaeus himself; the most important is the 1754 fifth edition. In it Linnaeus divided the plant Kingdom into 24 classes. One, Cryptogamia, included all the plants with concealed reproductive parts (algae, fungi, mosses and liverworts and ferns).
' (1751) was a summary of Linnaeus' thinking on plant classification and nomenclature, and an elaboration of the work he had previously published in ' (1736) and ' (1737). Other publications forming part of his plan to reform the foundations of botany include his ' and ': all were printed in Holland (as well as ' (1737) and "" (1735)), the "Philosophia" being simultaneously released in Stockholm.
Linnaean collections.
At the end of his lifetime the Linnean collection in Uppsala was considered as one of the finest collections of natural history objects in Sweden. Next to his own collection he had also built up a museum for the university of Uppsala, which was supplied by material donated by Carl Gyllenborg (in 1744–1745), crown-prince Adolf Fredrik (in 1745), Erik Petreus (in 1746), Claes Grill (in 1746), Magnus Lagerström (in 1748 and 1750) and Jonas Alströmer (in 1749). The relation between the museum and the private collection was not formalized and the steady flow of material from Linnean pupils were incorporated to the private collection rather than to the museum. Linnaeus felt his work was reflecting the harmony of nature and he said in 1754 'the earth is then nothing else but a museum of the all-wise creator's masterpieces, divided into three chambers'. He had turned his own estate into a microcosm of that 'world museum'.
In April 1766 parts of the town were destroyed by a fire and the Linnean private collection was subsequently moved to a barn outside the town, and shortly afterwards to a single-room stone building close to his countryhouse at Hammarby near Uppsala. This resulted in a physical separation between the two collections, the museum collection remained in the botanical garden of the university. Some material which needed special care (alcohol specimens) or ample storage space was moved from the private collection to the museum.
In Hammarby the Linnean private collections suffered seriously from damp and the depredations by mice and insects. Carl von Linné's son (Carl Linnaeus) inherited the collections in 1778 and retained them until his own death in 1783. Shortly after Carl von Linné's death his son confirmed that mice had caused "horrible damage" to the plants and that also moths and mould had caused considerable damage. He tried to rescue them from the neglect they had suffered during his father's later years, and also added further specimens. This last activity however reduced rather than augmented the scientific value of the original material.
In 1784 the botanist James Edward Smith purchased from the inheritants (the widow and daughter of Carl Linnaeus) nearly all of the Linnean private scientific effects and transferred them to London. Not all material in Linné's private collection was transported to England. 33 fish specimens preserved in alcohol were not sent away and were later lost.
In London Smith tended to neglect the zoological parts of the collection, he added some specimens and also gave some specimens away. Over the following centuries the Linnean collection in London suffered enormously at the hands of scientists who studied the collection, and in the process disturbed the original arrangement and labels, added specimens that did not belong to the original series and withdrew precious original type material.
Much material which had been intensively studied by Linné in his scientific career belonged to the collection of Queen Lovisa Ulrika (1720–1782) (in the Linnean publications referred to as "Museum Ludovicae Ulricae" or "M. L. U."). This collection was donated by his grandson King Gustav IV Adolf (1778–1837) to the museum in Uppsala in 1804. Another important collection in this respect was that of her husband King Adolf Fredrik (1710–1771) (in the Linnean sources known as "Museum Adolphi Friderici" or "Mus. Ad. Fr."), the wet parts (alcohol collection) of which were later donated to the Royal Swedish Academy of Sciences, and is today housed in the Swedish Museum of Natural History at Stockholm. The dry material was transferred to Uppsala.
Linnaean taxonomy.
The establishment of universally accepted conventions for the naming of organisms was Linnaeus' main contribution to taxonomy—his work marks the starting point of consistent use of binomial nomenclature. During the 18th century expansion of natural history knowledge, Linnaeus also developed what became known as the "Linnaean taxonomy"; the system of scientific classification now widely used in the biological sciences.
The Linnaean system classified nature within a nested hierarchy, starting with three kingdoms. Kingdoms were divided into classes and they, in turn, into orders, and thence into genera ("singular:" genus), which were divided into Species ("singular:" species). Below the rank of species he sometimes recognized taxa of a lower (unnamed) rank; these have since acquired standardised names such as "variety" in botany and "subspecies" in zoology. Modern taxonomy includes a rank of family between order and genus and a rank of phylum between kingdom and class that were not present in Linnaeus' original system.
Linnaeus' groupings were based upon shared physical characteristics, and not simply upon differences. Of his higher groupings, only those for animals are still in use, and the groupings themselves have been significantly changed since their conception, as have the principles behind them. Nevertheless, Linnaeus is credited with establishing the idea of a hierarchical structure of classification which is based upon observable characteristics and intended to reflect natural relationships. While the underlying details concerning what are considered to be scientifically valid "observable characteristics" have changed with expanding knowledge (for example, DNA sequencing, unavailable in Linnaeus' time, has proven to be a tool of considerable utility for classifying living organisms and establishing their evolutionary relationships), the fundamental principle remains sound.
Influences and economic beliefs.
Linnaeus' applied science was inspired not only by the instrumental utilitarianism general to the early Enlightenment, but also by his adherence to the older economic doctrine of Cameralism. Additionally, Linnaeus was a state interventionist. He supported tariffs, levies, export bounties, quotas, embargoes, navigation acts, subsidized investment capital, ceilings on wages, cash grants, state-licensed producer monopolies, and cartels.
Views on mankind.
According to German biologist Ernst Haeckel, the question of man's origin began with Linnaeus. He helped future research in the natural history of man by describing humans just as he described any other plant or animal.
Anthropomorpha.
" with a division between "Homo" and "Simia".]]
Linnaeus classified humans among the primates (as they were later called) beginning with the first edition of "". During his time at Hartekamp, he had the opportunity to examine several monkeys and noted similarities between them and man. He pointed out both species basically have the same anatomy; except for speech, he found no other differences. Thus he placed man and monkeys under the same category, "Anthropomorpha", meaning "manlike." This classification received criticism from other biologists such as Johan Gottschalk Wallerius, Jacob Theodor Klein and Johann Georg Gmelin on the ground that it is illogical to describe a human as 'like a man'. In a letter to Gmelin from 1747, Linnaeus replied:
"It does not please that I've placed Man among the Anthropomorpha, perhaps because of the term 'with human form', but man learns to know himself. Let's not quibble over words. It will be the same to me whatever name we apply. But I seek from you and from the whole world a generic difference between man and simian that [follows from the principles of Natural History.
"The greater number of naturalists who have taken into consideration the whole structure of man, including his mental faculties, have followed Blumenbach and Cuvier, and have placed man in a separate Order, under the title of the Bimana, and therefore on an equality with the orders of the Quadrumana, Carnivora, etc. Recently many of our best naturalists have recurred to the view first propounded by Linnaeus, so remarkable for his sagacity, and have placed man in the same Order with the Quadrumana, under the title of the Primates. The justice of this conclusion will be admitted: for in the first place, we must bear in mind the comparative insignificance for classification of the great development of the brain in man, and that the strongly marked differences between the skulls of man and the Quadrumana (lately insisted upon by Bischoff, Aeby, and others) apparently follow from their differently developed brains. In the second place, we must remember that nearly all the other and more important differences between man and the Quadrumana are manifestly adaptive in their nature, and relate chiefly to the erect position of man; such as the structure of his hand, foot, and pelvis, the curvature of his spine, and the position of his head."</ref> I absolutely know of none. If only someone might tell me a single one! If I would have called man a simian or vice versa, I would have brought together all the theologians against me. Perhaps I ought to have by virtue of the law of the discipline."
The theological concerns were twofold: first, putting man at the same level as monkeys or apes would lower the spiritually higher position that man was assumed to have in the great chain of being, and second, because the Bible says man was created in the image of God (theomorphism), if monkeys/apes and humans were not distinctly and separately designed, that would mean monkeys and apes were created in the image of God as well. This was something many could not accept. The conflict between world views that was caused by asserting man was a type of animal would simmer for a century until the much greater, and still ongoing, creation–evolution controversy began in earnest with the publication of "On the Origin of Species" by Charles Darwin in 1859.
After such criticism, Linnaeus felt he needed to explain himself more clearly. The 10th edition of ' introduced new terms, including "Mammalia" and "Primates", the latter of which would replace "Anthropomorpha" as well as giving humans the full binomial "Homo sapiens". The new classification received less criticism, but many natural historians still believed he had demoted humans from their former place to rule over nature, not be a part of it. Linnaeus believed that man biologically belongs to the animal kingdom and had to be included in it. In his book ', he said, "One should not vent one's wrath on animals, Theology decree that man has a soul and that the animals are mere 'aoutomata mechanica,' but I believe they would be better advised that animals have a soul and that the difference is of nobility."
Strange people in distant lands.
Linnaeus added a second species to the genus "Homo" in "" based on a figure and description by Jacobus Bontius from a 1658 publication: "Homo troglodytes" ("caveman") and published a third in 1771: "Homo lar". Swedish historian Gunnar Broberg states that the new human species Linnaeus described were actually simians or native people clad in skins to frighten colonial settlers, whose appearance had been exaggerated in accounts to Linnaeus.
In early editions of "", many well-known legendary creatures were included such as the phoenix, dragon and manticore as well as cryptids like the satyrus, which Linnaeus collected into the catch-all category "Paradoxa". Broberg thought Linnaeus was trying to offer a natural explanation and demystify the world of superstition. Linnaeus tried to debunk some of these creatures, as he had with the hydra; regarding the purported remains of dragons, Linnaeus wrote that they were either derived from lizards or rays. For "Homo troglodytes" he asked the Swedish East India Company to search for one, but they did not find any signs of its existence. "Homo lar" has since been reclassified as "Hylobates lar", the lar gibbon.
Four races.
In the first edition of "", Linnaeus subdivided the human species into four varieties based on continent and skin colour: "Europæus albus" (white European), "Americanus rubescens" (red American), "Asiaticus fuscus" (brown Asian) and "Africanus niger" (black African). In the tenth edition of Systema Naturae he further detailed stereotypical characteristics for each variety, based on the concept of the four temperaments from classical antiquity, and changed the description of Asians' skin tone to "luridus" (yellow). Additionally, Linnaeus created a wastebasket taxon "monstrosus" for "wild and monstrous humans, unknown groups, and more or less abnormal people".
Commemoration.
Anniversaries of Linnaeus' birth, especially in centennial years, have been marked by major celebrations. Linnaeus has appeared on numerous Swedish postage stamps and banknotes. There are numerous statues of Linnaeus in countries around the world. The Linnean Society of London has awarded the Linnean Medal for excellence in botany or zoology since 1888. Following approval by the Parliament of Sweden, Växjö University and Kalmar College merged on 1 January 2010 to become Linnaeus University. Other things named after Linnaeus include the twinflower genus "Linnaea", the crater Linné on the Earth's moon and the cobalt sulfide mineral Linnaeite.
Commentary on Linnaeus.
Andrew Dickson White wrote in "" (1896):
Linnaeus ... was the most eminent naturalist of his time, a wide observer, a close thinker; but the atmosphere in which he lived and moved and had his being was saturated with biblical theology, and this permeated all his thinking. ... Toward the end of his life he timidly advanced the hypothesis that all the species of one genus constituted at the creation one species; and from the last edition of his "Systema Naturæ" he quietly left out the strongly orthodox statement of the fixity of each species, which he had insisted upon in his earlier works. ... warnings came speedily both from the Catholic and Protestant sides.
The mathematical PageRank algorithm, applied to 24 multilingual Wikipedia editions in 2014, places Carl Linnaeus at the first position among top 100 historical figures.
External links.
"Biographies"
"Resources"
"Other"

</doc>
<doc id="5236" url="http://en.wikipedia.org/wiki?curid=5236" title="Coast">
Coast

A coastline or a seashore is the area where land meets the sea or ocean. A precise line that can be called a coastline cannot be determined due to the Coastline paradox
The term "coastal zone" is a region where interaction of the sea and land processes occurs. Both the terms coast and coastal are often used to describe a geographic location or region; for example, New Zealand's West Coast, or the East and West Coasts of the United States.
A pelagic coast refers to a coast which fronts the open ocean, as opposed to a more sheltered coast in a gulf or bay. A shore, on the other hand, can refer to parts of the land which adjoin any large body of water, including oceans (sea shore) and lakes (lake shore). Similarly, the somewhat related term "bank" refers to the land alongside or sloping down to a river (riverbank) or to a body of water smaller than a lake. "Bank" is also used in some parts of the world to refer to an artificial ridge of earth intended to retain the water of a river or pond; in other places this may be called a levee.
While many scientific experts might agree on a common definition of the term "coast", the delineation of the extents of a coast differ according to jurisdiction, with many scientific and government authorities in various countries differing for economic and social policy reasons. According to the UN atlas, 44% of people live within of the sea.
Formation.
Tides often determine the range over which sediment is deposited or eroded. Areas with high tidal ranges allow waves to reach farther up the shore, and areas with lower tidal ranges produce deprossosition at a smaller elevation interval. The tidal range is influenced by the size and shape of the coastline. Tides do not typically cause erosion by themselves; however, tidal bores can erode as the waves surge up river estuaries from the ocean.
Waves erode coastline as they break on shore releasing their energy; the larger the wave the more energy it releases and the more sediment it moves. Coastlines with longer shores have more room for the waves to disperse their energy, while coasts with cliffs and short shore faces give little room for the wave energy to be dispersed. In these areas the wave energy breaking against the cliffs is higher, and air and water are compressed into cracks in the rock, forcing the rock apart, breaking it down. Sediment deposited by waves comes from eroded cliff faces and is moved along the coastline by the waves. This forms an abrasion or cliffed coast.
Sediment deposited by rivers is the dominant influence on the amount of sediment located on a coastline. Today riverine deposition at the coast is often blocked by dams and other human regulatory devices, which remove the sediment from the stream by causing it to be deposited inland.
Like the ocean which shapes them, coasts are a dynamic environment with constant change. The Earth's natural processes, particularly sea level rises, waves and various weather phenomena, have resulted in the erosion, accretion and reshaping of coasts as well as flooding and creation of continental shelves and drowned river valleys (rias).
Environmental importance.
The coast and its adjacent areas on and off shore are an important part of a local ecosystem: the mixture of fresh water and salt water in estuaries provides many nutrients for marine life. Salt marshes and beaches also support a diversity of plants, animals and insects crucial to the food chain.
The high level of biodiversity creates a high level of biological activity, which has attracted human activity for thousands of years.
Human impacts.
Human uses of coasts.
More and more of the world's people live in coastal regions. Many major cities are on or near good harbors and have port facilities. Some landlocked places have achieved port status by building canals.
The coast is a frontier that nations have typically defended against military invaders, smugglers and illegal migrants. Fixed coastal defenses have long been erected in many nations and coastal countries typically have a navy and some form of coast guard.
Coasts, especially those with beaches and warm water, attract tourists. In many island nations such as those of the Mediterranean, South Pacific and Caribbean, tourism is central to the economy. Coasts offer recreational activities such as swimming, fishing, surfing, boating, and sunbathing. Growth management can be a challenge for coastal local authorities who often struggle to provide the infrastructure required by new residents.
Threats to a coast.
Coasts also face many human-induced environmental impacts. The human influence on climate change is thought to contribute to an accelerated trend in sea level rise which threatens coastal habitats.
Pollution can occur from a number of sources: garbage and industrial debris; the transportation of petroleum in tankers, increasing the probability of large oil spills; small oil spills created by large and small vessels, which flush bilge water into the ocean.
Fishing has declined due to habitat degradation, overfishing, trawling, bycatch and climate change. Since the growth of global fishing enterprises after the 1950s, intensive fishing has spread from a few concentrated areas to encompass nearly all fisheries. The scraping of the ocean floor in bottom dragging is devastating to coral, sponges and other long-lived species that do not recover quickly. This destruction alters the functioning of the ecosystem and can permanently alter species composition and biodiversity. Bycatch, the capture of unintended species in the course of fishing, is typically returned to the ocean only to die from injuries or exposure. Bycatch represents about a quarter of all marine catch. In the case of shrimp capture, the bycatch is five times larger than the shrimp caught.
It is believed that melting Arctic ice will cause sea levels to rise and flood costal areas.
Conservation.
Extraordinary population growth in the 20th century has placed stress on the planet's ecosystems. For example, on Saint Lucia, harvesting mangrove for timber and clearing for fishing reduced the mangrove forests, resulting in a loss of habitat and spawning grounds for marine life that was unique to the area. These forests also helped to stabilize the coastline. Conservation efforts since the 1980s have partially restored the ecosystem.
Types of coast.
According to one principle of classification, an emergent coastline is a coastline which has experienced a fall in sea level, because of either a global sea level change, or local uplift. Emergent coastlines are identifiable by the coastal landforms, which are above the high tide mark, such as raised beaches. In contrast, a submergent coastline is one where the sea level has risen, due to a global sea level change, local subsidence, or isostatic rebound. Submergent coastlines are identifiable by their submerged, or "drowned" landforms, such as rias (drowned valleys) and fjords.
According to a second principle of classification, a concordant coastline is a coastline where bands of different rock types run parallel to the shore. These rock types are usually of varying resistance, so the coastline forms distinctive landforms, such as coves. Discordant coastlines feature distinctive landforms because the rocks are eroded by ocean waves. The less resistant rocks erode faster, creating inlets or bays; the more resistant rocks erode more slowly, remaining as headlands or outcroppings.
Other coastal categories:
Coastal landforms.
The following articles describe some coastal landforms
Coastal processes.
The following articles describe the various geologic processes that affect a coastal zone:
Wildlife.
Animals.
Some of the animals live along a typical coast. There are animals like puffins, sea turtles and rockhopper penguins. Sea snails and various kinds of barnacles live on the coast and scavenge on food deposited by the sea. Most coastal animals are used to humans in developed areas, such as dolphins and seagulls who eat food thrown for them by tourists. Since the coastal areas are all part of the littoral zone, there is a profusion of marine life found just off-coast.
There are many kinds of seabirds on the coast. Pelicans and cormorants join up with terns and oystercatchers to forage for fish and shellfish on the coast. There are sea lions on the coast of Wales and other countries.
Plants.
Coastal areas are famous for their kelp beds. Kelp is a fast growing seaweed that grows up to a metre a day. Corals and sea anemones are true animals, but live a lifestyle similar to that of plants. Mangroves, seagrasses and salt marsh are important coastal vegetation types in tropical and temperate environments respectively.
Coastline statistics.
Coastline problem.
Shortly before 1951, Lewis Fry Richardson, in researching the possible effect of border lengths on the probability of war, noticed that the Portuguese reported their measured border with Spain to be 987 km, but the Spanish reported it as 1214 km. This was the beginning of the coastline problem, which is a mathematical uncertainty inherent in the measurement of boundaries that are irregular.
The prevailing method of estimating the length of a border (or coastline) was to lay out "n" equal straight-line segments of length "ℓ" with dividers on a map or aerial photograph. Each end of the segment must be on the boundary. Investigating the discrepancies in border estimation, Richardson discovered what is now termed the Richardson Effect: the sum of the segments is inversely proportional to the common length of the segments. In effect, the shorter the ruler, the longer the measured border; the Spanish and Portuguese geographers were simply using different-length rulers.
The result most astounding to Richardson is that, under certain circumstances, as "ℓ" approaches zero, the length of the coastline approaches infinity. Richardson had believed, based on Euclidean geometry, that a coastline would approach a fixed length, as do similar estimations of regular geometric figures. For example, the perimeter of a regular polygon inscribed in a circle approaches the circumference with increasing numbers of sides (and decrease in the length of one side). In geometric measure theory such a smooth curve as the circle that can be approximated by small straight segments with a definite limit is termed a rectifiable curve.
Measuring a coastline.
More than a decade after Richardson completed his work, Benoit Mandelbrot developed a new branch of mathematics, fractal geometry, to describe just such non-rectifiable complexes in nature as the infinite coastline. His own definition of the new figure serving as the basis for his study is:
A key property of the fractal is self-similarity; that is, at any scale the same general configuration appears. A coastline is perceived as bays alternating with promontories. In the hypothetical situation that a given coastline has this property of self-similarity, then no matter how greatly any one small section of coastline is magnified, a similar pattern of smaller bays and promontories superimposed on larger bays and promontories appears, right down to the grains of sand. At that scale the coastline appears as a momentarily shifting, potentially infinitely long thread with a stochastic arrangement of bays and promontories formed from the small objects at hand. In such an environment (as opposed to smooth curves) Mandelbrot asserts "coastline length turns out to be an elusive notion that slips between the fingers of those who want to grasp it."
There are different kinds of fractals. A coastline with the stated property is in "a first category of fractals, namely curves whose fractal dimension is greater than 1." That last statement represents an extension by Mandelbrot of Richardson's thought. Mandelbrot's statement of the Richardson Effect is:
where L, coastline length, a function of the measurement unit, ε, is approximated by the expression. F is a constant and D is a parameter that Richardson found depended on the coastline approximated by L. He gave no theoretical explanation but Mandelbrot identified D with a non-integer form of the Hausdorff dimension, later the fractal dimension. Rearranging the right side of the expression obtains:
where Fε-D must be the number of units ε required to obtain L. The fractal dimension is the number of the dimensions of the figure being used to approximate the fractal: 0 for a dot, 1 for a line, 2 for a square. D in the expression is between 1 and 2, for coastlines typically less than 1.5. The broken line measuring the coast does not extend in one direction nor does it represent an area, but is intermediate. It can be interpreted as a thick line or band of width 2ε. More broken coastlines have greater D and therefore L is longer for the same ε. Mandelbrot showed that D is independent of ε.

</doc>
<doc id="5237" url="http://en.wikipedia.org/wiki?curid=5237" title="Catatonia">
Catatonia

Catatonia is a state of neurogenic motor immobility and behavioral abnormality manifested by stupor. It was first described in 1874 by Karl Ludwig Kahlbaum, in "Die Katatonie oder das Spannungsirresein" ("Catatonia or Tension Insanity").
In the current "Diagnostic and Statistical Manual of Mental Disorders" published by the American Psychiatric Association (DSM-5) catatonia is not recognized as a separate disorder, but is associated with psychiatric conditions such as schizophrenia (catatonic type), bipolar disorder, post-traumatic stress disorder, depression and other mental disorders, as well as drug abuse or overdose (or both). It may also be seen in many medical disorders including infections (such as encephalitis), autoimmune disorders, focal neurologic lesions (including strokes), metabolic disturbances, alcohol withdrawal and abrupt or overly rapid benzodiazepine withdrawal.
It can be an adverse reaction to prescribed medication. It bears similarity to conditions such as encephalitis lethargica and neuroleptic malignant syndrome. There are a variety of treatments available; benzodiazepines are a first-line treatment strategy. Electro-convulsive therapy is also sometimes used. There is growing evidence for the effectiveness of NMDA antagonists for benzodiazepine resistant catatonia. Antipsychotics are sometimes employed but require caution as they can worsen symptoms and have serious adverse effects.
Clinical features.
Patients with catatonia may experience an extreme loss of motor skill or even constant hyperactive motor activity. Catatonic patients will sometimes hold rigid poses for hours and will ignore any external stimuli. Patients with catatonic excitement can suffer from exhaustion if not treated. Patients may also show stereotyped, repetitive movements.
They may show specific types of movement such as waxy flexibility, in which they maintain positions after being placed in them through someone else in which they resist movement in proportion to the force applied by the examiner. They may repeat meaningless phrases or speak only to repeat what the examiner says.
While catatonia is only identified as a symptom of schizophrenia in present psychiatric classifications, it is increasingly recognized as a syndrome with many faces. It appears as the Kahlbaum syndrome (motionless catatonia), malignant catatonia (neuroleptic malignant syndrome, toxic serotonin syndrome), and excited forms (delirious mania, catatonic excitement, oneirophrenia). 
It has also been recognized as grafted on to autism spectrum disorders.
Diagnostic criteria.
According to the DSM-V, "Catatonia Associated with Another Mental Disorder (Catatonia Specifier)" is diagnosed if the clinical picture is dominated by at least three of the following: 
Rating scale.
Fink and Taylor developed a catatonia rating scale to identify the syndrome. A diagnosis is verified by a benzodiazepine or barbiturate test. The diagnosis is validated by the quick response to either benzodiazepines or electroconvulsive therapy (ECT). While proven useful in the past, barbiturates are no longer commonly used in psychiatry; thus the option of either benzodiazepines or ECT.
Treatment.
Initial treatment is aimed at providing symptomatic relief. Benzodiazepines are the first line of treatment, and high doses are often required. A test dose of 1–2 mg of intramuscular lorazepam will often result in marked improvement within half an hour. In France, zolpidem has also been used in diagnosis, and response may occur within the same time period. Ultimately the underlying cause needs to be treated.
Electroconvulsive therapy (ECT) is an effective treatment for catatonia as well as for most of the underlying causes (e.g. psychosis, mania, depression). Antipsychotics should be used with care as they can worsen catatonia and are the cause of neuroleptic malignant syndrome, a dangerous condition that can mimic catatonia and requires immediate discontinuation of the antipsychotic.
Excessive glutamate activity is believed to be involved in catatonia; when first-line treatment options fail, NMDA antagonists such as amantadine or memantine are used. Amantadine may have an increased incidence of tolerance with prolonged use and can cause psychosis, due to its additional effects on the dopamine system. Memantine has a more targeted pharmacological profile for the glutamate system, reduced incidence of psychosis and may therefore be preferred for individuals who cannot tolerate amantadine. Topiramate, is another treatment option for resistant catatonia; it produces its therapeutic effects by producing glutamate antagonism via modulation of AMPA receptors.

</doc>
<doc id="5244" url="http://en.wikipedia.org/wiki?curid=5244" title="Cipher">
Cipher

In cryptography, a cipher (or cypher) is an algorithm for performing encryption or decryption—a series of well-defined steps that can be followed as a procedure. An alternative, less common term is "encipherment". To encipher or encode is to convert information from plain text into cipher or code. In non-technical usage, a 'cipher' is the same thing as a 'code'; however, the concepts are distinct in cryptography. In classical cryptography, ciphers were distinguished from codes. 
Codes generally substitute different length strings of characters in the output, while ciphers generally substitute the same number of characters as are input. There are exceptions and some cipher systems may use slightly more, or fewer, characters when output versus the number that were input. 
Codes operated by substituting according to a large codebook which linked a random string of characters or numbers to a word or phrase. For example, "UQJHSE" could be the code for "Proceed to the following coordinates". When using a cipher the original information is known as plaintext, and the encrypted form as ciphertext. The ciphertext message contains all the information of the plaintext message, but is not in a format readable by a human or computer without the proper mechanism to decrypt it.
The operation of a cipher usually depends on a piece of auxiliary information, called a key (or, in traditional NSA parlance, a "cryptovariable"). The encrypting procedure is varied depending on the key, which changes the detailed operation of the algorithm. A key must be selected before using a cipher to encrypt a message. Without knowledge of the key, it should be extremely difficult, if not impossible, to decrypt the resulting ciphertext into readable plaintext.
Most modern ciphers can be categorized in several ways
Etymology.
"Cipher" is alternatively spelled "cypher"; similarly "ciphertext" and "cyphertext", and so forth.
The word "cipher" in former times meant "zero" and had the same origin: Middle French as "cifre" and Medieval Latin as "cifra," from the Arabic صفر "ṣifr" = zero (see Zero—Etymology). "Cipher" was later used for any decimal digit, even any number. There are many theories about how the word "cipher" may have come to mean "encoding":It was firstly introduced by Abū ʿAbdallāh Muḥammad ibn Mūsā al-Khwārizmī.
Ibrahim Al-Kadi concluded that the Arabic word "sifr", for the digit zero, developed into the European technical term for encryption.
Versus codes.
In non-technical usage, a "(secret) code" typically means a "cipher". Within technical discussions, however, the words "code" and "cipher" refer to two different concepts. Codes work at the level of meaning—that is, words or phrases are converted into something else and this chunking generally shortens the message.
An example of this is the Telegraph Code which was used to shorten long telegraph messages which resulted from entering into commercial contracts using exchanges of Telegrams.
Ciphers, on the other hand, work at a lower level: the level of individual letters, small groups of letters, or, in modern schemes, individual bits and blocks of bits. Some systems used both codes and ciphers in one system, using superencipherment to increase the security. In some cases the terms codes and ciphers are also used synonymously to substitution and transposition.
Historically, cryptography was split into a dichotomy of codes and ciphers; and coding had its own terminology, analogous to that for ciphers: ""encoding", "codetext", "decoding"" and so on.
However, codes have a variety of drawbacks, including susceptibility to cryptanalysis and the difficulty of managing a cumbersome codebook. Because of this, codes have fallen into disuse in modern cryptography, and ciphers are the dominant technique.
Types.
There are a variety of different types of encryption. Algorithms used earlier in the history of cryptography are substantially different from modern methods, and modern ciphers can be classified according to how they operate and whether they use one or two keys.
Historical.
Historical pen and paper ciphers used in the past are sometimes known as classical ciphers. They include simple substitution ciphers (such as Rot 13) and transposition ciphers (such as a Rail Fence Cipher). For example "GOOD DOG" can be encrypted as "PLLX XLP" where "L" substitutes for "O", "P" for "G", and "X" for "D" in the message. Transposition of the letters "GOOD DOG" can result in "DGOGDOO". These simple ciphers and examples are easy to crack, even without plaintext-ciphertext pairs.
Simple ciphers were replaced by polyalphabetic substitution ciphers (such as the Vigenère) which changed the substitution alphabet for every letter. For example "GOOD DOG" can be encrypted as "PLSX TWF" where "L", "S", and "W" substitute for "O". With even a small amount of known or estimated plaintext, simple polyalphabetic substitution ciphers and letter transposition ciphers designed for pen and paper encryption are easy to crack. It is possible to create a secure pen and paper cipher based on a one-time pad though, but the usual disadvantages of one-time pads apply.
During the early twentieth century, electro-mechanical machines were invented to do encryption and decryption using transposition, polyalphabetic substitution, and a kind of "additive" substitution. In rotor machines, several rotor disks provided polyalphabetic substitution, while plug boards provided another substitution. Keys were easily changed by changing the rotor disks and the plugboard wires. Although these encryption methods were more complex than previous schemes and required machines to encrypt and decrypt, other machines such as the British Bombe were invented to crack these encryption methods.
Modern.
Modern encryption methods can be divided by two criteria: by type of key used, and by type of input data.
By type of key used ciphers are divided into:
In a symmetric key algorithm (e.g., DES and AES), the sender and receiver must have a shared key set up in advance and kept secret from all other parties; the sender uses this key for encryption, and the receiver uses the same key for decryption. The Feistel cipher uses a combination of substitution and transposition techniques. Most block cipher algorithms are based on this structure. In an asymmetric key algorithm (e.g., RSA), there are two separate keys: a "public key" is published and enables any sender to perform encryption, while a "private key" is kept secret by the receiver and enables only him to perform correct decryption.
Ciphers can be distinguished into two types by the type of input data:
Key size and vulnerability.
In a pure mathematical attack, (i.e., lacking any other information to help break a cipher) two factors above all count:
Since the desired effect is computational difficulty, in theory one would choose an algorithm and desired difficulty level, thus decide the key length accordingly.
An example of this process can be found at Key Length which uses multiple reports to suggest that a symmetric cipher with 128 bits, an asymmetric cipher with 3072 bit keys, and an elliptic curve cipher with 512 bits, all have similar difficulty at present.
Claude Shannon proved, using information theory considerations, that any theoretically unbreakable cipher must have keys which are at least as long as the plaintext, and used only once: one-time pad.

</doc>
