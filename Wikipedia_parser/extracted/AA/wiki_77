<doc id="5638" url="http://en.wikipedia.org/wiki?curid=5638" title="Combustion">
Combustion

Combustion or burning is the sequence of exothermic chemical reactions between a fuel and an oxidant accompanied by the production of heat and conversion of chemical species. The release of heat can produce light in the form of either glowing or a flame.
In a complete combustion reaction, a compound reacts with an oxidizing element, such as oxygen or fluorine, and the products are compounds of each element in the fuel with the oxidizing element. The process releases heat energy.
For example:
The standard enthalpy of reaction for methane combustion at and is .
A simple example can be seen in the combustion of hydrogen and oxygen, a reaction commonly used to fuel rocket engines:
The result is water vapor, with a standard enthalpy of reaction at and of .
Another example is burning of carbon, which is the main component of coal:
Complete combustion is almost impossible to achieve. As actual combustion reactions come to equilibrium, a wide variety of major and minor species will be present, such as carbon monoxide, hydrogen and even carbon (soot or ash). Additionally, any combustion at high temperatures in atmospheric air, which is 78 percent nitrogen, will also create small amounts of several nitrogen oxides, commonly referred to as NOx.
Combustion need not always involve oxygen; e.g., hydrogen burns in chlorine to form hydrogen chloride with the liberation of heat and light characteristic of combustion.
Types.
Complete vs. incomplete.
Complete.
[[File:Combustion reaction of methane.jpg|thumb|370px|The combustion of methane, a hydrocarbon.
Because the products are carbon dioxide and water, this is a complete combustion reaction.
In complete combustion, the reactant burns in oxygen, producing a limited number of products. When a hydrocarbon burns in oxygen, the reaction will primarily yield carbon dioxide and water. When elements are burned, the products are primarily the most common oxides. Carbon will yield carbon dioxide, sulfur will yield sulfur dioxide, and iron will yield iron(III) oxide. Nitrogen is not considered to be a combustible substance when oxygen is the oxidant, but small amounts of various nitrogen oxides (commonly designated NO
x species) form when air is the oxidant.
Combustion is not necessarily favorable to the maximum degree of oxidation, and it can be temperature-dependent. For example, sulfur trioxide is not produced quantitatively by the combustion of sulfur. NOx species appear in significant amounts above about 2,800 °F (1,540 °C), and more is produced at higher temperatures. The amount of NOx is also a function of oxygen excess.
In most industrial applications and in fires, air is the source of oxygen (O
2). In air, each mole of oxygen is mixed with approximately 3.71 mol of nitrogen. Nitrogen does not take part in combustion, but at high temperatures some nitrogen will be converted to NO
x (mostly NO, with much smaller amounts of NO
2). On the other hand, when there is insufficient oxygen to completely combust the fuel, some fuel carbon is converted to carbon monoxide and some of the hydrogen remains unreacted. A more complete set of equations for the combustion of a hydrocarbon in air therefore requires an additional calculation for the distribution of oxygen between the carbon and hydrogen in the fuel.
The amount of air required for complete combustion to take place is known as theoretical air. However, in practice the air used is 2-3x that of theoretical air.on, is a complete combustion reaction, as the products are carbon dioxide and water.]]
In complete combustion, the reactant burns in oxygen, producing a limited number of products. When a hydrocarbon burns in oxygen, the reaction will primarily yield carbon dioxide and water. When elements are burned, the products are primarily the most common oxides. Carbon will yield carbon dioxide, sulfur will yield sulfur dioxide, and iron will yield iron(III) oxide. Nitrogen is not considered to be a combustible substance when oxygen is the oxidant, but small amounts of various nitrogen oxides (commonly designated NOx species) form when air is the oxidant.
Combustion is not necessarily favorable to the maximum degree of oxidation, and it can be temperature-dependent. For example, sulfur trioxide is not produced quantitatively by the combustion of sulfur. NOx species appear in significant amounts above about , and more is produced at higher temperatures. The amount of NOx is also a function of oxygen excess.
In most industrial applications and in fires, air is the source of oxygen (). In air, each mole of oxygen is mixed with approximately of nitrogen. Nitrogen does not take part in combustion, but at high temperatures some nitrogen will be converted to NOx#Thermal (mostly Nitric oxide, with much smaller amounts of Nitrogen dioxide). On the other hand, when there is insufficient oxygen to completely combust the fuel, some fuel carbon is converted to carbon monoxide and some of the hydrogen remains unreacted. A more complete set of equations for the combustion of a hydrocarbon in air therefore requires an additional calculation for the distribution of oxygen between the carbon and hydrogen in the fuel.
The amount of air required for complete combustion to take place is known as theoretical air. However, in practice the air used is 2-3x that of theoretical air.
Incomplete.
Incomplete combustion will occur when there is not enough oxygen to allow the fuel to react completely to produce carbon dioxide and water. It also happens when the combustion is quenched by a heat sink, such as a solid surface or flame trap.
For most fuels, such as diesel oil, coal or wood, pyrolysis occurs before combustion. In incomplete combustion, products of pyrolysis remain unburnt and contaminate the smoke with noxious particulate matter and gases. Partially oxidized compounds are also a concern; partial oxidation of ethanol can produce harmful acetaldehyde, and carbon can produce toxic carbon monoxide.
The quality of combustion can be improved by the designs of combustion devices, such as burners and internal combustion engines. Further improvements are achievable by catalytic after-burning devices (such as catalytic converters) or by the simple partial return of the exhaust gases into the combustion process. Such devices are required by environmental legislation for cars in most countries, and may be necessary to enable large combustion devices, such as thermal power stations, to reach legal emission standards.
The degree of combustion can be measured and analyzed with test equipment. HVAC contractors, firemen and engineers use combustion analyzers to test the efficiency of a burner during the combustion process. In addition, the efficiency of an internal combustion engine can be measured in this way, and some U.S. states and local municipalities use combustion analysis to define and rate the efficiency of vehicles on the road today.
Smouldering/Slow.
Smouldering is the slow, low-temperature, flameless form of combustion, sustained by the heat evolved when oxygen directly attacks the surface of a condensed-phase fuel. It is a typically incomplete combustion reaction. Solid materials that can sustain a smouldering reaction include coal, cellulose, wood, cotton, tobacco, peat, duff, humus, synthetic foams, charring polymers (including polyurethane foam), and dust. Common examples of smouldering phenomena are the initiation of residential fires on upholstered furniture by weak heat sources (e.g., a cigarette, a short-circuited wire) and the persistent combustion of biomass behind the flaming fronts of wildfires.
Rapid.
Rapid combustion is a form of combustion, otherwise known as a fire, in which large amounts of heat and light energy are released, which often results in a flame. This is used in a form of machinery such as internal combustion engines and in thermobaric weapons. Such a combustion is frequently called an explosion, though for an internal combustion engine this is inaccurate. An internal combustion engine nominally operates on a controlled rapid burn. When the fuel-air mixture in an internal combustion engine explodes, that is known as detonation.
Spontaneous.
Spontaneous combustion is a type of combustion which occurs by self heating (increase in temperature due to exothermic internal reactions), followed by thermal runaway (self heating which rapidly accelerates to high temperatures) and finally, ignition.
Turbulent.
Combustion resulting in a turbulent flame is the most used for industrial application (e.g. gas turbines, gasoline engines, etc.) because the turbulence helps the mixing process between the fuel and oxidizer.
Microgravity.
Combustion processes behave differently in a microgravity environment than in Earth-gravity conditions due to the lack of buoyancy. For example, a candle's flame takes the shape of a sphere. Microgravity combustion research contributes to understanding of spacecraft fire safety and diverse aspects of combustion physics.
Micro-combustion.
Combustion processes which happen in very small volumes are considered micro-combustion. The high surface-to-volume ratio increases specific heat loss. Quenching distance plays a vital role in stabilizing the flame in such combustion chambers.
Chemical equations.
Stoichiometric combustion of a hydrocarbon in oxygen.
Generally, the chemical equation for stoichiometric combustion of a hydrocarbon in oxygen is:
formula_1
where "z" = "x" + ¼"y".
For example, the stoichiometric burning of propane in oxygen is:
formula_2
The simple word equation for the stoichiometric combustion of a hydrocarbon in oxygen is:
formula_3
Stoichiometric combustion of a hydrocarbon in air.
If the stoichiometric combustion takes place using air as the oxygen source, the nitrogen present in the air can be added to the equation (although it does not react) to show the composition of the resultant flue gas:
formula_4
where "z" = "x" + ¼"y".
For example, the stoichiometric combustion of propane in air is:
formula_5
The simple word equation for the stoichiometric combustion of a hydrocarbon in air is:
formula_6
Trace combustion products.
Various other substances begin to appear in significant amounts in combustion products when the flame temperature is above about . When excess air is used, nitrogen may oxidize to Nitric oxide and, to a much lesser extent, to Nitrogen dioxide. Carbon monoxide forms by disproportionation of Carbon dioxide, and Hydrogen and Hydroxyl radical form by disproportionation of Water.
For example, when of propane is burned with of air (120% of the stoichiometric amount), the combustion products contain 3.3% . At , the equilibrium combustion products contain 0.03% and 0.002% . At , the combustion products contain 0.17% , 0.05% , 0.01% , and 0.004% .
Diesel engines are run with an excess of oxygen to combust small particles that tend to form with only a stoichiometric amount of oxygen, necessarily producing nitrogen oxide emissions. Both the United States and European Union enforce limits to vehicle nitrogen oxide emissions, which necessitate the use of special catalytic converters or treatment of the exhaust with urea (see Diesel exhaust fluid).
Incomplete combustion of a hydrocarbon in oxygen.
The incomplete (partial) combustion of a hydrocarbon with oxygen produces a gas mixture containing mainly , , , and . Such gas mixtures are commonly prepared for use as protective atmospheres for the heat-treatment of metals and for gas carburizing. The general reaction equation for incomplete combustion of one mole of a hydrocarbon in oxygen is:
formula_7
The simple word equation for the incomplete combustion of a hydrocarbon in oxygen is:
formula_8
For stoichiometric (complete) combustion, "z" = "x" + ¼"y". When "z" falls below roughly 50% of the stoichiometric value, Methane can become an important combustion product; when "z" falls below roughly 35% of the stoichiometric value, elemental carbon may become stable.
The products of incomplete combustion can be calculated with the aid of a material balance, together with the assumption that the combustion products reach equilibrium. For example, in the combustion of one mole of propane () with four moles of , seven moles of combustion gas are formed, and "z" is 80% of the stoichiometric value. The three elemental balance equations are:
formula_9
formula_10
formula_11
These three equations are insufficient in themselves to calculate the combustion gas composition.
However, at the equilibrium position, the water gas shift reaction gives another equation:
formula_12
For example, at the value of "Keq" is 0.728. Solving, the combustion gas consists of 42.4% , 29.0% , 14.7% , and 13.9% . Carbon becomes a stable phase at and pressure when z is less than 30% of the stoichiometric value, at which point the combustion products contain more than 98% and and about 0.5% .
Fuels.
Substances or materials which undergo combustion are called fuels. The most common examples are natural gas, propane, kerosene, diesel, petrol, charcoal, coal, wood, etc.
Liquid fuels.
Combustion of a liquid fuel in an oxidizing atmosphere actually happens in the gas phase. It is the vapor that burns, not the liquid. Therefore, a liquid will normally catch fire only above a certain temperature: its flash point. The flash point of a liquid fuel is the lowest temperature at which it can form an ignitable mix with air. It is also the minimum temperature at which there is enough evaporated fuel in the air to start combustion.
Solid fuels.
The act of combustion consists of three relatively distinct but overlapping phases:
Combustion management.
Efficient process heating requires recovery of the largest possible part of a fuel’s heat of combustion into the material being processed. There are many avenues of loss in the operation of a heating process. Typically, the dominant loss is sensible heat leaving with the offgas (i.e., the flue gas). The temperature and quantity of offgas indicates its heat content (enthalpy), so keeping its quantity low minimizes heat loss.
In a perfect furnace, the combustion air flow would be matched to the fuel flow to give each fuel molecule the exact amount of oxygen needed to cause complete combustion. However, in the real world, combustion does not proceed in a perfect manner. Unburned fuel (usually and ) discharged from the system represents a heating value loss (as well as a safety hazard). Since combustibles are undesirable in the offgas, while the presence of unreacted oxygen there presents minimal safety and environmental concerns, the first principle of combustion management is to provide more oxygen than is theoretically needed to ensure that all the fuel burns. For methane () combustion, for example, slightly more than two molecules of oxygen are required.
The second principle of combustion management, however, is to not use too much oxygen. The correct amount of oxygen requires three types of measurement: first, active control of air and fuel flow; second, offgas oxygen measurement; and third, measurement of offgas combustibles. For each heating process there exists an optimum condition of minimal offgas heat loss with acceptable levels of combustibles concentration. Minimizing excess oxygen pays an additional benefit: for a given offgas temperature, the NOx level is lowest when excess oxygen is kept lowest.
Adherence to these two principles is furthered by making material and heat balances on the combustion process. The material balance directly relates the air/fuel ratio to the percentage of in the combustion gas. The heat balance relates the heat available for the charge to the overall net heat produced by fuel combustion. Additional material and heat balances can be made to quantify the thermal advantage from preheating the combustion air, or enriching it in oxygen.
Reaction mechanism.
Combustion in oxygen is a chain reaction in which many distinct radical intermediates participate. The high energy required for initiation is explained by the unusual structure of the dioxygen molecule. The lowest-energy configuration of the dioxygen molecule is a stable, relatively unreactive diradical in a triplet spin state. Bonding can be described with three bonding electron pairs and two antibonding electrons, whose spins are aligned, such that the molecule has nonzero total angular momentum. Most fuels, on the other hand, are in a singlet state, with paired spins and zero total angular momentum. Interaction between the two is quantum mechanically a "forbidden transition", i.e. possible with a very low probability. To initiate combustion, energy is required to force dioxygen into a spin-paired state, or singlet oxygen. This intermediate is extremely reactive. The energy is supplied as heat, and the reaction then produces additional heat, which allows it to continue.
Combustion of hydrocarbons is thought to be initiated by hydrogen atom abstraction (not proton abstraction) from the fuel to oxygen, to give a hydroperoxide radical (HOO). This reacts further to give hydroperoxides, which break up to give hydroxyl radicals. There are a great variety of these processes that produce fuel radicals and oxidizing radicals. Oxidizing species include singlet oxygen, hydroxyl, monatomic oxygen, and hydroperoxyl. Such intermediates are short-lived and cannot be isolated. However, non-radical intermediates are stable and are produced in incomplete combustion. An example is acetaldehyde produced in the combustion of ethanol. An intermediate in the combustion of carbon and hydrocarbons, carbon monoxide, is of special importance because it is a poisonous gas, but also economically useful for the production of syngas.
Solid and heavy liquid fuels also undergo a great number of pyrolysis reactions that give more easily oxidized, gaseous fuels. These reactions are endothermic and require constant energy input from the ongoing combustion reactions. A lack of oxygen or other poorly designed conditions result in these noxious and carcinogenic pyrolysis products being emitted as thick, black smoke.
The rate of combustion is the amount of a material that undergoes combustion over a period of time. It can be expressed in grams per second (g/s) or kilograms per second (kg/s).
Temperature.
Assuming perfect combustion conditions, such as complete combustion under adiabatic conditions (i.e., no heat loss or gain), the adiabatic combustion temperature can be determined. The formula that yields this temperature is based on the first law of thermodynamics and takes note of the fact that the heat of combustion is used entirely for heating the fuel, the combustion air or oxygen, and the combustion product gases (commonly referred to as the "flue gas").
In the case of fossil fuels burnt in air, the combustion temperature depends on all of the following:
The adiabatic combustion temperature (also known as the "adiabatic flame temperature") increases for higher heating values and inlet air and fuel temperatures and for stoichiometric air ratios approaching one.
Most commonly, the adiabatic combustion temperatures for coals are around (for inlet air and fuel at ambient temperatures and for formula_14), around for oil and for natural gas.
In industrial fired heaters, power station steam generators, and large gas-fired turbines, the more common way of expressing the usage of more than the stoichiometric combustion air is "percent excess combustion air". For example, excess combustion air of 15 percent means that 15 percent more than the required stoichiometric air is being used.
Instabilities.
Combustion instabilities are typically violent pressure oscillations in a combustion chamber. These pressure oscillations can be as high as 180 dB, and long term exposure to these cyclic pressure and thermal loads reduces the life of engine components. In rockets, such as the F1 used in the Saturn V program, instabilities led to massive damage of the combustion chamber and surrounding components. This problem was solved by re-designing the fuel injector. In liquid jet engines the droplet size and distribution can be used to attenuate the instabilities. Combustion instabilities are a major concern in ground-based gas turbine engines because of NOx emissions. The tendency is to run lean, an equivalence ratio less than 1, to reduce the combustion temperature and thus reduce the NOx emissions; however, running the combustion lean makes it very susceptible to combustion instability.
The Rayleigh Criterion is the basis for analysis of thermoacoustic combustion instability and is evaluated using the Rayleigh Index over one cycle of instability
formula_15
where q' is the heat release rate perturbation and p' is the pressure fluctuation.
When the heat release oscillations are in phase with the pressure oscillations, the Rayleigh Index is positive and the magnitude of the thermo acoustic instability is maximised. On the other hand, if the Rayleigh Index is negative, then thermoacoustic damping occurs. The Rayleigh Criterion implies that a thermoacoustic instability can be optimally controlled by having heat release oscillations 180 degrees out of phase with pressure oscillations at the same frequency. This minimizes the Rayleigh Index.

</doc>
<doc id="5639" url="http://en.wikipedia.org/wiki?curid=5639" title="Cyrillic script">
Cyrillic script

The Cyrillic script is an alphabetic writing system employed across Eurasia. It is based on the Early Cyrillic, which was developed in the First Bulgarian Empire during the 9th century AD at the Preslav Literary School. It is the basis of alphabets used in various languages, past and present, in parts of the Balkans and Northern Eurasia, especially those of Slavic origin, and non-Slavic languages influenced by Russian. , around 252 million people in Eurasia use it as the official alphabet for their national languages. About half of them are in Russia. Cyrillic is one of the most used writing systems in the world.
Cyrillic is derived from the Greek uncial script, augmented by letters from the older Glagolitic alphabet, including some ligatures. These additional letters were used for Old Church Slavonic sounds not found in Greek. The script is named in honor of the two Byzantine brothers, Saints Cyril and Methodius, who created the Glagolitic alphabet earlier on. Modern scholars believe that Cyrillic was developed and formalized by early disciples of Cyril and Methodius.
With the accession of Bulgaria to the European Union on 1 January 2007, Cyrillic became the third official script of the European Union, following the Latin and Greek scripts.
Letters.
Cyrillic script spread throughout the East and South Slavic territories, being adopted for writing local languages, such as the Old East Slavic. Its adaptation to local languages produced a number of Cyrillic alphabets, discussed hereafter.
Capital and lowercase letters were not distinguished in old manuscripts.
Yeri () was originally a ligature of Yer and I ( + = ). Iotation was indicated by ligatures formed with the letter I: (not ancestor of modern Ya, Я, which is derived from ), , (ligature of and ), , . Many letters had variant forms and commonly used ligatures, for example = = , = , = .
The letters also had numeric values, based not on Cyrillic alphabetical order, but inherited from the letters' Greek ancestors.
The early Cyrillic alphabet is difficult to represent on computers. Many of the letterforms differed from modern Cyrillic, varied a great deal in manuscripts, and changed over time. Few fonts include adequate glyphs to reproduce the alphabet. In accordance with Unicode policy, the standard does not include letterform variations or ligatures found in manuscript sources unless they can be shown to conform to the Unicode definition of a character.
The Unicode 5.1 standard, released on 4 April 2008, greatly improves computer support for the early Cyrillic and the modern Church Slavonic language. In Microsoft Windows, Segoe UI is notable for having complete support for the archaic Cyrillic letters since Windows 8.
Letterforms and typography.
The development of Cyrillic typography passed directly from the medieval stage to the late Baroque, without a Renaissance phase as in Western Europe. Late Medieval Cyrillic letters (still found on many icon inscriptions today) show a marked tendency to be very tall and narrow, with strokes often shared between adjacent letters.
Peter the Great, Czar of Russia, mandated the use of westernized letter forms in the early 18th century. Over time, these were largely adopted in the other languages that use the script. Thus, unlike the majority of modern Greek fonts that retained their own set of design principles for lower-case letters (such as the placement of serifs, the shapes of stroke ends, and stroke-thickness rules, although Greek capital letters do use Latin design principles), modern Cyrillic fonts are much the same as modern Latin fonts of the same font family. The development of some Cyrillic computer typefaces from Latin ones has also contributed to the visual Latinization of Cyrillic type.
Cyrillic uppercase and lowercase letter forms are not as differentiated as in Latin typography. Upright Cyrillic lowercase letters are essentially small capitals (with exceptions: Cyrillic , , , and adopted Western lowercase shapes, lowercase is typically designed under the influence of Latin , lowercase is a traditional handwritten form), although a good-quality Cyrillic typeface will still include separate small-caps glyphs.
Cyrillic fonts, as well as Latin ones, have roman and italic types (practically all popular modern fonts include parallel sets of Latin and Cyrillic letters, where many glyphs, uppercase as well as lowercase, are simply shared by both). However, the native font terminology in Slavic languages (for example, in Russian) does not use the words "roman" and "italic" in this sense. Instead, the nomenclature follows German naming patterns:
As in Latin typography, a sans-serif face may have a mechanically sloped oblique type ("naklonniy shrift"—"sloped", or "slanted type") instead of italic.
Similarly to Latin fonts, italic and cursive types of many Cyrillic letters (typically lowercase; uppercase only for hand-written or stylish types) are very different from their upright roman types. In certain cases, the correspondence between uppercase and lowercase glyphs does not coincide in Latin and Cyrillic fonts: for example, italic Cyrillic is the lowercase counterpart of not of .
A boldfaced type is called "poluzhirniy shrift" ("semi-bold type"), because there existed fully boldfaced shapes that have been out of use since the beginning of the 20th century.
A bold italic combination (bold slanted) does not exist for all font families.
In Standard Serbian, as well as in Macedonian, some italic and cursive letters are different from those used in other languages. These letter shapes are often used in upright fonts as well, especially for advertisements, road signs, inscriptions, posters and the like, less so in newspapers or books. The Cyrillic lowercase has a slightly different design both in the roman and italic types, which is similar to the lowercase Greek letter delta, .
The following table shows the differences between the upright and italic Cyrillic letters of the Russian alphabet. Italic forms significantly different from their upright analogues, or especially confusing to users of a Latin alphabet, are highlighted.
Note: in some fonts or styles, lowercase italic Cyrillic () may look like Latin and lowercase italic Cyrillic () may look exactly like a capital italic (), only small.
Cyrillic alphabets.
Among others, Cyrillic is the standard script for writing the following languages:
The Cyrillic script has also been used for languages of Alaska, Slavic Europe (except for Western Slavic and some Southern Slavic), the Caucasus, Siberia, and the Russian Far East.
The first alphabet derived from Cyrillic was Abur, used for the Komi language. Other Cyrillic alphabets include the Molodtsov alphabet for the Komi language and various alphabets for Caucasian languages.
Name.
Because the script was conceived and popularised by the followers of Cyril and Methodius, rather than by Cyril and Methodius themselves, its name denotes homage rather than authorship. The name "Cyrillic" often confuses people who are not familiar with the script's history, because it does not identify a country of origin (in contrast to the "Greek alphabet"). Some call it the "Russian alphabet" because Russian is the most popular and influential alphabet based on the script. Some Bulgarian intellectuals, notably Stefan Tsanev, have expressed concern over this, and have suggested that the Cyrillic script be called the "Bulgarian alphabet" instead, for the sake of historical accuracy.
In Bulgarian, Macedonian, Russian, and Serbian, the Cyrillic script is also known as "azbuka", derived from the old names of the first two letters of most Cyrillic alphabets (just as the term "alphabet" came from the first two Greek letters "alpha" and "beta").
History.
The Cyrillic script was created in the First Bulgarian Empire and is derived from the Greek uncial script letters, augmented by ligatures and consonants from the older Glagolitic alphabet for sounds not found in Greek. Tradition holds that Cyrillic and Glagolitic were formalized either by the two Bulgarian brothers, whose birth names are Tsurho and Strahota, born in Thessaloniki, Saints Cyril and Methodius who brought Christianity to the southern Slavs, or by their disciples. Paul Cubberley posits that although Cyril may have codified and expanded Glagolitic, it was his students in the First Bulgarian Empire that developed Cyrillic from the Greek letters in the 890s as a more suitable script for church books. Later Cyrillic spread among other Slavic peoples: Russians, Serbs and others, as well as among non-Slavic Vlachs and Moldavians.
Cyrillic and Glagolitic were used for the Church Slavonic language, especially the Old Church Slavonic variant. Hence expressions such as "И is the tenth Cyrillic letter" typically refer to the order of the Church Slavonic alphabet; not every Cyrillic alphabet uses every letter available in the script.
The Cyrillic script came to dominate Glagolitic in the 12th century. The literature produced in the Old Bulgarian language soon spread north and became the lingua franca of the Balkans and Eastern Europe, where it came to also be known as Old Church Slavonic. The alphabet used for the modern Church Slavonic language in Eastern Orthodox and Eastern Catholic rites still resembles early Cyrillic. However, over the course of the following millennium, Cyrillic adapted to changes in spoken language, developed regional variations to suit the features of national languages, and was subjected to academic reform and political decrees. Today, many languages in the Balkans, Eastern Europe, and northern Eurasia are written in Cyrillic alphabets.
Relationship to other writing systems.
Latin script.
A number of languages written in a Cyrillic alphabet have also been written in a Latin alphabet, such as Azerbaijani, Uzbek and Romanian (in Republic of Moldova in 1989, in Romania throughout the 19th century). After the disintegration of the Soviet Union in 1991, some of the former republics officially shifted from Cyrillic to Latin. The transition is complete in most of Moldova (except breakaway region of Transnistria, where Moldovan Cyrillic is official), Turkmenistan, and Azerbaijan, but Uzbekistan still uses both systems. The Russian government has mandated that Cyrillic must be used for all public communications in all federal subjects of Russia, to promote closer ties across the federation. This act was controversial for speakers of many Slavic languages; for others, such as Chechen and Ingush speakers, the law had political ramifications. For example, the separatist Chechen government mandated a Latin script which is still used by many Chechens. Those in the diaspora especially refuse to use the Chechen Cyrillic alphabet, which they associate with Russian imperialism.
Standard Serbian uses both the Cyrillic and Latin scripts. Cyrillic is nominally the official script of Serbia's administration according to the Serbian constitution; however, the law does not regulate scripts in standard language, or standard language itself by any means. In practice the scripts are equal, with Latin being used more often in less official capacity.
The Zhuang alphabet, used between the 1950s and 1980s in portions of the People's Republic of China, used a mixture of Latin, phonetic, numeral-based, and Cyrillic letters. The non-Latin letters, including Cyrillic, were removed from the alphabet in 1982 and replaced with Latin letters that closely resembled the letters they replaced.
Romanization.
There are various systems for Romanization of Cyrillic text, including transliteration to convey Cyrillic spelling in Latin letters, and transcription to convey pronunciation.
Standard Cyrillic-to-Latin transliteration systems include:
See also Romanization of Belarusian, Bulgarian, Kyrgyz, Russian, Macedonian and Ukrainian.
Cyrillization.
Representing other writing systems with Cyrillic letters is called Cyrillization.
Computer encoding.
Unicode.
In Unicode 6.0, letters of Cyrillic, including national and historical alphabets, are represented by four blocks:
Two more Cyrillic(-derived) characters are U+1D2B and U+1D78.
The characters in the range U+0400 to U+045F are basically the characters from ISO 8859-5 moved upward by 864 positions. The characters in the range U+0460 to U+0489 are historic letters, not used now. The characters in the range U+048A to U+052F are additional letters for various languages that are written with Cyrillic script.
Unicode as a general rule does not include accented Cyrillic letters. A few exceptions are:
To indicate stressed or long vowels, combining diacritical marks can be used after the respective letter (for example, : etc.).
Some languages, including Church Slavonic, are still not fully supported.
Unicode 5.1, released on 4 April 2008, introduces major changes to the Cyrillic blocks. Revisions to the existing Cyrillic blocks, and the addition of Cyrillic Extended A (2DE0...2DFF) and Cyrillic Extended B (A640...A69F), significantly improve support for the early Cyrillic alphabet, Abkhaz, Aleut, Chuvash, Kurdish, and Mordvin.
Other.
Punctuation for Cyrillic text is similar to that used in European Latin-alphabet languages.
Other character encoding systems for Cyrillic:
Keyboard layouts.
Each language has its own standard keyboard layout, adopted from typewriters. With the flexibility of computer input methods, there are also transliterating or phonetic/homophonic keyboard layouts made for typists who are more familiar with other layouts, like the common English qwerty keyboard. When practical Cyrillic keyboard layouts or fonts are not available, computer users sometimes use transliteration or look-alike "volapuk" encoding to type languages that are normally written with the Cyrillic alphabet.

</doc>
<doc id="5641" url="http://en.wikipedia.org/wiki?curid=5641" title="Consonant">
Consonant

In articulatory phonetics, a consonant is a speech sound that is articulated with complete or partial closure of the vocal tract. Examples are , pronounced with the lips; , pronounced with the front of the tongue; , pronounced with the back of the tongue; , pronounced in the throat; and , pronounced by forcing air through a narrow channel (fricatives); and and , which have air flowing through the nose (nasals). Contrasting with consonants are vowels. 
Since the number of possible sounds in all of the world's languages is much greater than the number of letters in any one alphabet, linguists have devised systems such as the International Phonetic Alphabet (IPA) to assign a unique and unambiguous symbol to each attested consonant. In fact, the English alphabet has fewer consonant letters than English has consonant sounds, so digraphs like "ch", "sh", "th", and "zh" are used to extend the alphabet, and some letters and digraphs represent more than one consonant. For example, the sound spelled "th" in "this" is a different consonant than the "th" sound in "thin". (In the IPA they are transcribed and , respectively.)
Terminology.
The word "consonant" comes from Latin oblique stem "cōnsonant-," from "cōnsonāns (littera)" "sounding-together (letter)", a calque of Greek σύμφωνον "sýmphōnon" (plural "sýmphōna").
Dionysius Thrax calls consonants "sýmphōna" "pronounced with" because they can only be pronounced with a vowel. He divides them into two subcategories: "hēmíphōna", semivowels ("half-pronounced"), which correspond to continuants, not semivowels, and "áphōna", mute or silent consonants ("unvoiced"), which correspond to stops, not voiceless consonants.
This description does not apply to some human languages, such as the Salishan languages, in which stops sometimes occur without vowels (see Nuxálk), and the modern conception of consonant does not require co-occurrence with vowels.
Letters.
The word consonant is also used to refer to a letter of an alphabet that denotes a consonant sound. The 21 consonant letters in the English alphabet are B, C, D, F, G, H, J, K, L, M, N, P, Q, R, S, T, V, X, Z, and usually W and Y: The letter Y stands for the consonant in "yoke", the vowel in "myth", the vowel in "funny", and the diphthong in "my". W always represents a consonant except in combination with a vowel letter, as in "growth", "raw", and "how", and in a few loanwords from Welsh, like "crwth" or "cwm".
In some other languages, such as Finnish, "y" only represents a vowel sound.
Consonants versus vowels.
Consonants and vowels correspond to distinct parts of a syllable: The most sonorous part of the syllable (that is, the part that's easiest to sing), called the "syllabic peak" or "nucleus," is typically a vowel, while the less sonorous margins (called the "onset" and "coda") are typically consonants. Such syllables may be abbreviated CV, V, and CVC, where C stands for consonant and V stands for vowel. This can be argued to be the only pattern found in most of the world's languages, and perhaps the primary pattern in all of them. However, the distinction between consonant and vowel is not always clear cut: there are syllabic consonants and non-syllabic vowels in many of the world's languages.
One blurry area is in segments variously called "semivowels, semiconsonants," or "glides". On the one side, there are vowel-like segments that are not in themselves syllabic but that form diphthongs as part of the syllable nucleus, as the "i" in English "boil" . On the other, there are approximants that behave like consonants in forming onsets but are articulated very much like vowels, as the "y" in English "yes" . Some phonologists model these as both being the underlying vowel , so that the English word "bit" would phonemically be , "beet" would be , and "yield" would be phonemically . Likewise, "foot" would be , "food" would be , "wood" would be , and "wooed" would be . However, there is a (perhaps allophonic) difference in articulation between these segments, with the in "yes" and "yield" and the of "wooed" having more constriction and a more definite place of articulation than the in "boil" or "bit" or the of "foot". 
The other problematic area is that of syllabic consonants, segments articulated as consonants but occupying the nucleus of a syllable. This may be the case for words such as "church" in rhotic dialects of English, although phoneticians differ in whether they consider this to be a syllabic consonant, , or a rhotic vowel, : Some distinguish an approximant that corresponds to a vowel , for "rural" as or ; others see these as a single phoneme, . 
Other languages use fricative and often trilled segments as syllabic nuclei, as in Czech and several languages in Democratic Republic of the Congo, and China, including Mandarin Chinese. In Mandarin, they are historically allophones of , and spelled that way in Pinyin. Ladefoged and Maddieson call these "fricative vowels" and say that "they can usually be thought of as syllabic fricatives that are allophones of vowels." That is, phonetically they are consonants, but phonemically they behave as vowels. 
Many Slavic languages allow the trill and the lateral as syllabic nuclei (see Words without vowels). In languages like Nuxalk, it is difficult to know what the nucleus of a syllable is, or if all syllables even have nuclei. If the concept of 'syllable' applies in Nuxalk, there are syllabic consonants in words like (?) 'seal fat'. Miyako in Japan is similar, with 'to build' and 'to pull'.
Features.
Each spoken consonant can be distinguished by several phonetic "features":
All English consonants can be classified by a combination of these features, such as "voiceless alveolar stop" . In this case, the airstream mechanism is omitted.
Some pairs of consonants like "p::b", "t::d" are sometimes called fortis and lenis, but this is a phonological rather than phonetic distinction.
Consonants are scheduled by their features in a number of IPA charts:
Examples.
The recently extinct Ubykh language had only 2 or 3 vowels but 84 consonants; the Taa language has 87 consonants under one analysis, 164 under another, plus some 30 vowels and tone. The types of consonants used in various languages are by no means universal. For instance, nearly all Australian languages lack fricatives; a large percentage of the world's languages lack voiced stops as phonemes such as , , and . Most languages, however, do include one or more fricatives, with being the most common, and a liquid consonant or two, with the most common. The approximant is also widespread, and virtually all languages have one or more nasals, though a very few, such as the Central dialect of Rotokas, lack even these. This last language has the smallest number of consonants in the world, with just six.
Most common.
The most common consonants around the world are the three voiceless stops , , , and the two nasals , . However, even these common five are not universal. Several languages in the vicinity of the Sahara Desert, including Arabic, lack . Several languages of North America, such as Mohawk, lack both of the labials and . The Wichita language of Oklahoma and some West African languages such as Ijo lack the consonant on a phonemic level, but do use it as an allophone of another consonant (of in the case of Ijo, and of in Wichita). A few languages on Bougainville Island and around Puget Sound, such as Makah, lack both of the nasals and . The 'click language' Nǁng lacks , and colloquial Samoan lacks both alveolars, and . Despite the 80-odd consonants of Ubykh, it lacks the plain velar in native words, as do the related Adyghe and Kabardian languages. But with a few striking exceptions, such as Xavante and Tahitian—which have no dorsal consonants whatsoever—nearly all other languages have at least one velar consonant: the few languages that do not have a simple usually have a consonant that is very similar. For instance, an areal feature of the Pacific Northwest coast is that historical * has become palatalized in many languages, so that Saanich for example has and but no plain ; similarly, historical * in the Northwest Caucasian languages became palatalized to in Ubykh and in most Circassian dialects.
The most "frequent" consonant (that is, the one appearing most often in speech) in many languages is .
Audio samples.
The following are consonant charts with links to audio samples.
References.
Ian Maddieson, "Patterns of Sounds", Cambridge University Press, 1984. ISBN 0-521-26536-3

</doc>
<doc id="5642" url="http://en.wikipedia.org/wiki?curid=5642" title="Costume jewelry">
Costume jewelry

Costume jewelry, trinkets, fashion jewelry, junk jewelry, fake jewelry, or fallalery is jewelry manufactured as ornamentation to complement a particular fashionable costume or garment as opposed to "real" (fine) jewelry which may be regarded primarily as collectibles, keepsakes, or investments.
Etymology.
The term costume jewelry dates back to the early 20th century. It reflects the use of the word "costume" to refer to what is now called an "outfit".
Components.
Originally, costume or fashion jewelry was made of inexpensive simulated gemstones, such as rhinestones or lucite, set in pewter, silver, nickel or brass. During the depression years, rhinestones were even down-graded by some manufacturers to meet the cost of production.
During the World War II era, sterling silver was often incorporated into costume jewelry designs primarily because:
This resulted in a number of years during which sterling silver costume jewelry was produced and some can still be found in today's vintage jewelry marketplace.
Modern costume jewelry incorporates a wide range of materials. High end crystals, cubic zirconia simulated diamonds, and some semi-precious stones are used in place of precious stones. Metals include gold- or silver-plated brass, and sometimes vermeil or sterling silver. Lower-priced jewelry may still use gold plating over pewter, nickel or other metals; items made in countries outside the United States may contain lead. Some pieces incorporate plastic, acrylic, leather or wood.
Historical expression.
Costume jewelry can be characterized by the period in history in which it was made.
Art Deco period (1920–1930s).
The Art Deco movement was an attempt to combine the harshness of mass production with the sensitivity of art and design. It was during this period that Coco Chanel introduced costume jewelry to complete the costume. The Art Deco movement died with the onset of the Great Depression and the outbreak of World War II.
According to Schiffer, some of the characteristics of the costume jewelry in the Art Deco period were: 
Retro period (1935 to 1950).
In the Retro period, designers struggled with the art versus mass production dilemma. Natural materials merged with plastics. The retro period primarily included American-made jewelry, which has a distinct American look. With the war in Europe, many European jewelry firms were forced shut down. Many European designers immigrated to the U.S. since the economy was recovering. 
According to Schiffer, some of the characteristics of the costume jewelry in the Retro period were:
Art Modern period (1945 to 1960).
In the Art Modern period following World War II, jewelry designs became more traditional and understated. The big, bold styles of the Retro period went out of style and were replaced by the more tailored styles of the 1950s and 1960s.
According to Schiffer, some of the characteristics of the costume jewelry in the Art Modern period were:
With the advent of the Mod period came "Body Jewelry". Carl Schimel of Kim Craftsmen Jewelry was at the forefront of this style. While Kim Craftsmen closed in the early 1990s, many collectors still forage for their items at antique shows and flea markets.
The Boston Museaum Of Fine Art recently displayed Carl Schimel's "Chastity Belt" created in 1969 in their "When High Fashion Inhaled The '60s—'Hippie Chic'" At MFA.http://educators.mfa.org/node/398878 This piece and exhibit was reviewed by Gregg Cook of Boston NPR "Carl Schimel’s circa 1969 base metal “Chastity Belt”—displayed here atop a black bodystocking—imitates medieval designs in its erotic chains and medallion (“a container meant to hold birth control pills,” according to the MFA). (Greg Cook) http://artery.wbur.org/2013/07/20/hippie-chic-mfa a photo of this piece can be seen at http://media.wbur.org/wordpress/18/files/2013/07/picHippieChicCook_0176.jpg
General history.
Costume jewelry has been part of culture for almost 300 years. During the 18th century, jewelers began making pieces with inexpensive glass. In the 19th century, costume jewelry made of semi-precious material came into the market. Jewels made of semi-precious material were more affordable, and this affordability gave common people the chance to own costume jewelry.
But the real golden era for the costume jewelry began in the middle of the 20th century. The new middle class wanted beautiful, but affordable, jewelry. The demand for jewelry of this type coincided with the machine-age and the industrial revolution. The revolution made the production of carefully executed replicas of admired heirloom pieces possible.
As the class structure in America changed, so did measures of real wealth. Women in all social stations, even the working-class woman, could own a small piece of costume jewelry. The average town and country woman could acquire and wear a considerable amount of this mass-produced jewelry that was both affordable and stylish.
Costume jewelry was also made popular by various designers in the mid-20th century. Some of the most remembered names in costume jewelry include both the high and low priced brands: Crown Trifari, Dior, Chanel, Miriam Haskell, Monet, Napier, Corocraft, Coventry, and Kim Craftsmen.
A significant factor in the popularization of costume jewelry was the Hollywood movie. The leading female stars of the 1940s and 1950s often wore and then endorsed the pieces produced by a range of designers. If you admired a necklace worn by Bette Davis in "The Private Lives of Elizabeth and Essex", you could buy a copy from Joseff of Hollywood, who made the original. Stars such as Vivien Leigh, Elizabeth Taylor and Jane Russell appeared in adverts for the pieces and the availability of the collections in shops such as Woolworth made it possible for ordinary women to own and wear such jewelry.
Coco Chanel greatly popularized the use of faux jewelry in her years as a fashion designer, bringing costume jewelry to life with gold and faux pearls.
Kenneth Jay Lane has since the 1960s been known for creating unique pieces for Jackie Onassis, Elizabeth Taylor, Diana Vreeland, and Audrey Hepburn. He is probably best known for his three-strand faux pearl necklace worn by Barbara Bush to her husband's inaugural ball.
In many instances, high-end fashion jewelry has achieved a "collectible" status, and increases in value over time. Today, there is a substantial secondary market for vintage fashion jewelry. The main collecting market is for 'signed pieces', that is pieces which have the maker's mark, usually stamped on the reverse. Amongst the most sought after are Miriam Haskell, Coro, Butler and Wilson, Crown Trifari and Sphinx. However, there is also demand for good quality 'unsigned' pieces, especially if they are of an unusual design.
Business and industry.
Costume jewelry is considered a discrete category of fashion accessory, and displays many characteristics of a self-contained industry. Costume jewelry manufacturers are located throughout the world, with a particular concentration in parts of China and India, where entire city-wide and region-wide economies are dominated by the trade of these goods. There has been considerable controversy in the United States and elsewhere about the lack of regulations in the manufacture of such jewelry—these range from human rights issues surrounding the treatment of labor, to the use of manufacturing processes in which small, but potentially harmful, amounts of toxic metals are added during production. In 2010, the Associated Press released the story that toxic levels of the metal cadmium. were found in children's jewelry. An AP investigation found some pieces contained more than 80 percent of cadmium.. The wider issues surrounding imports, exports, trade laws, and globalization also apply to the costume jewelry trade.
As part of the supply chain, wholesalers in the United States and other nations purchase costume jewelry from manufacturers and typically import or export it to wholesale distributors and suppliers who deal directly with retailers. Wholesale costume jewelry merchants would traditionally seek out new suppliers at trade shows. As the Internet has become increasingly important in global trade, the trade-show model has changed. Retailers can now select from a large number of wholesalers with sites on the World Wide Web. Some of these sites also market directly to consumers, who can purchase costume jewelry at greatly reduced prices. Some of these sites include fashion jewelry as a separate category, while some use this term in favor of costume jewelry. The trend of jewelry-making at home by hobbyists for personal enjoyment or for sale on sites like Etsy has resulted in the common practice of buying wholesale costume jewelry in bulk and using it for parts.

</doc>
<doc id="5643" url="http://en.wikipedia.org/wiki?curid=5643" title="Channel Islands">
Channel Islands

The Channel Islands (Norman: "Îles d'la Manche", French: "Îles Anglo-Normandes" or "Îles de la Manche") are an archipelago of British Crown Dependencies in the English Channel, off the French coast of Normandy. They include two separate bailiwicks: the Bailiwick of Jersey and the Bailiwick of Guernsey. They are considered the remnants of the Duchy of Normandy, and are not part of the United Kingdom. They have a total population of about 168,000 and their respective capitals, Saint Peter Port and Saint Helier, have populations of 16,488 and 33,500, respectively. The total area of the islands is 194 km2.
Both Bailiwicks have been administered separately since the late 13th century; each has its own independent laws, elections, and representative bodies (although in modern times, politicians from the islands' legislatures are in regular contact). Any institution common to both is the exception rather than the rule.
Geography.
The permanently inhabited islands of the Channel Islands are:
All of these except Jersey are in the Bailiwick of Guernsey. There are also several uninhabited islets. Four are part of the Bailiwick of Jersey:
These lie off Alderney:
These lie off Guernsey:
In general the larger islands have the "-ey" suffix, and the smaller ones have the "-hou" suffix; these are believed to be from the Old Norse "ey" and "holmr", respectively which means island and islet.
The Chausey Islands south of Jersey are not generally included in the geographical definition of the Channel Islands but are occasionally described in English as 'French Channel Islands' in view of their French jurisdiction. They were historically linked to the Duchy of Normandy, but they are part of the French territory along with continental Normandy, and not part of the British Isles or of the Channel Islands in a political sense. They are an incorporated part of the commune of Granville (Manche). While they are popular with visitors from France, Channel Islanders rarely visit them as there are no direct transport links from the other islands.
In official Jersey French, the islands are called 'Îles de la Manche', while in France, the term 'Îles Anglo-normandes' (Anglo-Norman isles) is used to refer to the British 'Channel Islands' in contrast to other islands in the Channel. Chausey is referred to as an 'Île normande' (as opposed to "anglo-normande"). 'Îles Normandes' and 'Archipel Normand' have also, historically, been used in Channel Island French to refer to the islands as a whole.
The very large tidal variation provides an environmentally rich inter-tidal zone around the islands, and some islands such as Burhou, the Écréhous, and the Minquiers have been designated Ramsar sites.
The waters around the islands include the following:
The highest point in the islands is Les Platons in Jersey at 143 metres (469 ft) above sea level. The lowest point is the Atlantic Ocean (sea level).
History.
Prehistory.
The earliest evidence of human occupation of the Channel Islands has been dated to 250,000 years ago when they were attached to the landmass of continental Europe. The islands became detached by rising sea levels in the Neolithic period. The numerous dolmens and other archaeological sites extant and recorded in history demonstrate the existence of a population large enough and organised enough to undertake constructions of considerable size and sophistication, such as the burial mound at La Hougue Bie in Jersey or the statue menhirs of Guernsey.
From the Iron Age.
Hoards of Armorican coins have been excavated, providing evidence of trade and contact in the Iron Age period. Evidence for Roman settlement is sparse, although evidently the islands were visited by Roman officials and traders. The traditional Latin names of the islands (Caesarea for Jersey, Sarnia for Guernsey, Riduna for Alderney) derive (possibly mistakenly) from the Antonine Itinerary. Gallo-Roman culture was adopted to an unknown extent in the islands.
In the 6th century Christian missionaries visited the islands. Samson of Dol, Helier, Marculf and Magloire are among saints associated with the islands. Although originally included within the diocese of Dol, in the 6th century the islands were transferred to the diocese of Coutances, perhaps under the influence of Prætextatus.
From the beginning of the 9th century Norse raiders appeared on the coasts. Norse settlement succeeded initial attacks, and it is from this period that many place names of Norse origin appear, including the modern names of the islands.
From the Duchy of Normandy.
The islands were annexed to the Duchy of Normandy in 933. In 1066, William II of Normandy, invaded and conquered England, becoming William I of England, also known as William the Conqueror. In the period 1204–1214, King John lost the Angevin lands in northern France, including mainland Normandy, to King Philip II of France; in 1259 his successor, Henry III, officially surrendered his claim and title to the Duchy of Normandy, while retaining the Channel Islands. Since then, the Channel Islands have been governed as possessions of the Crown separate from the Kingdom of England and its successor kingdoms of Great Britain and the United Kingdom.
The islands were invaded by the French in 1338, who held some territory until 1345. Owain Lawgoch, a mercenary leader of a Free Company in the service of the French Crown, attacked Jersey and Guernsey in 1372, and in 1373 Bertrand du Guesclin besieged Mont Orgueil. Jersey was occupied by the French in the Wars of the Roses from 1461 to 1468. In 1483 a Papal bull decreed that the islands would be neutral during time of war. This privilege of neutrality enabled islanders to trade with both France and England and was respected until 1689 when it was abolished by Order in Council following the Glorious Revolution in Great Britain.
Various attempts to transfer the islands from the diocese of Coutances (to Nantes (1400), Salisbury (1496), and Winchester (1499)) had little effect until an Order in Council of 1569 brought the islands formally into the diocese of Winchester. Control by the bishop of Winchester was ineffectual as the islands had turned overwhelmingly Calvinist and the episcopacy was not restored until 1620 in Jersey and 1663 in Guernsey.
Sark in the 16th century was uninhabited until colonised from Jersey in the 1560s. The grant of seigneurship from Elizabeth I of England forms the basis of Sark's constitution today.
Over a dozen windmills are known to have existed in the Channel Isles. They were mostly tower mills used for grinding corn.
From the 17th century.
During the Wars of the Three Kingdoms, Jersey held out strongly for the Royalist cause, providing refuge for Charles, Prince of Wales in 1646 and 1649–1650, while the more strongly Presbyterian Guernsey more generally favoured the parliamentary cause (although Castle Cornet was, on 15 December 1651, the last Royalist stronghold in the British Isles to surrender).
The islands acquired commercial and political interests in the North American colonies. Islanders became involved with the Newfoundland fisheries in the 17th century. In recognition for all the help given to him during his exile in Jersey in the 1640s, Charles II gave George Carteret, Bailiff and governor, a large grant of land in the American colonies, which he promptly named New Jersey, now part of the United States of America. Sir Edmund Andros of Guernsey was an early colonial governor in North America, and head of the short-lived Dominion of New England.
In the 19th century, wealthy French émigrés fleeing the Revolution sought residency in the islands. Many of the town domiciles existing today were built in that time. In Saint Peter Port, a large part of the harbour had been built by 1865.
20th century.
World War II.
The islands were the only part of the British Commonwealth to be occupied by the German Army during World War II.
The British Government demilitarised the islands in June 1940 and the Lieutenant-Governors were withdrawn on 21 June, leaving the insular administrations to continue government as best they could under impending military occupation.
Before German troops landed, between 30 June and 4 July 1940, evacuation took place (many young men had already left to join the Allied armed forces): 6,600 out of 50,000 left Jersey while 17,000 out of 42,000 left Guernsey. Thousands of children were evacuated with their schools to England and Scotland.
The population of Sark largely remained where they were; but in Alderney, the entire population, save for six persons, left. In Alderney, the occupying Germans built four concentration camps in which over 700 people out of a total prisoner population of about 6,000 died. Due to the destruction of documents, it is impossible to state how many forced workers died in the other islands. These were the only Nazi concentration camps on British soil.
The Royal Navy blockaded the islands from time to time, particularly following the Invasion of Normandy in June 1944. There was considerable hunger and privation during the five years of German occupation, particularly in the final months when the population was close to starvation. Intense negotiations resulted in some humanitarian aid being sent via the Red Cross, leading to the arrival of the Red Cross supply ship "Vega" in December 1944.
The German occupation of 1940–45 was harsh: over 2,000 Islanders were deported by the Germans, and Jews were sent to concentration camps; British Bobbies assisted the Nazi occupiers in rounding up the Jewish population. Partisan resistance and retribution, accusations of collaboration, and slave labour also occurred. Many Spaniards, initially refugees from the Spanish Civil War, were brought to the islands to build fortifications. Later, Russians and Eastern Europeans continued the work. Many land mines were laid, with 65,718 land mines laid in Jersey alone.
There was no resistance movement in the Channel Islands on the scale of that in mainland France. This has been ascribed to a range of factors including the physical separation of the Islands, the density of troops (up to one German for every two Islanders), the small size of the Islands precluding any hiding places for resistance groups, and the absence of the Gestapo from the occupying forces. Moreover, much of the population of military age had joined the British Army already.
The end of the occupation came after VE-Day on 8 May 1945, Jersey and Guernsey being liberated on 9 May. The German garrison in Alderney did not surrender until 16 May, and it was one of the last of the Nazi German remnants to surrender. The first evacuees returned on the first sailing from Great Britain on 23 June, but the people of Alderney were unable to start returning until December 1945. Many of the evacuees who returned home had difficulty reconnecting with their families after five years of separation.
Post-1945.
Following the liberation of 1945, reconstruction led to a transformation of the economies of the islands, attracting immigration and developing tourism. The legislatures were reformed and non-party governments embarked on social programmes, aided by the incomes from offshore finance, which grew rapidly from the 1960s.
The islands decided not to join the European Economic Community when the UK joined, and remain outside.
Since the 1990s declining profitability of agriculture and tourism have challenged the governments of the islands.
Politics.
The Channel Islands fall into two separate self-governing bailiwicks, the Bailiwick of Guernsey and the Bailiwick of Jersey. Both are British Crown Dependencies, and neither is part of the United Kingdom. They have been part of the Duchy of Normandy since the 10th century and Queen Elizabeth II is often referred to by her traditional and conventional title of Duke of Normandy. However, pursuant to the Treaty of Paris (1259), she governs in her right as The Queen (the "Crown in right of Jersey", and the "Crown in right of the "république" of the Bailiwick of Guernsey"), and not as the Duke. This notwithstanding, it is a matter of local pride for monarchists to treat the situation otherwise: the Loyal Toast at formal dinners is to 'The Queen, our Duke', rather than to 'Her Majesty, The Queen' as in the UK.
A bailiwick is a territory administered by a Bailiff. Although the words derive from a common root ('bail' = 'to give charge of') there is a vast difference between the meaning of the word 'bailiff' (English) and 'Bailiff' (CI). (The former is a court appointed private debt-collector authorised to collect judgment debts, while the latter is the most important citizen within his or her Bailwick.) The Bailiff in each Bailiwick is the civil head, presiding officer of the States, and also head of the judiciary.
In the early part of the twenty-first century, the existence of governmental offices such as the Bailiffs' which incorporate multiple roles straddling the different branches of Government came under increased scrutiny for their apparent contravention of the doctrine of separation of powers—most notably in the Guernsey case of McGonnell -v- United Kingdom (2000) 30 EHRR 289 which following final judgement at the European Court of Human Rights became part of the impetus for much recent constitutional change, particularly the Constitutional Reform Act 2005 (2005 c.4) in the UK itself, including the separation of the roles of the Lord Chancellor, the abolition of the House of Lords' judicial role, and its replacement by the UK Supreme Court. The Islands' Bailiffs however, still retain their historic roles.
The systems of government in the Islands date from Norman times, which accounts for the names of the legislatures, the States, derived from the Norman 'États' or 'estates' (i.e. the Crown, the Church, and the people). The States have evolved over the centuries into democratic parliaments.
Each island has its own primary legislature, known as the States of Guernsey and the States of Jersey, with Chief Pleas in Sark and the States of Alderney - the Channel Islands are not represented in the UK Parliament. Laws passed by the States are given Royal Assent by The Queen in Council, to whom the islands' governments are responsible.
The islands are not part of the European Union, but are part of the Customs Territory of the European Community by virtue of Protocol Three to the Treaty on European Union. In September 2010, a Channel Islands Brussels Office was set up jointly by the two Bailiwicks to develop the Channel Islands' influence with the EU, to advise the Channel Islands' governments on European matters, and to promote economic links with the EU.
Both Bailiwicks are members of the British-Irish Council, and Jèrriais and Guernésiais are recognised regional languages of the Isles.
The legal courts are separate; separate courts of appeal have been in place since 1961. Among the legal heritage from Norman law is the Clameur de Haro.
Islanders are full British citizens, and therefore European citizens. Any British citizen who applies for a passport in Jersey or Guernsey receives a passport bearing the words "British Islands, Bailiwick of Jersey" or "British Islands, Bailiwick of Guernsey". Under the provisions of Protocol Three, Channel Islanders who do not have a close connection with the UK (no parent or grandparent from the UK, and have never been resident in the UK for a five-year period) do not automatically benefit from the EU provisions on free movement within the EU and their passports receive an endorsement to that effect. This affects only a minority of islanders.
Under the UK Interpretation Act 1978, the Channel Islands are deemed to be part of the British Islands, not to be confused with the British Isles. For the purposes of the British Nationality Act 1981, the “British Islands” include the United Kingdom (Great Britain and Northern Ireland), the Channel Islands and the Isle of Man, taken together, unless the context otherwise requires.
Economy.
Tourism is the major industry in the smaller islands (with some agriculture). However Jersey and Guernsey have, since the 1960s, become major offshore financial centers & tax havens on the scale of the Cayman Islands or Bermuda. Guernsey's horticultural and greenhouse activities have been more significant than in Jersey, and Guernsey has maintained light industry as a higher proportion of its economy than Jersey. Jersey's economy since the 1980s has been substantially more reliant on finance. 
Both islands are now heavily dependent on the finance industry, this along with a high cost of living has resulted in a general widening between the rich and poor.
Both Bailiwicks issue their own banknotes and coins, which circulate freely in all the islands alongside UK coinage and Bank of England and Scottish banknotes.
There are many exports, largely consisting of crafted goods and farmed produce.
Transport and communications.
Post.
Since 1969, Jersey and Guernsey have operated postal administrations independently of the UK's Royal Mail, with their own postage stamps, which can be used for postage only in their respective Bailiwicks. UK stamps are no longer valid, but mail to the islands, and to the Isle of Man, is charged at UK inland rates. It was not until the early 1990s that the islands joined the UK's postcode system, Jersey postcodes using the initials JE and Guernsey GY.
Transport.
Road.
Each of the three largest islands has a distinct vehicle registration scheme:
In Sark, where most motor traffic is prohibited, the few vehiclesnearly all tractorsdo not display plates. Bicycles display tax discs.
Sea.
In the 1960s, names used for the cross-Channel ferries plying the mail route between the islands and Weymouth, Dorset, were taken from the popular Latin names for the islands: "Caesarea" (Jersey), "Sarnia" (Guernsey) and "Riduna" (Alderney). Fifty years later, the ferry route between the Channel Islands and the UK is operated by Condor Ferries from both St Helier, Jersey and St Peter Port, Guernsey, using high-speed catamaran fast craft to Weymouth and Poole in the UK. A regular passenger ferry service on the Commodore Clipper goes from both Channel Island ports to Portsmouth daily, and carries both passengers and freight.
Ferry services to Normandy are operated by Manche Îles Express, and services between Jersey and Saint Malo are operated by Compagnie Corsaire and Condor Ferries.
The Isle of Sark Shipping Company operates small ferries to Sark.
On 20 August 2013, Huelin-Renouf, which had operated a "lift-on lift-off" container service for 80 years between the Port of Southampton and the Port of Jersey, ceased trading. Senator Alan Maclean, a Jersey politician had previously tried to save the 90-odd jobs furnished by the company to no avail. On 20 September, it was announced that Channel Island Lines would continue this service, and would purchase the MV Huelin Dispatch from Associated British Ports who in turn had purchased them from the receiver in the bankruptcy. The new operator was to be funded by Rockayne Limited, a closely held association of Jersey businesspeople.
Air.
There are three airports in the Channel Islands; Alderney Airport, Guernsey Airport and Jersey Airport, which are directly connected to each other by services operated by Blue Islands and Aurigny.
Rail.
Historically there have been railway networks on Jersey, Guernsey, and Alderney, but all of the lines on Jersey and Guernsey have been closed and dismantled. Today there are three working railways in the Channel Islands, of which the Alderney Railway is the only one providing a regular timetabled passenger service. The other two are a gauge miniature railway, also on Alderney, and the heritage steam railway operated on Jersey as part of the Pallot Heritage Steam Museum.
Media.
Regional television and radio broadcasts are available in the islands. These services are provided by BBC Radio Jersey, BBC Radio Guernsey, BBC Channel Islands, ITV Channel Television, Island FM, and Channel 103.
Television programmes are broadcast from the Frémont Point transmitting station.
There are some local newspapers including the Guernsey Press and the Jersey Evening Post.
Telephone.
Jersey always operated its own telephone services independently of Britain's national system, but Guernsey did not establish its own telephone services until 1969. Both islands still form part of the British telephone numbering plan, but Ofcom on the mainlines does not have responsibility for telecommunications regulatory and licensing issues on the islands. It is responsible for wireless telegraphy licensing throughout the islands, and by agreement, for broadcasting regulation in the two large islands only.
Internet.
Modern broadband speeds are available in all the islands, including VDSL for home and business. Providers include Sure (Cable & Wireless) and JT (Jersey Telecome).
The two Bailiwicks each have their own internet domain .GG (Guernsey, Alderney, Sark) and .JE (Jersey) which are managed by CHANNELISLES.NET.
Culture.
The Norman language predominated in the islands until the 19th century, when increasing influence from English-speaking settlers and easier transport links led to Anglicisation. There are four main dialects/languages of Norman in the islands, Auregnais (Alderney, extinct in late 20th century), Dgèrnésiais (Guernsey), Jèrriais (Jersey) and Sercquiais (Sark, an offshoot of Jèrriais).
Victor Hugo spent many years in exile, first in Jersey and then in Guernsey, where he finished "Les Misérables". Guernsey is the setting of Hugo's later novel, "Les Travailleurs de la Mer" ("The Toilers of the Sea"). A "Guernsey-man" also makes an appearance in chapter 91 of Herman Melville's "Moby-Dick".
The annual "Muratti", the inter-island football match, is considered the sporting event of the year, although, due to broadcast coverage, it no longer attracts the crowds of spectators, travelling between the islands, that it did during the 20th century.
Cricket is popular in the Channel Islands. The Jersey cricket team and the Guernsey cricket team are both Associate members of the International Cricket Council. The teams have played each other in the Inter-insular match since 1957. In 2001 and 2002, the Channel Islands entered a team into the MCCA Knockout Trophy, the one-day tournament of the Minor counties of English and Welsh cricket.
Channel Island sportsmen and women compete in the Commonwealth Games for their respective islands and the islands have also been enthusiastic supporters of the Island Games. Shooting is a popular sport, in which islanders have won Commonwealth medals.
Guernsey's traditional colour for sporting and other purposes is green and Jersey's is red.
The main islanders have traditional animal nicknames:
Faith and religious history.
Christianity was brought to the islands around the 6th century; according to tradition, Jersey was evangelised by St Helier, Guernsey by St Samson of Dol, and the smaller islands were occupied at various times by monastic communities representing strands of Celtic Christianity. At the Reformation, the islands turned Calvinist under the influence of an influx of French-language pamphlets published in Geneva. Anglicanism was imposed in the 17th century, but the Non-Conformist tendency re-emerged with a strong adoption of Methodism. The presence of long-term Catholic communities from France and seasonal workers from Brittany and Normandy added to the mix of denominations. In late 20th Century, a strong Roman Catholic presence re-emerged with the many Portuguese workers (both from Mainland Portugal and the Island of Madeira) coming to live in the islands then more recently Polish Roman Catholics and other Eastern Europe worshipers. Services in a number of languages can be found along with many new more evangelical churches.
Other islands in the English Channel.
There are other islands in the English Channel which are not part of the Channel Islands. Among these are the French islands Bréhat, Île de Batz, Chausey, Tatihou and Îles Saint-Marcouf, and the Isle of Wight which is part of England.

</doc>
<doc id="5644" url="http://en.wikipedia.org/wiki?curid=5644" title="Comedy film">
Comedy film

Comedy is a genre of film in which the main emphasis is on humour. These films are designed to entertain the audience through amusement, and most often work by exaggerating characteristics of real life for humorous effect. 
Films in this style traditionally have a happy ending (the black comedy being an exception). One of the oldest genres in film, some of the very first silent movies were comedies, as slapstick comedy often relies on visual depictions, without requiring sound. Comedy, unlike other film genres, puts much more focus on individual stars, with many former stand-up comics transitioning to the film industry due to their popularity. While many comic films are lighthearted stories with no intent other than to amuse, others contain political or social commentary (such as "Wag the Dog" and "Man of the Year").
Types.
A Comedy of manners film satirises the manners and affectations of a social class, often represented by stock characters. Also, the plot of the comedy is often concerned with an illicit love affair or some other scandal. However, the plot is generally less important than its witty dialogue. This form of comedy has a long ancestry, dating back at least as far as "Much Ado about Nothing" created by William Shakespeare.
Slapstick (The Three Stooges is an excellent example of this kind of comedy) relies predominately on visual depictions of events, and therefore does not require sound. Accordingly, the subgenre was ideal for silent movies.
In a fish out of water comedy film, the main character or character finds himself in an unusual environment, which drives most of the humour. Situations can be swapping gender roles, as in "Tootsie" (1982); an age changing role, as in "Big" (1988); a freedom-loving individual fitting into a structured environment, as in "Police Academy" (1984); a rural backwoodsman in the big city, as in ""Crocodile" Dundee", and so forth. The Coen Brothers are known for using this technique in all of their films, though not always to comic effect. Some films including people fitting the "fish-out-of-water" bill include "The Big Lebowski" and "A Serious Man".
A parody or spoof film is a comedy that satirizes other film genres or classic films. Such films employ sarcasm, stereotyping, mockery of scenes from other films, and the obviousness of meaning in a character's actions. Examples of this form include "Blazing Saddles" (1974), "Airplane!" (1980), "Young Frankenstein" (1974) and "Scary Movie (2000).
The anarchic comedy film, as its name suggests, is a random or stream-of-consciousness type of humour which often lampoons a form of authority. The genre dates from the silent era, and the most famous examples of this type of film would be those produced by Monty Python. Others include "Duck Soup" (1933) and "National Lampoon's Animal House" .
The black comedy film deals with normally taboo subjects, including, death, murder, sexual relations, suicide and war, in a satirical manner. Examples include "Arsenic and Old Lace" (1944), "Monsieur Verdoux" (1947), "Kind Hearts and Coronets" (1949), "The Ladykillers" (1955), "" (1964), "The Loved One" (1965), "MASH" (1970), "Monty Python's The Meaning of Life" (1983), "Brazil" (1985), "The War of the Roses" (1989), "Heathers" (1989), "Your Friends & Neighbors" (1998), "Keeping Mum" (2005), and "Burn After Reading" (2008).
Gross out films are a relatively recent development, and rely heavily on vulgar, sexual or "toilet" humour. Examples include "Porky's" (1982), "Dumb and Dumber" (1994), "There's Something About Mary" (1998), and "American Pie" (1999).
The romantic comedy film sub-genre typically involves the development of a relationship between a man and a woman. The stereotyped plot line follows the "boy-gets-girl", "boy-loses-girl", "boy gets girl back again" sequence. Naturally there are innumerable variants to this plot, and much of the generally light-hearted comedy lies in the social interactions and sexual tensions between the pair. Examples of this style of film include "It's a Wonderful World" (1939), "The Shop Around the Corner" (1940), "Sabrina" (1954), "Annie Hall" (1977), "When Harry Met Sally..." (1989), "Pretty Woman" (1990), and "Four Weddings and a Funeral" (1994).
It was not uncommon for the early romantic comedy film to also be a screwball comedy film. This form of comedy film was particularly popular during the 1930s and 1940s. There is no consensus definition of this film style, and it is often loosely applied to slapstick or romantic comedy films. Typically it can include a romantic element, an interplay between people of different economic strata, quick and witty repartee, some form of role reversal, and a happy ending. Some examples of the screwball comedy are: "It Happened One Night" (1934), "Bringing Up Baby" (1938), "Philadelphia Story" (1940), "His Girl Friday" (1940), and more recently "What's Up, Doc?" (1972).
Hybrid genres.
Action comedy.
Films in this sub-genre blend comic antics and action where the film stars combine wit and one-liners with a thrilling plot and daring stunts. The genre became a specific draw in North America in the eighties when comedians such as Eddie Murphy started taking more action oriented roles such as in "48 Hrs." and "Beverly Hills Cop". These type of films are often buddy films, with mismatched partners such as in "Midnight Run", "Rush Hour", "21 Jump Street", "Bad Boys", and "Hot Fuzz". Slapstick martial arts films became a mainstay of Hong Kong action cinema through the work of Jackie Chan among others. It may also focus on superheroes such as "The Incredibles", "Hancock" or "Kick-Ass".
Comedy horror.
Comedy horror is a type of horror film in which the usual dark themes are treated with a humorous approach. These films are either use goofy horror clichés such as in "Scream", "Young Frankenstein", "Little Shop of Horrors", "Haunted Mansion" and "Scary Movie" where campy styles are favoured. Some are much more subtle and don't parody horror, such as "An American Werewolf In London". Another style of comedy horror can also rely on over the top violence and gore such as in "Dead Alive" (1992), "Evil Dead" (1981), and "Club Dread" - such films are sometimes known as "splatstick", a portmanteau of the words "splatter" and "slapstick". It would be reasonable to put "Ghostbusters" in this category.
Fantasy comedy.
Fantasy comedy films are types of films that uses magic, supernatural and or mythological figures for comic purposes. Most fantasy comedy includes an element of parody, or satire, turning many of the fantasy conventions on their head such as the hero becoming a cowardly fool, the princess being a klutz. Examples of these films include "Being John Malkovich", "Night at the Museum", "Groundhog Day", "Click" and "Shrek".
heropanthi 
Sci-fi comedy.
Sci-fi comedy films, like most hybrid genre of comedy use the elements of science fiction films to over the top extremes and exaggerated science fiction stereotypical characters. Examples of these types of films include "Back to the Future", "Spaceballs", "Ghostbusters", "Evolution", "Innerspace", "Galaxy Quest", "Mars Attacks!", "Men in Black" and "The World's End".
Military comedy.
Military comedy films involve comic situations in a military setting. When a film is primarily about the experience of civilians called into military service and still feeling out of place, it may be referred to as a "service comedy". Because war is such a grim subject, many military comedies are set in peacetime or during wartime but away from battle zones. Military and service comedies include: 
History.
1895–1930.
Comic films began to appear in significant numbers during the era of silent films, roughly 1895 to 1930. The visual humour of many of these silent films relied on slapstick and burlesque. A very early comedy short was "Watering the Gardener" (1895) by the Lumière brothers. In American film, the most prominent comic actors of the silent era were Charlie Chaplin (although born in England, his success was principally in the U.S.), Buster Keaton and Harold Lloyd. In his native France and throughout the world, Max Linder was a major comic feature and might qualify as the first true film star.
A popular trend during the 1920s and afterward was comedy in the form of animated cartoons. Several popular characters of the period received the cartoon treatment. Among these were Felix the Cat, Mickey Mouse, Oswald the Lucky Rabbit, and Betty Boop.
1930–1950s.
Toward the end of the 1920s, the introduction of sound into movies made possible dramatic new film styles and the use of verbal humour. During the 1930s, the silent film comedy was replaced by dialogue from film comedians such as the W. C. Fields and the Marx Brothers. Stan Laurel and Oliver Hardy, who had made a number of very popular short silent films, used the arrival of sound to deepen their well-formed screen characterizations and enhance their visual humour, and went on to great success in talking films. The comedian Charlie Chaplin was one of the last silent film hold-outs, and his films during the 1930s were devoid of dialogue, although they did employ sound effects.
Screwball comedies, such as produced by Frank Capra, exhibited a pleasing, idealized climate that portrayed reassuring social values and a certain optimism about everyday life. Movies still included slapstick humour and other physical comedy, but these were now frequently supplemental to the verbal interaction. Another common comic production from the 1930s was the short subject. Hal Roach Studio specialized in this form. While Columbia was prolific, producing 190 Three Stooges releases, alone. These non-feature productions only went into decline in the 1950s when they were migrated to the television.
In the United Kingdom, film adaptations of stage farces were popular in the early 1930s, while the music hall tradition strongly influenced film comedy into the 1940s with Will Hay and George Formby among the top comedy stars of the time. In England in the late 1940s, Ealing Studios achieved popular success as well as critical acclaim with a series of films known collectively as the "Ealing comedies", from 1947 to 1957. They usually included a degree of social comment, and featured ensemble casts which often included Alec Guinness or Stanley Holloway. Among the most famous examples were "Kind Hearts and Coronets" (1949), "The Lavender Hill Mob" (1951) and "
The Ladykillers" (1955).
With the entry of the United States into World War II, Hollywood became focused on themes related to the conflict. Comedies portrayed military themes such as service, civil defense, boot-camp and shore-leave. The war-time restrictions on travel made this a boom time for Hollywood, and nearly a quarter of the money spent on attending movies.
The post-war period was an age of reflection on the war, and the emergence of a competing medium, the television. In 1948, television began to acquire commercial momentum and by the following year there were nearly a hundred television transmitters in American cities.
By the 1950s, the television industry had become a serious competition for the movie industry. Despite the technological limitations of the TV medium at the time, more and more people chose to stay home to watch the television. The Hollywood studios at first viewed the television as a threat, and later as a commercial market. Several comic forms that had previously been a staple of movie theaters transitioned to the television. Both the short subject and the cartoon now appeared on the television rather than in the theater, and the "B" movie also found its outlet on the television.
As television became filled with family-oriented comedies, the 1950s saw a trend toward more adult social situations. Only the Walt Disney studios continued to steadily release family comedies. The release of comedy films also went into a decline during this decade. In 1947 almost one in five films had been comic in nature, but by 1954 this was down to ten percent.
The 1950s saw the decline of past comedy stars and a certain paucity of new talent in Hollywood. Among the few popular new stars during this period were Judy Holliday and the comedy team phenom of Dean Martin and Jerry Lewis. Lewis followed the legacy of such comedians as Keaton and Harold Lloyd, but his work was not well received by critics in the United States (in contrast to France where he proved highly popular.)
The British film industry produced a number of highly successful film series, however, including the Doctor series, the St. Trinian's films and the increasingly bawdy "Carry On" films. John and Roy Boulting also wrote and directed a series of successful satires, including "Private's Progress" (1956) and "I'm All Right, Jack" (1959). As in the United States, in the next decade much of this talent would move into television.
A number of French comedians were also able to find an English speaking audience in the 1950s, including Fernandel and Jacques Tati.
1960s–1980s.
The next decade saw an increasing number of broad, star-packed comedies including "It's a Mad, Mad, Mad, Mad World" (1963), "Those Magnificent Men in Their Flying Machines" (1965) and "The Great Race" (1965). By the middle of the decade, some of the 1950s generation of American comedians, such as Jerry Lewis, went into decline, while Peter Sellers found success with international audiences in his first American film "The Pink Panther". The bumbling Inspector Clouseau was a character Sellers would continue to return to over the next decade.
Toward the end of the 1950s, darker humour and more serious themes had begun to emerge, including satire and social commentary. "Dr. Strangelove" (1964) was a satirical comedy about Cold War paranoia, while "The Apartment" (1960), "Alfie" (1966) and "The Graduate" (1967) featured sexual themes in a way that would have been impossible only a few years previously.
In 1970, the black comedies "Catch 22" and "M*A*S*H" reflected the anti-war sentiment then prevalent, as well as treating the sensitive topic of suicide. "M*A*S*H" would be toned down and brought to television in the following decade as a long-running series.
Among the leading lights in comedy films of the next decade were Woody Allen and Mel Brooks. Both wrote, directed and appeared in their movies. Brooks' style was generally slapstick and zany in nature, often parodying film styles and genres, including Universal horror films ("Young Frankenstein"), westerns ("Blazing Saddles") and Hitchcock films ("High Anxiety"). Following his success on Broadway and on film with "The Odd Couple" playwright and screenwriter Neil Simon would also be prominent in the 1970s, with films like "The Sunshine Boys" and "California Suite". Other notable film comedians who appeared later in the decade were Richard Pryor, Steve Martin and Burt Reynolds.
Most British comedy films of the early 1970s were spin-offs of television series, including "Dad's Army" and "On the Buses". The greatest successes, however, came with the films of the Monty Python team, including "And Now for Something Completely Different" (1971), "Monty Python and the Holy Grail" (1975) and "Monty Python's Life of Brian" in 1979.
In 1980, the gag-based comedy "Airplane!", a spoof of the previous decade's disaster film series was released and paved the way for more of the same including "Top Secret!" (1984) and the "Naked Gun" films. Popular comedy stars in the 1980s included Dudley Moore, Tom Hanks, Eddie Murphy and Dan Aykroyd. Many had come to prominence on the American TV series "Saturday Night Live", including Bill Murray, Steve Martin and Chevy Chase. Eddie Murphy made a success of comedy-action films including "48 Hrs." (1982) and the "Beverly Hills Cop" series (1984–1993).
Also popular were the films of John Hughes such as "Ferris Bueller's Day Off". He would later become best known for the "Home Alone" series of the early 1990s. The latter film helped a revival in comedies aimed at a family audience, along with "Honey, I Shrunk the Kids" and its sequels.
1990s–2010s.
One of the major developments of the 1990s was the re-emergence of the romantic comedy film, encouraged by the success of "When Harry Met Sally..." in 1989. Other examples included "Sleepless in Seattle" (1993), "Clueless" (1995) and "You've Got Mail" (1998) from the United States, and "Four Weddings and a Funeral" (1994), "Sliding Doors" (1998) and "Notting Hill" (1999) from the United Kingdom. Spoofs remained popular as well, especially with the "Scary Movie" series and "Not Another Teen Movie" series.
Probably more representative of British humour were the working class comedies "Brassed Off" (1996) and "The Full Monty" (1997). Other British comedies examined the role of the Asian community in British life, including "Bhaji on the Beach" (1993), "East Is East" (1999), "Bend It Like Beckham" (2002), "Anita and Me" (2003) and "Death at a Funeral".
Also there were "stoner" comedies, which usually involve two guys on an adventure with random things happening to them along the way. Big movies of this sub-genre would be "The Big Lebowski", "Dude, Where's My Car", "Big Nothing", "Harold & Kumar Go to White Castle", and "Pineapple Express". These movies usually have drug-related jokes and crude content.
Another development was the increasing use of "gross-out humour" usually aimed at a younger audience, in films like "There's Something About Mary", "American Pie" and its sequels, and "Freddy Got Fingered". In mid-2000s, the trend of "gross-out" movies is continuing, with adult-oriented comedies picking up the box office. But serious black comedies (also known as dramatic comedies or dramedies) were performing also well, such as "The Weather Man", "Broken Flowers" and "Shopgirl". In late 2006, "" blended vulgar humour with cultural satire.

</doc>
<doc id="5645" url="http://en.wikipedia.org/wiki?curid=5645" title="Cult film">
Cult film

A cult film, also commonly referred to as a cult classic, is a film that has acquired a cult following. Cult films are known for their dedicated, passionate fanbase, an elaborate subculture that engage in repeated viewings, quoting dialogue, and audience participation. Inclusive definitions allow for major studio productions, especially box office bombs, while exclusive definitions focus more on obscure, transgressive films shunned by the mainstream. The difficulty in defining the term and subjectivity of what qualifies as a cult film mirror classificatory disputes about art. The term "cult film" itself was first used in the 1970s to describe the culture that surrounded underground films and midnight movies, though "cult" was in common use in film analysis for decades prior to that.
Cult films trace their origin back to controversial and suppressed films kept alive by dedicated fans. In some cases, reclaimed or rediscovered films have acquired cult followings decades after their original release, occasionally for their camp value. Other cult films have since become well-respected or reassessed as classics; there is debate as to whether these popular and accepted films are still cult films. After failing in the cinema, some cult films have become regular fixtures on cable television or profitable sellers on home video. Others have inspired their own film festivals. Cult films can both appeal to specific subcultures and form their own subcultures. Other media that reference cult films can easily identify which demographics they desire to attract and offer savvy fans an opportunity to demonstrate their knowledge.
Cult films frequently break cultural taboos, and many feature excessive displays of violence, gore, sexuality, profanity, or combinations thereof. This can lead to controversy, censorship, and outright bans; less transgressive films may attract similar amounts of controversy when critics call them frivolous or incompetent. Films that fail to attract requisite amounts of controversy may face resistance when labeled as cult films. Mainstream films and big budget blockbusters have attracted cult followings similar to more underground and lesser known films; fans of these films often emphasize the films' niche appeal and reject the more popular aspects. Fans who like the films for the wrong reasons, such as perceived elements that represent mainstream appeal and marketing, will often be ostracized or ridiculed. Likewise, fans who stray from accepted subcultural scripts may experience similar rejection.
Since the late 1970s, cult films have become increasingly more popular. Films that once would have been limited to obscure cult followings are now capable of breaking into the mainstream, and showings of cult films have proved to be a profitable business venture. Overbroad usage of the term has resulted in controversy, as purists state it has become a meaningless descriptor applied to any film that is the slightest bit weird or unconventional; others accuse Hollywood studios of trying to artificially create cult films or use the term as a marketing tactic. Films are frequently stated to be an "instant cult classic" now, occasionally before they are released. Fickle fans on the Internet have latched on to unreleased films only to abandon them later on release. At the same time, other films have acquired massive, quick cult followings, thanks to spreading virally through social media. Easy access to cult films via video on demand and peer-to-peer file sharing has led some critics to pronounce the death of cult films.
Definition.
A cult film is any film that has a cult following, although the term is not easily defined and can be applied to a wide variety of films. The definition is occasionally expanded to exclude films that have been released by major studios or have big budgets, try specifically to become cult films, or become accepted by mainstream audiences and critics. Cult films are defined by audience reaction as much as they are content. This may take the form of elaborate and ritualized audience participation, film festivals, or cosplay. Over time, the definition has become more vague and inclusive as it drifts away from earlier, stricter views. Increasing use of the term by mainstream publications has resulted in controversy, as cinephiles argue that the term has become meaningless or "elastic, a catchall for anything slightly maverick or strange". Mark Shiel has criticized the term itself as being a weak concept, reliant on subjectivity; different groups can interpret films in their own terms. According to feminist Joanne Hollows, this subjectivity causes films with strong female cult followings to be perceived as too mainstream and not transgressive enough to qualify as a cult film. Mike Chopra‑Gant says that cult films become decontextualized when studied as a group, and Shiel criticizes this recontextualization as cultural commodification.
In 2008, "Cineaste" asked a range of academics for their definition of a cult film. Several people defined cult films primarily in terms of their opposition to mainstream films and conformism, explicitly requiring a transgressive element, though others disputed the transgressive potential, given the demographic appeal to white males and mainstreaming of cult films. Jeffrey Andrew Weinstock instead called them mainstream films with transgressive elements. Most definitions also required a strong community aspect, such as obsessed fans or ritualistic behavior. Citing misuse of the term, Mikel J. Koven took a self-described hard-line stance that rejected definitions that use any other criteria. Matt Hills instead stressed the need for an open-ended definition rooted in structuration, where the film and the audience reaction are interrelated and neither is prioritized. Ernest Mathijs focused on the accidental nature of cult followings, arguing that cult film fans consider themselves too savvy to be marketed to, while Jonathan Rosenbaum rejected the continued existence of cult films and called the term a marketing buzzword. Mathijs suggests that cult films help to understand ambiguity and incompleteness in life given the difficulty in even defining the term. That cult films can have opposing qualities – such as good and bad, failure and success, innovative and retro – helps to illustrate that art is subjective and never self-evident. This ambiguity leads critics of postmodernism to accuse cult films of being beyond criticism, as the emphasis is now on personal interpretation rather than critical analysis or metanarratives. However, these inherent dichotomies can lead critics to pronounce beloved cult films to be "so bad it's good" or for audiences to be split between ironic and earnest fans.
Quoting Jeffrey Sconce, Jancovich et al. define cult films in terms of paracinema, marginal films that exist outside critical and cultural acceptance: everything from exploitation to beach party musicals to softcore pornography. However, they reject cult films as having a single unifying feature; instead, they state that cult films are united in their "subcultural ideology" and opposition to mainstream tastes, itself a vague and undefinable term. Cult followings themselves can range from adoration to contempt, and they have little in common except for their celebration of nonconformity – even the bad films ridiculed by fans are artistically nonconformist, albeit unintentionally. At the same time, they state that bourgeois, masculine tastes are frequently reinforced, which makes cult films more of an internal conflict within the bourgeoisie, rather than a rebellion against it. This results in an anti-academic bias despite the use of formal methodologies, such as defamiliarization. This contradiction exists in many subcultures, especially those dependent on defining themselves in terms of opposition to the mainstream. This nonconformity is eventually co-opted by the dominant forces, such as Hollywood, and marketed to the mainstream. Xavier Mendik also defines cult films as opposing the mainstream and further proposes that films can become cult by virtue of their genre or content, especially if it is transgressive. Due to their rejection of mainstream appeal, Mendik says cult films can be more creative and political; times of relative political instability produce more interesting films.
General overview.
Cult films have existed since the early days of cinema. Harry Allan Potamkin traces them back to 1910s France and the reception of Pearl White, William S. Hart, and Charlie Chaplin, described as "a dissent from the popular ritual". "Nosferatu" (1922) was an unauthorized adaptation of Bram Stoker's "Dracula". Stoker's widow sued the production company and drove it to bankruptcy. All known copies of the film were destroyed, and "Nosferatu" become an early cult film, kept alive by a cult following that circulated illegal bootlegs. Chuck Kleinhans identifies the Marx Brothers as making other early cult films. On their original release, some highly regarded classics from the Golden Age of Hollywood were panned by critics and audiences, relegated to cult status. "The Night of the Hunter" (1955) was a cult film for years, quoted often and championed by fans, before it was reassessed as an important and influential classic. During this time, American exploitation films and imported European art films were marketed similarly. Although critics Pauline Kael and Arthur Knight argued against arbitrary divisions into high and low culture, American films settled into rigid genres; European art films continued to push the boundaries of simple definitions, and these exploitative art films and artistic exploitation films would go on to influence American cult films. Much like later cult films, these early exploitation films encouraged audience participation, influenced by live theater and vaudeville.
Modern cult films grew from 1960s counterculture and underground films, popular among those who rejected mainstream Hollywood films. These underground film festivals led to the creation of midnight movies, which attracted cult followings. The term "cult film" itself was an outgrowth of this movement and was first used in the 1970s, though "cult" had been in use for decades in film analysis with both positive and negative connotations. These films were more concerned with cultural significance than the social justice sought by earlier avant-garde films. Midnight movies became more popular and mainstream, peaking with the release of "The Rocky Horror Picture Show" (1975), which finally found its audience several years after its release. Eventually, the rise of home video would marginalize midnight movies once again, after which many directors joined the burgeoning independent film scene or went back underground. Home video would give a second life to box office flops, as positive word-of-mouth or excessive replay on cable television led these films to develop an appreciative audience, as well as obsessive replay and study. For example, "The Beastmaster" (1982), despite its failure at the box office, became one of the most played movies on American cable television and developed into a cult film. Home video and television broadcasts of cult films were initially greeted with hostility. Joanne Hollows states that they were seen as turning cult films mainstream – in effect, feminizing them by opening them to distracted, passive audiences.
Releases from major studios – such as "The Big Lebowski" (1998), which was distributed by Universal Studios – can become cult films when they fail at the box office and develop a cult following through reissues, such as midnight movies, festivals, and home video. Hollywood films, due to their nature, are more likely to attract this kind of attention, which leads to a mainstreaming effect of cult culture. With major studios behind them, even financially unsuccessful films can be re-released multiple times, which plays into a trend to capture audiences through repetitious reissues. The constant use of profanity and drugs in otherwise mainstream, Hollywood films, such as "The Big Lebowski", can alienate critics and audiences yet lead to a large cult following among more open-minded demographics not often associated with cult films, such as Wall Street bankers and professional soldiers. Thus, even comparatively mainstream films can satisfy the traditional demands of a cult film, perceived by fans as transgressive, niche, and uncommercial. Discussing his reputation for making cult films, Bollywood director Anurag Kashyap said, "I didn't set out to make cult films. I wanted to make box-office hits." According to Mathijs and Sexton, this acceptance of mainstream culture and commercialism is not out of character, as cult audiences have a more complex relationship to these concepts: they are more opposed to mainstream values and excessive commercialism than they are anything else.
In a global context, popularity can vary widely by territory, especially with regard to limited releases. "Mad Max" (1979) was an international hit – except in America where it became an obscure cult favorite, ignored by critics and available for years only in a dubbed version though it earned over $100M internationally. Foreign cinema can put a different spin on popular genres, such as Japanese horror, which was initially a cult favorite in America. Asian imports to the West are often marketed as exotic cult films and of interchangeable national identity, which Chi-Yun Shin criticizes as reductive. Foreign influence can affect fan response, especially on genres tied to a national identity; when they become more global in scope, questions of authenticity may arise. Filmmakers and films ignored in their own country can become the objects of cult adoration in another, which produces perplexed reactions in their native country. Cult films can also establish an early viability for more mainstream films both for filmmakers and national cinema. The early cult horror films of Peter Jackson were so strongly associated with his homeland that they affected the international reputation of New Zealand and its cinema. As more artistic films emerged, New Zealand was perceived as a legitimate competitor to Hollywood, which mirrored Jackson's career trajectory. "Heavenly Creatures" (1994) acquired its own cult following, became a part of New Zealand's national identity, and paved the way for big-budget, Hollywood-style epics, such as Jackson's "Lord of the Rings" trilogy.
Ernest Mathijs states that cult films and fandom frequently involve nontraditional elements of time and time management. Fans will often watch films obsessively, an activity that is viewed by the mainstream as wasting time yet can be seen as resisting the commodification of leisure time. They may also watch films idiosyncratically: sped up, slowed down, frequently paused, or at odd hours. Cult films themselves subvert traditional views of time – time travel, non-linear narratives, and ambiguous establishments of time are all popular. Mathijs also identifies specific cult film viewing habits, such as viewing horror films on Halloween, sentimental melodrama on Christmas, and romantic films on Valentine's Day. These films are often viewed as marathons where fans can gorge themselves on their favorites. Mathijs states that cult films broadcast on Christmas have a nostalgic factor. These films, ritually watched every season, give a sense of community and shared nostalgia to viewers. New films often have trouble making inroads against the institutions of "It's A Wonderful Life" (1946) and "Miracle on 34th Street" (1947). These films provide mild criticism of consumerism while encouraging family values. Halloween, on the other hand, allows flaunting society's taboos and testing one's fears. Horror films have appropriated the holiday, and many horror films debut on Halloween. Mathijs criticizes the over-cultified, commercialized nature of Halloween and horror films, which feed into each other so much that Halloween has turned into an image or product with no real community. Mathijs states that Halloween horror conventions can provide the missing community aspect.
Despite their oppositional nature, cult films can produce celebrities. Like cult films themselves, authenticity is an important aspect of their popularity. Actors can become typecast as they become strongly associated with such iconic roles. Tim Curry, despite his acknowledged range as an actor, found casting difficult after he achieved fame in "The Rocky Horror Picture Show". Even when discussing unrelated projects, interviewers frequently bring up the role, which causes him to tire of discussing it. Mary Woronov, known for her transgressive roles in cult films, eventually transitioned to mainstream films. She was expected to recreate the transgressive elements of her cult films within the confines of mainstream cinema. Instead of the complex gender deconstructions of her Andy Warhol films, she became typecast as a lesbian or domineering woman. Sylvia Kristel, after starring in "Emmanuelle" (1974), found herself highly associated with the film and the sexual liberation of the 1970s. Caught between the transgressive elements of her cult film and the mainstream appeal of soft-core pornography, she was unable to work in anything but exploitation films and "Emmanuelle" sequels. Despite her immense popularity and cult following, she would rate only a footnote in most histories of European cinema if she was even mentioned. Similarly, Chloë Sevigny has struggled with her reputation as a cult independent film star famous for her daring roles in transgressive films. Cult films can also trap directors. Leonard Kastle, who directed "The Honeymoon Killers" (1969), never directed another film again. Despite his cult following, which included François Truffaut, he was unable to find financing for any of his other screenplays. Qualities that bring cult films to prominence – such as an uncompromising, unorthodox vision – caused Alejandro Jodorowsky to languish in obscurity for years.
Transgression and censorship.
Transgressive films as a distinct artistic movement began in the 1970s. Unconcerned with genre distinctions, they drew inspiration equally from the nonconformity of European art cinema and experimental film, the gritty subject matter of Italian neorealism, and the shocking images of 1960s exploitation. Some used hardcore pornography and horror, occasionally at the same time. In the 1980s, Nick Zedd identified this movement as the Cinema of Transgression and later wrote a manifesto. Popular in midnight showings, they were mainly limited to large urban areas, which led Joan Hawkins to label them as "downtown culture". These films acquired a legendary reputation as they were discussed and debated in alternative weeklies, such as "The Village Voice". Home video would finally allow general audiences to see them, which gave many people their first taste of underground film. Ernest Mathijs says that cult films often disrupt viewer expectations, such as giving characters transgressive motivations or focusing attention on elements outside the film. Cult films can also transgress national stereotypes and genre conventions, such as "Battle Royale" (2000), which broke many rules of teenage slasher films. The reverse – when films based on cult properties lose their transgressive edge – can result in derision and rejection by fans. Audience participation itself can be transgressive, such as breaking long-standing taboos against talking during films and throwing things at the screen.
According to Mathijs, critical reception is important to a film's perception as cult, through topicality and controversy. Topicality, which can be regional (such as objection to government funding of the film) or critical (such as philosophical objections to the themes), enables attention and a contextual response. Cultural topics make the film relevant and can lead to controversy, such as a moral panic, which provides opposition. Cultural values transgressed in the film, such as sexual promiscuity, can be attacked by proxy, through attacks on the film. These concerns can vary from culture to culture, and they need not be at all similar. However, Mathijs says the film must invoke metacommentary for it to be more than simply culturally important. While referencing previous arguments, critics may attack its choice of genre or its very right to exist. Taking stances on these varied issues, critics assure their own relevance while helping to elevate the film to cult status. Perceived racist and reductive remarks by critics can rally fans and raise the profile of cult films, such as Rex Reed's comments about Korean culture. Critics can also polarize audiences and lead debates, such as Joe Bob Briggs and Roger Ebert dueling over "I Spit On Your Grave" (1978). Briggs would later contribute a commentary track to the DVD release in which he defends it as a feminist film. Films which do not attract enough controversy may be ridiculed and rejected when suggested as cult films.
Peter Hutchings, noting the many definitions of a cult film that require transgressive elements, states that cult films are known in part for their excesses. Both subject matter and its depiction are portrayed in extreme ways that break taboos of good taste and aesthetic norms. Violence, gore, sexual perversity, and even the music can be pushed to stylistic excess far beyond that allowed by mainstream cinema. Film censorship can make these films obscure and difficult to find, common criteria used to define cult films. Despite this, these films remain well-known and prized among collectors. Fans will occasionally express frustration with dismissive critics and conventional analysis, which they believe marginalizes and misinterprets paracinema. In marketing these films, young men are predominantly targeted. Horror films in particular can draw fans who seek the most extreme films. Audiences can also ironically latch on to offensive themes, such as misogyny, using these films as catharsis for the things that they hate most in life. Exploitative, transgressive elements can be pushed to excessive extremes for both humor and satire. Frank Henenlotter faced censorship and ridicule, but he found acceptance among audiences receptive to themes that Hollywood was reluctant to touch, such as violence, drug addiction, and misogyny. Lloyd Kaufman sees his films' political statements as more populist and authentic than the hypocrisy of mainstream films and celebrities. Despite being drenched in fake blood, vomit, and diarrhea, Kaufman's films have attracted positive attention from critics and academics. Excess can also exist as camp, such as films that highlight the excesses of 1980s fashion and commercialism.
Films that are influenced by unpopular styles or genres can become cult films. Jean Rollin worked within "cinéma fantastique", an unpopular genre in modern France. Influenced by American films and early French fantasists, he drifted between art, exploitation, and pornography. His films were reviled by critics, but he retained a cult following drawn by the nudity and eroticism. Similarly, Jess Franco chafed under fascist censorship in Spain but became influential in Spain's horror boom of the 1960s. These transgressive films that straddle the line between art and horror may have overlapping cult followings, each with their own interpretation and reasons for appreciating it. The films that followed Jess Franco were unique in their rejection of mainstream art. Popular among fans of European horror for their subversiveness and obscurity, these later Spanish films allowed political dissidents to criticize the fascist regime within the cloak of exploitation and horror. Unlike most exploitation directors, they were not trying to establish a reputation. They were already established in the art-house world and intentionally chose to work within paracinema as a reaction against the New Spanish Cinema, an artistic revival supported by the fascists. As late as the 1980s, critics still cited Pedro Almodóvar's anti-macho iconoclasm as a rebellion against fascist mores, as he grew from countercultural rebel to mainstream respectability. Transgressive elements that limit a director's appeal in one country can be celebrated or highlighted in another. Takashi Miike has been marketed in the West as a shocking and avant-garde filmmaker despite his many family-friendly comedies, which have not been imported.
The transgressive nature of cult films can lead to their censorship. During the 1970s and early 1980s, a wave of explicit, graphic exploitation films caused controversy. Called "video nasties" within the UK, they ignited calls for censorship and stricter laws on home video releases, which were largely unregulated. Consequently, the British Board of Film Classification banned many popular cult films due to issues of sex, violence, and incitement to crime. Released during the cannibal boom, "Cannibal Holocaust" (1980) was banned in dozens of countries and caused the director to be briefly jailed over fears that it was a real snuff film. Although opposed to censorship, director Ruggero Deodato would later agree with cuts made by the BBFC which removed unsimulated animal killings, which limited the film's distribution. Frequently banned films may introduce questions of authenticity as fans question whether they have seen a truly uncensored cut. Cult films have been falsely claimed to have been banned to increase their transgressive reputation and explain their lack of mainstream penetration. Marketing campaigns have also used such claims to raise interest among curious audiences. Home video has allowed cult film fans to import rare or banned films, finally giving them a chance to complete their collection with imports and bootlegs. Cult films previously banned are sometimes released with much fanfare and the fans assumed to be already familiar with the controversy. Personal responsibility is often highlighted, with a strong anti-censorship message. Previously lost scenes cut by studios can be re-added and restore a director's original vision, which draws similar fanfare and acclaim from fans. Imports are sometimes censored to remove elements that would be controversial, such as references to Islamic spirituality in Indonesian cult films.
Academics David Church and Chuck Kleinhans describe an uncritical celebration of transgressive themes in cult films, including misogyny and racism. David Church has also criticized gendered descriptions of transgressive content that celebrate masculinity. Joanne Hollows further identifies a gendered component to the celebration of transgressive themes in cult films, where male terms are used to describe films outside the mainstream while female terms are used to describe mainstream, conformist cinema. Jacinda Read's expansion states that cult films, despite their potential for empowerment of the marginalized, are more often used by politically incorrect males. Knowledgeable about feminism and multiculturalism, they seek a refuge from the academic acceptance of these progressive ideals. Their playful and ironic acceptance of regressive lad culture invites, and even dares, condemnation from academics and the uncool. Thus, cult films become a tool to reinforce mainstream values through transgressive content; Rebecca Feasy states that cultural hierarchies can also be reaffirmed through mockery of films perceived to be lacking masculinity. However, the sexploitation films of Doris Wishman took a feminist approach which avoids and subverts the male gaze and traditional goal-oriented methods. Wishman's subject matter, though exploitative and transgressive, was always framed in terms of female empowerment and the feminine spectator. Her use of common cult film motifs – female nudity and ambiguous gender – were repurposed to comment on feminist topics. Similarly, the films of Russ Meyer were a complicated combination of transgressive, mainstream, progressive, and regressive elements. They attracted both acclaim and denouncement from critics and progressives. Transgressive films imported from cultures that are recognizably different yet still relatable can be used to progressively examine issues in another culture.
Subcultural appeal and fandom.
Cult films can be used to help define or create groups as a form of subcultural capital; knowledge of cult films proves that one is "authentic" or "non-mainstream". They can be used to provoke an outraged response from the mainstream, which further defines the subculture, as only members could possibly tolerate such deviant entertainment. More accessible films have less subcultural capital; among extremists, banned films will have the most. By referencing cult films, media can identify desired demographics, strengthen bonds with specific subcultures, and stand out among those who understand the intertextuality. Popular films from previous eras may be reclaimed by genre fans long after they have been forgotten by the original audiences. This can be done for authenticity, such as horror fans who seek out now-obscure titles from the 1950s instead of the modern, well-known remakes. Authenticity may also drive fans to deny genre categorization to films perceived as too mainstream or accessible. Authenticity in performance and expertise can drive fan acclaim. Authenticity can also drive fans to decry the mainstream in the form of hostile critics and censors. Especially when promoted by enthusiastic and knowledgeable programmers, choice of venue can be an important part of expressing individuality. Besides creating new communities, cult films can link formerly disparate groups, such as fans and critics. As these groups intermix, they can influence each other, though this may be resisted by older fans, unfamiliar with these new references. In extreme cases, cult films can lead to the creation of religions, such as Dudeism. For their avoidance of mainstream culture and audiences, enjoyment of irony, and celebration of obscure subcultures, Martin Roberts compared cult film fans to hipsters.
A film can become the object of a cult following within a particular region or culture if it has unusual significance. For example, Norman Wisdom's films, friendly to Marxist interpretation, amassed a cult following in Albania, as they were among the few Western films allowed by the country's Communist rulers. "The Wizard of Oz" (1939) and its star, Judy Garland, hold special significance to American and British gay culture, although it is a widely viewed and historically important film in greater American culture. Similarly, James Dean and his brief film career have become icons of alienated youth. Cult films can have such niche appeal that they are only popular within certain subcultures, such as "Reefer Madness" (1936) and "Hemp for Victory" (1942) among stoner subculture. Beach party musicals, popular among American surfers, failed to find an equivalent audience when imported to the United Kingdom. When films target subcultures like this, they may seem unintelligible without the proper cultural capital. Films which appeal to teenagers may offer subcultural identities that are easily recognized and differentiate various subcultural groups. Films which appeal to stereotypical male activities, such as sports, can easily gain strong male cult followings. Sports metaphors are often used in the marketing of cult films to males, such as emphasizing the "extreme" nature of the film, which increases the appeal to youth subcultures fond of extreme sports.
Matt Hills' concept of the "cult blockbuster" involves cult followings inside larger, mainstream films. Although these are big budget, mainstream films, they still attract cult followings. The cult fans differentiate themselves from ordinary fans in several ways: longstanding devotion to the film, distinctive interpretations, and fan works. Hills identifies three different cult followings for "The Lord of the Rings", each with their own fandom separate from the mainstream. Emma Pett identifies "Back to the Future" (1985) as another example of a cult blockbuster. Although the film topped the charts when it was released, it has developed a nostalgic cult following over the years. The hammy acting by Christopher Lloyd and quotable dialogue draw a cult following, as they mimic traditional cult films. Blockbuster science fiction films that include philosophical subtexts, such as "The Matrix", allow cult film fans to enjoy them on a higher level than the mainstream. "Star Wars", with its large cult following in geek subculture, has been cited as both a cult blockbuster or a cult film. Although a mainstream epic, "Star Wars" has provided its fans with a spirituality and culture outside of the mainstream. Fans, in response to the popularity of these blockbusters, will claim elements for themselves while rejecting others. Jar Jar Binks, for example, is rejected not because of racial stereotyping but because he represents mainstream appeal and marketing. Also, instead of valuing textual rarity, fans of cult blockbusters will value repeat viewings. They may also engage in behaviors more traditional for fans of cult television and other serial media, as cult blockbusters are often franchised, preconceived as a film series, or both. To reduce mainstream accessibility, a film series can be self-reflexive and full of in-jokes that only longtime fans can understand.
Cult films can create their own subculture. "The Rocky Horror Picture Show"s cult following has established its own customs and norms of behavior, which are often at direct odds with mainstream society. These less restrictive norms encourage individuality and experimentation, a theme seen in the film. Although fans received mainstream disapproval for their activities, they stuck together and formed long-lasting bonds. Eventually, peer pressure drove fans to engage in standardized audience participation rituals. Although often described as primarily composed of obsessed fans, cult film fandom can include many newer, less experienced members. Familiar with the film's reputation and having watched clips on YouTube, these fans may take the next step and enter the film's fandom. If they are the majority, they may alter or ignore long-standing traditions, such as audience participation rituals; rituals which lack perceived authenticity may be criticized, but accepted rituals bring subcultural capital to veteran fans who introduce them to the newer members. Fans who flaunt their knowledge receive negative reactions. Newer fans may cite the film itself as their reason for attending a showing, but longtime fans often cite the community. Organized fandoms may spread and become popular as a way of introducing new people to the film, as well as theatrical screenings being privileged by the media and fandom itself. Fandom can also be used as a process of legitimation. Fans of cult films, as in media fandom, are frequently producers instead of mere consumers. Unconcerned with traditional views on intellectual property, these fan works are often unsanctioned, transformative, and ignore fictional canon.
Like cult films themselves, magazines and websites dedicated to cult films revel in their self-conscious offensiveness. They maintain a sense of exclusivity by offending mainstream audiences with misogyny, gore, and racism. Obsessive trivia can be used to bore mainstream audiences while building up subcultural capital. Specialist stores on the fringes of society (or websites which prominently partner with hardcore pornographic sites) can be used to reinforce the outsider nature of cult film fandom, especially when they use erotic or gory imagery. By assuming a preexisting knowledge of trivia, non-fans can be excluded. Previous articles and controversies can also be alluded to without explanation. Casual readers and non-fans will thus be left out of discussions and debates, as they lack enough information to meaningfully contribute. When fans like a cult film for the wrong reasons, such as casting or characters aimed at mainstream appeal, they may be ridiculed. Thus, fandom can keep the mainstream at bay while defining themselves in terms of the "the Other". Commercial aspects of fandom (such as magazines or books) can also be defined in terms of "otherness" and thus valid to consume: consumers purchasing independent or niche publications are discerning consumers, but the mainstream is denigrated. Irony or self-deprecating humor can also be used. In online communities, different subcultures attracted to transgressive films can clash over values and criteria for subcultural capital. Even within subcultures, fans who break subcultural scripts, such as denying the affectivity of a "sick film", will be ridiculed for their lack of authenticity.
Types.
"So bad it's good".
The critic Michael Medved characterized examples of the "so bad it's good" class of low-budget cult film through books such as "The Golden Turkey Awards". These films include financially fruitless and critically scorned films that have become inadvertent comedies to film buffs, such as "Plan 9 from Outer Space" (1958) and "The Room" (2003). Similarly, Paul Verhoeven's "Showgirls" (1995) bombed in theaters but developed a cult following on video. Catching on, MGM capitalized on the film's ironic appeal and marketed it as a cult film. Films which attract the derision of audiences can turn into outlets for fan creativity where fans are the final arbiters of humor rather than the film's creators. Jacob deNobel states that films can be perceived as nonsensical or inept when audiences misunderstand avant-garde filmmaking or misinterpret parody. Films such as "Rocky Horror" can be misinterpreted as "weird for weirdness sake" by people unfamiliar with the cult films that it parodies. deNoble ultimately rejects the use of the label "so bad it's good" as mean-spirited and often misapplied. Alamo Drafthouse programmer Zack Carlson has further said that any film which succeeds in entertaining an audience is good, regardless of irony. The rise of the Internet and on-demand films has led critics to question whether "so bad it's good" films have a future now that people have such diverse options in both availability and catalog, though fans eager to experience the worst films ever made can lead to lucrative showings for local theaters and merchandisers.
Camp and guilty pleasures.
Chuck Kleinhans states that the difference between a guilty pleasure and a cult film can be as simple as the number of fans; David Church raises the question of how many people it takes to form a cult following, especially now that home video makes fans difficult to count. As these cult films become more popular, they can bring varied responses from fans that depend on different interpretations, such as camp, irony, genuine affection, or combinations thereof. Earnest fans, who recognize and accept the film's faults, can make minor celebrities of the film's cast, though the benefits are not always clear. Cult film stars known for their camp can inject subtle parody or signal when films should not be taken seriously. Campy actors can also provide comic book supervillains for serious, artistic-minded films. This can draw fan acclaim and obsession more readily than subtle, method-inspired acting. Mark Chalon Smith says technical faults may be forgiven if a film makes up for them in other areas, such as camp or transgressive content. Smith states that the early films of John Waters are amateurish and less influential than claimed, but Waters' outrageous vision cements his place in cult cinema. Films like "Myra Breckinridge" (1970) and "Beyond the Valley of the Dolls" (1970) can experience critical reappraisal later, once their camp excess and avant-garde filmmaking are better accepted, and films that are initially dismissed as frivolous are often reassessed as campy. Films that intentionally try to appeal to fans of camp may end up alienating them, as the films become perceived as trying too hard or not authentic.
Nostalgia.
According to Brigid Cherry, nostalgia "is a strong element of certain kinds of cult appeal." When Veoh added many cult films to their site, they cited nostalgia as a factor for their popularity. I. Q. Hunter describes cult films as "New Hollywood "in extremis"" and a form of nostalgia for that period. Ernest Mathijs instead states that cult films use nostalgia as a form of resistance against progress and capitalistic ideas of a time-based economy. By virtue of the time travel plot, "Back to the Future" permits nostalgia for both the 1950s and 1980s. Many members of its nostalgic cult following are too young to have been alive during those periods, which Emma Pett interprets as fondness for retro aesthetics, nostalgia for when they saw the film rather than when it was released, and looking to the past to find a better time period. Similarly, John Hughes films have taken hold in midnight movie venues, trading off of nostalgia for the 1980s and an ironic appreciation for their optimism. Mathijs and Sexton describe "Grease" (1978) as a film nostalgic about an imagined past that has acquired a nostalgic cult following. Other cult films, such as "Streets of Fire" (1984), create a new fictional world based on nostalgic views of the past. Cult films may also subvert nostalgia, such as "The Big Lebowski", which introduces many nostalgic elements and then reveals them as fake and hollow. China Miéville praises the use of satire in "Donnie Darko" for its avoidance of falling into facile and comforting nostalgia, but Nathan Lee identifies the retro aesthetic and nostalgic pastiche as factors in its popularity among midnight movie crowds.
Midnight movies.
Tomas Crowder-Taraborrelli describes midnight movies as a reaction against the political and cultural conservatism in America, and Joan Hawkins identifies the movement as running the gamut from anarchist to Libertarian, united in their anti-establishment attitude and punk aesthetic. These films are resistant to simple categorization, defined by the fanaticism and ritualistic behaviors of their audiences. Midnight movies require a night life and an audience willing to invest themselves actively. Hawkins states that these films took a rather bleak point of view due to the living conditions of the artists and the economic prospects of the 1970s. Like the surrealists and dadaists, they not only satirically attacked society but also the very structure of film – a counter-cinema that deconstructs narrative and traditional processes. In the late 1980s and 1990s, midnight movies transitioned from underground showings to home video viewings; eventually, a desire for community brought a resurgence, and "The Big Lebowski" kick-started a new generation. Demographics shifted, and more hip and mainstream audiences were drawn to them. Although studios expressed skepticism, large audiences were drawn to box office flops, such as "Donnie Darko" (2001) and "Office Space" (1999). Today, midnight movies are more popular than ever and are strongly diverging from mainstream films shown at midnight. Mainstream cinemas, eager to disassociate themselves from negative associations and increase profits, have begun abandoning midnight screenings. Although classic midnight movies have dropped off in popularity, they still bring reliable crowds.
Art and exploitation.
Although seemingly at odds with each other, art and exploitation films are frequently treated as equal and interchangeable in cult fandom, listed alongside each other and described in similar terms: their ability to provoke a response. The most exploitative aspects of art films are thus played up and their academic recognition ignored. This flattening of culture follows the popularity of post-structuralism, which rejects a hierarchy of artistic merit and equates exploitation and art. Mathijs and Sexton state that although cult films are not synonymous with exploitation, as is occasionally assumed, this is a key component; they write that exploitation, which exists on the fringes of the mainstream and deals with taboo subjects, is well-suited for cult followings. David Andrews writes that cult softcore films are "the most masculinized, youth-oriented, populist, and openly pornographic softcore area." The sexploitation films of Russ Meyer were among the first to abandon all hypocritical pretenses of morality and were technically proficient enough to gain a cult following. His persistent vision saw him received as an auteur worthy of academic study; John Waters attributes this to Meyer's ability to create complicated, sexually charged films without resorting to explicit sex. Myrna Oliver described Doris Wishman's exploitation films as "crass, coarse, and camp ... perfect fodder for a cult following." The sickest and most disturbing films have their own cult following; these films transcend their roots in exploitation, horror, and art films. In 1960s and 1970s America, exploitation and art films shared audiences and marketing, especially in New York City's grindhouse cinemas.
B and genre films.
Mathijs and Sexton state that genre is an important part of cult films; cult films will often mix, mock, or exaggerate the tropes associated with traditional genres. Science fiction, fantasy, and horror are known for their large and dedicated cult followings; as science fiction films become more popular, fans emphasize non-mainstream and less commercial aspects of it. B films, which are often conflated with exploitation, are as important to cult films as exploitation. Teodor Reljic of "Malta Today" states that cult B films are a realistic goal for Malta's burgeoning film industry. Genre films, B films that strictly adhere to genre limitations, can appeal to cult film fans: given their transgressive excesses, horror films are likely to become to cult films; films like "Galaxy Quest" (1999) highlight the importance of cult followings and fandom to science fiction; and authentic martial arts skills in Hong Kong action films can drive them to become cult favorites. Cult musicals can range from the traditional, such as "Singin' in the Rain" (1952), which appeal to cult audiences through nostalgia, camp, and spectacle, to the more non-traditional, such as "Cry-Baby" (1990), which parodies musicals, and "Rocky Horror", which uses a rock soundtrack. Romantic fairy tale "The Princess Bride" (1987) failed to attract audiences in its original release, as the studio did not know how to market it. The freedom and excitement associated with cars can be an important part of drawing cult film fans to genre films, and they can signify action and danger with more ambiguity than a gun. "Ad Week" writes that cult B films, when released on home video, market themselves and need only enough advertising to raise curiosity or nostalgia.
Animation.
Animation can provide wide open vistas for stories; the French film "Fantastic Planet" (1972) explored ideas beyond the limits of traditional, live-action science fiction films. Phil Hoad identifies "Akira" (1988) as introducing violent, adult Japanese animation (known as anime) to the West and paving the way for later works. Anime, according to Brian Ruh, is not a cult genre, but the lack of individual fandoms inside anime fandom itself lends itself to a bleeding over of cult attention and can help spread works internationally. Anime, which is often highly franchised, provides its fans with alternative fictional canons and points of view that can drive fan activity. The "Ghost in the Shell" franchise, for example, provided Japanese fans with enough bonus material and spinoffs that it encouraged cult tendencies. Markets that did not support the sale of these materials saw less cult activity. Ralph Bakshi's career has been marked with controversy: "Fritz the Cat" (1972) provoked outrage as the first X-rated animated film, and "Coonskin" (1975) was decried as racist. Bakshi recalls that older animators had tired of "kid stuff" and desired edgier work, whereas younger animators hated his work for "destroying the Disney images." Eventually, his work would be reassessed and cult followings, including Quentin Tarantino and Robert Rodriguez, developed around several of his films. "Heavy Metal" (1981) faced similar denunciations from critics; Donald Liebenson cites the violence and sexual imagery as alienating critics, who didn't know what to make of the film. It would go on to become a popular midnight movie and frequently bootlegged by fans, as licensing issues kept it from being released on video for many years.
Nonfiction.
Sensationalistic documentaries called mondo films replicate the most shocking and transgressive elements of exploitation films; they are usually modeled after "sick films" and cover similar subject matter. According to Mathijs and Mendik, these documentaries often present non-Western societies as "stereotypically mysterious, seductive, immoral, deceptive, barbaric or savage". Though they can be interpreted as racist, Mathijs and Mendik state that they also "exhibit a liberal attitude towards the breaking of cultural taboos". Mondo films like "Faces of Death" mix real and fake footage freely, and they gain their cult following through the outrage and debate over authenticity that results. Like "so bad it's good" cult films, old propaganda and government hygiene films may be enjoyed ironically by more modern audiences, who enjoy the camp value of the outdated themes and outlandish claims made about perceived social threats, such as drug use. Barry K. Grant states that Frank Capra's "Why We Fight" World War II propaganda films are explicitly not cult, because they are "slickly made and have proven their ability to persuade an audience." The sponsored film "Mr. B Natural" became a cult hit when it was broadcast on "Mystery Science Theater 3000"; these educational shorts were the favorite of MST3K cast member Trace Beaulieu. Mark Jancovich states that cult audiences are drawn to these films because of their "very banality or incoherence of their political positions", unlike traditional cult films, which achieve popularity through auteurist radicalism.
Mainstream popularity.
Mark Shiel explains the rising popularity of cult films as an attempt by cinephiles and scholars to escape the oppressive conformity and mainstream appeal of even independent film, as well as a lack of condescension in both critics and the films; Donna de Ville says it is a chance to subvert the dominance of academics and cinephiles. According to Xavier Mendik, "academics have been really interested in cult movies for quite a while now." Mendik has sought to bring together academic interest and fandom through Cine-Excess, a film festival. Academic I. Q. Hunter states that "it's much easier to be a cultist now, but it is also rather more inconsequential." Citing the mainstream availability of "Cannibal Holocaust", Jeffrey Sconce rejects definitions of cult films based on controversy and excess, as they've now become meaningless. Cult films have influenced such diverse industries as cosmetics, music videos, and fashion. Cult films have shown up in less expected places; as a sign of his popularity, a bronze statue of Ed Wood has been proposed in his hometown, and "L'Osservatore Romano", the official newspaper of the Holy See, has courted controversy for its endorsement of cult films and pop culture. When cities attempt to renovate neighborhoods, fans have called attempts to demolish iconic settings from cult films "cultural vandalism". Cult films can also drive tourism, even when it is unwanted.
As far back as the 1970s, "Attack of the Killer Tomatoes" (1978) was designed specifically to be a cult film, and "The Rocky Horror Picture Show" was produced by 20th Century Fox, a major Hollywood studio. Over its decades-long release, "Rocky Horror" became the seventh highest grossing R-rated film when adjusted for inflation; Matt Singer has questioned whether "Rocky Horror"s popularity invalidates its cult status. Founded in 1974, Troma Entertainment, an independent studio, would become known for both its cult following and cult films. In the 1980s, Danny Peary's "Cult Movies" (1981) would influence director Edgar Wright and film critic Scott Tobias, of "The A.V. Club". The rise of home video would have a mainstreaming effect on cult films and cultish behavior, though some collectors would be unlikely to self-identify as cult film fans. Film critic Joe Bob Briggs began reviewing drive-in theater and cult films, though he faced much criticism as an early advocate of exploitation and cult films. Briggs highlights the mainstreaming of cult films by pointing out the respectful obituaries that cult directors have received from formerly hostile publications and acceptance of politically incorrect films at mainstream film festivals. This acceptance is not universal, though, and some critics have resisted this mainstreaming of paracinema. Beginning in the 1990s, Quentin Tarantino would have the greatest success in turning cult films mainstream. Tarantino later used his fame to champion obscure cult films that had influenced him and set up the short-lived Rolling Thunder Pictures, which distributed several of his favorite cult films. Tarantino's clout led Phil Hoad of "The Guardian" to call Tarantino the world's most influential director.
As major Hollywood studios and audiences both become savvy to cult films, productions once limited to cult appeal have instead become popular hits, and cult directors have become hot properties known for more mainstream and accessible films. Remarking on the popular trend of remaking cult films, Claude Brodesser-Akner of "New York" magazine states that Hollywood studios have been superstitiously hoping to recreate past successes rather than trading on nostalgia. Their popularity would bring some critics to proclaim the death of cult films now that they have finally become successful and mainstream, are too slick to attract a proper cult following, lack context, or are too easily found online. In response, David Church says that cult film fans have retreated to more obscure and difficult to find films, often using illegal distribution methods, which preserves the outlaw status of cult films. Virtual spaces, such as online forums and fan sites, replace the traditional fanzines and newsletters. Cult film fans consider themselves collectors, rather than consumers, as they associate consumers with mainstream, Hollywood audiences. This collecting can take the place of fetishization of a single film. Addressing concerns that DVDs have revoked the cult status of films like "Rocky Horror", Mikel J. Koven states that small scale screenings with friends and family can replace midnight showings. Koven also identifies television shows, such as "Twin Peaks", as retaining more traditional cult activities inside popular culture. David Lynch himself has not ruled out another television series, as studios have become reluctant to take chances on non-mainstream ideas. Despite this, the Alamo Drafthouse has capitalized on cult films and the surrounding culture through inspiration drawn from "Rocky Horror" and retro promotional gimmickry. They sell out their shows regularly and have acquired a cult following of their own.
Bob Batchelor states that the Internet has democratized cult culture and destroyed the line between cult and mainstream. Fans of even the most obscure films can communicate online with each other in vibrant communities. Although known for their big-budget blockbusters, Steven Spielberg and George Lucas have criticized the current Hollywood system of gambling everything on the opening weekend of these productions. Geoffrey Macnab of "The Independent" instead suggests that Hollywood look to capitalize on cult films, which have exploded in popularity on the Internet. The rise of social media has been a boon to cult films. Sites such as Twitter have displaced traditional venues for fandom and courted controversy from cultural critics who are unamused by campy cult films. After a clip from one of his films went viral, director-producer Roger Corman made a distribution deal with YouTube. Found footage which had originally been distributed as cult VHS collections eventually went viral on YouTube, which opened them to new generations of fans. Films such as "Birdemic" (2008) and "The Room" (2003) gained quick, massive popularity, as prominent members of social networking sites discussed them. Their rise as "instant cult classics" bypasses the years of obscurity that most cult films labor under. In response, critics have described the use of viral marketing as astroturfing and an attempt to manufacture cult films.
I. Q. Hunter identifies a prefabricated cult film style which includes "deliberately, insulting bad films", "slick exercises in dysfunction and alienation", and mainstream films "that sell themselves as worth obsessing over". Scott Tobias states that Don Coscarelli, whose previous films effortlessly attracted cult followings, has drifted into this realm. Tobias criticizes Coscarelli as trying too hard to appeal to cult audiences and sacrificing internal consistency for calculated quirkiness. Influenced by the successful online hype of "The Blair Witch Project" (1999), other films have attempted to draw online cult fandom with the use of prefabricated cult appeal. "Snakes on a Plane" (2006) is an example that attracted massive attention from curious fans. Uniquely, its cult following preceded the film's release and included speculative parodies of what fans imagined the film might be. This reached the point of convergence culture when fan speculation began to impact on the film's production. Although it was proclaimed a cult film and major game-changer before it was released, it failed to win either mainstream audiences or maintain its cult following. In retrospect, critic Spencer Kornhaber would call it a serendipitous novelty and a footnote to a "more naive era of the Internet". However, it became influential in both marketing and titling. This trend of "instant cult classics" which are hailed yet fail to attain a lasting following is described by Matt Singer, who states that the phrase is an oxymoron.
Cult films are often approached in terms of auteur theory, which states that the director's creative vision drives a film. This has fallen out of favor in academia, creating a disconnect between cult film fans and critics. Matt Hills states that auteur theory can help to create cult films; fans that see a film as continuing a director's creative vision are likely to accept it as cult. According to Greg Taylor, auteur theory also helped to popularize cult films when middlebrow audiences found an accessible way to approach avant-garde film criticism. Auteur theory provided an alternative culture for cult film fans while carrying the weight of scholarship. By requiring repeated viewings and extensive knowledge of details, auteur theory naturally appealed to cult film fans. Taylor further states that this was instrumental in allowing cult films to break through to the mainstream. Joe Tompkins states that this auteurism is often highlighted when mainstream success occurs. This may take the place of – and even ignore – political readings of the director. Cult films and directors may be celebrated for their transgressive content, daring, and independence, but Tompkins argues that mainstream recognition requires they be palatable to corporate interests who stand to gain much from the mainstreaming of cult film culture. While critics may champion revolutionary aspects of filmmaking and political interpretation, Hollywood studios and other corporate interests will instead highlight only the aspects that they wish to legitimize in their own films, such as sensational exploitation. Someone like George Romero, whose films are both transgressive and subversive, will have the transgressive aspects highlighted while the subversive aspects are ignored.

</doc>
<doc id="5646" url="http://en.wikipedia.org/wiki?curid=5646" title="Constantinople">
Constantinople

Constantinople (; ; ; modern ) was the capital city of the Roman, Byzantine, Latin, and Ottoman empires. It was reinaugurated in 324 AD at ancient Byzantium, as the new capital of the Roman Empire by Constantine the Great, after whom it was named, and dedicated on 11 May 330. In the 12th century, the city was the largest and wealthiest European city. Eventually, the Byzantine Empire in the east was reduced to just its capital and its environs, falling to the Ottoman Empire in 1453. Following the Muslim conquest, the city prospered as the Islamic capital of the Ottoman period. After the founding of the modern Republic of Turkey — the successor state of the Ottoman Empire — the city was renamed İstanbul in 1923. 
Constantinople was famed for its massive defenses. Although besieged on numerous occasions by various peoples, it was taken only in 1204 by the army of the Fourth Crusade, in 1261 by Michael VIII Palaiologos, and in 1453 by the Ottoman Sultan Mehmed II . A first wall was erected by Constantine I, and the city was surrounded by a double wall lying about to the west of the first wall, begun during the 5th century by Theodosius II. The city was built on seven hills as well as on the Golden Horn and the Sea of Marmara and thus presented an impregnable fortress enclosing magnificent palaces, domes, and towers.
It was also famed for architectural masterpieces such as the church of Hagia Sophia, the sacred palace of the emperors, the hippodrome, and the Golden Gate, lining the arcaded avenues and squares. Constantinople contained numerous artistic and literary treasures before it was sacked in 1204 and 1453. It was virtually depopulated when it fell to the Ottoman Turks, but the city recovered rapidly, becoming once again by the mid-1600s the world's largest city as the Ottoman capital.
Names.
The city was originally founded as a Greek colony under the name of "Byzantium" in the 7th century BC. It took on the name of "Konstantinoupolis" ("city of Constantine", "Constantinople") after its re-foundation under Roman emperor Constantine I, who transferred the imperial capital from its historic base, Rome, to Byzantium in 330 AD and designated his new capital "Nova Roma" or "New Rome." The modern Turkish name for the city, "İstanbul", derives from the Greek phrase "eis tin polin" (εις την πόλιν), meaning "into the City" or "to the City". This pattern is used for other Greek cities conquered by the Ottomans (e.g. Izmir, "eis Smyrnen"; Iznik, "eis Nikaian") This name was used in Turkish alongside "Kostantiniyye", the more formal adaptation of the original "Constantinople", during the period of Ottoman rule, while western languages mostly continued to refer to the city as Constantinople until the early 20th century. After the creation of the Republic of Turkey in 1923, the Turkish government began to formally object to the use of "Constantinople" in other languages and ask that others use the more common name for the city.
The name "Constantinople" is still used by members of the Eastern Orthodox Church in the title of one of their most important leaders, the Orthodox patriarch based in the city, referred to as "His Most Divine All-Holiness the Archbishop of Constantinople New Rome and Ecumenical Patriarch."
History.
Byzantium.
Constantinople was founded by the Roman Emperor Constantine I (272–337 AD) in 324 on the site of an already-existing city, Byzantium, which was settled in the early days of Greek colonial expansion, around 671–662 BC. The site lay astride the land route from Europe to Asia and the seaway from the Black Sea to the Mediterranean, and had in the Golden Horn an excellent and spacious harbour.
306–337.
Constantine had altogether more colourful plans. Having restored the unity of the Empire, and, being in course of major governmental reforms as well as of sponsoring the consolidation of the Christian church, he was well aware that Rome was an unsatisfactory capital. Rome was too far from the frontiers, and hence from the armies and the imperial courts, and it offered an undesirable playground for disaffected politicians. Yet it had been the capital of the state for over a thousand years, and it might have seemed unthinkable to suggest that the capital be moved to a different location. Nevertheless, Constantine identified the site of Byzantium as the right place: a place where an emperor could sit, readily defended, with easy access to the Danube or the Euphrates frontiers, his court supplied from the rich gardens and sophisticated workshops of Roman Asia, his treasuries filled by the wealthiest provinces of the Empire.
Constantinople was built over 6 years, and consecrated on 11 May 330. Constantine divided the expanded city, like Rome, into 14 regions, and ornamented it with public works worthy of an imperial metropolis. Yet, at first, Constantine's new Rome did not have all the dignities of old Rome. It possessed a proconsul, rather than an urban prefect. It had no praetors, tribunes, or quaestors. Although it did have senators, they held the title "clarus", not "clarissimus", like those of Rome. It also lacked the panoply of other administrative offices regulating the food supply, police, statues, temples, sewers, aqueducts, or other public works. The new programme of building was carried out in great haste: columns, marbles, doors, and tiles were taken wholesale from the temples of the empire and moved to the new city. In similar fashion, many of the greatest works of Greek and Roman art were soon to be seen in its squares and streets. The emperor stimulated private building by promising householders gifts of land from the imperial estates in Asiana and Pontica and on 18 May 332 he announced that, as in Rome, free distributions of food would be made to the citizens. At the time, the amount is said to have been 80,000 rations a day, doled out from 117 distribution points around the city.
Constantine laid out a new square at the centre of old Byzantium, naming it the Augustaeum. The new senate-house (or Curia) was housed in a basilica on the east side. On the south side of the great square was erected the Great Palace of the Emperor with its imposing entrance, the Chalke, and its ceremonial suite known as the Palace of Daphne. Nearby was the vast Hippodrome for chariot-races, seating over 80,000 spectators, and the famed Baths of Zeuxippus. At the western entrance to the Augustaeum was the Milion, a vaulted monument from which distances were measured across the Eastern Roman Empire.
From the Augustaeum led a great street, the Mese (Greek: Μέση [Οδός] lit. "Middle "), lined with colonnades. As it descended the First Hill of the city and climbed the Second Hill, it passed on the left the Praetorium or law-court. Then it passed through the oval Forum of Constantine where there was a second Senate-house and a high column with a statue of Constantine himself in the guise of Helios, crowned with a halo of seven rays and looking toward the rising sun. From there the Mese passed on and through the Forum Tauri and then the Forum Bovis, and finally up the Seventh Hill (or Xerolophus) and through to the Golden Gate in the Constantinian Wall. After the construction of the Theodosian Walls in the early 5th century, it was extended to the new Golden Gate, reaching a total length of seven Roman miles.
337–529.
The first known Prefect of the City of Constantinople was Honoratus, who took office on 11 December 359 and held it until 361. The emperor Valens built the Palace of Hebdomon on the shore of the Propontis near the Golden Gate, probably for use when reviewing troops. All the emperors up to Zeno and Basiliscus were crowned and acclaimed at the Hebdomon. Theodosius I founded the Church of John the Baptist to house the skull of the saint (today preserved at the Topkapı Palace in Istanbul, Turkey), put up a memorial pillar to himself in the Forum of Taurus, and turned the ruined temple of Aphrodite into a coach house for the Praetorian Prefect; Arcadius built a new forum named after himself on the Mese, near the walls of Constantine.
The importance of Constantinople gradually increased. After the shock of the Battle of Adrianople in 378, in which the emperor Valens with the flower of the Roman armies was destroyed by the Visigoths within a few days' march, the city looked to its defences, and in 413–414, Theodosius II built the 18-meter (60-foot)-tall triple-wall fortifications, which were never to be breached until the coming of gunpowder. Theodosius also founded a University near the Forum of Taurus, on 27 February 425.
Uldin, a prince of the Huns, appeared on the Danube about this time and advanced into Thrace, but he was deserted by many of his followers, who joined with the Romans in driving their king back north of the river. Subsequent to this, new walls were built to defend the city, and the fleet on the Danube improved.
In due course, the barbarians overran the Western Roman Empire: Its emperors retreated to Ravenna, and it diminished to nothing. Thereafter, Constantinople became in truth the largest city of the Roman Empire and of the world. Emperors were no longer peripatetic between various court capitals and palaces. They remained in their palace in the Great City, and sent generals to command their armies. The wealth of the eastern Mediterranean and western Asia flowed into Constantinople.
527–565.
The emperor Justinian I (527–565) was known for his successes in war, for his legal reforms and for his public works. It was from Constantinople that his expedition for the reconquest of the former Diocese of Africa set sail on or about 21 June 533. Before their departure, the ship of the commander Belisarius was anchored in front of the Imperial palace, and the Patriarch offered prayers for the success of the enterprise. After the victory, in 534, the Temple treasure of Jerusalem, looted by the Romans in 70 AD and taken to Carthage by the Vandals after their sack of Rome in 455, was brought to Constantinople and deposited for a time, perhaps in the Church of St. Polyeuctus, before being returned to Jerusalem in either the Church of the Resurrection or the New Church.
Chariot-racing had been important in Rome for centuries. In Constantinople, the hippodrome became over time increasingly a place of political significance. It was where (as a shadow of the popular elections of old Rome) the people by acclamation showed their approval of a new emperor, and also where they openly criticized the government, or clamoured for the removal of unpopular ministers. In the time of Justinian, public order in Constantinople became a critical political issue.
Throughout the late Roman and early Byzantine periods, Christianity was resolving fundamental questions of identity, and the dispute between the orthodox and the monophysites became the cause of serious disorder, expressed through allegiance to the horse-racing parties of the Blues and the Greens. The partisans of the Blues and the Greens were said to affect untrimmed facial hair, head hair shaved at the front and grown long at the back, and wide-sleeved tunics tight at the wrist; and to form gangs to engage in night-time muggings and street violence. At last these disorders took the form of a major rebellion of 532, known as the "Nika" riots (from the battle-cry of "Victory!" of those involved).
Fires started by the Nika rioters consumed Constantine's basilica of St Sophia, the city's principal church, which lay to the north of the Augustaeum. Justinian commissioned Anthemius of Tralles and Isidore of Miletus to replace it with a new and incomparable St Sophia. This was the great cathedral of the Orthodox Church, whose dome was said to be held aloft by God alone, and which was directly connected to the palace so that the imperial family could attend services without passing through the streets. The dedication took place on 26 December 537 in the presence of the emperor, who exclaimed, "O Solomon, I have outdone thee!" St Sophia was served by 600 people including 80 priests, and cost 20,000 pounds of gold to build.
Justinian also had Anthemius and Isidore demolish and replace the original Church of the Holy Apostles built by Constantine with a new church under the same dedication. This was designed in the form of an equal-armed cross with five domes, and ornamented with beautiful mosaics. This church was to remain the burial place of the Emperors from Constantine himself until the 11th century. When the city fell to the Turks in 1453, the church was demolished to make room for the tomb of Mehmet II the Conqueror. Justinian was also concerned with other aspects of the city's built environment, legislating against the abuse of laws prohibiting building within of the sea front, in order to protect the view.
During Justinian I's reign, the city's population reached about 500,000 people. However, the social fabric of Constantinople was also damaged by the onset of the Plague of Justinian between 541–542 AD. It killed perhaps 40% of the city's inhabitants.
Survival, 565–717.
In the early 7th century, the Avars and later the Bulgars overwhelmed much of the Balkans, threatening Constantinople from the west. Simultaneously, the Persian Sassanids overwhelmed the Prefecture of the East and penetrated deep into Anatolia. Heraclius, son to the exarch of Africa, set sail for the city and assumed the purple. He found the military situation so dire that he is said at first to have contemplated withdrawing the imperial capital to Carthage, but relented after the people of Constantinople begged him to stay. The citizens lost their right to free grain in 618 when Heraclius realised that the city no longer could be supplied from Egypt as a result of the Persian wars: the population dropped substantially in size as a result.
While the city withstood a siege, Heraclius campaigned deep into Persian territory and briefly restored the "status quo" in 628, when the Persians surrendered all their conquests. However, further sieges followed in the course of attacks from the Arabs, a first from 674 to 678, and a second from 717 to 718. At this time the Theodosian Walls kept the city impregnable from the land, while a newly discovered incendiary substance known as "Greek Fire" allowed the Byzantine navy to destroy the Arab fleets and keep the city supplied. In the second siege, the Second ruler of Bulgaria, Khan Tervel (also called St. Triveliy) rendered decisive help.
717–1025.
In the 730s Leo III carried out extensive repairs of the Theodosian walls, which had been damaged by frequent and violent attacks; this work was financed by a special tax on all the subjects of the Empire.
Theodora, widow of the Emperor Theophilus (died 842), acted as regent during the minority of her son Michael III, who was said to have been introduced to dissolute habits by her brother Bardas. When Michael assumed power in 856, he became known for excessive drunkenness, appeared in the hippodrome as a charioteer and burlesqued the religious processions of the clergy. He removed Theodora from the Great Palace to the Carian Palace and later to the monastery of Gastria, but, after the death of Bardas, she was released to live in the palace of St Mamas; she also had a rural residence at the Anthemian Palace, where Michael was assassinated in 867.
In 860, an attack was made on the city by a new principality set up a few years earlier at Kiev by Askold and Dir, two Varangian chiefs: Two hundred small vessels passed through the Bosporus and plundered the monasteries and other properties on the suburban Prince's Islands. Oryphas, the admiral of the Byzantine fleet, alerted the emperor Michael, who promptly put the invaders to flight; but the suddenness and savagery of the onslaught made a deep impression on the citizens.
In 980, the emperor Basil II received an unusual gift from Prince Vladimir of Kiev: 6,000 Varangian warriors, which Basil formed into a new bodyguard known as the Varangian Guard. They were known for their ferocity, honour, and loyalty. It is said that, in 1038, they were dispersed in winter quarters in the Thracesian theme when one of their number attempted to violate a countrywoman, but in the struggle she seized his sword and killed him; instead of taking revenge, however, his comrades applauded her conduct, compensated her with all his possessions, and exposed his body without burial as if he had committed suicide. However, following the death of an Emperor, they became known also for plunder in the Imperial palaces. Later in the 11th Century the Varangian Guard became dominated by Anglo-Saxons who preferred this way of life to subjugation by the new Norman kings of England.
The "Book of the Eparch", which dates to the 10th century, gives a detailed picture of the city's commercial life and its organization at that time. The corporations in which the tradesmen of Constantinople were organised were supervised by the Eparch, who regulated such matters as production, prices, import, and export. Each guild had its own monopoly, and tradesmen might not belong to more than one. It is an impressive testament to the strength of tradition how little these arrangements had changed since the office, then known by the Latin version of its title, had been set up in 330 to mirror the urban prefecture of Rome.
In the 9th and 10th centuries, Constantinople had a population of between 500,000 and 800,000.
Iconoclast controversy.
In the 8th and 9th centuries, the iconoclast movement caused serious political unrest throughout the Empire. The emperor Leo III issued a decree in 726 against images, and ordered the destruction of a statue of Christ over one of the doors of the Chalke, an act that was fiercely resisted by the citizens. Constantine V convoked a church council in 754, which condemned the worship of images, after which many treasures were broken, burned, or painted over with depictions of trees, birds or animals: One source refers to the church of the Holy Virgin at Blachernae as having been transformed into a "fruit store and aviary". Following the death of her son Leo IV in 780, the empress Irene restored the veneration of images through the agency of the Second Council of Nicaea in 787.
The iconoclast controversy returned in the early 9th century, only to be resolved once more in 843 during the regency of Empress Theodora, who restored the icons. These controversies contributed to the deterioration of relations between the Western and the Eastern Churches.
Prelude to the Comnenian period, 1025–1081.
In the late 11th century catastrophe struck with the unexpected and calamitous defeat of the imperial armies at the Battle of Manzikert in Armenia in 1071. The Emperor Romanus Diogenes was captured. The peace terms demanded by Alp Arslan, sultan of the Seljuk Turks, were not excessive, and Romanus accepted them. On his release, however, Romanus found that enemies had placed their own candidate on the throne in his absence; he surrendered to them and suffered death by torture, and the new ruler, Michael VII Ducas, refused to honour the treaty. In response, the Turks began to move into Anatolia in 1073. The collapse of the old defensive system meant that they met no opposition, and the empire's resources were distracted and squandered in a series of civil wars. Thousands of Turkoman tribesmen crossed the unguarded frontier and moved into Anatolia. By 1080, a huge area had been lost to the Empire, and the Turks were within striking distance of Constantinople.
1081–1185.
Under the Comnenian dynasty (1081–1185), Byzantium staged a remarkable recovery. In 1090–91, the nomadic Pechenegs reached the walls of Constantinople, where Emperor Alexius I with the aid of the Kipchaks annihilated their army. In response to a call for aid from Alexius, the First Crusade assembled at Constantinople in 1096, but declining to put itself under Byzantine command set out for Jerusalem on its own account. John II built the monastery of the Pantocrator (Almighty) with a hospital for the poor of 50 beds.
With the restoration of firm central government, the empire became fabulously wealthy. The population was rising (estimates for Constantinople in the 12th century vary from some 100,000 to 500,000), and towns and cities across the realm flourished. Meanwhile, the volume of money in circulation dramatically increased. This was reflected in Constantinople by the construction of the Blachernae palace, the creation of brilliant new works of art, and general prosperity at this time: an increase in trade, made possible by the growth of the Italian city-states, may have helped the growth of the economy. It is certain that the Venetians and others were active traders in Constantinople, making a living out of shipping goods between the Crusader Kingdoms of Outremer and the West, while also trading extensively with Byzantium and Egypt. The Venetians had factories on the north side of the Golden Horn, and large numbers of westerners were present in the city throughout the 12th century. Toward the end of Manuel I Komnenos's reign, the number of foreigners in the city reached about 60,000–80,000 people out of a total population of about 400,000 people. In 1171, Constantinople also contained a small community of 2,500 Jews. In 1182, all Latin (Western European) inhabitants of Constantinople were massacred.
In artistic terms, the 12th century was a very productive period. There was a revival in the mosaic art, for example: Mosaics became more realistic and vivid, with an increased emphasis on depicting three-dimensional forms. There was an increased demand for art, with more people having access to the necessary wealth to commission and pay for such work. According to N.H. Baynes ("Byzantium, An Introduction to East Roman Civilization"):
1185–1261.
On 25 July 1197, Constantinople was struck by a severe fire which burned the Latin Quarter and the area around the Gate of the Droungarios (Turkish: Odun Kapısı) on the Golden Horn. Nevertheless, the destruction wrought by the 1197 fire paled in comparison with that brought by the Crusaders. In the course of a plot between Philip of Swabia, Boniface of Montferrat and the Doge of Venice, the Fourth Crusade was, despite papal excommunication, diverted in 1203 against Constantinople, ostensibly promoting the claims of Alexius, son of the deposed emperor Isaac. The reigning emperor Alexius III had made no preparation. The Crusaders occupied Galata, broke the defensive chain protecting the Golden Horn and entered the harbour, where on 27 July they breached the sea walls: Alexius III fled. But the new Alexius IV found the Treasury inadequate, and was unable to make good the rewards he had promised to his western allies. Tension between the citizens and the Latin soldiers increased. In January 1204, the "protovestiarius" Alexius Murzuphlus provoked a riot, it is presumed, to intimidate Alexius IV, but whose only result was the destruction of the great statue of Athena, the work of Phidias, which stood in the principal forum facing west.
In February, the people rose again: Alexius IV was imprisoned and executed, and Murzuphlus took the purple as Alexius V. He made some attempt to repair the walls and organise the citizenry, but there had been no opportunity to bring in troops from the provinces and the guards were demoralised by the revolution. An attack by the Crusaders on 6 April failed, but a second from the Golden Horn on 12 April succeeded, and the invaders poured in. Alexius V fled. The Senate met in St Sophia and offered the crown to Theodore Lascaris, who had married into the Angelid family, but it was too late. He came out with the Patriarch to the Golden Milestone before the Great Palace and addressed the Varangian Guard. Then the two of them slipped away with many of the nobility and embarked for Asia. By the next day the Doge and the leading Franks were installed in the Great Palace, and the city was given over to pillage for three days.
Sir Steven Runciman, historian of the Crusades, wrote that the sack of Constantinople is “unparalleled in history”.
For the next half-century, Constantinople was the seat of the Latin Empire. Under the rulers of the Latin Empire, the city declined, both in population and the condition of its buildings. Alice-Mary Talbot cites an estimated population for Constantinople of 400,000 inhabitants; after the destruction wrought by the Crusaders on the city, about one third were homeless, and numerous courtiers, nobility, and higher clergy, followed various leading personages into exile. "As a result Constantinople became seriously depopulated," Talbot concludes.
The Latins took over at least 20 churches and 13 monasteries, most prominently the Hagia Sophia, which became the cathedral of the Latin Patriarch of Constantinople. It is to these that E.H. Swift attributed the construction of a series of flying buttresses to shore up the walls of the church, which had been weakened over the centuries by earthquake tremors. However, this act of maintenance is an exception: for the most part, the Latin occupiers were too few to maintain all of the buildings, either secular and sacred, and many became targets for vandalism or dismantling. Bronze and lead were removed from the roofs of abandoned buildings and melted down and sold to provide money to the chronically under-funded Empire for defense and to support the court; Deno John Geanokoplos writes that "it may well be that a division is suggested here: Latin laymen stripped secular buildings, ecclesiastics, the churches." Buildings were not the only targets of officials looking to raise funds for the impoverished Latin Empire: the monumental sculptures which adorned the Hippodrome and fora of the city were pulled down and melted for coinage. "Among the masterpieces destroyed, writes Talbot, "were a Herakles attributed to the fourth-century B.C. sculptor Lysippos, and monumental figures of Hera, Paris, and Helen."
The Nicaean emperor John III Vatatzes reportedly saved several churches from being dismantled for their valuable building materials; by sending money to the Latins "to buy them off" ("exonesamenos"), he prevented the destruction of several churches. According to Talbot, these included the churches of Blachernae, Rouphinianai, and St. Michael at Anaplous. He also granted funds for the restoration of the Church of the Holy Apostles, which had been seriously damaged in an earthquake.
The Byzantine nobility scattered, many going to Nicaea, where Theodore Lascaris set up an imperial court, or to Epirus, where Theodore Angelus did the same; others fled to Trebizond, where one of the Comneni had already with Georgian support established an independent seat of empire. Nicaea and Epirus both vied for the imperial title, and tried to recover Constantinople. In 1261, Constantinople was captured from its last Latin ruler, Baldwin II, by the forces of the Nicaean emperor Michael VIII Palaiologos.
1261–1453 and the Fall of Constantinople.
Although Constantinople was retaken by Michael VIII Palaiologos, the Empire had lost many of its key economic resources, and struggled to survive. The palace of Blachernae in the north-west of the city became the main Imperial residence, with the old Great Palace on the shores of the Bosporus going into decline. When Michael VIII captured the city, its population was 35,000 people, but, by the end of his reign, he had succeeded in increasing the population to about 70,000 people. The Emperor achieved this by summoning former residents having fled the city when the Crusaders captured it, and by relocating Greeks from the recently reconquered Peloponnese to the capital. In 1347, the Black Death spread to Constantinople. In 1453, when the Ottoman Turks captured the city, it contained approximately 50,000 people.
On 29 May 1453, Turkish sultan Mehmed II, "the Conqueror", entered Constantinople after a 53–day siege during which his 30 inch caliber & 27 feet long barrel cannon had torn a huge hole in the Walls of Theodosius II. Constantinople (Ottoman Turkish: "Kostantiniyye") became the third capital of the Ottoman state.
Mehmed had begun the siege on 6 April 1453. He had hired engineers to build cannons and bombs for the occasion. He also acquired scholars and imams to encourage the soldiers. He gave the Byzantine emperor Constantine Palaeologus (1449–1453) three chances to surrender the city, a duty enjoined by the Shariah (Muslim Holy Law). Mehmed guaranteed that the city's residents, including their riches, beliefs and honor, would be safe. However, Constantine did not accept these terms of surrender.
After more than a month of fighting, Mehmed’s advisors were beginning to lose hope. Against their counsel, Mehmed continued to fight. City was surrounded by all sides,Turkish ships were stopped by long chains near golden horn. Here Mehmed practised the decisive move in the history of naval warfare. He ordered to move the ships by land on slippery trunks of trees by bypassing the long chains in strait of Bosphorus. By travelling few kilometers on land ships were finally shifted into sea near the fort. Turk cannons ultimately started breaking the fort walls. The night before the final assault, he studied the previous attempts to take the city. He was comparing ways that would work and where they would not. On the morning of 29 May 1453, the sultan ordered the call of Azan (call to prayer). This was not a regular prayer session for religious reasons but rather a scare tactic. When the Byzantine forces saw the entire Ottoman army get on their knees to pray, the Byzantine army was witnessing how united the Ottoman Turks were, and this worried them. As Eversley put it, "Their minds were defeated before their bodies."
The city was plundered for three days. In the end, the population which had not been able to escape was deported to Edirne, Bursa and other Ottoman cities, leaving the city deserted except for the Jews of Balat and the Genoese of Pera.
Other than the variations in their stories, the battle occurs in the same manner and neither the Byzantine nor Muslim narratives portrays the opposition in a brutal, negative light. As expected, however, each narrative portrays its side in a heroic light. The historical narrative from the Ottoman Turkish point of view depicts Mehmed as a clever, strong conqueror: "The sultan ordered the setting up of his secret weapon which he had invented himself." By contrast, the narrative from the Byzantine side portrays final emperor Constantine Palaeologus as a valiant leader who gave his life for the cause of Orthodoxy. As quoted from a Byzantine essay: " charged into the sea of the enemy soldiers, hitting left and right in a final act of defiance."
Ottoman era 1453–1922.
Finally, the Christian Orthodox city of Constantinople was under Ottoman control. When Mehmed II finally entered Constantinople through what is now known as the Topkapi Gate, he immediately rode his horse to the Hagia Sophia, which he ordered to be sacked. He ordered that an imam meet him there in order to chant the Shahada, the Islamic creed which declares belief in the oneness of God and acceptance of Muhammad as God's prophet” This act transformed the Orthodox cathedral into a Muslim mosque, solidifying Islamic rule in Constantinople.
Mehmed’s main concern with Constantinople had to do with rebuilding the city’s defenses and repopulation. Building projects were commenced immediately after the conquest, which included the repair of the walls, construction of the citadel, and building a new palace. Mehmed issued orders across his empire that Muslims, Christians, and Jews should resettle the city; he demanded that five thousand households needed to be transferred to Constantinople by September. From all over the Islamic empire, prisoners of war and deported people were sent to the city: these people were called "Sürgün" in Turkish (). Two centuries later, Ottoman traveler Evliya Çelebi gave a list of groups introduced into the city with their respective origins. Even today, many quarters of Istanbul, such as Aksaray, Çarşamba, bear the names of the places of origin of their inhabitants. However, many people escaped again from the city, and there were several outbreaks of plague, so that in 1459 Mehmet allowed the deported Greeks to come back to the city.
Importance.
Culture.
Constantinople was the largest and richest urban center in the Eastern Mediterranean Sea during the late Eastern Roman Empire, mostly as a result of its strategic position commanding the trade routes between the Aegean Sea and the Black Sea. It would remain the capital of the eastern, Greek-speaking empire for over a thousand years. At its peak, roughly corresponding to the Middle Ages, it was the richest and largest European city, exerting a powerful cultural pull and dominating economic life in the Mediterranean. Visitors and merchants were especially struck by the beautiful monasteries and churches of the city, in particular, Hagia Sophia, or the Church of Holy Wisdom: A Russian 14th-century traveler, Stephen of Novgorod, wrote, "As for St Sophia, the human mind can neither tell it nor make description of it."
It was especially important for preserving in its libraries manuscripts of Greek and Latin authors throughout a period when instability and disorder caused their mass-destruction in western Europe and north Africa: On the city's fall, thousands of these were brought by refugees to Italy, and played a key part in stimulating the Renaissance, and the transition to the modern world. The cumulative influence of the city on the west, over the many centuries of its existence, is incalculable. In terms of technology, art and culture, as well as sheer size, Constantinople was without parallel anywhere in Europe for a thousand years.
Armenians, Syrians, Slavs and Georgians were part of the Byzantine social hierarchy.
International status.
The city provided a defence for the eastern provinces of the old Roman Empire against the barbarian invasions of the 5th century. The 18-meter-tall walls built by Theodosius II were, in essence, impregnable to the barbarians coming from south of the Danube river, who found easier targets to the west rather than the richer provinces to the east in Asia. From the 5th century, the city was also protected by the Anastasian Wall, a 60-kilometer chain of walls across the Thracian peninsula. Many scholars argue that these sophisticated fortifications allowed the east to develop relatively unmolested while Ancient Rome and the west collapsed. With the emergence of Christianity and the rise of Islam, Constantinople became the last bastion of Christian Europe, standing at the fore of Islamic expansion, and repelling its influence. As the Byzantine Empire was situated in-between the Islamic world and the Christian west, so did Constantinople act as Europe’s first line-of-defence against Arab advances in the 7th and 8th centuries. The city, and the Empire, would ultimately fall to the Ottomans by 1453, but its enduring legacy had provided Europe centuries of resurgence following the collapse of Rome.
Architecture.
The Byzantine Empire used Roman and Greek architectural models and styles to create its own unique type of architecture. The influence of Byzantine architecture and art can be seen in the copies taken from it throughout Europe. Particular examples include St Mark's Basilica in Venice, the basilicas of Ravenna, and many churches throughout the Slavic East. Also, alone in Europe until the 13th-century Italian florin, the Empire continued to produce sound gold coinage, the solidus of Diocletian becoming the bezant prized throughout the Middle Ages. Its city walls were much imitated (for example, see Caernarfon Castle) and its urban infrastructure was moreover a marvel throughout the Middle Ages, keeping alive the art, skill and technical expertise of the Roman Empire. In the Ottoman period Islamic architecture and symbolism were used.
Religion.
Constantine's foundation gave prestige to the Bishop of Constantinople, who eventually came to be known as the Ecumenical Patriarch, and made it a prime center of Christianity alongside Rome. This contributed to cultural and theological differences between Eastern and Western Christianity eventually leading to the Great Schism that divided Western Catholicism from Eastern Orthodoxy from 1054 onwards. Constantinople is also of great religious importance to Islam, as the conquest of Constantinople is one of the signs of the End time in Islam.

</doc>
<doc id="5647" url="http://en.wikipedia.org/wiki?curid=5647" title="Columbus">
Columbus

Columbus is a Latinized version of the Italian surname ""Colombo"". It most commonly refers to:
Columbus may also refer to:
Places named Columbus.
In the United States, Columbus may refer to:

</doc>
<doc id="5648" url="http://en.wikipedia.org/wiki?curid=5648" title="Cornwall">
Cornwall

Cornwall (British English pronunciation: or ) is a ceremonial county and unitary authority area of England, within the United Kingdom. Cornwall is a peninsula bordered to the north and west by the Celtic Sea, to the south by the English Channel, and to the east by the county of Devon, over the River Tamar. Cornwall has a population of and covers an area of . The administrative centre, and only city in Cornwall, is Truro, although the town of St Austell has the largest population.
Cornwall forms the westernmost part of the south-west peninsula of the island of Great Britain, and a large part of the Cornubian batholith is within Cornwall. This area was first inhabited in the Palaeolithic and Mesolithic periods. It continued to be occupied by Neolithic and then Bronze Age peoples, and later (in the Iron Age) by Brythons with distinctive cultural relations to neighbouring Wales and Brittany. There is little evidence that Roman rule was effective west of Exeter and few Roman remains have been found. Cornwall was the home of a division of the Dumnonii tribe – whose tribal centre was in the modern county of Devon – known as the Cornovii, separated from the Brythons of Wales after the Battle of Deorham, often coming into conflict with the expanding English kingdom of Wessex before King Athelstan in AD 936 set the boundary between English and Cornish at the Tamar. From the early Middle Ages, British language and culture was apparently shared by Brythons trading across both sides of the Channel, evidenced by the corresponding high medieval Breton kingdoms of Domnonee and Cornouaille and the Celtic Christianity common to both territories.
Historically tin mining was important in the Cornish economy, becoming increasingly significant during the High Middle Ages and expanding greatly during the 19th century when rich copper mines were also in production. In the mid-19th century, however, the tin and copper mines entered a period of decline. Subsequently china clay extraction became more important and metal mining had virtually ended by the 1990s. Traditionally fishing (particularly of pilchards), and agriculture (particularly of dairy products and vegetables), were the other important sectors of the economy. The railways led to the growth of tourism during the 20th century, however, Cornwall's economy struggled after the decline of the mining and fishing industries. The area is noted for its wild moorland landscapes, its long and varied coastline, its many place-names derived from the Cornish language, and its very mild climate. Extensive stretches of Cornwall's coastline, and Bodmin Moor, are protected as an Area of Outstanding Natural Beauty.
Cornwall is the traditional homeland of the Cornish people and is recognised as one of the Celtic nations, retaining a distinct cultural identity that reflects its history. Some people question the present constitutional status of Cornwall, and a nationalist movement seeks greater autonomy within the United Kingdom in the form of a devolved legislative assembly. On 24 April 2014 it was announced that Cornish people will be granted minority status under the European Framework Convention for the Protection of National Minorities.
Toponymy.
The name "Cornwall" derives from the combination of two separate terms from different languages. The "Corn-" part comes from the hypothesised original tribal name of the Celtic people who had lived here since the Iron Age, the "Cornovii". The second element "-wall" derives from the Old English "w(e)alh", meaning a "foreigner" or "Welshman". The name first appears in the "Anglo-Saxon Chronicle" in 891 as "On Corn walum". In the Domesday Book it was referred to as "Cornualia" and in c. 1198 as "Cornwal".
A latinisation of the name as "Cornubia" first appears in a mid-9th-century deed purporting to be a copy of one dating from c. 705. Another variation, with "Wales" reinterpreted as "Gallia", thus: "Cornugallia", is first attested in 1086. Finally, the Cornish language form of the name, "Kernow", which first appears around 1400, derives directly from the original "Cornowii". which is postulated from a single mention in the Ravenna Cosmography of around 700 (but based on earlier sources) of "Purocoronavis". This is considered to be a corruption of "Durocornovium", 'a fort or walled settlement of the Cornovii'. Its location is unidentified, but Tintagel or Carn Brea have been suggested.
In pre-Roman times, Cornwall was part of the kingdom of Dumnonia, and was later known to the Anglo-Saxons as ""West" Wales", to distinguish it from "North Wales" (modern-day Wales).
History.
Prehistory, Roman and post-Roman periods.
The present human history of Cornwall begins with the reoccupation of Britain after the last Ice Age. The area now known as Cornwall was first inhabited in the Palaeolithic and Mesolithic periods. It continued to be occupied by Neolithic and then Bronze Age peoples. According to John T. Koch and others, Cornwall in the Late Bronze Age was part of a maritime trading-networked culture called the Atlantic Bronze Age, in modern-day Ireland, England, France, Spain and Portugal. During the British Iron Age Cornwall, like all of Britain south of the Firth of Forth, was inhabited by a Celtic people known as the Britons with distinctive cultural relations to neighbouring Wales and Brittany. The Common Brittonic spoken at the time eventually developed into several distinct tongues, including Cornish.
The first account of Cornwall comes from the Sicilian Greek historian Diodorus Siculus (c. 90 BCE – c. 30 BCE), supposedly quoting or paraphrasing the 4th-century BCE geographer Pytheas, who had sailed to Britain:
The identity of these merchants is unknown. It has been theorised that they were Phoenicians, but there is no evidence for this. (For further discussion of tin mining see the section on the economy below.)
There is little evidence that Roman rule was effective west of Exeter in Devon and few Roman remains have been found. However after 410, Cornwall appears to have reverted to rule by Romano-Celtic chieftains of the Cornovii tribe as part of Dumnonia including one Marcus Cunomorus with at least one significant power base at Tintagel. 'King' Mark of Cornwall is a semi-historical figure known from Welsh literature, the Matter of Britain, and in particular, the later Norman-Breton medieval romance of Tristan and Yseult where he is regarded as a close kinsman of King Arthur; himself usually considered to be born of the Cornish people in folklore traditions derived from Geoffrey of Monmouth's Historia Regum Britanniae.
Archaeology supports ecclesiatical, literary and legendary evidence for some relative economic stability and close cultural ties between the sub-Roman Westcountry, South Wales, Brittany and Ireland through the fifth and sixth centuries.
Conflict with Wessex.
The Battle of Deorham in 577 saw the separation of Dumnonia (and therefore Cornwall) from Wales, following which the Dumnonii often came into conflict with the expanding English kingdom of Wessex. The "Annales Cambriae" report that in 722 AD the Britons of Cornwall won a battle at "Hehil". It seems likely that the enemy the Cornish fought was a West Saxon force, as evidenced by the naming of King Ine of Wessex and his kinsman Nonna in reference to an earlier Battle of Lining in 710.
The "Anglo-Saxon Chronicle" stated in 815 (adjusted date) "and in this year king Ecgbryht raided in Cornwall from east to west." and thenceforth apparently held it as a ducatus or dukedom annexed to his regnum or kingdom of Wessex, but not wholly incorporated with it. The "Anglo-Saxon Chronicle" states that in 825 (adjusted date) a battle took place between the Wealas (Cornish) and the Defnas (men of Devon) at Gafulforda. In the same year Ecgbert, as a later document expresses it, "disposed of their territory as it seemed fit to him, giving a tenth part of it to God." In other words he incorporated Cornwall ecclesiastically with the West Saxon diocese of Sherborne, and endowed Ealhstan, his fighting bishop, who took part in the campaign, with an extensive Cornish estate consisting of Callington and Lawhitton, both in the Tamar valley, and Pawton near Padstow.
In 838, the Cornish and their Danish allies were defeated by Egbert at Hengestesdune (probably Hingston Down in Cornwall). In 875, the last recorded king of Cornwall, Dumgarth, is said to have drowned. Around the 880s, Anglo-Saxons from Wessex had established modest land holdings in the eastern part of Cornwall; notably Alfred the Great who had acquired a few estates. William of Malmesbury, writing around 1120, says that King Athelstan of England (924–939) fixed the boundary between English and Cornish people at the east bank of the River Tamar.
Norman-Breton period.
One interpretation of the Domesday Book is that by this time the native Cornish landowning class had been almost completely dispossessed and replaced by English landowners, particularly Harold Godwinson himself. However, the Bodmin manumissions show that two leading Cornish figures nominally had Saxon names, but these were both glossed with native Cornish names. Naming evidence cited by medievalist Edith Ditmas suggests that many post-Conquest landowners in Cornwall were Breton allies of the Normans and further proposed this period for the early composition of the Tristan and Iseult cycle by poets such as Beroul from a pre-existing shared Brittonic oral tradition.
Soon after the Norman conquest most of the land was transferred to the new Breton-Norman aristocracy, with the lion's share going to Robert, Count of Mortain, half-brother of King William and the largest landholder in England after the king with his stronghold at Trematon Castle near the mouth of the Tamar. Cornwall and Devon west of Dartmoor showed a very different type of settlement pattern from that of Saxon Wessex and places continued, even after 1066, to be named in the Celtic Cornish tradition with Saxon architecture being uncommon.
Later medieval administration and society.
Subsequently, however, Norman absentee landlords became replaced by a new Cornu-Norman elite including scholars such as Richard Rufus of Cornwall. These families eventually became the new ruling class of Cornwall (typically speaking Norman French, Cornish, Latin and eventually English), many becoming involved in the operation of the Stannary Parliament system, Earldom and eventually the Duchy. The Cornish language continued to be spoken and it acquired a number of characteristics establishing its identity as a separate language from Breton.
Christianity in Cornwall.
Many place names in Cornwall are associated with Christian missionaries described as coming from Ireland and Wales in the 5th century AD and usually called saints ("See" List of Cornish saints). The historicity of some of these missionaries is problematic. The patron saint of Wendron Parish Church, "Saint Wendrona" is another example. and it has been pointed out by Canon Doble that it was customary in the Middle Ages to ascribe such geographical origins to saints. Some of these saints are not included in the early lists of saints.
Saint Piran, after whom Perranporth is named, is generally regarded as the patron saint of Cornwall. However in early Norman times it is likely that Saint Michael the Archangel was recognised as the patron saint and is still recognised by the Anglican Church as the "Protector of Cornwall". The title has also been claimed for Saint Petroc who was patron of the Cornish diocese prior to the Normans.
Celtic and Anglo-Saxon times.
The church in Cornwall until the time of Athelstan of Wessex observed more or less orthodox practices, being completely separate from the Anglo-Saxon church until then (and perhaps later). The See of Cornwall continued until much later: Bishop Conan apparently in place previously, but (re-?) consecrated in 931 AD by Athelstan. However, it is unclear whether he was the sole Bishop for Cornwall or the leading Bishop in the area. The situation in Cornwall may have been somewhat similar to Wales where each major religious house corresponded to a cantref (this has the same meaning as Cornish keverang) both being under the supervision of a Bishop. However if this was so the status of keverangow before the time of King Athelstan is not recorded. However it can be inferred from the districts included at this period that the minimum number would be three: Triggshire; Wivelshire; and the remaining area. Penwith, Kerrier, Pydar and Powder meet at a central point (Scorrier) which some have believed indicates a fourfold division imposed by Athelstan on a sub-kingdom.
Middle Ages.
The whole of Cornwall was in this period in the Archdeaconry of Cornwall within the Diocese of Exeter. From 1267 the archdeacons had a house at Glasney near Penryn. Their duties were to visit and inspect each parish annually and to execute the bishop's orders. Archdeacon Roland is recorded in the Domesday Book of 1086 as having land holdings in Cornwall but he was not Archdeacon of Cornwall, just an archdeacon in the Diocese of Exeter. In the episcopate of William Warelwast (1107–37) the first Archdeacon of Cornwall was appointed )possibly Hugo de Auco). Most of the parish churches in Cornwall in Norman times were not in the larger settlements, and the medieval towns which developed thereafter usually had only a chapel of ease with the right of burial remaining at the ancient parish church. Over a hundred holy wells exist in Cornwall, each associated with a particular saint, though not always the same one as the dedication of the church.
Various kinds of religious houses existed in mediaeval Cornwall though none of them were nunneries; the benefices of the parishes were in many cases appropriated to religious houses within Cornwall or elsewhere in England or France.
From the Reformation to the Victorian period.
In the 16th century there was some violent resistance to the replacement of Catholicism with Protestantism in the Prayer Book Rebellion. In 1548 the college at Glasney, a centre of learning and study established by the Bishop of Exeter, had been closed and looted (many manuscripts and documents were destroyed) which aroused resentment among the Cornish. They, among other things, objected to the English language Book of Common Prayer, protesting that the English language was still unknown to many at the time. The Prayer Book Rebellion was a cultural and social disaster for Cornwall; the reprisals taken by the forces of the Crown have been estimated to account for 10–11% of the civilian population of Cornwall. Culturally speaking, it saw the beginning of the slow decline of the Cornish language.
From that time Christianity in Cornwall was in the main within the Church of England and subject to the national events which affected it in the next century and a half. Roman Catholicism never became extinct, though openly practised by very few; there were some converts to Puritanism, Anabaptism and Quakerism in certain areas though they suffered intermittent persecution which more or less came to an end in the reign of William and Mary. During the 18th century Cornish Anglicanism was very much in the same state as Anglicanism in most of England. Wesleyan Methodist missions began during John Wesley's lifetime and had great success over a long period during which Methodism itself divided into a number of sects and established a definite separation from the Church of England.
From the early 19th to the mid-20th century Methodism was the leading form of Christianity in Cornwall but it is now in decline. The Church of England was in the majority from the reign of Queen Elizabeth until the Methodist revival of the 19th century: before the Wesleyan missions dissenters were very few in Cornwall. The county remained within the Diocese of Exeter until 1876 when the Anglican Diocese of Truro was created (the first Bishop was appointed in 1877). Roman Catholicism was virtually extinct in Cornwall after the 17th century except for a few families such as the Arundells of Lanherne. From the mid-19th century the church reestablished episcopal sees in England, one of these being at Plymouth. Since then immigration to Cornwall has brought more Roman Catholics into the population.
Physical geography.
Cornwall forms the tip of the south-west peninsula of the island of Great Britain, and is therefore exposed to the full force of the prevailing winds that blow in from the Atlantic Ocean. The coastline is composed mainly of resistant rocks that give rise in many places to impressive cliffs. Cornwall has a border with only one other county, Devon.
Coastal areas.
The north and south coasts have different characteristics. The north coast on the Celtic Sea, part of the Atlantic Ocean, is more exposed and therefore has a wilder nature. The prosaically named "High Cliff", between Boscastle and St Gennys, is the highest sheer-drop cliff in Cornwall at . However, there are also many extensive stretches of fine golden sand which form the beaches that are so important to the tourist industry, such as those at Bude, Polzeath, Watergate Bay, Perranporth, Porthtowan, Fistral Beach, Newquay, St Agnes, St Ives, and on the south coast Gyllyngvase beach in Falmouth. There are two river estuaries on the north coast: Hayle Estuary and the estuary of the River Camel, which provides Padstow and Rock with a safe harbour.
The south coast, dubbed the "Cornish Riviera", is more sheltered and there are several broad estuaries offering safe anchorages, such as at Falmouth and Fowey. Beaches on the south coast usually consist of coarser sand and shingle, interspersed with rocky sections of wave-cut platform. Also on the south coast, the picturesque fishing village of Polperro, at the mouth of the Pol River, and the fishing port of Looe on the River Looe are both popular with tourists.
Inland areas.
The interior of the county consists of a roughly east-west spine of infertile and exposed upland, with a series of granite intrusions, such as Bodmin Moor, which contains the highest land within Cornwall. From east to west, and with approximately descending altitude, these are Bodmin Moor, the area north of St Austell, the area south of Camborne, and the Penwith or Land's End peninsula. These intrusions are the central part of the granite outcrops that form the exposed parts of the Cornubian batholith of south-west Britain, which also includes Dartmoor to the east in Devon and the Isles of Scilly to the west, the latter now being partially submerged.
The intrusion of the granite into the surrounding sedimentary rocks gave rise to extensive metamorphism and mineralisation, and this led to Cornwall being one of the most important mining areas in Europe until the early 20th century. It is thought tin was mined here as early as the Bronze Age, and copper, lead, zinc and silver have all been mined in Cornwall. Alteration of the granite also gave rise to extensive deposits of China Clay, especially in the area to the north of St Austell, and the extraction of this remains an important industry.
The uplands are surrounded by more fertile, mainly pastoral farmland. Near the south coast, deep wooded valleys provide sheltered conditions for flora that like shade and a moist, mild climate. These areas lie mainly on Devonian sandstone and slate. The north east of Cornwall lies on Carboniferous rocks known as the Culm Measures. In places these have been subjected to severe folding, as can be seen on the north coast near Crackington Haven and in several other locations.
The Lizard Peninsula.
The geology of the Lizard peninsula is unusual, in that it is mainland Britain's only example of an ophiolite, a section of oceanic crust now found on land. Much of the peninsula consists of the dark green and red Precambrian serpentinite, which forms spectacular cliffs, notably at Kynance Cove, and carved and polished serpentine ornaments are sold in local gift shops. This ultramafic rock also forms a very infertile soil which covers the flat and marshy heaths of the interior of the peninsula. This is home to rare plants, such as the Cornish Heath, which has been adopted as the county flower.
Ecology.
Cornwall has varied habitats including terrestrial and marine ecosystems. One noted species in decline locally is the Reindeer lichen, which species has been made a priority for protection under the national UK Biodiversity Action Plan.
Botanists divide Cornwall and Scilly into two vice-counties: West (1) and East (2). The standard flora is by F. H. Davey "Flora of Cornwall" (1909). Davey was assisted by A. O. Hume and he thanks Hume, his companion on excursions in Cornwall and Devon, and for help in the compilation of that Flora, publication of which was financed by him.
Climate.
Cornwall has a temperate Oceanic climate (Köppen climate classification: Cfb) and has the mildest and sunniest climate in the United Kingdom, as a result of its southerly latitude and the influence of the Gulf Stream. The average annual temperature in Cornwall ranges from 11.6 °C (53 °F) on the Isles of Scilly to 9.8 °C (50 °F) in the central uplands. Winters are amongst the warmest in the country due to the southerly latitude and moderating effects of the warm ocean currents, and frost and snow are very rare at the coast and are also rare in the central upland areas. The surrounding sea and its southwesterly position mean that Cornwall's weather can be relatively changeable.
Cornwall is one of the sunniest areas in the UK, with over 1541 hours of sunshine per year, with the highest average of 7.6 hours of sunshine per day in July. The moist, mild air coming from the south west brings higher amounts of rainfall than in eastern Great Britain, at 1051 to 1290 mm (41.4 to 50.8 in) per year, however not as much as in more northern areas of the west coast. The Isles of Scilly, for example, where there are on average less than 2 days of air frost per year, is the only area in the UK to be in the USDA Hardiness zone 10. In Scilly there is on average less than 1 day of air temperature exceeding 30 °C per year and it is in the AHS Heat Zone 1. Extreme temperatures in Cornwall are particularly rare; however, extreme weather in the form of storms and floods is common.
Politics and administration.
Local politics.
With the exception of the Isles of Scilly, Cornwall is now governed by a unitary authority known as Cornwall Council which is based in Truro. The only Crown Court is based at the Courts of Justice in Truro. Magistrates' Courts are to be found in Truro (but at a different location to the Crown Court); Bodmin; Penzance and Liskeard.
The Isles of Scilly form part of the ceremonial county of Cornwall and have, at times, been served by the same county administration. However, since 1890 they have been administered by their own unitary authority, now known as the Council of the Isles of Scilly. They are still grouped with Cornwall for other administrative purposes, such as the National Health Service and Devon and Cornwall Police.
Before reorganisation on 1 April 2009, council functions throughout the rest of Cornwall were organised on a two-tier basis, with a county council and district councils for the six districts of Caradon, Carrick, Kerrier, North Cornwall, Penwith, and Restormel. While projected to streamline services, cut red tape and save around £17 million a year, the reorganisation was met with wide opposition, with a poll in 2008 giving a result of 89% disapproval from Cornish residents.
The first elections for the new unitary authority were held on 4 June 2009. The new council has 123 seats; the largest party (in 2012) is the Conservative Party with 46, followed by the Liberal Democrats with 37, Independents with 23, Mebyon Kernow with 6 seats, 1 standalone independent and Labour with 1 seat. There are currently 2 vacant seats.
Before the creation of the new unitary council, the former county council had 82 seats, the majority of which were held by the Liberal Democrats, elected at the 2005 county council elections. The six former districts in Cornwall had a total of 249 council seats, and the groups with greatest numbers of councillors were Liberal Democrats, Conservatives, and Independents.
Parliament and national politics.
Following a review by the Boundary Commission for England taking effect at the 2010 general election, Cornwall is divided into six county constituencies to elect MPs to the House of Commons of the United Kingdom.
Before the 2010 boundary changes there were five constituencies in Cornwall, all of which were won by Liberal Democrats in the 2005 general election. However, at the 2010 general election Liberal Democrat candidates won three constituencies and Conservative candidates won three constituencies ("see also 2010 United Kingdom general election result in Cornwall").
Until 1832, Cornwall had 44 MPs – more than any other county – reflecting the importance of tin to the Crown. Most of the increase came between 1529 and 1584 after which there was no change until 1832.
The chief registered parties contesting elections in Cornwall are Conservatives, Greens, Labour, Liberal Democrats, Mebyon Kernow, Liberal Party and the United Kingdom Independence Party (UKIP).
In 2007 David Cameron, leader of the Conservative Party, in a departure from the Conservative Party's traditionally unionist stance, appointed Cornishman Mark Prisk, MP for Hertford, Hertfordshire, England, as "Shadow Minister for Cornwall". This appointment was called "the fictional minister for Cornwall", by a Liberal Democrat MP, as there was no government minister to shadow. The post was not continued following the 2010 election, and no longer exists.
Self-rule movement.
Cornish nationalists have organised into two political parties: Mebyon Kernow, formed in 1951, and the Cornish Nationalist Party. In addition to the political parties, there are various interest groups such as the Revived Cornish Stannary Parliament and the Celtic League. The Cornish Constitutional Convention was formed in 2000 as a cross-party organisation including representatives from the private, public and voluntary sectors to campaign for the creation of a Cornish Assembly, along the lines of the National Assembly for Wales, Northern Ireland Assembly and the Scottish Parliament. Between 5 March 2000 and December 2001, the campaign collected the signatures of 41,650 Cornish residents endorsing the call for a devolved assembly, along with 8,896 signatories from outside Cornwall. The resulting petition was presented to the then Prime Minister, Tony Blair. The Liberal Democrats recognise Cornwall's claims for greater autonomy, as do the Liberal Party.
An additional political issue is the recognition of the Cornish people as a minority.
Cornish national identity.
Cornwall is recognised by several organisations, including the Cornish nationalist party Mebyon Kernow, the Celtic League and the International Celtic Congress, as one of the six Celtic nations, alongside Brittany, Ireland, the Isle of Man, Scotland and Wales. Alongside Asturias and Galicia, Cornwall is also recognised as one of the eight Celtic nations by the Isle of Man Government and the Welsh Government. Cornwall is represented, as one of the Celtic nations, at the "Festival Interceltique de Lorient", an annual celebration of Celtic culture held in Brittany.
Cornwall Council consider Cornwall's unique cultural heritage and distinctiveness to be one of the area's major assets. They see Cornwall's language, landscape, Celtic identity, political history, patterns of settlement, maritime tradition, industrial heritage, and non-conformist tradition, to be among the features comprising its "distinctive" culture. However, it is uncertain how many of the people living in Cornwall consider themselves to be Cornish; results from different surveys (including the national census) have varied. In the 2001 census, 7 percent of people in Cornwall identified themselves as Cornish, rather than British or English. However, activists have argued that this underestimated the true number as there was no explicit "Cornish" option included in the official census form. Subsequent surveys have suggested that as many as 44 percent identify as Cornish. Many people in Cornwall say that this issue would be resolved if a Cornish option became available on the census. The question and content recommendations for the 2011 Census provided an explanation of the process of selecting an ethnic identity which is relevant to the understanding of the often quoted figure of 37,000 who claim Cornish identity.
On 24 April 2014 it was announced that Cornish people would be granted minority status under the European Framework Convention for the Protection of National Minorities.
Settlements and communication.
Cornwall's only city, and the home of the council headquarters, is Truro. Nearby Falmouth is notable as a port, while other ports such as Penzance, the most westerly coastal town in England, St Ives and Padstow have declined. Newquay on the north coast is famous for its beaches and is a popular surfing destination, as is Bude further north. St Austell is Cornwall's largest town and more populous than the capital Truro; it was the centre of the china clay industry in Cornwall. Redruth and Camborne together form the largest urban area in Cornwall, and both towns were significant as centres of the global tin mining industry in the 19th century (nearby copper mines were also very productive during that period).
Cornwall borders the county of Devon at the River Tamar. Major road links between Cornwall and the rest of Great Britain are the A38 which crosses the Tamar at Plymouth via the Tamar Bridge and the town of Saltash, the A39 road (Atlantic Highway) from Barnstaple, passing through North Cornwall to end eventually in Falmouth, and the A30 which crosses the border south of Launceston. Torpoint Ferry also links Plymouth with the town of Torpoint on the opposite side of the Hamoaze. A rail bridge, the Royal Albert Bridge, built by Isambard Kingdom Brunel (1859) provides the only other major transport link. The major city of Plymouth being the large urban centre closest to east Cornwall has made it an important location for such services as hospitals, department stores, road and rail transport, and cultural venues.
Newquay Cornwall International Airport provides an airlink to the rest of the UK, Ireland and Europe.
Cardiff and Swansea, across the Bristol Channel, are connected to Cornwall by ferry, usually to Padstow. Swansea in particular has several boat companies who can arrange boat trips to north Cornwall, which allow the traveller to pass by the north Cornish coastline, including Tintagel Castle and Padstow harbour. Very occasionally, the "Waverley" and "Balmoral" paddle steamers cruise from Swansea or Bristol to Padstow.
The Isles of Scilly are served by ferry (from Penzance) and by aeroplane (Land's End Airport, near St Just) and from Newquay Airport. Further flights to St. Mary's Airport, Isles of Scilly, are available from Exeter International Airport in Devon.
Flag.
Saint Piran's Flag is regarded by many as the national flag of Cornwall, and an emblem of the Cornish people; and by others as the county flag. The banner of Saint Piran is a white cross on a black background (in terms of heraldry 'sable, a cross argent'). Saint Piran is supposed to have adopted these two colours from seeing the white tin in the black coals and ashes during his supposed discovery of tin. Davies Gilbert in 1826 described it as anciently the flag of St Piran and the banner of Cornwall, and another history of 1880 said that: "The white cross of St. Piran was the ancient banner of the Cornish people." The Cornish flag is an exact reverse of the former Breton national flag (black cross) and is known by the same name "Kroaz Du".
There are also claims that the patron saint of Cornwall is Saint Michael or Saint Petroc, but Saint Piran is by far the most popular of the three and his emblem is internationally recognised as the flag of Cornwall. St Piran's Day (5 March) is celebrated by the Cornish diaspora around the world.
Heraldry.
For the heraldry of Cornwall see:
Economy.
Cornwall is one of the poorest parts of the United Kingdom in terms of per capita GDP and average household incomes. At the same time, parts of the county, especially on the coast, have high house prices, driven up by demand from relatively wealthy retired people and second-home owners. The GVA per head was 65% of the UK average for 2004. The GDP per head for Cornwall and the Isles of Scilly was 79.2% of the EU-27 average for 2004, the UK per head average was 123.0%.
Historically mining of tin (and later also of copper) was important in the Cornish economy. The first reference to this appears to be by Pytheas: "see above". Julius Caesar was the last classical writer to mention the tin trade, which appears to have declined during the Roman occupation. The tin trade revived in the Middle Ages and its importance to the Kings of England resulted in certain privileges being granted to the tinners; the Cornish Rebellion of 1497 is attributed to grievances of the tin miners. In the mid-19th century, however, the tin trade again fell into decline. Other primary industries that have declined since the 1960s include china clay production, fishing and farming.
Today, the Cornish economy depends heavily on its tourist industry, which makes up around a quarter of the economy. The official measures of deprivation and poverty at district and 'sub-ward' level show that there is great variation in poverty and prosperity in Cornwall with some areas among the poorest in England and others among the top half in prosperity. For example, the ranking of 32,482 sub-wards in England in the index of multiple deprivation (2006) ranged from 819th (part of Penzance East) to 30,899th (part of Saltash Burraton in Caradon), where the lower number represents the greater deprivation.
Cornwall is one of four UK areas that qualify for poverty-related grants from the EU: it was granted Objective 1 status by the European Commission, followed by a further round of funding known as 'Convergence Funding'.
Tourism.
Tourism is estimated to contribute up to 24% of Cornwall's gross domestic product. Cornwall's unique culture, spectacular landscape and mild climate make it a popular tourist destination, despite being somewhat distant from the United Kingdom's main centres of population. Surrounded on three sides by the English Channel and Celtic Sea, Cornwall has many miles of beaches and cliffs; the South West Coast Path follows a complete circuit of both coasts. Other tourist attractions include moorland, country gardens, museums, historic and prehistoric sites, and wooded valleys. Five million tourists visit Cornwall each year, mostly drawn from within the UK. Visitors to Cornwall are served by airports at Newquay and Exeter, whilst private jets, charters and helicopters are also served by Perranporth airfield; nightsleeper and daily rail services run between Cornwall, London and other regions of the UK. Cornwall has a tourism-based seasonal economy
Newquay and Porthtowan are popular destinations for surfers. In recent years, the Eden Project near St Austell has been a major financial success, drawing one in eight of Cornwall's visitors.
Internet.
Although Cornwall is remote and residential broadband is less common than in other parts of the UK it houses one of the world's fastest high-speed transatlantic fibre optic cables, making Cornwall an important hub within Europe's Internet infrastructure.
Other industries.
Other industries are fishing, although this has been significantly re-structured by EU fishing policies (the Southwest Handline Fishermen's Association has started to revive the fishing industry), and agriculture, which has also declined significantly. Mining of tin and copper was also an industry, but today the derelict mine workings survive only as a World Heritage Site. However, the Camborne School of Mines, which was relocated to Penryn in 2004, is still a world centre of excellence in the field of mining and applied geology and the grant of World Heritage status has attracted funding for conservation and heritage tourism. China clay extraction has also been an important industry in the St Austell area, but this sector has been in decline, and this, coupled with increased mechanisation, has led to a decrease in employment in this sector, although the industry still employs around 2,133 people in Cornwall, and generates over £80 Million to the local economy
Demographics.
Cornwall's population was 537,400 at the last count (2012), and population density 144 people per square kilometre, ranking it 40th and 41st respectively compared with the other 47 counties of England. Cornwall is 95.7% White British and has a relatively high level of population growth. At 11.2% in the 1980s and 5.3% in the 1990s, it has the fifth highest population growth of the English counties. The natural change has been a small population decline, and the population increase is due to inward migration into Cornwall. According to the 1991 census, the population was 469,800.
Cornwall has a relatively high retired population, with 22.9% of pensionable age, compared with 20.3% for the United Kingdom. This may be due to a combination of Cornwall's rural and coastal geography increasing its popularity as a retirement location, and outward migration of younger residents to more economically diverse areas.
Education system.
Cornwall has a comprehensive education system, with 31 state and 8 independent secondary schools. There are three further education colleges: Penwith College (a former sixth form college), Cornwall College (occupying the former home of the Camborne School of Mines) and Truro College. The Isles of Scilly only has one school while the former Restormel district has the highest school population, and school year sizes are around 200, with none above 270.
Higher education is provided by Falmouth University, the University of Exeter (including Camborne School of Mines), the Combined Universities in Cornwall, and by Truro College, Penwith College (which combined in 2008 to make Truro and Penwith College) and Cornwall College.
Languages and dialects.
English is the main language used in Cornwall, although the revived Cornish language may be seen on road signs and is spoken fluently by a small minority of people.
Cornish language.
The Cornish language is closely related to the other Brythonic languages of Welsh and Breton, and less so to the Goidelic languages of Irish, Scots Gaelic and Manx. The language continued to function visibly as a community language in parts of Cornwall until the late 18th century, and it was claimed in 2011 that the last native speaker did not die until 1914.
There has been a revival of the language since Henry Jenner's "Handbook of the Cornish Language" was published in 1904. A study in 2000 suggested that there were around 300 people who spoke Cornish fluently. Cornish, however, had no legal status in the UK until 2002. Nevertheless, the language is taught in about twelve primary schools, and occasionally used in religious and civic ceremonies. In 2002 Cornish was officially recognised as a UK minority language and in 2005 it received limited Government funding. A Standard Written Form was agreed in 2008.
Several Cornish mining words are still in use in English language mining terminology, such as costean, gunnies, vug, kibbal, gossan and kieve.
Four of the current members in the Parliament of the United Kingdom, Andrew George, MP for St Ives, Dan Rogerson, MP for North Cornwall, Stephen Gilbert, MP for St Austell and Newquay, and Sarah Newton, MP for Truro and Falmouth repeated their Parliamentary oaths in Cornish.
Culture.
Visual arts.
Since the 19th century, Cornwall, with its unspoilt maritime scenery and strong light, has sustained a vibrant visual art scene of international renown. Artistic activity within Cornwall was initially centred on the art-colony of Newlyn, most active at the turn of the 20th century. This Newlyn School is associated with the names of Stanhope Forbes, Elizabeth Forbes, Norman Garstin and Lamorna Birch. Modernist writers such as D. H. Lawrence and Virginia Woolf lived in Cornwall between the wars, and Ben Nicholson, the painter, having visited in the 1920s came to live in St Ives with his then wife, the sculptor Barbara Hepworth, at the outbreak of the second world war. They were later joined by the Russian emigrant Naum Gabo, and other artists. These included Peter Lanyon, Terry Frost, Patrick Heron, Bryan Wynter and Roger Hilton. St Ives also houses the Leach Pottery, where Bernard Leach, and his followers championed Japanese inspired studio pottery. Much of this modernist work can be seen in Tate St Ives. The Newlyn Society and Penwith Society of Arts continue to be active, and contemporary visual art is documented in a dedicated online journal.
Music and festivals.
Cornwall has a full and vibrant folk music tradition which has survived into the present and is well known for its unusual folk survivals such as Mummers Plays, the Furry Dance in Helston played by the famous Helston Town Band, and Obby Oss in Padstow.
As in other former mining districts of Britain, male voice choirs and Brass Bands, e.g. "Brass on the Grass" concerts during the summer at Constantine, are still very popular in Cornwall: Cornwall also has around 40 brass bands, including the six-times National Champions of Great Britain, Camborne Youth Band, and the bands of Lanner and St Dennis.
Cornish players are regular participants in inter-Celtic festivals, and Cornwall itself has several lively inter-Celtic festivals such as Perranporth's Lowender Peran folk festival.
On a more modern note, contemporary musician Richard D. James (also known as Aphex Twin) grew up in Cornwall, as did Luke Vibert and Alex Parks winner of Fame Academy 2003. Roger Taylor, the drummer from the band Queen was also raised in the county, and currently lives not far from Falmouth. The American singer/songwriter Tori Amos now resides predominantly in North Cornwall not far from Bude with her family. The lutenist, lutarist, composer and festival director Ben Salfield currently lives near Redruth.
Literature.
Cornwall's rich heritage and dramatic landscape have inspired writers since the 19th century.
Fiction.
Sir Arthur Quiller-Couch, author of many novels and works of literary criticism, lived in Fowey: his novels are mainly set in Cornwall. Daphne du Maurier lived at Menabilly near Fowey and many of her novels had Cornish settings, including "Rebecca", "Jamaica Inn", "Frenchman's Creek", "My Cousin Rachel", and "The House on the Strand". She is also noted for writing "Vanishing Cornwall". Cornwall provided the inspiration for "The Birds", one of her terrifying series of short stories, made famous as a film by Alfred Hitchcock. 
Medieval Cornwall is the setting of the trilogy by Monica Furlong, "Wise Child", "Juniper", and "Colman", as well as part of Charles Kingsley's "Hereward the Wake".
Conan Doyle's "The Adventure of the Devil's Foot" featuring Sherlock Holmes is set in Cornwall. Winston Graham's series "Poldark", Kate Tremayne's Adam Loveday series, Susan Cooper's novels "Over Sea, Under Stone" and "Greenwitch", and Mary Wesley's "The Camomile Lawn" are all set in Cornwall. Writing under the pseudonym of Alexander Kent, Douglas Reeman sets parts of his Richard Bolitho and Adam Bolitho series in the Cornwall of the late 18th and the early 19th centuries, particularly in Falmouth.
Hammond Innes's novel, "The Killer Mine"; Charles de Lint's novel "The Little Country"; and Chapters 24 and 25 of J. K. Rowling's "Harry Potter and the Deathly Hallows" take place in Cornwall (the Harry Potter story at Shell Cottage, which is on the beach outside the fictional village of Tinworth in Cornwall).
David Cornwell, who writes espionage novels under the name John le Carré, lives and writes in Cornwall. Nobel Prize-winning novelist William Golding was born in St Columb Minor in 1911, and returned to live near Truro from 1985 until his death in 1993. D. H. Lawrence spent a short time living in Cornwall. Rosamunde Pilcher grew up in Cornwall, and several of her books take place there.
Poetry.
The late Poet Laureate Sir John Betjeman was famously fond of Cornwall and it featured prominently in his poetry. He is buried in the churchyard at St Enodoc's Church, Trebetherick.
Charles Causley, the poet, was born in Launceston and is perhaps the best known of Cornish poets. Jack Clemo and the scholar A. L. Rowse were also notable Cornishmen known for their poetry; The Rev. R. S. Hawker of Morwenstow wrote some poetry which was very popular in the Victorian period. The Scottish poet W. S. Graham lived in West Cornwall from 1944 until his death in 1986.
The poet Laurence Binyon wrote "For the Fallen" (first published in 1914) while sitting on the cliffs between Pentire Point and The Rumps and a stone plaque was erected in 2001 to commemorate the fact. The plaque bears the inscription "FOR THE FALLEN / Composed on these cliffs, 1914". The plaque also bears below this the fourth stanza (sometimes referred to as "The Ode") of the poem:
Other literary works.
Cornwall produced a substantial number of passion plays such as the Ordinalia during the Middle Ages. Many are still extant, and provide valuable information about the Cornish language. See also Cornish literature
Prolific writer Colin Wilson, best known for his debut work "The Outsider" (1956) and for "The Mind Parasites" (1967), lives in Gorran Haven, a small village on the southern Cornish coast. The writer D. M. Thomas was born in Redruth but lived and worked in Australia and the United States before returning to his native Cornwall. He has written novels, poetry, and other works, including translations from Russian.
Thomas Hardy's drama "The Queen of Cornwall" (1923) is a version of the Tristan story; the second act of Richard Wagner's opera "Tristan und Isolde" takes place in Cornwall, as do Gilbert and Sullivan's operettas "The Pirates of Penzance" and "Ruddigore". A level of "", a game dealing with Arthurian Legend, takes place in Cornwall at a tacky museum above King Arthur's tomb.
The fairy tale Jack the Giant Killer takes place in Cornwall.
Sports and games.
With its comparatively small, and largely rural population, major contribution by the Cornish to national sport in the United Kingdom has been limited. There are no teams affiliated to the Cornwall County Football Association that play in the Football League of England and Wales, and the Cornwall County Cricket Club plays as one of the minor counties of English cricket. Viewed as an "important identifier of ethnic affiliation", rugby union has become a sport strongly tied to notions of Cornishness. and since the 20th century, rugby union in Cornwall has emerged as one of the most popular spectator and team sports in Cornwall (perhaps the most popular), with professional Cornish rugby footballers being described as a "formidable force", "naturally independent, both in thought and deed, yet paradoxically staunch English patriots whose top players have represented England with pride and passion". In 1985, sports journalist Alan Gibson made a direct connection between love of rugby in Cornwall and the ancient parish games of hurling and wrestling that existed for centuries before rugby officially began. Among Cornwall's native sports are a distinctive form of Celtic wrestling related to Breton wrestling, and Cornish hurling, a kind of mediaeval football played with a silver ball (distinct from Irish Hurling). Cornish Wrestling is Cornwall's oldest sport and as Cornwall's native tradition it has travelled the world to places like Victoria, Australia and Grass Valley, California following the miners and gold rushes. Cornish hurling now takes place at St. Columb Major, St Ives, and less frequently at Bodmin.
Surfing and other water sports.
Due to its long coastline, various maritime sports are popular in Cornwall, notably sailing and surfing. International events in both are held in Cornwall. Cornwall hosted the Inter-Celtic Watersports Festival in 2006. Surfing in particular is very popular, as locations such as Bude and Newquay offer some of the best surf in the UK. Pilot gig rowing has been popular for many years and the World championships takes place annually on the Isles of Scilly. On 2 September 2007, 300 surfers at Polzeath beach set a new world record for the highest number of surfers riding the same wave as part of the Global Surf Challenge and part of a project called Earthwave to raise awareness about global warming.
Cuisine.
Cornwall has a strong culinary heritage. Surrounded on three sides by the sea amid fertile fishing grounds, Cornwall naturally has fresh seafood readily available; Newlyn is the largest fishing port in the UK by value of fish landed. Television chef Rick Stein has long operated a fish restaurant in Padstow for this reason, and Jamie Oliver recently chose to open his second restaurant, Fifteen, in Watergate Bay near Newquay. MasterChef host and founder of Smiths of Smithfield, John Torode, in 2007 purchased Seiners in Perranporth. One famous local fish dish is Stargazy pie, a fish-based pie in which the heads of the fish stick through the piecrust, as though "star-gazing". The pie is cooked as part of traditional celebrations for Tom Bawcock's Eve, but is not generally eaten at any other time.
Cornwall is perhaps best known though for its pasties, a savoury dish made with pastry. Today's pasties usually contain a filling of beef steak, onion, potato and swede with salt and white pepper, but historically pasties had a variety of different fillings. "Turmut, 'tates and mate" (i.e. "Turnip, potatoes and meat", turnip being the Cornish and Scottish term for swede, itself an abbreviation of 'Swedish Turnip') describes a filling once very common. For instance, the licky pasty contained mostly leeks, and the herb pasty contained watercress, parsley, and shallots. Pasties are often locally referred to as "oggies". Historically, pasties were also often made with sweet fillings such as jam, apple and blackberry, plums or cherries.
The wet climate and relatively poor soil of Cornwall make it unsuitable for growing many arable crops. However, it is ideal for growing the rich grass required for dairying, leading to the production of Cornwall's other famous export, clotted cream. This forms the basis for many local specialities including Cornish fudge and Cornish ice cream. Cornish clotted cream has Protected Geographical Status under EU law, and cannot be made anywhere else. Its principal manufacturer is Rodda's, based at Scorrier.
Local cakes and desserts include Saffron cake, Cornish heavy ("hevva") cake, Cornish fairings biscuits, figgy 'obbin, Cream tea and whortleberry pie.
There are also many types of beers brewed in Cornwall – those produced by Sharp's Brewery, Skinner's Brewery and St Austell Brewery are the best-known – including stouts, ales and other beer types. There is some small scale production of wine, mead and cider.

</doc>
<doc id="5649" url="http://en.wikipedia.org/wiki?curid=5649" title="Constitutional monarchy">
Constitutional monarchy

Constitutional monarchy is a form of democratic government in which a monarch acts as a nonpolitical head of state within the boundaries of a constitution, whether written or unwritten. While the monarch may hold formal reserve powers and government may officially take place in the monarch's name, they do not set public policy or choose political leaders. Political scientist Vernon Bogdanor, paraphrasing Thomas Macaulay, has defined a constitutional monarch as "a sovereign who reigns but does not rule." This form of government differs from absolute monarchy, in which the monarch controls political decision-making and is not effectively restricted by constitutional constraints.
Constitutional monarchies are sometimes referred to as limited monarchies, crowned republics or parliamentary monarchies.
In addition to acting as a visible symbol of national unity, a constitutional monarch may hold formal powers such as dissolving parliament or giving Royal Assent to legislation. However, the exercise of such powers is generally a formality rather than an opportunity for the sovereign to enact personal political preference. In "The English Constitution", British political theorist Walter Bagehot identified three main political rights which a constitutional monarch could freely exercise: the right to be consulted, the right to advise, and the right to warn.
The United Kingdom and fifteen of its former colonies are constitutional monarchies with a Westminster system of government. Three states—Malaysia, Cambodia, and the Holy See—employ true elective monarchies, where the ruler is periodically selected by a small, aristocratic electoral college.
The country to transition from an absolute to a constitutional monarchy was Bhutan, in 2007-8.
Constitutional and absolute monarchy.
In England, the Glorious Revolution of 1688 led to a Constitutional English Monarchy restricted by laws such as the Bill of Rights 1689 and the Act of Settlement 1701, although limits on the power of the monarch ('a limited monarchy') are much older than that (see Magna Carta). At the same time, in Scotland, the Convention of Estates enacted the Claim of Right Act 1689 which placed similar limits on the Scottish monarchy. With the Hanoverian accession in Britain onwards, monarchs saw their powers pass further to their ministers, and Royal neutrality in politics became cemented from around the start of the reign of Queen Victoria (though she had her personal favorites) and enlargements to the franchise. Today, the role is by convention effectively ceremonial. Instead, the British Parliament and the Government - chiefly in the office of Prime Minister - exercise their powers under 'Royal (or Crown) Prerogative': on behalf of the Monarch and through powers still formally possessed by the Monarch. No person may accept significant public office without swearing an oath of allegiance to the Queen. 
Constitutional monarchy occurred first in continental Europe, briefly in the early years of the French revolution, but much more widely afterwards. Napoleon Bonaparte is considered the first monarch proclaiming "himself" as an embodiment of the nation, rather than as a divinely-appointed ruler; this interpretation of monarchy is germane to continental constitutional monarchies. G.W.F. Hegel, in his "Elements of the Philosophy of Right" (1820), gave it a philosophical justification that concurred with evolving contemporary political theory and the Protestant Christian view of natural law. Hegel's forecast of a constitutional monarch with very limited powers whose function is to embody the national character and provide constitutional continuity in times of emergency was reflected in the development of constitutional monarchies in Europe and Japan. His forecast of the form of government suitable to the modern world may be seen as prophetic: the largely ceremonial offices of president in some modern parliamentary republics in Europe and e.g. Israel can be perceived as elected or appointed versions of Hegel's constitutional monarch; the Russian and French presidents, with their stronger powers, may also be regarded in Hegelian terms as wielding powers suitable to the embodiment of the national will.
Executive monarchy versus Ceremonial monarchy.
There exist at least two different types of constitutional monarchies in the modern world - executive and ceremonial. In executive monarchies, the monarch wields significant (though not absolute) power. The monarchy under this system of government is a powerful political (and social) institution. By contrast, in ceremonial monarchies, the monarch holds little actual power or direct political influence.
Executive monarchies: Bhutan, Bahrain, Jordan, Kuwait, Liechtenstein, Monaco, Morocco, Tonga, Swaziland and the United Arab Emirates.
Ceremonial monarchies: Andorra, Antigua and Barbuda, Australia, The Bahamas, Barbados, Belgium, Belize, Cambodia, Canada, Denmark, Grenada, Jamaica, Japan, Lesotho, Luxembourg, Malaysia, the Netherlands, New Zealand, Norway, Papua New Guinea, Saint Kitts and Nevis, Saint Lucia, Saint Vincent and the Grenadines, Solomon Islands, Spain, Sweden, Thailand, Tuvalu and the United Kingdom.
Ceremonial and executive monarchy, should not be confused with democratic and non-democratic monarchical systems. For example, Liechtenstein and Monaco are considered democratic states, yet the ruling monarchs in these countries wield significant executive power yet they are NOT Absolute Monarchs.
Modern constitutional monarchy.
As originally conceived, a constitutional monarch was head of the executive branch and quite a powerful figure, even though his or her power was limited by the constitution and the elected parliament. Some of the framers of the US Constitution may have envisaged the president as an elected constitutional monarch, as the term was then understood, following Montesquieu's account of the separation of powers.
The present-day concept of a constitutional monarchy developed in the United Kingdom, where the democratically elected parliaments, and their leader, the prime minister, exercise power, with the monarchs having ceded power and remaining as a titular position. In many cases the monarchs, while still at the very top of the political and social hierarchy, were given the status of "servants of the people" to reflect the new, egalitarian position. In the course of France's July Monarchy, Louis-Philippe I was styled "King of the French" rather than "King of France".
Following the Unification of Germany, Otto von Bismarck rejected the British model. In the constitutional monarchy established under the Constitution of the German Empire which Bismarck inspired, the Kaiser retained considerable actual executive power, while the Imperial Chancellor needed no parliamentary vote of confidence and ruled solely by the imperial mandate. However this model of constitutional monarchy was discredited and abolished following Germany's defeat in the First World War. Later, Fascist Italy could also be considered as a "constitutional monarchy" of a kind, in that there was a king as the titular head of state while actual power was held by Benito Mussolini under a constitution. This eventually discredited the Italian monarchy and led to its abolition in 1946. After the Second World War, surviving European monarchies almost invariably adopted some variant of the constitutional monarchy model originally developed in Britain.
Nowadays a parliamentary democracy that is a constitutional monarchy is considered to differ from one that is a republic only in detail rather than in substance. In both cases, the titular head of state - monarch or president - serves the traditional role of embodying and representing the nation, while the government is carried on by a cabinet composed predominantly of elected Members of Parliament.
However, three important factors distinguish monarchies such as the United Kingdom from systems where greater power might otherwise rest with Parliament. These are: the Royal Prerogative under which the monarch may exercise power under certain very limited circumstances; Sovereign Immunity under which they are considered to have "done no wrong" under the law, and may avoid both taxation and planning permission, for example; and considerable ceremonial power where the executive, judiciary, police and armed forces owe allegiance to the Crown.
Today slightly more than a quarter of constitutional monarchies are Western European countries, including the United Kingdom, the Netherlands, Belgium, Norway, Denmark, Spain, Luxembourg, Monaco, Liechtenstein, and Sweden. However, the two most populous constitutional monarchies in the world are in Asia: Japan and Thailand. In these countries the prime minister holds the day-to-day powers of governance, while the King or Queen (or other monarch, such as a Grand Duke, in the case of Luxembourg, or Prince in the case of Monaco and Liechtenstein) retains residual (but not always insignificant) powers. The powers of the monarch differ between countries. In Denmark and in Belgium, for example, the Monarch formally appoints a representative to preside over the creation of a coalition government following a parliamentary election, while in Norway the King chairs special meetings of the cabinet.
In nearly all cases, the monarch is still the nominal chief executive, but is bound by constitutional convention to act on the advice of the Cabinet. Only a few monarchies (most notably Japan and Sweden) have amended their constitutions so that the monarch is no longer even the nominal chief executive.
There are sixteen constitutional monarchies under Queen Elizabeth II, which are known as Commonwealth realms. Unlike some of their continental European counterparts, the Monarch and her Governors-General in the Commonwealth realms hold significant "reserve" or "prerogative" powers, to be wielded in times of extreme emergency or constitutional crises, usually to uphold parliamentary government. An instance of a Governor-General exercising such power occurred during the 1975 Australian constitutional crisis, when the Australian Prime Minister, Gough Whitlam, was dismissed by the Governor-General. The Australian senate had threatened to block the Government's budget by refusing to pass the necessary appropriation bills. On 11 November 1975, Whitlam intended to call a half-Senate election in an attempt to break the deadlock. When he sought the Governor-General's approval of the election, the Governor-General instead dismissed him as Prime Minister, and shortly thereafter installed leader of the opposition Malcolm Fraser in his place. Acting quickly before all parliamentarians became aware of the change of government, Fraser and his allies secured passage of the appropriation bills, and the Governor-General dissolved Parliament for a double dissolution election. Fraser and his government were returned with a massive majority. This led to much speculation among Whitlam's supporters as to whether this use of the Governor-General's reserve powers was appropriate, and whether Australia should become a republic. Among supporters of constitutional monarchy, however, the experience confirmed the value of the monarchy as a source of checks and balances against elected politicians who might seek powers in excess of those conferred by the constitution, and ultimately as a safeguard against dictatorship.
In Thailand's constitutional monarchy, the monarch is recognized as the Head of State, Head of the Armed Forces, Upholder of the Buddhist Religion, and Defender of the Faith. The current King, Bhumibol Adulyadej, is the longest reigning current monarch in the world and in all of Thailand's history. Bhumibol has reigned through several political changes in the Thai government. He has played an influential role in each incident, often acting as mediator between disputing political opponents. (See Bhumibol's role in Thai Politics.) Among the powers retained by the monarch under the constitution, lèse majesté protects the image of the monarch and enables him to play a role in politics. It carries strict criminal penalties for violators. Generally, the Thai people are reverent of Bhumibol. Much of his social influence arises from this reverence and from the socio-economic improvement efforts undertaken by the royal family.
In both the United Kingdom and elsewhere, a frequent debate centers on when it is appropriate for a monarch to use his or her political powers. When a monarch does act, political controversy can often ensue, partially because the neutrality of the crown is seen to be compromised in favour of a partisan goal, while some political scientists champion the idea of an "interventionist monarch" as a check against possible illegal action by politicians. There are currently 44 monarchies, and most of them are constitutional monarchies.

</doc>
<doc id="5653" url="http://en.wikipedia.org/wiki?curid=5653" title="Clarke's three laws">
Clarke's three laws

Clarke's Three Laws are three "laws" of prediction formulated by the British writer Arthur C. Clarke. They are:
Origins.
The first Clarke's Law was proposed by Arthur C. Clarke in the essay "Hazards of Prophecy: The Failure of Imagination", in "Profiles of the Future" (1962).
The second law is offered as a simple observation in the same essay. Status as Clarke's Second Law was conferred by others. In a 1973 revision of his compendium of essays, "Profiles of the Future", Clarke acknowledged the Second Law and proposed the Third. "As three laws were good enough for Newton, I have modestly decided to stop there".
The Third Law is the best known and most widely cited. Also appearing in Clarke's Essay "Hazards of Prophecy: The Failure of Imagination", it may be an echo of a statement in a 1942 story by Leigh Brackett: "Witchcraft to the ignorant, ... Simple science to the learned". Even earlier examples of this sentiment may be found in Wild Talents by author Charles Fort where he makes the statement: "...a performance that may some day be considered understandable, but that, in these primitive times, so transcends what is said to be the known; that it is what I mean by magic."
One of the characters in Ben Bova's novel "Orion and King Arthur" credits the saying to "a very wise man". On page 687 of Angie Sage's last novel of the Septimus Heap series, Fyre, this law is stated as "Arthur C. Clarke's Third Law: Any sufficiently advanced technology is indistinguishable from Magyk."
In novels ("The City and the Stars") and short stories ("The Sentinel" upon which "" was based), Clarke presents ultra-advanced technologies. In "Against the Fall of Night", the human race regresses after a full billion years of civilization, and faces remnants of past glories such as roadways. Physical possibilities are inexplicable from their perspective.
A fourth law has been added to the canon, despite Sir Arthur Clarke's declared intention of not going one better than Sir Isaac Newton. Geoff Holder quotes: "For every expert, there is an equal and opposite expert" in his book "101 Things to Do with a Stone Circle" (The History Press, 2009), and offers as his source, Arthur C. Clarke's "Profiles of the Future" (new edition, 1999).
Similarly structured 'laws'.
These are offered here to demonstrate, by analogy, Clarke's meaning of the phrase "sufficiently advanced".

</doc>
<doc id="5654" url="http://en.wikipedia.org/wiki?curid=5654" title="Caspar David Friedrich">
Caspar David Friedrich

[[File:Caspar David Friedrich - Wanderer above the sea of fog.jpg|thumb|"Wanderer above the Sea of Fog" (1818). 94.8 × 74.8 cm, Kunsthalle Hamburg. This well-known and especially Romantic masterpiece was described by the historian John Lewis Gaddis as leaving a contradictory impression, "suggesting at once mastery over a landscape and the insignificance of the individual within it. We see no face, so it's impossible to know whether the prospect facing the young man is exhilarating, or terrifying, or both."]]
Caspar David Friedrich (September 5, 1774 – May 7, 1840) was a 19th-century German Romantic landscape painter, generally considered the most important German artist of his generation. He is best known for his mid-period allegorical landscapes which typically feature contemplative figures silhouetted against night skies, morning mists, barren trees or Gothic ruins. His primary interest as an artist was the contemplation of nature, and his often symbolic and anti-classical work seeks to convey a subjective, emotional response to the natural world. Friedrich's paintings characteristically set a human presence in diminished perspective amid expansive landscapes, reducing the figures to a scale that, according to the art historian Christopher John Murray, directs "the viewer's gaze towards their metaphysical dimension".
Friedrich was born in the Pomeranian town of Greifswald at the Baltic Sea, where he began his studies in art as a young man. He studied in Copenhagen until 1798, before settling in Dresden. He came of age during a period when, across Europe, a growing disillusionment with materialistic society was giving rise to a new appreciation of spirituality. This shift in ideals was often expressed through a reevaluation of the natural world, as artists such as Friedrich, J.M.W. Turner (1775–1851) and John Constable (1776–1837) sought to depict nature as a "divine creation, to be set against the artifice of human civilization".
Friedrich's work brought him renown early in his career, and contemporaries such as the French sculptor David d'Angers (1788–1856) spoke of him as a man who had discovered "the tragedy of landscape". Nevertheless, his work fell from favour during his later years, and he died in obscurity, and in the words of the art historian Philip Miller, "half mad". As Germany moved towards modernisation in the late 19th century, a new sense of urgency characterised its art, and Friedrich's contemplative depictions of stillness came to be seen as the products of a bygone age. The early 20th century brought a renewed appreciation of his work, beginning in 1906 with an exhibition of thirty-two of his paintings and sculptures in Berlin. By the 1920s his paintings had been discovered by the Expressionists, and in the 1930s and early 1940s Surrealists and Existentialists frequently drew ideas from his work. The rise of Nazism in the early 1930s again saw a resurgence in Friedrich's popularity, but this was followed by a sharp decline as his paintings were, by association with the Nazi movement, misinterpreted as having a nationalistic aspect. It was not until the late 1970s that Friedrich regained his reputation as an icon of the German Romantic movement and a painter of international importance.
Life.
Early years and family.
Caspar David Friedrich was born on September 5, 1774, in Greifswald, Swedish Pomerania, on the Baltic coast of Germany. The sixth of ten children, he was brought up in the strict Lutheran creed of his father Adolf Gottlieb Friedrich, a candle-maker and soap boiler. Records of the family's financial circumstances are contradictory; while some sources indicate the children were privately tutored, others record that they were raised in relative poverty. Caspar David was familiar with death from an early age. His mother, Sophie Dorothea Bechly, died in 1781 when he was just seven. A year later, his sister Elisabeth died, while a second sister, Maria, succumbed to typhus in 1791. Arguably the greatest tragedy of his childhood was the 1787 death of his brother Johann Christoffer: at the age of thirteen, Caspar David witnessed his younger brother fall through the ice of a frozen lake and drown. Some accounts suggest that Johann Christoffer perished while trying to rescue Caspar David, who was also in danger on the ice.
Friedrich began his formal study of art in 1790 as a private student of artist Johann Gottfried Quistorp at the University of Greifswald in his home city, at which the art department is now named in his honour ("Caspar-David-Friedrich-Institut"). Quistorp took his students on outdoor drawing excursions; as a result, Friedrich was encouraged to sketch from life at an early age. Through Quistorp, Friedrich met and was subsequently influenced by the theologian Ludwig Gotthard Kosegarten, who taught that nature was a revelation of God. Quistorp introduced Friedrich to the work of the German 17th-century artist Adam Elsheimer, whose works often included religious subjects dominated by landscape, and nocturnal subjects. During this period he also studied literature and aesthetics with Swedish professor Thomas Thorild. Four years later Friedrich entered the prestigious Academy of Copenhagen, where he began his education by making copies of casts from antique sculptures before proceeding to drawing from life. Living in Copenhagen afforded the young painter access to the Royal Picture Gallery's collection of 17th-century Dutch landscape painting. At the Academy he studied under teachers such as Christian August Lorentzen and the landscape painter Jens Juel. These artists were inspired by the "Sturm und Drang" movement and represented a midpoint between the dramatic intensity and expressive manner of the budding Romantic aesthetic and the waning neo-classical ideal. Mood was paramount, and influence was drawn from such sources as the Icelandic legend of Edda, the poems of Ossian and Norse mythology.
Friedrich settled permanently in Dresden in 1798. During this early period, he experimented in printmaking with etchings and designs for woodcuts which his furniture-maker brother cut. By 1804 he had produced 18 etchings and four woodcuts; they were apparently made in small numbers and only distributed to friends. Despite these forays into other media, he gravitated toward working primarily with ink, watercolour and sepias. With the exception of a few early pieces, such as "" (1797), he did not work extensively with oils until his reputation was more established. Landscapes were his preferred subject, inspired by frequent trips, beginning in 1801, to the Baltic coast, Bohemia, the Krkonoše and the Harz Mountains. Mostly based on the landscapes of northern Germany, his paintings depict woods, hills, harbors, morning mists and other light effects based on a close observation of nature. These works were modeled on sketches and studies of scenic spots, such as the cliffs on Rügen, the surroundings of Dresden and the river Elbe. He executed his studies almost exclusively in pencil, even providing topographical information, yet the subtle atmospheric effects characteristic of Friedrich's mid-period paintings were rendered from memory. These effects took their strength from the depiction of light, and of the illumination of sun and moon on clouds and water: optical phenomena peculiar to the Baltic coast that had never before been painted with such an emphasis.
Move to Dresden.
Friedrich established his reputation as an artist when he won a prize in 1805 at the Weimar competition organised by the writer, poet, and dramatist Johann Wolfgang von Goethe. At the time, the Weimar competition tended to draw mediocre and now long-forgotten artists presenting derivative mixtures of neo-classical and pseudo-Greek styles. The poor quality of the entries began to prove damaging to Goethe's reputation, so when Friedrich entered two sepia drawings—"Procession at Dawn" and "Fisher-Folk by the Sea"—the poet responded enthusiastically and wrote, "We must praise the artist's resourcefulness in this picture fairly. The drawing is well done, the procession is ingenious and appropriate... his treatment combines a great deal of firmness, diligence and neatness... the ingenious watercolour... is also worthy of praise."
Friedrich completed the first of his major paintings in 1807, at the age of 34. "The Cross in the Mountains", today known as the "Tetschen Altar" (Galerie Neue Meister, Dresden), is an altarpiece panel commissioned by the Countess of Thun for her family's chapel in Tetschen, Bohemia. It was to be one of the few commissions the artist received. The altar panel depicts the crucified Christ in profile at the top of a mountain, alone and surrounded by nature. The cross reaches the highest point in the pictorial plane but is presented from an oblique and a distant viewpoint, unusual for a crucifixion scene in Western art. Nature dominates the scene and for the first time in Christian art, an altarpiece showcases a landscape. According to the art historian Linda Siegel, the design of the altarpiece is the "logical climax of many earlier drawings of his which depicted a cross in nature's world."
The work was first exhibited on Christmas Day, 1808. Although it was generally coldly received, it was nevertheless Friedrich's first painting to receive wide publicity. The artist's friends publicly defended the work, while art critic Basilius von Ramdohr published a lengthy article rejecting Friedrich's use of landscape in such a context; he wrote that it would be "a veritable presumption, if landscape painting were to sneak into the church and creep onto the altar". Ramdohr fundamentally challenged the concept that pure landscape painting could convey explicit meaning. Friedrich responded with a programme describing his intentions. In his 1809 commentary on the painting, he compared the rays of the evening sun to the light of the Holy Father. The sinking of the sun suggests that the era when God revealed himself directly to man has passed. This statement marked the only time Friedrich recorded a detailed interpretation of his own work.
Friedrich was elected a member of the Berlin Academy in 1810 following the purchase of two of his paintings by the Prussian Crown Prince. Yet in 1816, he sought to distance himself from Prussian authority, and that June applied for Saxon citizenship. The move was unexpected by his friends, as the Saxon government of the time was pro-French, while Friedrich's paintings to date were seen as generally patriotic and distinctly anti-French. Nevertheless, with the aid of his Dresden-based friend Graf Vitzthum von Eckstädt, Friedrich attained not only citizenship, but in 1818, a place in the Saxon Academy as a member with a yearly dividend of 150 thalers. Although he hoped to receive a full Professorship, it was never awarded him as, according to the German Library of Information, "it was felt that his painting was too personal, his point of view too individual to serve as a fruitful example to students." Politics too may have played a role in the stalling of his career: Friedrich's decidedly Germanic choice of subject and costuming frequently clashed with the prevailing pro-French attitudes of the time.
Marriage.
On January 21, 1818, Friedrich married Caroline Bommer, the twenty-five-year-old daughter of a dyer from Dresden. The couple had three children, with their first, Emma, arriving in 1820. Physiologist and painter Carl Gustav Carus notes in his biographical essays that marriage did not impact significantly on either Friedrich's life or personality, yet his canvasses from this period, including "Chalk Cliffs on Rügen"—painted after his honeymoon—display a new sense of levity, while his palette is brighter and less austere. Human figures appear with increasing frequency in the paintings of this period, which Siegel interprets as a reflection that "the importance of human life, particularly his family, now occupies his thoughts more and more, and his friends, his wife, and his townspeople appear as frequent subjects in his art."
Around this time, the artist found support from two sources in Russia. In 1820, Grand Duke Nikolai Pavlovich, at the behest of his wife Alexandra Feodorovna, visited Friedrich's studio and returned to Saint Petersburg with a number of his paintings. The exchange marked the beginning of a patronage that continued for many years. Not long thereafter, the poet Vasily Zhukovsky, tutor to Alexander II, met Friedrich in 1821 and found in him a kindred spirit. For decades Zhukovsky helped Friedrich both by purchasing his work himself and by recommending his art to the royal family; his assistance toward the end of Friedrich's career proved invaluable to the ailing and impoverished artist. Zhukovsky remarked that his friend's paintings "please us by their precision, each of them awakening a memory in our mind."
Friedrich was acquainted with Philipp Otto Runge (1777–1810), another leading German painter of the Romantic period. He was also a friend of Georg Friedrich Kersting (1785–1847), who painted him at work in his unadorned studio, and of the Norwegian painter Johan Christian Clausen Dahl (1788–1857). Dahl was close to Friedrich during the artist's final years, and he expressed dismay that to the art-buying public, Friedrich's pictures were only "curiosities". While the poet Zhukovsky appreciated Friedrich's psychological themes, Dahl praised the descriptive quality of Friedrich's landscapes, commenting that "artists and connoisseurs saw in Friedrich's art only a kind of mystic, because they themselves were only looking out for the mystic... They did not see Friedrich's faithful and conscientious study of nature in everything he represented".
During this period Friedrich frequently sketched memorial monuments and sculptures for mausoleums, reflecting his obsession with death and the afterlife; he even created designs for some of the funerary art in Dresden's cemeteries. Some of these works were lost in the fire that destroyed Munich's Glass Palace (1931) and later in the 1945 bombing of Dresden.
Later life and death.
Friedrich's reputation steadily declined over the final fifteen years of his life. As the ideals of early Romanticism passed from fashion, he came to be viewed as an eccentric and melancholy character, out of touch with the times. Gradually his patrons fell away. By 1820, he was living as a recluse and was described by friends as the "most solitary of the solitary". Towards the end of his life he lived in relative poverty and was increasingly dependent on the charity of friends. He became isolated and spent long periods of the day and night walking alone through woods and fields, often beginning his strolls before sunrise.
In June 1835, Friedrich suffered his first stroke, which left him with minor limb paralysis and greatly reduced his ability to paint. As a result he was unable to work in oil; instead he was limited to watercolour, sepia and reworking older compositions. Although his vision remained strong, he had lost the full strength of his hand. Yet he was able to produce a final 'black painting', "Seashore by Moonlight" (1835–36), described by Vaughan as the "darkest of all his shorelines, in which richness of tonality compensates for the lack of his former finesse". Symbols of death appeared in his other work from this period. Soon after his stroke, the Russian royal family purchased a number of his earlier works, and the proceeds allowed him to travel to Teplitz—in today's Czech Republic—to recover.
During the mid-1830s, Friedrich began a series of portraits and he returned to observing himself in nature. As the art historian William Vaughan has observed, however, "He can see himself as a man greatly changed. He is no longer the upright, supportive figure that appeared in "Two Men Contemplating the Moon" in 1819. He is old and stiff... he moves with a stoop". 
By 1838, he was capable only of working in a small format. He and his family were living in poverty and grew increasingly dependent for support on the charity of friends.
Friedrich died in Dresden on May 7, 1840, and was buried in Dresden's Trinity Cemetery (the entrance to which he had painted some 15 years earlier). By then, his reputation and fame were waning, and his passing was little noticed within the artistic community. His artwork had certainly been acknowledged during his lifetime, but not widely. While the close study of landscape and an emphasis on the spiritual elements of nature were commonplace in contemporary art, his work was too original and personal to be well understood. By 1838, his work no longer sold or received attention from critics; the Romantic movement had been moving away from the early idealism that the artist had helped found. 
After his death, Carl Gustav Carus wrote a series of articles which paid tribute to Friedrich's transformation of the conventions of landscape painting. However, Carus' articles placed Friedrich firmly in his time, and did not place the artist within a continuing tradition. Only one of his paintings had been reproduced as a print, and that was produced in very few copies.
Themes.
Landscape and the sublime.
The visualisation and portrayal of landscape in an entirely new manner was Friedrich's key innovation. He sought not just to explore the blissful enjoyment of a beautiful view, as in the classic conception, but rather to examine an instant of sublimity, a reunion with the spiritual self through the contemplation of nature. Friedrich was instrumental in transforming landscape in art from a backdrop subordinated to human drama to a self-contained emotive subject. Friedrich's paintings commonly employed the "Rückenfigur"—a person seen from behind, contemplating the view. The viewer is encouraged to place himself in the position of the "Rückenfigur", by which means he experiences the sublime potential of nature, understanding that the scene is as perceived and idealised by a human. Friedrich created the notion of a landscape full of romantic feeling—"die romantische Stimmungslandschaft". His art details a wide range of geographical features, such as rock coasts, forests, and mountain scenes. He often used the landscape to express religious themes. During his time, most of the best-known paintings were viewed as expressions of a religious mysticism.
Friedrich said, "The artist should paint not only what he sees before him, but also what he sees within him. If, however, he sees nothing within him, then he should also refrain from painting that which he sees before him. Otherwise, his pictures will be like those folding screens behind which one expects to find only the sick or the dead." Expansive skies, storms, mist, forests, ruins and crosses bearing witness to the presence of God are frequent elements in Friedrich's landscapes. Though death finds symbolic expression in boats that move away from shore—a Charon-like motif—and in the poplar tree, it is referenced more directly in paintings like "The Abbey in the Oakwood" (1808–10), in which monks carry a coffin past an open grave, toward a cross, and through the portal of a church in ruins.
He was one of the first artists to portray winter landscapes in which the land is rendered as stark and dead. Friedrich's winter scenes are solemn and still—according to the art historian Hermann Beenken, Friedrich painted winter scenes in which "no man has yet set his foot. The theme of nearly all the older winter pictures had been less winter itself than life in winter. In the 16th and 17th centuries, it was thought impossible to leave out such motifs as the crowd of skaters, the wanderer... It was Friedrich who first felt the wholly detached and distinctive features of a natural life. Instead of many tones, he sought the one; and so, in his landscape, he subordinated the composite chord into one single basic note".
Bare oak trees and tree stumps, such as those in "Raven Tree" (c. 1822), "" (c. 1833), and "Willow Bush under a Setting Sun" (c. 1835), are recurring elements of Friedrich's paintings, symbolizing death. Countering the sense of despair are Friedrich's symbols for redemption: the cross and the clearing sky promise eternal life, and the slender moon suggests hope and the growing closeness of Christ. In his paintings of the sea, anchors often appear on the shore, also indicating a spiritual hope. German literature scholar Alice Kuzniar finds in Friedrich's painting a temporality—an evocation of the passage of time—that is rarely highlighted in the visual arts. For example, in "The Abbey in the Oakwood", the movement of the monks away from the open grave and toward the cross and the horizon imparts Friedrich's message that the final destination of man's life lies beyond the grave.
With dawn and dusk constituting prominent themes of his landscapes, Friedrich's own later years were characterized by a growing pessimism. His work becomes darker, revealing a fearsome monumentality. "The Wreck of the Hope"—also known as "The Polar Sea" or "The Sea of Ice" (1823–24)—perhaps best summarizes Friedrich's ideas and aims at this point, though in such a radical way that the painting was not well received. Completed in 1824, it depicted a grim subject, a shipwreck in the Arctic Ocean; "the image he produced, with its grinding slabs of travertine-colored floe ice chewing up a wooden ship, goes beyond documentary into allegory: the frail bark of human aspiration crushed by the world's immense and glacial indifference."
Friedrich's written commentary on aesthetics was limited to a collection of aphorisms set down in 1830, in which he explained the need for the artist to match natural observation with an introspective scrutiny of his own personality. His best-known remark advises the artist to "close your bodily eye so that you may see your picture first with the spiritual eye. Then bring to the light of day that which you have seen in the darkness so that it may react upon others from the outside inwards." He rejected the overreaching portrayals of nature in its "totality", as found in the work of contemporary painters like Adrian Ludwig Richter (1803–84) and Joseph Anton Koch (1768–1839).
Loneliness and death.
Both Friedrich's life and art have at times been perceived by some to have been marked with an overwhelming sense of loneliness. Art historians and some of his contemporaries attribute such interpretations to the losses suffered during his youth to the bleak outlook of his adulthood, while Friedrich's pale and withdrawn appearance helped reinforce the popular notion of the "taciturn man from the North".
Friedrich suffered depressive episodes in 1799, 1803–1805, c.1813, in 1816 and between 1824 and 1826. There are noticeable thematic shifts in the works he produced during these episodes, which see the emergence of such motifs and symbols as vultures, owls, graveyards and ruins. From 1826 these motifs became a permanent feature of his output, while his use of color became more dark and muted. Carus wrote in 1929 that Friedrich "is surrounded by a thick, gloomy cloud of spiritual uncertainty", though the noted art historian and curator Hubertus Gassner disagrees with such notions, seeing in Friedrich's work a positive and life-affirming subtext inspired by Freemasonry and religion.
Germanic folklore.
Reflecting Friedrich's patriotism and resentment during the 1813 French occupation of the dominion of Pomerania, motifs from German folklore became increasingly prominent in his work. An anti-French German nationalist, Friedrich used motifs from his native landscape to celebrate Germanic culture, customs and mythology. He was impressed by the anti-Napoleonic poetry of Ernst Moritz Arndt and Theodor Körner, and the patriotic literature of Adam Müller and Heinrich von Kleist. Moved by the deaths of three friends killed in battle against France, as well as by Kleist's 1808 drama "Die Hermannsschlacht", Friedrich undertook a number of paintings in which he intended to convey political symbols solely by means of the landscape—a first in the history of art.
In ' (1812), a dilapidated monument inscribed "Arminius" invokes the Germanic chieftain, a symbol of nationalism, while the four tombs of fallen heroes are slightly ajar, freeing their spirits for eternity. Two French soldiers appear as small figures before a cave, lower and deep in a grotto surrounded by rock, as if farther from heaven. A second political painting, ' (c. 1813), depicts a lost French soldier dwarfed by a dense forest, while on a tree stump a raven is perched—a prophet of doom, symbolizing the anticipated defeat of France.
Legacy.
Influence.
Alongside other Romantic painters, Friedrich helped position landscape painting as a major genre within Western art. Of his contemporaries, Friedrich's style most influenced the painting of Johan Christian Dahl (1788–1857). Among later generations, Arnold Böcklin (1827–1901) was strongly influenced by his work, and the substantial presence of Friedrich's works in Russian collections influenced many Russian painters, in particular Arkhip Kuindzhi (c. 1842–1910) and Ivan Shishkin (1832–98). Friedrich's spirituality anticipated American painters such as Albert Pinkham Ryder (1847–1917), Ralph Blakelock (1847–1919), the painters of the Hudson River School and the New England Luminists.
At the turn of the 20th century, Friedrich was rediscovered by the Norwegian art historian Andreas Aubert (1851–1913), whose writing initiated modern Friedrich scholarship, and by the Symbolist painters, who valued his visionary and allegorical landscapes. The Norwegian Symbolist Edvard Munch (1863–1944) would have seen Friedrich's work during a visit to Berlin in the 1880s. Munch's 1899 print "The Lonely Ones" echoes Friedrich's "Rückenfigur (back figure)", although in Munch's work the focus has shifted away from the broad landscape and toward the sense of dislocation between the two melancholy figures in the foreground.
Friedrich's landscapes exercised a strong influence on the work of German artist Max Ernst (1891–1976), and as a result other Surrealists came to view Friedrich as a precursor to their movement. In 1934, the Belgian painter René Magritte (1898–1967) paid tribute in his work "The Human Condition", which directly echoes motifs from Friedrich's art in its questioning of perception and the role of the viewer. A few years later, the Surrealist journal "Minotaure" featured Friedrich in a 1939 article by critic Marie Landsberger, thereby exposing his work to a far wider circle of artists. The influence of "The Wreck of Hope" (or "The Sea of Ice") is evident in the 1940–41 painting "Totes Meer" by Paul Nash (1889–1946), a fervent admirer of Ernst. Friedrich's work has been cited as an inspiration by other major 20th-century artists, including Mark Rothko (1903–70), Gerhard Richter (b. 1932) and Anselm Kiefer (b. 1945). Friedrich's Romantic paintings have also been singled out by writer Samuel Beckett (1906–89), who, standing before "Man and Woman Contemplating the Moon", said "This was the source of "Waiting for Godot", you know."
In his 1961 article "The Abstract Sublime", originally published in ARTnews, the art historian Robert Rosenblum drew comparisons between the Romantic landscape paintings of both Friedrich and Turner with the Abstract Expressionist paintings of Mark Rothko. Rosenblum specifically describes Friedrich's 1809 painting "The Monk by the Sea", Turner's "The Evening Star" and Rothko's 1954 "Light, Earth and Blue" as revealing affinities of vision and feeling. According to Rosenblum, "Rothko, like Friedrich and Turner, places us on the threshold of those shapeless infinities discussed by the aestheticians of the Sublime. The tiny monk in the Friedrich and the fisher in the Turner establish a poignant contrast between the infinite vastness of a pantheistic God and the infinite smallness of His creatures. In the abstract language of Rothko, such literal detail—a bridge of empathy between the real spectator and the presentation of a transcendental landscape—is no longer necessary; we ourselves are the monk before the sea, standing silently and contemplatively before these huge and soundless pictures as if we were looking at a sunset or a moonlit night."
Critical opinion.
Until 1890, and especially after his friends had died, Friedrich's work lay in near-oblivion for decades. Yet, by 1890, the symbolism in his work began to ring true with the artistic mood of the day, especially in central Europe. However, despite a renewed interest and an acknowledgment of his originality, his lack of regard for "painterly effect" and thinly rendered surfaces jarred with the theories of the time.
During the 1930s, Friedrich's work was used in the promotion of Nazi ideology, which attempted to fit the Romantic artist within the nationalistic "Blut und Boden". It took decades for Friedrich's reputation to recover from this association with Nazism. His reliance on symbolism and the fact that his work fell outside the narrow definitions of modernism contributed to his fall from favour. In 1949, art historian Kenneth Clark wrote that Friedrich "worked in the frigid technique of his time, which could hardly inspire a school of modern painting", and suggested that the artist was trying to express in painting what is best left to poetry. Clark's dismissal of Friedrich reflected the damage the artist's reputation sustained during the late 1930s.
Friedrich's reputation suffered further damage when his imagery was adopted by a number of Hollywood directors, such as Walt Disney, built on the work of such German cinema masters as Fritz Lang and F. W. Murnau, within the horror and fantasy genres. His rehabilitation was slow, but enhanced through the writings of such critics and scholars as Werner Hofmann, Helmut Börsch-Supan and Sigrid Hinz, who successfully rejected and rebutted the political associations ascribed to his work, and placed it within a purely art-historical context. By the 1970s, he was again being exhibited in major galleries across the world, as he found favour with a new generation of critics and art historians.
Today, his international reputation is well established. He is a national icon in his native Germany, and highly regarded by art historians and art connoisseurs across the Western World. He is generally viewed as a figure of great psychological complexity, and according to Vaughan, "a believer who struggled with doubt, a celebrator of beauty haunted by darkness. In the end, he transcends interpretation, reaching across cultures through the compelling appeal of his imagery. He has truly emerged as a butterfly—hopefully one that will never again disappear from our sight".
Work.
Friedrich was a prolific artist who produced more than 500 attributed works. In line with the Romantic ideals of his time, he intended his paintings to function as pure aesthetic statements, so he was cautious that the titles given to his work were not overly descriptive or evocative. It is likely that some of today's more literal titles, such as "The Stages of Life", were not given by the artist himself, but were instead adopted during one of the revivals of interest in Friedrich. Complications arise when dating Friedrich's work, in part because he often did not directly name or date his canvases. He kept a carefully detailed notebook on his output, however, which has been used by scholars to tie paintings to their completion dates.

</doc>
<doc id="5655" url="http://en.wikipedia.org/wiki?curid=5655" title="Courtney Love">
Courtney Love

Courtney Michelle Love (born Courtney Michelle Harrison, July 9, 1964) is an American singer-songwriter, musician, actress, and artist. The daughter of psychotherapist Linda Carroll and writer and ex-Grateful Dead manager Hank Harrison, Love began her career as an actress in her early twenties, landing roles in Alex Cox's cult films "Sid and Nancy" (1986) and "Straight to Hell" (1987), --> and later rose to international prominence as frontwoman of alternative rock band Hole, which she formed in 1989. Her uninhibited stage presence and confrontational lyrics, combined with publicity surrounding her 1992 marriage to Kurt Cobain, made her a noticeable and divisive figure in the alternative music scene of the 1990s, with "Rolling Stone" once branding her the "most controversial woman in the history of rock."
Love received critical adulation for Hole's 1991 hardcore punk-influenced debut album, and the band's second release, "Live Through This" (1994), went certified platinum and received wide critical acclaim. Love returned to film in 1995, and received critical recognition for her performance in Miloš Forman's "The People vs. Larry Flynt", which earned her a Golden Globe Nomination. Following this, Hole's third release, "Celebrity Skin" (1998), was nominated for three Grammy awards before the band went dormant in 2000. Love continued to occasionally act in films, including roles in "Man on the Moon" (1999), and "Trapped" (2002), before releasing a solo album, "America's Sweetheart" (2004), which met with mostly positive reviews but underwhelming sales. Love re-formed Hole in 2009 with new members, and released the album "Nobody's Daughter" (2010), which met mixed-positive reviews. In April 2014, Love confirmed that she had been rehearsing material with Hole's founding lead guitarist Eric Erlandson, as well as former bandmates Melissa Auf der Maur and Patty Schemel, though confirmation of a reunion was not given. Later that month, Love debuted her new solo song, "You Know My Name" on BBC Radio 6, which was released May 4, 2014.
Biography.
1964–81: Early life and education.
Love was born Courtney Michelle Harrison in San Francisco, California, to psychotherapist Linda Carroll (née Risi) and Hank Harrison, publisher and brief manager of the Grateful Dead. Love's mother is the daughter of novelist Paula Fox and an unidentified father, who is rumoured to be Marlon Brando. Carroll and Harrison divorced in 1969 and her father's custody withdrawn after her mother alleged that he had fed LSD to Love as a toddler. In 1970, her mother moved the family to the rural community of Marcola, Oregon, where they lived on a commune. Love was legally adopted by her stepfather, Frank Rodriguez, with whom her mother had Love's two half-sisters and adopted a brother; another male half-sibling died in infancy of a heart defect when Love was ten.
Love attended elementary school in Eugene, where she struggled academically and had trouble making friends, though was described as a "creative" child. At age nine, she was diagnosed with mild autism.
In 1972, Love's mother divorced Rodriguez and moved the family to New Zealand, where she enrolled Love at Nelson College for Girls, but Love was ultimately sent back to live in Portland, Oregon, with her former stepfather and numerous family friends. She auditioned for the Mickey Mouse Club at age twelve, and was rejected after reading a Sylvia Plath poem for her audition. At age fourteen, Love was arrested for shoplifting a t-shirt and was sent to Hillcrest Correctional Facility. She spent the following several years in foster care before becoming legally emancipated at age sixteen. Love supported herself by working as a stripper at Mary's Club in Portland, a DJ, and various odd jobs, and intermittently took classes at Portland State University studying English and philosophy. Love has said that she "didn't have a lot of social skills", and that she learned them while frequenting gay clubs in Portland.
In 1981, Love was granted a small trust fund through her adoptive grandparents, which she used to travel to Ireland; there, she was accepted into Trinity College, and studied theology for two semesters. In the United Kingdom, she became acquainted with musician Julian Cope in Liverpool and moved into his house briefly before returning to the United States. Love continued to relocate between Oregon and California, enrolling at San Francisco State University and the San Francisco Art Institute, where she studied film with George Kuchar. She later took stint jobs doing erotic dancing in Japan. In 1986, Love landed roles in two Alex Cox films (see "Acting career"), and then quit acting and retreated to Anchorage, Alaska for several months where she returned to stripping to support herself.
1982–88: Faith No More, early projects.
Love initially began several music projects in the 1980s, first forming Sugar Babydoll in Portland with friends Ursula Wehr and Robin Barbur. In 1982, Love attended a Faith No More concert in San Francisco, and "convinced" the members to let her join as a singer. The group recorded material with Love as a vocalist, but, according to Roddy Bottum, wanted a "male energy", and Love was subsequently kicked out of the band; she and Bottum, however, maintained a friendship in the years after. Love later formed the Pagan Babies with friend Kat Bjelland, whom she met at the Satyricon club in Portland in 1984: "The best thing that ever happened to me in a way, was Kat," Love said. Love asked Bjelland to start a band with her as a guitarist, and the two moved to San Francisco in June 1985, where they recruited Love's friend, bassist Jennifer Finch, and drummer Janis Tanaka. According to Bjelland, " didn't play an instrument at the time" aside from keyboards, so Bjelland would transpose Love's musical ideas on guitar for her. The group played several house shows and recorded one 4-track demo before disbanding in late 1985. Following Pagan Babies, Love moved to Minneapolis where Bjelland had formed the group Babes In Toyland, and briefly worked as a concert promoter before returning to California.
1989–96: Hole, critical success.
In 1989, Love taught herself to play guitar and relocated to Los Angeles, where she placed an ad in a local music zine, reading: "I want to start a band. My influences are Big Black, Sonic Youth, and Fleetwood Mac" to which guitarist Eric Erlandson replied. Love recruited Erlandson as lead guitarist, Lisa Roberts, her neighbor, as bassist, and drummer Caroline Rue. Love named the band Hole after a line from Euripedes' "Medea". Hole played their first show in November 1989 at Raji's after three months of rehearsal. The band's debut single, "Retard Girl", was issued in April 1990 through the Long Beach indie label Sympathy for the Record Industry, and was given air-time by Rodney Bingenheimer's local station, KROQ. The following year, the band released their second single, "Dicknail" through Sub Pop Records.
With Love influenced by no wave, noise rock and grindcore bands, Hole's first studio album, "Pretty on the Inside", captured a particularly abrasive sound and contained disturbing lyrics, described by "Q Magazine" as "confrontational genuinely uninhibited." The record was released in September 1991 on Caroline Records, produced by Kim Gordon of Sonic Youth, with assistant production from Gumball's Don Fleming. Though Love would later say it was "unlistenable" and "[unmelodic," the album received generally positive critical reception from indie and punk rock critics and was labeled one of the 20 best albums of the year by "Spin Magazine". It also gained a following in the United Kingdom, charting at 59 on the UK Albums Chart, as well as its lead single, "Teenage Whore" entering the country's indie chart at number one. The underlying pro-feminist slant of the album's songs led many to mistakenly tag the band as being part of the riot grrl movement, a movement that Love did not associate with. In support of the record, the band toured in Europe headlining with Mudhoney, and extensively in the United States opening for The Smashing Pumpkins, including shows at the Whisky A Go Go opening for Sonic Youth, and performances at CBGB. Love designed and distributed flyers promoting the shows, which included cutouts of women and young girls, as well as scattered lyrics and quotes from poems.
After the release of "Pretty on the Inside", Love began dating Kurt Cobain and became pregnant, which temporarily put her music career on hold. During Love's pregnancy, Hole recorded a cover of "Over the Edge" for a Wipers tribute album, and recorded their fourth single, "Beautiful Son", which was released in April 1993. On September 8, 1993, Love and husband Kurt Cobain made their only public performance together at the Rock Against Rape benefit in Hollywood, California, performing two duets, both acoustic versions, of "Pennyroyal Tea" and "Where Did You Sleep Last Night." Love also performed electric versions of two of Hole's new songs, "Doll Parts" and "Miss World," both of which were written for the band's upcoming second release.
In October 1993, Hole recorded their second album, titled "Live Through This", in Atlanta, Georgia. The album featured a new lineup, with bassist Kristen Pfaff and drummer Patty Schemel. "Live Through This" was released on Geffen's subsidiary DGC label in April 1994, four days after Love's husband, Cobain, was found dead of a self-inflicted shotgun wound in their home. Two months later, in June 1994, bassist Kristen Pfaff died of a heroin overdose, and Love recruited Melissa Auf der Maur for the band's impending tour. Throughout the months preceding the tour, Love was rarely seen in public, spending time at her Seattle home, or visiting the Namgyal Buddhist Monastery in New York.
"Live Through This" was a commercial and critical success, hitting platinum sales in April 1995 and receiving unanimous critical accolades. At their August 26, 1994 performance at the Reading Festival — Love's first public performance following her husband's death — she appeared onstage, tear-drenched, with outstretched arms, mimicking crucifixion. John Peel wrote in "The Guardian" that Love's disheveled appearance "would have drawn whistles of astonishment in Bedlam," and that her performance "verged on the heroic... Love steered her band through a set which dared you to pity either her recent history or that of the band...the band teetered on the edge of chaos, generating a tension which I cannot remember having felt before from any stage." Three days later, "Spin" reviewed another "devastating" performance by the band in Cleveland, Ohio, writing:
During "Doll Parts," after moaning "He only loves those things because he loves to see "me" break" (instead of "them"), wobbled back from the mic almost punch-drunk. By this time, the band had bailed, and Love was alone singing, "Someday you will ache like I ache," again and again, her voice a faint sob. She finally stumbled over to a huge speaker and leaned into it like she was about to pass out. Then, while being led off by an assistant, she stepped back, pulled up her top, then flipped us off with both hands.
In February 1995, Hole performed a well-reviewed acoustic set on "MTV Unplugged" at the Brooklyn Academy of Music, and continued to tour late into the year, concluding their world tour with an appearance at the 1995 "MTV Video Music Awards", in which they performed "Violet," and were nominated for Best Music Video for "Doll Parts."
1997–2000: Mainstream success.
After Love spent the majority of 1996 acting in films, which included a leading role in "The People vs. Larry Flynt", Hole released a compilation album, "My Body, The Hand Grenade" as well as an EP titled "The First Session" which consisted of the band's earliest recordings. In September 1998, Hole released their third studio album, "Celebrity Skin", which marked something of a transformation for Love, featuring a stark power pop sound as opposed to the group's earlier punk rock influences. Love divulged her ambition of making an album where "art meets commerce ... there are no compromises made, it has commercial appeal, and it sticks to the original vision." She claimed to have been influenced by Neil Young, Fleetwood Mac, and My Bloody Valentine when writing the album. "Celebrity Skin" was well received by critics; "Rolling Stone" called the album "accessible, fiery and intimate—often at the same time ... a basic guitar record that's anything but basic." "Celebrity Skin" went on to go multi-platinum, and topped "Best of Year" lists at "Spin", the "Village Voice", and other periodicals. The album garnered the band their only No. 1 hit single on the Modern Rock Tracks chart with the title track "Celebrity Skin". The band made various appearances promoting the album, including MTV performances and at the 1998 Billboard Music Awards. Hole toured with Marilyn Manson on the Beautiful Monsters Tour in 1999, but dropped out of the tour nine dates in after a dispute over production costs between Love and Manson; Hole resumed touring with Imperial Teen.
Prior to the release and promotion of "Celebrity Skin", Love and Fender designed a low-priced Squier brand guitar, called Vista Venus. The instrument featured a shape inspired by Mercury, Stratocaster, and Rickenbacker's solidbodies and had a single-coil and a humbucker pickup, and was available in 6-string and 12-string versions. In an early 1999 interview, Love said about the Venus: "I wanted a guitar that sounded really warm and pop, but which required just one box to go dirty (... ) And something that could also be your first band guitar. I didn't want it all teched out. I wanted it real simple, with just one pickup switch."
After touring for "Celebrity Skin" finished, Auf der Maur left the band to tour with the Smashing Pumpkins; Hole's touring drummer Samantha Maloney left soon after. Love and Erlandson continued with the band, and released the single "Be A Man"— an outtake from the "Celebrity Skin" sessions— for the soundtrack of the Oliver Stone film "Any Given Sunday" (1999). The group became dormant in the following two years, and, in May 2002, officially announced their breakup amid continuing litigation with Universal Music Group over their record contract.
2001–07: Solo career, "America's Sweetheart".
With Hole in disarray, Love began a "punk rock femme supergroup" called Bastard during autumn 2001, enlisting Schemel, Veruca Salt co-frontwoman Louise Post, and bassist Gina Crosley. Though a demo was completed, the project never reached fruition.
In 2002, Love began composing an album with Linda Perry, titled "America's Sweetheart", also reuniting with drummer Patty Schemel. Love signed with Virgin Records to release it, and initially recorded it in France, but was forced by the label to re-record the entire album in the summer of 2003. "America's Sweetheart" was released in February 2004, and was embraced by critics with mixed reviews. "Spin" called it a "jaw-dropping act of artistic will and a fiery, proper follow-up to 1994's "Live Through This"" and awarded it eight out of ten stars, while "Rolling Stone" suggested that, "for people who enjoy watching celebrities fall apart, "America's Sweetheart" should be more fun than an Osbournes marathon." The album sold 86,000 copies in its first three months, with the singles "Mono" and "Hold on to Me", both of which earned competent spots on album charts. Love has publicly expressed her regret over the record several times, calling it "a crap record" and reasoning that her drug issues at the time were to blame. Shortly after the record was released, Love told Kurt Loder on TRL: "I cannot exist as a solo artist. It's a joke."
In 2006, Love started recording what was going to be her second solo album, "How Dirty Girls Get Clean", collaborating again with Perry and Billy Corgan in the writing and recording. Love had written several songs, including an anti-cocaine song titled "Loser Dust", during her time in rehab in 2005. Love told "Billboard": "My hand-eye coordination was so bad the drug use, I didn't even know chords anymore. It was like my fingers were frozen. And I wasn't allowed to make noise rehab ... I never thought I would work again." Some tracks and demos from the album (initially planned for release in 2008) were leaked on the internet in 2006, and a documentary entitled "The Return of Courtney Love", detailing the making of the album, aired on the British television network in the fall of that year. A rough acoustic version of "Never Go Hungry Again", recorded during an interview for "The Times" in November, was also released. Incomplete audio clips of the song "Samantha", originating from an interview with NPR, were also distributed on the internet in 2007.
2008–present: "Nobody's Daughter", Hole reunion.
On June 17, 2009, "NME" reported that Hole would be reuniting. Former Hole guitarist Erlandson stated in "Spin" magazine that contractually no reunion can take place without his involvement; therefore "Nobody's Daughter" would remain Love's solo record, as opposed to a "Hole" record. Love responded to Erlandson's comments in a Twitter post, claiming "he's out of his mind, Hole is my band, my name, and my Trademark". "Nobody's Daughter" was released worldwide as a Hole album on April 27, 2010. For the new line-up, Love recruited guitarist Micko Larkin, Shawn Dailey (bass guitar), and Stu Fisher (drums, percussion). "Nobody's Daughter" featured a great deal of material written and recorded for Love's aborted solo album, "How Dirty Girls Get Clean", including "Pacific Coast Highway", "Letter to God", "Samantha", and "Never Go Hungry", although they were re-produced with Larkin. The first single from "Nobody's Daughter" was "Skinny Little Bitch", which became a hit on alternative rock radio in early March 2010.
The album received mixed reviews. "Rolling Stone" gave the album three out of five stars, saying that Love "worked hard on these songs, instead of just babbling a bunch of druggy bullshit and assuming people would buy it, the way she did on her 2004 flop, "America's Sweetheart"." "Slant Magazine" also gave the album three out of five stars, saying "It's Marianne Faithfull's substance-ravaged voice that comes to mind most often while listening to songs like "Honey" and "For Once in Your Life." The latter track is, in fact, one of Love's most raw and vulnerable vocal performances to date. Co-penned by Linda Perry, the song offers a rare glimpse into the mind of a woman who, for the last 15 years, has been as famous for being a rock star as she's been for being a victim." The album's subject matter was largely centered on Love's tumultuous life between 2003 and 2007, and featured a polished folk-rock sound with much more acoustic work than previous Hole albums. Love and the band toured internationally from 2010 into late 2012 promoting the record, after which she dropped the Hole name and returned to a solo career.
Following a two year tour to promote "Nobody's Daughter", Love collaborated with Michael Stipe for the track "Rio Grande", and also contributed guest vocals and co-wrote a track on Fall Out Boy's album, "Save Rock and Roll" (2013). A music video, starring Love and the band, was released for the song in March 2014. After solo performances in December 2012 and January 2013, Love toured North America in 2013, starting in Philadelphia in June. Initially, the tour had been conceived to promote Love's new album, but was consequently dubbed a "greatest hits" tour due to the impending release of new material. Love told "Billboard" that she had recorded eight songs in the studio, which she planned to release in the near future. "songs are not my usual (style)," Love said. "I don't have any Fleetwood Mac references on it. Usually I always have a Fleetwood Mac reference as well as having, like, Big Black references. These are very unique songs that sort of magically happened." However, in an April 2014 interview with BBC, Love revealed that she and former Hole guitarist Eric Erlandson had reconciled, and had been rehearsing new material together, along with former bassist Melissa Auf der Maur and drummer Patty Schemel, though did not confirm a reunion of the band.
On April 22, 2014, Love debuted the song "You Know My Name" on BBC Radio 6 to promote her tour of the United Kingdom. It was released as a double A-side single with the song "Wedding Day" on May 4, 2014 on her own label Cherry Forever Records via Kobalt Label Services. The tracks were produced by Michael Beinhorn, and feature Tommy Lee on drums.
On May 1, 2014, in an interview with "Pitchfork", Love commented further on the possibility of Hole reuniting, saying: 
I'm not going to commit to it happening, because we want an element of surprise. There's a lot of i's to be dotted and t's to be crossed. It's next year's concern, but we've hung out, we've sat down, we've met, we've jammed. There's some caveats, there's some things people need. We're older—we're all mainlining vegan food, you know what I mean? Nobody smokes other than me. No one's on drugs [...] the reason it's not happening this year is because I was too late to come to the conclusion that it should be done, and to find the manager we all agree on. To make it have some ass-kicking. No one's been dormant. Patty teaches drumming and drums in three indie bands. Melissa has her metal-nerd thing going on—her dream is to play Castle Donington with Dokken. Eric hasn't flipped—I jammed with him, he's still doing his Thurston -crazy tunings, still corresponding with Kevin Shields. We all get along great.
Musicianship.
Influences and style.
Love was exposed to the music of Patti Smith and the Pretenders in juvenile hall, which she was greatly influenced by: "You had these two iconic women, and I realized that you could do something that was completely subversive that didn't involve violence felonies," said Love. "I stopped making trouble. I stopped." As a teenager, Love named Flipper, Kate Bush, Soft Cell, Lou Reed, and Dead Kennedys among her favorite artists. Most prominently, Love was influenced by a multitude of new wave and post-punk bands, such as Echo and the Bunnymen, The Smiths, The Teardrop Explodes, Bauhaus, and Joy Division. While in Ireland at age fifteen, she saw the The Virgin Prunes perform live in Dublin, and said the experience "framed her [music career." Her varying genre interests were illustrated in a 1991 interview with "Flipside", in which she stated: "There's a part of me that wants to have a grindcore band and another that wants to have a Raspberries-type pop band", also citing her admiration for Neil Young. Conversely, Love also embraced the influence of experimental and punk rock groups, including Sonic Youth, Swans, Big Black, The Germs, and The Stooges. Love has also expressed great admiration for Fleetwood Mac, with Hole covering their track "Gold Dust Woman" in 1996, as well as using sampling from "Rhiannon" on their noise track "Starbelly" from "Pretty on the Inside".
Musically, it was remarked in an October 1991 review of her first album that Love's layering of harsh and abrasive riffs buried more sophisticated musical arrangements. Hole's incorporation of both punk rock and power pop sounds illustrates the band's often divergent musical style, which drew influence from alternating genres. In 1998, Love stated that Hole had "always been a pop band. We always had a subtext of pop. I always talked about it, if you go back ... what'll sound like some weird Sonic Youth tuning back then to you was sounding like the Raspberries to me, in my demented pop framework."
Love possesses a contralto vocal range, and her vocal style has been described as "raw and distinctive." According to Love, she "never wanted to be a singer," but rather aspired to be a skilled guitarist: "I'm such a lazy bastard though that I never did that," Love said. "You have to stay in your room and play every Zeprecord, and I didn't ... [it ended up that I was always the only person with the nerve to sing, and so I got stuck with it." She has been oft noted by critics for her husky vocals, and was, in Hole's earliest years, noted for her screaming abilities and punk singing. Her vocals have been compared to those of Johnny Rotten, and "Rolling Stone" described them as "lung-busting" and "a corrosive, lunatic wail." Upon the release of Hole's 2010 album, "Nobody's Daughter", critics compared Love's raspy, unpolished vocals to those of Bob Dylan.
Love writes from a female's point of view, and her earlier work, particularly on Hole's first two albums, was noted for being notably aggressive and critical toward cultural definitions of women. Her lyrics have been noted by scholars for "articulating a third-wave feminist consciousness." Common themes present in Love's songs during her early career included body image, rape, suicide, misogyny, conformity, elitism, pregnancy, prostitution, and death. In a 1991 interview with Everett True, Love said: "I try to place imagery next to fucked up imagery, because that's how I view things ... I sometimes feel that no one's taken the time to write about certain things in rock, that there's a certain female point of view that's never been given space." Charles Cross has referred to her lyrics on "Live Through This" as being "true extensions of her diary," and she has admitted that a great deal of the writing for "Pretty on the Inside" were excisions from her journals. Her later work was more lyrically introspective. "Celebrity Skin" and "America's Sweetheart" deal with celebrity life, Hollywood, and drug addiction, while continuing Love's interest in vanity and body image. "Nobody's Daughter" was lyrically reflective of Love's past relationships and her struggle to sobriety, with the majority of its lyrics having been written while she was in rehab in 2006.
Equipment and gear.
Love has frequently played a multitude of Fender guitars, including a Jaguar and a vintage 1965 Jazzmaster, the latter of which was purchased by the Hard Rock Cafe and is on display in New York City. Love is seen playing her Jazzmaster in the music video for "Miss World." Earlier in Hole's career, between 1989 and 1991, Love primarily played a Rickenbacker 425 because she "preferred the 3/4 neck," but she destroyed the guitar onstage at a 1991 concert opening for The Smashing Pumpkins. She also often played a guitar made by Mercury, an obscure company that manufactured custom guitars, which she purchased in 1992. Fender's Vista Venus, designed by Love in 1998, was partially inspired by Rickenbacker guitars as well as her Mercury. Love's setup has included Fender tube gear, Matchless, Ampeg, Silvertone and a solid-state 1976 Randall Commander. During her 2010 and more recent tours, Love has played a Rickenbacker 360 onstage.
Acting career.
Love's first acting role was in a 1984 student short film titled "Club Vatican" directed by her tutor George Kuchar, while studying at the San Francisco Art Institute. In 1985, she submitted an audition tape for the role of Nancy Spungen in the Sid Vicious biopic "Sid and Nancy" (1986), and was given a minor supporting role by director Alex Cox. Cox then cast her in a leading role in his following film, "Straight to Hell" (1987), which caught the attention of artist Andy Warhol. That year, Love appeared in an episode of "Andy Warhol's Fifteen Minutes" with Robbie Nevil in a segment titled "C'est la Vie". She also had a part in the 1988 Ramones music video for "I Wanna Be Sedated," appearing as a bride among dozens of party guests. In 1989, Love abandoned her career as an actress to pursue music.
In 1996, Love began obtaining small acting parts again in "Basquiat" and "Feeling Minnesota" (1996), before landing the co-starring role of Larry Flynt's wife, Althea, in Miloš Forman's 1996 film "The People vs. Larry Flynt", against Columbia Pictures' reluctance due to her low profile and "troubled" past. Love received critical acclaim, a Golden Globe nomination for Best Actress, and a New York Film Critics Circle Award for Best Supporting Actress, for what film critic Roger Ebert called "quite a performance; Love proves she is not a rock star pretending to act, but a true actress." She won several other awards from various film critic associations for the performance.
Other roles include: starring opposite Jim Carrey in the Andy Kaufman biopic "Man on the Moon" (1999); as Joan Vollmer in "Beat" (2000) alongside Kiefer Sutherland; and a leading role in "Julie Johnson" (2001) as Lili Taylor's lesbian lover, for which she won an Outstanding Actress award at L.A.'s Outfest. She followed with another leading part in the thriller film "Trapped" (2002), alongside Kevin Bacon and Charlize Theron.
Love has also appeared in a multitude of documentary films as herself, including "", "Not Bad for a Girl", "Mayor of the Sunset Strip", "Bob and the Monster", and "Hit So Hard", which documents Hole drummer Patty Schemel's time in the band with Love, Erlandson, Kristen Pfaff, and Melissa Auf der Maur.
Other endeavors.
In between the recording of Hole's "Live Through This" and "Celebrity Skin", Love acted as an executive soundtrack coordinator and assembled the soundtrack to the 1995 film "Tank Girl", based on a comic series of which Love was a fan. In 2004, Love collaborated with illustrators Misaho Kujiradou and Ai Yazawa to create a manga comic, "Princess Ai". The story is based in part on Love's life, and involves the main character's search for her place in the world; it was written by Stu Levy under the name D.J. Milky, and released by his publishing company Tokyopop.
In 2006, Love published a memoir titled "", consisting of diary entries, poems, letters, drawings, personal photos, and lyric compositions spanning from Love's childhood up until the year 2006, shortly after her release from a six-month rehab sentence. The book was generally well reviewed by critics, and Love did book readings in promotion for it.
In May 2012, Love debuted an art show at Fred Torres Collaborations in New York titled ""And She's Not Even Pretty"", which contained over forty drawings and paintings by Love composed in ink, colored pencil, pastels, and watercolors. The works were of various women in different emotional states, some accompanied by poems and song lyrics. She is also writing a memoir, "Girl With the Most Cake".
Public image.
Love has consistently attracted media attention for her brash and outspoken nature, and for "subverting mainstream expectations of how a woman should look, act, and sound." She has also received considerable media scrutiny over her battles with drug addiction, most notably in 1992 when "Vanity Fair" published an article by journalist Lynn Hirschberg which alluded that Love was addicted to heroin during her pregnancy; this resulted in the custody of Love and Cobain's newborn daughter, Frances, being temporarily withdrawn in a Los Angeles County court and placed with Love's sister. Love claimed she was misquoted in the piece, and asserted that she had immediately quit using the drug during her first trimester after she discovered she was pregnant.
On the opening date of Lollapalooza in 1995, Love notoriously got into a physical fight backstage with Kathleen Hanna and punched her in the face. In retrospect, Love said: "I was completely high on dope that time— I cannot remember much about it." She later criticised her own behavior, saying: "I pictures of how I looked. It's disgusting. I'm ashamed. There's death and there's disease and there's misery and there's giving up your soul ... The human spirit mixed with certain powders is not the person, it's [a demonic presence."
Love's aesthetic image, particularly in the early 1990s, often consisted of "thrift shop" babydoll dresses, and her face adorned with smeared makeup; MTV reporter Kurt Loder described her as looking like "a debauched rag doll" onstage. The style, widely popularized by Love, was later dubbed "kinderwhore". Love later claimed to have been influenced by the fashion of Chrissy Amphlett of the Divinyls when assembling the look. In the later 1990s, surrounding the release of Hole's "Celebrity Skin" and Love's budding film career, she embraced a more polished and glamorous look, becoming involved in high fashion and modeling for Versace advertisements in April 1998.
As a result of Love's high-profile marriage to Kurt Cobain, comparisons have been made of her to Yoko Ono. Shortly following their marriage, and particularly after Cobain's suicide, she was often negatively compared to John Lennon's widow Ono by Cobain's fans. This media comparison was addressed by Love in 1992, prior to Cobain's death, in the song "20 Years in the Dakota", which she explicitly wrote about Ono. Love again commented on the comparison in a 1993 interview with "NME", which drew several parallels to her public image in relationship to Cobain's, saying: People hate her Ono, they really do. Did you know that to 'Yoko someone' is a verb in America? It is something that boys say if they're hanging out with you too much and they're going to school or they have a band. It's almost a myth that's used to suppress women. Y'know, 'You're gonna Yoko me. You're gonna destroy me.' And this woman put up with racial inequality from Fleet Street, she put up with being accused of breaking up the best band in the world Beatles, she put up with people's idea that she castrated this man and then, worst of all, she had her best friend, her husband, the person she lived for, die in her arms in front of a fortress that she'd hidden herself in for twenty years. And I just feel that the world media should apologize to her because she handled it with so much dignity.
Personal life.
Love has been a practicing Buddhist since 1989, and has studied and practiced both Tibetan and Nichiren Buddhism. She is a member of Sōka Gakkai, an international lay Buddhist organization. Love is a Democrat. In 2000, she gave a speech at the Million Mom March to advocate stricter gun control laws in the United States, and has advocated for LGBT rights since the early 1990s. Love is a self-identified feminist, and has been noted throughout her career for her subversive feminism and "self-conscious parody of female sex roles."
Love has struggled with substance abuse problems for a great deal of her life. She experimented with numerous opiates in her early adult years, and tried cocaine at age 19. She became addicted to heroin in the early 1990s, but quit in 1996 at the insistence of director Miloš Forman when she landed a leading role in "The People vs. Larry Flynt". Love was ordered to take multiple urine tests under the supervision of Columbia Pictures while filming the movie, and passed all of them. On July 9, 2004, Love's 40th birthday, she attempted to commit suicide at her Manhattan apartment, and was taken to Bellevue Hospital, allegedly incoherent, and put on a 72-hour watch. According to police, she was believed to be a potential "danger to herself", but was deemed mentally sound and released to a rehab facility two days later. In 2005 and 2006, after making several public appearances clearly intoxicated (namely on the "Late Show with David Letterman" and the "Comedy Central Roast" of Pamela Anderson) and suffering drug-related arrests and probation violations, Love was sentenced to six months in lock down rehab due to struggles with prescription drugs and cocaine. She has stated she has been sober since 2007, and in May 2011, confirmed her sobriety.
Love has a chain of flowers tattooed around her left ankle, and several cherry blossoms tattooed across her chest and arms, each representing a person she's "truly loved." She has the phrase "Let It Bleed" tattooed across her right arm, and has a letter "K" tattooed in the center of her abdomen which she had partially faded, representing husband Kurt Cobain. She also has the phrase "don't dream it, be it" tattooed on her right wrist, referencing The Rocky Horror Picture Show. She also had an angel tattooed on the back of her right shoulder in the late 1990s, which she had removed in recent years.
Family and relationships.
Love has publicly acknowledged her estrangement from her parents, Linda Carroll and Hank Harrison, as well as her maternal grandmother, Paula Fox, who gave up Love's mother Linda for adoption after having her out of wedlock. According to Love, she has not been in contact with her father since age fifteen, and has "never forgiven" her mother over the way she was raised; she has, however, maintained relationships with her half-siblings.
She was briefly married to James Moreland (vocalist of The Leaving Trains) in 1989 for several months, but has said that Moreland was a transvestite and that their marriage was "a joke", ending in an annulment filed by Love. After forming Hole in 1989, Love and bandmate Eric Erlandson had a romantic relationship for over a year, and she also briefly dated Billy Corgan in 1991, with whom she has maintained a volatile friendship over the years.
Her most documented romantic relationship was with Kurt Cobain. It is uncertain when they first met; according to Love, she first met Cobain at a Dharma Bums show in Portland where she was doing a spoken word performance. According to Michael Azerrad, the two met at the Satyricon nightclub in Portland in 1989, though Cobain biographer Charles Cross stated the date was actually February 12, 1990, and that Cobain playfully wrestled Love to the floor after she commented to him in passing that he looked like Dave Pirner of Soul Asylum. Love's bandmate Eric Erlandson stated that both he and Love were formally introduced to Cobain in a parking lot after a Butthole Surfers concert at the Hollywood Palladium in 1991. The two later became reacquainted through Jennifer Finch, one of Love's longtime friends and former bandmates. Love and Cobain officially began dating in the fall of 1991 during Hole's "Pretty on the Inside" tours, and were married on Waikiki Beach in Honolulu, Hawaii, on February 24, 1992. Love wore a satin and lace dress once owned by actress Frances Farmer, and Cobain wore green pajamas. Six months later, on August 18, the couple's only child, a daughter, Frances Bean Cobain, was born. In April 1994, Cobain committed suicide in their Seattle home while Love was in rehab in Los Angeles. During their marriage, and after Cobain's death, Love became something of a hate-figure among some of Cobain's fans. After his cremation, Love divided portions of Cobain's ashes, some of which she kept in a teddy bear and in an urn. Another portion of his ashes was taken by Love to the Namgyal Buddhist Monastery in Ithaca, New York in 1994, where they were ceremonially blessed by Buddhist monks and mixed into clay which was made into memorial sculptures.
Between 1996 and 1999, Love dated actor Edward Norton, and was also linked to comedian Steve Coogan in the early 2000s.
Charitable work.
In 1993, Love and husband Kurt Cobain performed an acoustic set together at the Rock Against Rape benefit in Los Angeles, California, which raised awareness and provided resources for victims of sexual abuse. Love has also contributed to amfAR's AIDS research benefits and held live musical performances at their events. In 2009, Love performed a benefit concert for the RED Campaign at Carnegie Hall alongside Laurie Anderson, Rufus Wainright, and Scarlett Johansson, with proceeds going to AIDS research. In May 2011, she attended Mariska Hargitay's Joyful Heart Foundation event for victims of child abuse, rape, and domestic violence, donating six of husband Kurt Cobain's personal vinyl records for auction.
Love has also worked with LGBT and LGBT youth charities; specifically with the Los Angeles Gay and Lesbian Center, where she has taken part in performances at the center's "An Evening with Women", where she headlined the event's debut in 2007. The proceeds of the event help provide food and shelter for homeless youth; services for seniors; legal assistance; domestic violence services; health and mental health services, and cultural arts programs. Love participated with Linda Perry for the event again in 2012, relating her experiences as a nomadic teenager and having to live on the street: This really resonates with me, I was a kid from Oregon, and I came to Hollywood like a lot of people do, and you know, what happens is that we end up on the street... and if you're gay, or lesbian, or transgendered— the more "outside" you are, the more screwed you are in a lot of ways. I stripped. Everybody knows that. [... My point is that I survived it and thrived through it, but I only did it because I had great friends. These kids from Iowa, they aren't so lucky. Seven thousand kids in Los Angeles a year go out on the street, and forty percent of those kids are gay, lesbian, or transgendered. They come out to their parents, and become homeless. charity helps them get sent to the right foster care, they can get medical help, food, clothing... and for whatever reason, I don't really know why, but gay men have a lot of foundations, I've played many of them— but the lesbian side of it doesn't have as much money and/or donors, so we're excited that this has grown to cover women and women's affairs.
In culture.
The artist Barbara Kruger used one of her quotes on her NYC bus project. There is also a band named after her.

</doc>
<doc id="5657" url="http://en.wikipedia.org/wiki?curid=5657" title="Cow (disambiguation)">
Cow (disambiguation)

Cow usually refers to an adult female cattle.
Cow or cows may also refer to:

</doc>
<doc id="5658" url="http://en.wikipedia.org/wiki?curid=5658" title="Cannibalism">
Cannibalism

Cannibalism (from "Caníbales", the Spanish name for the Caribs, a West Indies tribe formerly well known for practicing cannibalism) is the act or practice of humans eating the flesh or internal organs of other human beings. It is also called anthropophagy. A person who practices cannibalism is called a cannibal. The expression "cannibalism" has been extended into zoology to mean one individual of a species consuming all or part of another individual of the same species as food, including sexual cannibalism.
The Island Carib people of the Lesser Antilles, from whom the word cannibalism derives, acquired a long-standing reputation as cannibals following the recording of their legends in the 17th century. Some controversy exists over the accuracy of these legends and the prevalence of actual cannibalism in the culture. Cannibalism was widespread in the past among humans in many parts of the world, continuing into the 19th century in some isolated South Pacific cultures, and to the present day in parts of tropical Africa. In a few cases in insular Melanesia, indigenous flesh-markets existed. Fiji was once known as the 'Cannibal Isles'. Cannibalism has been well documented around the world, from Fiji to the Amazon Basin to the Congo to Māori New Zealand. Neanderthals are believed to have practiced cannibalism, and Neanderthals may have been eaten by anatomically modern humans.
Cannibalism has recently been both practiced and fiercely condemned in several wars, especially in Liberia and Congo. As of 2006, the Korowai were one of very few tribes still believed to eat human flesh as a cultural practice. It is also still known to be practiced as a ritual and in war in various Melanesian tribes. Historically, allegations of cannibalism were used by the colonial powers as a tool of empire to justify the subjugation of what were seen as primitive peoples. Cannibalism has been said to test the bounds of cultural relativism as it challenges anthropologists "to define what is or is not beyond the pale of acceptable human behavior".
Cannibalism has been occasionally practiced as a last resort by people suffering from famine, including in modern times. A famous example is the ill-fated Westward expedition of the Donner Party, and more recently the crash of Uruguayan Air Force Flight 571, after which some survivors ate the bodies of dead passengers. Also, some mentally ill people obsess about eating others and actually do so, such as Jeffrey Dahmer and Albert Fish. There is resistance to formally labeling cannibalism as a mental disorder.
Reasons.
In some societies, especially tribal societies, cannibalism is a cultural norm. Consumption of a person from within the same community is called endocannibalism; ritual cannibalism of the recently deceased can be part of the grieving process, or a way of guiding the souls of the dead into the bodies of living descendants. 
Exocannibalism is the consumption of a person from outside the community, usually as a celebration of victory against a rival tribe. Both types of cannibalism can also be fueled by the belief that eating a person's flesh or internal organs will endow the cannibal with some of the characteristics of the deceased.
In most parts of the world, cannibalism is not a societal norm, but is sometimes resorted to in situations of extreme necessity. The survivors of the shipwrecks of the "Essex" and "Méduse" in the 19th century are said to have engaged in cannibalism, as are the members of Franklin's lost expedition and the Donner Party. Such cases generally involve necro-cannibalism (eating the corpse of someone who is already dead) as opposed to homicidal cannibalism (killing someone for food). In English law, the latter is always considered a crime, even in the most trying circumstances. The case of "R v Dudley and Stephens", in which two men were found guilty of murder for killing and eating a cabin boy while adrift at sea in a lifeboat, set the precedent that necessity is no defence to a charge of murder.
There are numerous examples of murderers consuming their victims, often deriving some degree of sexual satisfaction from the act of cannibalism. Notable examples include Albert Fish, Issei Sagawa and Jeffrey Dahmer. These individuals are usually considered to be mentally ill, although the compulsion to eat human flesh is not formally listed as a mental disorder in the "Diagnostic and Statistical Manual of Mental Disorders" (DSM). Cases of autophagia, or self-cannibalism, have also been reported.
In pre-modern medicine, the explanation given by the now-discredited theory of humorism for cannibalism was that it came about within a black acrimonious humour, which, being lodged in the linings of the ventricle, produced the voracity for human flesh.
Medical aspects.
A well known case of mortuary cannibalism is that of the Fore tribe in New Guinea which resulted in the spread of the prion disease kuru. Although the Fore's mortuary cannibalism was well documented, the practice had ceased before the cause of the disease was recognized. However, some scholars argue that although post-mortem dismemberment was the practice during funeral rites, cannibalism was not. Marvin Harris theorizes that it happened during a famine period coincident with the arrival of Europeans and was rationalized as a religious rite.
In 2003 a publication in "Science" received a large amount of press attention when it suggested that early humans may have practiced extensive cannibalism. According to this research, genetic markers commonly found in modern humans worldwide suggest that today many people carry a gene that evolved as protection against the brain diseases that can be spread by consuming human brain tissue. A 2006 reanalysis of the data questioned this hypothesis, as it claimed to have found a data collection bias, which led to an erroneous conclusion. This claimed bias came from incidents of cannibalism used in the analysis not being due to local cultures, but having been carried out by explorers, stranded seafarers or escaped convicts. The original authors published a subsequent paper in 2008 defending their conclusions.
Myths, legends and folklore.
Cannibalism features in the folklore and legends of many cultures and is most often attributed to evil characters or as extreme retribution for some wrong. Examples include the witch in "Hansel and Gretel", Lamia of Greek mythology and Baba Yaga of Slavic folklore.
A number of stories in Greek mythology involve cannibalism, in particular cannibalism of close family members, for example the stories of Thyestes, Tereus and especially Cronus, who was Saturn in the Roman pantheon. The story of Tantalus also parallels this.
The wendigo is a creature appearing in the legends of the Algonquian people. It is thought of variously as a malevolent cannibalistic spirit that could possess humans or a monster that humans could physically transform into. Those who indulged in cannibalism were at particular risk, and the legend appears to have reinforced this practice as taboo.
As used to demonize colonized or other groups.
Unsubstantiated reports of cannibalism disproportionately relate cases of cannibalism among cultures that are already otherwise despised, feared, or are little known. In antiquity, Greek reports of cannibalism, (often called "anthropophagy" in this context) were related to distant non-Hellenic barbarians, or else relegated in Greek mythology to the 'primitive' chthonic world that preceded the coming of the Olympian gods: see the explicit rejection of human sacrifice in the cannibal feast prepared for the Olympians by Tantalus of his son Pelops. All South Sea Islanders were cannibals so far as their enemies were concerned. When the whaleship Essex was rammed and sunk by a whale in 1820, the captain opted to sail 3000 miles upwind to Chile rather than 1400 miles downwind to the Marquesas because he had heard the Marquesans were cannibals. Ironically many of the survivors of the shipwreck resorted to cannibalism to survive.
However, Herman Melville happily lived with the Marquesan Typees for a time after the other two tribes on the island told him they were cannibals. In his semi-autobiographical novel "Typee", he reports seeing shrunken heads and having strong evidence that the tribal leaders ceremonially consumed the bodies of killed warriors of the neighboring tribe after a skirmish.
William Arens, author of "The Man-Eating Myth: Anthropology and Anthropophagy", questions the credibility of reports of cannibalism and argues that the description by one group of people of another people as cannibals is a consistent and demonstrable ideological and rhetorical device to establish perceived cultural superiority. Arens bases his thesis on a detailed analysis of numerous "classic" cases of cultural cannibalism cited by explorers, missionaries, and anthropologists. He asserted that many were steeped in racism, unsubstantiated, or based on second-hand or hearsay evidence. Arens' findings are controversial, and have been cited as an example of postcolonial revisionism.
Conversely, Michel de Montaigne's essay "Of cannibals" introduced a new multicultural note in European civilization. Montaigne wrote that "one calls 'barbarism' whatever he is not accustomed to".
Accounts.
Among modern humans, cannibalism has been practiced by various groups. In the past, it was practiced by humans in Europe, South America, among Iroquoian peoples in North America, Maori in New Zealand, the Solomon Islands, parts of West Africa and Central Africa, some of the islands of Polynesia, New Guinea, Sumatra, and Fiji. Evidence of cannibalism has been found in ruins associated with the Anasazi culture of the Southwestern United States as well.
Pre-history.
Some anthropologists, such as Tim White, suggest that ritual cannibalism was common in human societies prior to the beginning of the Upper Paleolithic period. This theory is based on the large amount of "butchered human" bones found in Neanderthal and other Lower/Middle Paleolithic sites. Cannibalism in the Lower and Middle Paleolithic may have occurred because of food shortages. It has been also suggested that removing dead bodies through ritual cannibalism might been a means of predator control, aiming to eliminate predators' and scavengers' access to hominid (and early human) bodies. Jim Corbett proposed that after major epidemics, when human corpses are easily accessible to predators, there are more cases of man-eating leopards, so removing dead bodies through ritual cannibalism (before the cultural traditions of burying and burning bodies appeared in human history) might have had practical reasons for hominids and early humans to control predation.
In Gough's Cave, England, remains of human bones and skulls, around 15,000 years old, suggest that cannibalism took place amongst the people living in or visiting the cave, and that they may have used human skulls as drinking vessels.
Researchers have found physical evidence of cannibalism in ancient times. In 2001, archaeologists at the University of Bristol found evidence of Iron Age cannibalism in Gloucestershire. Cannibalism was practiced as recently as 2000 years ago in Great Britain. In Germany, Emil Carthaus and Dr. Bruno Bernhard have observed 1,891 signs of cannibalism in the caves at the Hönne (1000 – 700 BC)
Early history.
Cannibalism is mentioned many times in early history and literature. Cannibalism was reported by Flavius Josephus during the siege of Jerusalem by Rome in 70 AD, and according to Appian, the population of Numantia during the Roman Siege of Numantia in the 2nd century BC was reduced to cannibalism and suicide.
St. Jerome, in his letter "Against Jovinianus", discusses how people come to their present condition as a result of their heritage, and then lists several examples of peoples and their customs. In the list, he mentions that he has heard that Atticoti eat human flesh and that Massagetae and "Derbices" (a people on the borders of India) kill and eat old people. 
Middle Ages.
Reports of cannibalism were recorded during the First Crusade, as Crusaders were alleged to have fed on the bodies of their dead opponents following the Siege of Ma'arrat al-Numan. Amin Maalouf also alleges further cannibalism incidents on the march to Jerusalem, and to the efforts made to delete mention of these from western history. During Europe's Great Famine of 1315–1317 there were many reports of cannibalism among the starving populations. In North Africa, as in Europe, there are references to cannibalism as a last resort in times of famine.
The Moroccan Muslim explorer Ibn Batutta reported that one African king advised him that nearby people were cannibals (although this may have been a prank played on Ibn Batutta by the king to fluster his guest). However Batutta reported that Arabs and Christians were safe, as their flesh was "unripe" and would cause the eater to fall ill.
For a brief time in Europe, an unusual form of cannibalism occurred when thousands of Egyptian mummies preserved in bitumen were ground up and sold as medicine. The practice developed into a wide-scale business which flourished until the late 16th century. This "fad" ended because the mummies were revealed actually to be recently killed slaves. Two centuries ago, mummies were still believed to have medicinal properties against bleeding, and were sold as pharmaceuticals in powdered form (see human mummy confection and mummia).
In China during the Tang Dynasty, cannibalism was supposedly resorted to by rebel forces early in the period (who were said to raid neighboring areas for victims to eat), as well as both soldiers and civilians besieged during the rebellion of An Lushan. Eating an enemy's heart and liver was also claimed to be a feature of both official punishments and private vengeance. References to cannibalizing the enemy has also been seen in poetry written in the Song Dynasty, (for example, in "Man Jiang Hong") although the cannibalizing is perhaps poetic symbolism, expressing hatred towards the enemy.
While there is universal agreement that some Mesoamerican people practiced human sacrifice, there is a lack of scholarly consensus as to whether cannibalism in pre-Columbian America was widespread. At one extreme, anthropologist Marvin Harris, author of "Cannibals and Kings", has suggested that the flesh of the victims was a part of an aristocratic diet as a reward, since the Aztec diet was lacking in proteins. While most pre-Columbian historians believe that there was ritual cannibalism related to human sacrifices, they do not support Harris's thesis that human flesh was ever a significant portion of the Aztec diet. Others have hypothesized that cannibalism was part of a blood revenge in war.
Early modern era.
European explorers and colonizers brought home many stories of cannibalism practiced by the native peoples they encountered. The friar Diego de Landa reported about Yucatán instances, and there have been similar reports by Purchas from Popayán, Colombia, and from the Marquesas Islands of Polynesia, where human flesh was called ""long pig"". According to Hans Egede, when the Inuit killed a woman accused of witchcraft, they ate a portion of her heart. It is recorded about the natives of the captaincy of Sergipe in Brazil: "They eat human flesh when they can get it, and if a woman miscarries devour the abortive immediately. If she goes her time out, she herself cuts the navel-string with a shell, which she boils along with the secondine, and eats them both."
The 1913 "Handbook of Indians of Canada" (reprinting 1907 material from the Bureau of American Ethnology), claims that North American natives practicing cannibalism included "... the Montagnais, and some of the tribes of Maine; the Algonkin, Armouchiquois, Iroquois, and Micmac; farther west the Assiniboine, Cree, Foxes, Chippewa, Miami, Ottawa, Kickapoo, Illinois, Sioux, and Winnebago; in the South the people who built the mounds in Florida, and the Tonkawa, Attacapa, Karankawa, Caddo, and Comanche (?); in the Northwest and West, portions of the continent, the Thlingchadinneh and other Athapascan tribes, the Tlingit, Heiltsuk, Kwakiutl, Tsimshian, Nootka, Siksika, some of the Californian tribes, and the Ute. There is also a tradition of the practice among the Hopi, and mentions of the custom among other tribes of New Mexico and Arizona. The Mohawk, and the Attacapa, Tonkawa, and other Texas tribes were known to their neighbours as 'man-eaters.'" The forms of cannibalism described included both resorting to human flesh during famines and ritual cannibalism, the latter usually consisting of eating a small portion of an enemy warrior.
As with most lurid tales of native cannibalism, these stories are treated with a great deal of scrutiny, as accusations of cannibalism were often used as justifications for the subjugation or destruction of "savages". However, there were several well-documented cultures that engaged in regular eating of the dead, such as New Zealand's Māori. In an 1809 incident known as the Boyd massacre, about 66 passengers and crew of the "Boyd" were killed and eaten by Māori on the Whangaroa peninsula, Northland. Cannibalism was already a regular practice in Māori wars. In another instance, on July 11, 1821 warriors from the Ngapuhi tribe killed 2,000 enemies and remained on the battlefield "eating the vanquished until they were driven off by the smell of decaying bodies". Māori warriors fighting the New Zealand government in Titokowaru's War in New Zealand's North Island in 1868–69 revived ancient rites of cannibalism as part of the radical Hauhau movement of the Pai Marire religion.
Other islands in the Pacific were home to cultures that allowed cannibalism to some degree. In parts of Melanesia, cannibalism was still practiced in the early 20th century, for a variety of reasons — including retaliation, to insult an enemy people, or to absorb the dead person's qualities. One tribal chief, Ratu Udre Udre in Rakiraki, Fiji, is said to have consumed 872 people and to have made a pile of stones to record his achievement. The ferocity of the cannibal lifestyle deterred European sailors from going near Fijian waters, giving Fiji the name "Cannibal Isles". The dense population of Marquesas Islands, Polynesia, was concentrated in the narrow valleys, and consisted of warring tribes, who sometimes practiced cannibalism on their enemies. W. D. Rubinstein wrote:
This period of time was also rife with instances of explorers and seafarers resorting to cannibalism for survival. The survivors of the sinking of the French ship "Méduse" in 1816 resorted to cannibalism after four days adrift on a raft and their plight was made famous by Théodore Géricault's painting Raft of the Medusa. After the sinking of the "Essex" of Nantucket by a whale, on November 20, 1820, (an important source event for Herman Melville's "Moby-Dick") the survivors, in three small boats, resorted, by common consent, to cannibalism in order for some to survive. Sir John Franklin's lost polar expedition is another example of cannibalism out of desperation. On land, the Donner Party found itself stranded by snow in a high mountain pass in California without adequate supplies during the Mexican-American War, leading to several instances of cannibalism. Another notorious cannibal was mountain man Boone Helm, who was known as "The Kentucky Cannibal" for eating several of his fellow travelers, from 1850 until his eventual hanging in 1864.
The case of "R v. Dudley and Stephens" (1884) 14 QBD 273 (QB) is an English case which dealt with four crew members of an English yacht, the "Mignonette", who were cast away in a storm some from the Cape of Good Hope. After several days, one of the crew, a seventeen-year-old cabin boy, fell unconscious due to a combination of the famine and drinking seawater. The others (one possibly objecting) decided then to kill him and eat him. They were picked up four days later. Two of the three survivors were found guilty of murder. A significant outcome of this case was that necessity was determined to be no defence against a charge of murder.
American consul James W. Davidson described in his 1903 book, "The Island of Formosa", how the Chinese in Taiwan ate and traded in the flesh of Taiwanese aboriginals.
Roger Casement, writing to a consular colleague in Lisbon on August 3, 1903 from Lake Mantumba in the Congo Free State, said: "The people round here are all cannibals. You never saw such a weird looking lot in your life. There are also dwarfs (called Batwas) in the forest who are even worse cannibals than the taller human environment. They eat man flesh raw! It's a fact." Casement then added how assailants would "bring down a dwarf on the way home, for the marital cooking pot ... The Dwarfs, as I say, dispense with cooking pots and eat and drink their human prey fresh cut on the battlefield while the blood is still warm and running. These are not fairy tales my dear Cowper but actual gruesome reality in the heart of this poor, benighted savage land."
World War II.
Many instances of cannibalism by necessity were recorded during World War II. For example, during the 872-day Siege of Leningrad, reports of cannibalism began to appear in the winter of 1941–1942, after all birds, rats and pets were eaten by survivors. Leningrad police even formed a special division to combat cannibalism.
Some 2.8 million Soviet POWs died in Nazi custody in less than eight months of 1941–42. According to the USHMM, by the winter of 1941, "starvation and disease resulted in mass death of unimaginable proportions". This deliberate starvation led to many incidents of cannibalism.
Following the Soviet victory at Stalingrad it was found that some German soldiers in the besieged city, cut off from supplies, resorted to cannibalism. Later, following the German surrender in January 1943, roughly 100,000 German soldiers were taken prisoner of war (POW). Almost all of them were sent to POW camps in Siberia or Central Asia where, due to being chronically underfed by their Soviet captors, many resorted to cannibalism. Fewer than 5,000 of the prisoners taken at Stalingrad survived captivity.
The Australian War Crimes Section of the Tokyo tribunal, led by prosecutor William Webb (the future Judge-in-Chief), collected numerous written reports and testimonies that documented Japanese soldiers' acts of cannibalism among their own troops, on enemy dead, and on Allied prisoners of war in many parts of the Greater East Asia Co-Prosperity Sphere. In September 1942, Japanese daily rations on New Guinea consisted of 800 grams of rice and tinned meat. However, by December, this had fallen to 50 grams. According to historian Yuki Tanaka, "cannibalism was often a systematic activity conducted by whole squads and under the command of officers".
In some cases, flesh was cut from living people. An Indian POW, Lance Naik Hatam Ali (later a citizen of Pakistan), testified that in New Guinea: "the Japanese started selecting prisoners and every day one prisoner was taken out and killed and eaten by the soldiers. I personally saw this happen and about 100 prisoners were eaten at this place by the Japanese. The remainder of us were taken to another spot away where 10 prisoners died of sickness. At this place, the Japanese again started selecting prisoners to eat. Those selected were taken to a hut where their flesh was cut from their bodies while they were alive and they were thrown into a ditch where they later died."
Another well-documented case occurred in Chichijima in February 1945, when Japanese soldiers killed and consumed five American airmen. This case was investigated in 1947 in a war crimes trial, and of 30 Japanese soldiers prosecuted, five (Maj. Matoba, Gen. Tachibana, Adm. Mori, Capt. Yoshii, and Dr. Teraki) were found guilty and hanged. In his book "", James Bradley details several instances of cannibalism of World War II Allied prisoners by their Japanese captors. The author claims that this included not only ritual cannibalization of the livers of freshly killed prisoners, but also the cannibalization-for-sustenance of living prisoners over the course of several days, amputating limbs only as needed to keep the meat fresh.
New Guinea.
The Korowai tribe of south-eastern Papua could be one of the last surviving tribes in the world engaging in cannibalism. However, recent reports suggest that certain clans have been coaxed into encouraging tourism by perpetuating the myth that it is still an active practice.
Africa.
During the 1892–1894 war between the Congo Free State and the Swahili-Arab city-states of Nyangwe and Kasongo in Eastern Congo, there were reports of widespread cannibalization of the bodies of defeated Arab combatants by the Batetela allies of Belgian commander Francis Dhanis. The Batetela, "like most of their neighbors were inveterate cannibals." According to Dhanis' medical officer, Captain Hinde, their town of Ngandu had "at least 2,000 polished human skulls" as a "solid white pavement in front" of its gates, with human skulls crowning every post of the stockade.
In April 1892, 10,000 of the Batetela, under the command of Gongo Lutete, joined forces with Dhanis in a campaign against the Swahili-Arab leaders Sefu and Mohara. After one early skirmish in the campaign, Hinde "noticed that the bodies of both the killed and wounded had vanished." When fighting broke out again, Hinde saw his Batetela allies drop human arms, legs and heads on the road. One young Belgian officer wrote home: "Happily Gongo's men ate them up a few hours. It's horrible but exceedingly useful and hygenic ... I should have been horrified at the idea in Europe! But it seems quite natural to me here. Don't show this letter to anyone indiscreet." After the massacre at Nyangwe, Lutete "hid himself in his quarters, appalled by the sight of thousands of men smoking human hands and human chops on their camp fires, enough to feed his army for many days."
In the 1980s, Médecins Sans Frontières, the international medical charity, supplied photographic and other documentary evidence of ritualized cannibal feasts among the participants in Liberia's internecine strife to representatives of Amnesty International who were on a fact-finding mission to the neighboring state of Guinea. However, Amnesty International declined to publicize this material; the Secretary-General of the organization, Pierre Sane, said at the time in an internal communication that "what they do with the bodies after human rights violations are committed is not part of our mandate or concern". The existence of cannibalism on a wide scale in Liberia was subsequently verified.
The self-declared Emperor of the Central African Republic, Jean-Bédel Bokassa (Emperor Bokassa I), was tried on October 24, 1986 for several cases of cannibalism although he was never convicted. Between April 17, and April 19, 1979 a number of elementary school students were arrested after they had protested against wearing the expensive, government-required school uniforms. Around 100 were killed. Bokassa is said to have participated in the massacre, beating some of the children to death with his cane and allegedly ate some of his victims.
Cannibalism has been reported in several recent African conflicts, including the Second Congo War, and the civil wars in Liberia and Sierra Leone. A UN human rights expert reported in July 2007 that sexual atrocities against Congolese women go "far beyond rape" and include sexual slavery, forced incest, and cannibalism. This may be done in desperation, as during peacetime cannibalism is much less frequent; at other times, it is consciously directed at certain groups believed to be relatively helpless, such as Congo Pygmies, even considered subhuman by some other Congolese. It is also reported by some that witch doctors sometimes use the body parts of children in their medicine. In the 1970s the Ugandan dictator Idi Amin was reputed to practice cannibalism.
In Uganda, the Lord's Resistance Army have been accused of routinely engaging in ritual or magical cannibalism.
North Korea.
Reports of widespread cannibalism began to emerge from North Korea during the famine of the 1990s and subsequent ongoing starvation. Kim Jong-il was reported to have ordered a crackdown on cannibalism in 1996. Chinese travellers reported in 1998 that cannibalism had occurred. Three people in North Korea were reported to have been executed for selling or eating human flesh in 2006. Further reports of cannibalism emerged in early 2013, including reports of a man executed for killing his two children for food.
There are competing claims about how widespread cannibalism was in North Korea. While refugees reported that it was widespread Barbara Demick in her 2010 book "Nothing to Envy: Ordinary Lives in North Korea" wrote that it did not seem to be.
Modern era.
Further instances include cannibalism as ritual practice, in times of drought, famine and other destitution, as well as those being criminal acts and war crimes throughout the 20th century, and also 21st century.
In West Africa, the Leopard Society was a secret society active into the mid-1900s and one that practiced cannibalism. Centered in Sierra Leone, Liberia and Côte d'Ivoire, the "Leopard men" would dress in leopard skins, waylaying travelers with sharp claw-like weapons in the form of leopards' claws and teeth. The victims' flesh would be cut from their bodies and distributed to members of the society.
As in some other Papuan societies, the Urapmin people engaged in cannibalism in war. Notably, the Urapmin also had a system of food taboo wherein dogs could not be eaten and had to be kept from breathing on food, unlike humans who could be eaten and with whom food could be shared.
The Aghoris are Indian ascetics who believe that eating human flesh confers spiritual and physical benefits, such as prevention of aging. They claim to only eat those who have voluntarily willed their body to the sect upon their death, although an Indian TV crew witnessed one Aghori feasting on a corpse discovered floating in the Ganges, and a member of the Dom caste reports that Aghoris often take bodies from the cremation "ghat" (or funeral pyre).
During the 1930s, multiple acts of cannibalism were reported from Ukraine and Russia's Volga, South Siberian and Kuban regions during the Soviet famine of 1932–1933.
Survival was a moral as well as a physical struggle. A woman doctor wrote to a friend in June 1933 that she had not yet become a cannibal, but was "not sure that I shall not be one by the time my letter reaches you." The good people died first. Those who refused to steal or to prostitute themselves died. Those who gave food to others died. Those who refused to eat corpses died. Those who refused to kill their fellow man died. ... At least 2,505 people were sentenced for cannibalism in the years 1932 and 1933 in Ukraine, though the actual number of cases was certainly much higher.
Cannibalism is documented to have occurred in China during the Great Leap Forward, when rural China was hit hard by drought and famine.
Prior to 1931, "New York Times" reporter William Buehler Seabrook, allegedly in the interests of research, obtained from a hospital intern at the Sorbonne a chunk of human meat from the body of a healthy human killed in an accident, then cooked and ate it. He reported, "It was like good, fully developed veal, not young, but not yet beef. It was very definitely like that, and it was not like any other meat I had ever tasted. It was so nearly like good, fully developed veal that I think no person with a palate of ordinary, normal sensitiveness could distinguish it from veal. It was mild, good meat with no other sharply defined or highly characteristic taste such as for instance, goat, high game, and pork have. The steak was slightly tougher than prime veal, a little stringy, but not too tough or stringy to be agreeably edible. The roast, from which I cut and ate a central slice, was tender, and in color, texture, smell as well as taste, strengthened my certainty that of all the meats we habitually know, veal is the one meat to which this meat is accurately comparable."
In his book, "The Gulag Archipelago", Soviet writer Aleksandr Solzhenitsyn described cases of cannibalism in 20th-century USSR. Of the famine in Povolzhie (1921–1922) he wrote: "That horrible famine was up to cannibalism, up to consuming children by their own parents — the famine, which Russia had never known even in Time of Troubles 1601–1603 ..."
He said of the Siege of Leningrad (1941–1944): "Those who consumed human flesh, or dealt with the human liver trading from dissecting rooms ... were accounted as the political criminals ..." And of the building of Northern Railway Prisoners Camp ("SevZhelDorLag") Solzhenitsyn reports, "An ordinary hard working political prisoner almost could not survive at that penal camp. In the camp SevZhelDorLag (chief: colonel Klyuchkin) in 1946–47 there were many cases of cannibalism: they cut human bodies, cooked and ate."
The Soviet journalist Yevgenia Ginzburg was a former long-term political prisoner who spent time in the Soviet prisons, Gulag camps and settlements from 1938 to 1955. She described in her memoir, "Harsh Route" (or "Steep Route"), of a case which she was directly involved in during the late 1940s, after she had been moved to the prisoners' hospital.
...The chief warder shows me the black smoked pot, filled with some food: 'I need your medical expertise regarding this meat.' I look into the pot, and hardly hold vomiting. The fibres of that meat are very small, and don't resemble me anything I have seen before. The skin on some pieces bristles with black hair (...) A former smith from Poltava, Kulesh worked together with Centurashvili. At this time, Centurashvili was only one month away from being discharged from the camp (...) And suddenly he surprisingly disappeared. The wardens looked around the hills, stated Kulesh's evidence, that last time Kulesh had seen his workmate near the fireplace, Kulesh went out to work and Centurashvili left to warm himself more; but when Kulesh returned to the fireplace, Centurashvili had vanished; who knows, maybe he got frozen somewhere in snow, he was a weak guy (...) The wardens searched for two more days, and then assumed that it was an escape case, though they wondered why, since his imprisonment period was almost over (...) The crime was there. Approaching the fireplace, Kulesh killed Centurashvili with an axe, burned his clothes, then dismembered him and hid the pieces in snow, in different places, putting specific marks on each burial place. ... Just yesterday, one body part was found under two crossed logs.
When Uruguayan Air Force Flight 571 crashed into the Andes on October 13, 1972, the survivors resorted to eating the deceased during their 72 days in the mountains. Their story was later recounted in the books ' and ' as well as the film "Alive", by Frank Marshall, and the documentaries ' (1993) and ' (2008).
Cannibalism was reported by the journalist Neil Davis during the South East Asian wars of the 1960s and 1970s. Davis reported that Cambodian troops ritually ate portions of the slain enemy, typically the liver. However he, and many refugees, also report that cannibalism was practiced non-ritually when there was no food to be found. This usually occurred when towns and villages were under Khmer Rouge control, and food was strictly rationed, leading to widespread starvation. Any civilian caught participating in cannibalism would have been immediately executed.
On July 23, 1988, Rick Gibson ate the flesh of another person in public. Because England does not have a specific law against cannibalism, he legally ate a canapé of donated human tonsils in Walthamstow High Street, London. A year later, on April 15, 1989, he publicly ate a slice of human testicle in Lewisham High Street, London. When he tried to eat another slice of human testicle at the Pitt International Galleries in Vancouver on July 14, 1989, the Vancouver police confiscated the testicle hors d'œuvre. However, the charge of publicly exhibiting a disgusting object was dropped and he finally ate the piece of human testicle on the steps of the Vancouver court house on September 22, 1989.

</doc>
<doc id="5659" url="http://en.wikipedia.org/wiki?curid=5659" title="Chemical element">
Chemical element

A chemical element is a pure chemical substance consisting of a single type of atom distinguished by its atomic number, which is the number of protons in its atomic nucleus. Elements are divided into metals, metalloids, and non-metals. 
The lightest chemical elements, including hydrogen, helium and smaller amounts of lithium, beryllium and boron, are thought to have been produced by various cosmic processes during the Big Bang and cosmic-ray spallation. Production of heavier elements, from carbon to the very heaviest elements, proceeded by stellar nucleosynthesis, and these were made available for later solar system and planetary formation by planetary nebulae and supernovae, which blast these elements into space. The high abundance of oxygen, silicon, and iron on Earth reflects their common production in such stars. While most elements are generally stable, a small amount of natural transformation of one element to another also occurs in the decay of radioactive elements as well as other natural nuclear processes.
The history of the discovery and use of the elements began with primitive human societies that found native elements like copper and gold and extracted (smelted) iron and a few other metals from their ores. Alchemists and chemists subsequently identified many more, with nearly all of the naturally-occurring elements becoming known by 1900. The properties of the chemical elements are often summarized using the periodic table, which organizes the elements by increasing atomic number into rows ("periods") in which the columns ("groups") share recurring ("periodic") physical and chemical properties. Save for unstable radioactive elements with short half lives, all of the elements are available industrially, most of them in high degrees of purity.
Hydrogen and helium are by far the most abundant elements in the universe. However, iron is the most abundant element (by mass) making up the Earth, and oxygen is the most common element in Earth's crust. Although all known chemical matter is composed of these elements, chemical matter itself is hypothesized to constitute only about 15% of the matter in the universe. The remainder is believed to be dark matter, a range of substances whose composition is largely unknown and not composed of chemical elements, since it lacks protons, neutrons or electrons. Dark matter may also include normal baryonic matter and neutrinos.
When two or more distinct elements are chemically combined, with the atoms held together by chemical bonds, the result is termed a chemical compound. Two thirds of the chemical elements occur naturally on Earth only as compounds, and in the remaining third, often the compound forms of the element are most common. Chemical compounds may be composed of elements combined in exact whole-number ratios of atoms, as in water, table salt, and minerals such as quartz, calcite, and some ores. However, chemical bonding of many types of elements results in crystalline solids and metallic alloys for which exact chemical formulas do not exist. Relatively pure samples of isolated elements are uncommon in nature. While 98 naturally-occurring elements (1 to 98, up to californium) have been identified in mineral samples from Earth's crust, only a small minority of elements are found as recognizable, relatively pure minerals. Among the more common of such "native elements" are copper, silver, gold, carbon (as coal, graphite, or diamonds), sulfur, and mercury. All but a few of the most inert elements, such as noble gases and noble metals, are usually found on Earth in chemically combined form, as chemical compounds. While about 32 of the chemical elements occur on Earth in native uncombined forms, most of these occur as mixtures. For example, atmospheric air is primarily a mixture of nitrogen, oxygen, and argon, and native solid elements occur in alloys, such as that of iron and nickel.
As of November 2011, 118 elements have been identified, the latest being ununseptium in 2010. Of the 118 known elements, only the first 98 are known to occur naturally on Earth; 80 of them are stable, while the others are radioactive, decaying into lighter elements over various timescales from fractions of a second to billions of years. 
The 18 radioactive elements that occur naturally are either very long-lived isotopes (such as uranium and thorium) or else radioactive decay daughters or nuclear reaction products formed from these elements combining with naturally occurring neutrons. Those elements that do not occur naturally on Earth have been produced artificially as the synthetic products of nuclear reactions.
Description.
The lightest of the chemical elements are hydrogen and helium, both created by Big Bang nucleosynthesis during the first 20 minutes of the universe in a ratio of around 3:1 by mass (or 12:1 by number of atoms). Almost all other elements found in nature were made by various natural methods of nucleosynthesis. On Earth, small amounts of new atoms are naturally produced in nucleogenic reactions, or in cosmogenic processes, such as cosmic ray spallation. New atoms are also naturally produced on Earth as radiogenic daughter isotopes of ongoing radioactive decay processes such as alpha decay, beta decay, spontaneous fission, cluster decay, and other rarer modes of decay.
Of the 98 naturally occurring elements, those with atomic numbers 1 through 40 are all considered stable. At least one isotope of each element with atomic numbers 41 through 82 are apparently stable (except technetium, element 43 and promethium, element 61, which have no stable isotopes) but theoretically unstable (in that their fission would release energy) and thus possibly mildly radioactive. The half-lives of elements 41 through 82 are so long, however, that their radioactive decay remains undetected by experiment. These "theoretical radionuclides" have half-lives at least 100 million times longer than the estimated age of the universe. Elements with atomic numbers 83 through 98 are unstable to the point that their radioactive decay can be detected. Some of these elements, notably thorium (atomic number 90), uranium (atomic number 92) and plutonium (atomic number 94), have one or more isotopes with half-lives long enough to survive as remnants of the explosive stellar nucleosynthesis that produced the heavy elements before the formation of our solar system. For example, at over 1.9 years, over a billion times longer than the current estimated age of the universe, bismuth-209 (atomic number 83) has the longest known alpha decay half-life of any naturally occurring element. The very heaviest elements (those beyond californium, atomic number 98) undergo radioactive decay with half-lives so short that they do not occur in nature and must be synthesized.
As of 2010, there are 118 known elements (in this context, "known" means observed well enough, even from just a few decay products, to have been differentiated from other elements). Of these 118 elements, 98 occur naturally on Earth. Ten of these occur in extreme trace quantities: technetium, atomic number 43; promethium, number 61; astatine, number 85; francium, number 87; neptunium, number 93; plutonium, number 94; americium, number 95; curium, number 96; berkelium, number 97; and californium, number 98. These 98 elements have been detected in the universe at large, in the spectra of stars and also supernovae, where short-lived radioactive elements are newly being made. The first 98 elements have been detected directly on Earth as primordial nuclides present from the formation of the solar system, or as naturally-occurring fission or transmutation products of uranium and thorium.
The remaining 20 heavier elements, not found today either on Earth or in astronomical spectra, have been produced artificially: these are all radioactive, with very short half-lives; if any atoms of these elements were present at the formation of Earth, they are extremely likely, to the point of certainty, to have already decayed, and if present in novae, have been in quantities too small to have been noted. Technetium was the first purportedly non-naturally occurring element synthesized, in 1937, although trace amounts of technetium have since been found in nature (and also the element may have been discovered naturally in 1925). This pattern of artificial production and later natural discovery has been repeated with several other radioactive naturally-occurring rare elements.
Lists of the elements are available by name, by symbol, by atomic number, by density, by melting point, and by boiling point as well as ionization energies of the elements. The nuclides of stable and radioactive elements are also available as a list of nuclides, sorted by length of half-life for those that are unstable. One of the most convenient, and certainly the most traditional presentation of the elements, is in the form of the periodic table, which groups together elements with similar chemical properties (and usually also similar electronic structures).
Atomic number.
The atomic number of an element is equal to the number of protons in each atom, and defines the element. For example, all carbon atoms contain 6 protons in their atomic nucleus; so the atomic number of carbon is 6. Carbon atoms may have different numbers of neutrons; atoms of the same element having different numbers of neutrons are known as isotopes of the element.
The number of protons in the atomic nucleus also determines its electric charge, which in turn determines the number of electrons of the atom in its non-ionized state. The electrons are placed into atomic orbitals that determine the atom's various chemical properties. The number of neutrons in a nucleus usually has very little effect on an element's chemical properties (except in the case of hydrogen and deuterium). Thus, all carbon isotopes have nearly identical chemical properties because they all have six protons and six electrons, even though carbon atoms may, for example, have 6 or 8 neutrons. That is why the atomic number, rather than mass number or atomic weight, is considered the identifying characteristic of a chemical element.
The symbol for atomic number is "Z".
Isotopes.
Isotopes are atoms of the same element (that is, with the same number of protons in their atomic nucleus), but having "different" numbers of neutrons. Most (66 of 94) naturally occurring elements have more than one stable isotope. Thus, for example, there are three main isotopes of carbon. All carbon atoms have 6 protons in the nucleus, but they can have either 6, 7, or 8 neutrons. Since the mass numbers of these are 12, 13 and 14 respectively, the three isotopes of carbon are known as carbon-12, carbon-13, and carbon-14, often abbreviated to 12C, 13C, and 14C. Carbon in everyday life and in chemistry is a mixture of 12C, 13C, and (a very small fraction of) 14C atoms.
Except in the case of the isotopes of hydrogen (which differ greatly from each other in relative mass—enough to cause chemical effects), the isotopes of a given element are chemically nearly indistinguishable.
All of the elements have some isotopes that are radioactive (radioisotopes), although not all of these radioisotopes occur naturally. The radioisotopes typically decay into other elements upon radiating an alpha or beta particle. If an element has isotopes that are not radioactive, these are termed "stable" isotopes. All of the known stable isotopes occur naturally (see primordial isotope). The many radioisotopes that are not found in nature have been characterized after being artificially made. Certain elements have no stable isotopes and are composed "only" of radioactive isotopes: specifically the elements without any stable isotopes are technetium (atomic number 43), promethium (atomic number 61), and all observed elements with atomic numbers greater than 82.
Of the 80 elements with at least one stable isotope, 26 have only one single stable isotope. The mean number of stable isotopes for the 80 stable elements is 3.1 stable isotopes per element. The largest number of stable isotopes that occur for a single element is 10 (for tin, element 50).
Isotopic mass and atomic mass.
The mass number of an element, "A", is the number of nucleons (protons and neutrons) in the atomic nucleus. Different isotopes of a given element are distinguished by their mass numbers, which are conventionally written as a superscript on the left hand side of the atomic symbol (e.g., 238U). The mass number is always a simple whole number and has units of "nucleons." An example of a referral to a mass number is "magnesium-24," which is an atom with 24 nucleons (12 protons and 12 neutrons).
Whereas the mass number simply counts the total number of neutrons and protons and is thus a natural (or whole) number, the atomic mass of a single atom is a real number for the mass of a particular isotope of the element, the unit being u. In general, when expressed in u it differs in value slightly from the mass number for a given nuclide (or isotope) since the mass of the protons and neutrons is not exactly 1 u, since the electrons contribute a lesser share to the atomic mass as neutron number exceeds proton number, and (finally) because of the nuclear binding energy. For example, the atomic mass of chlorine-35 to five significant digits is 34.969 u and that of chlorine-37 is 36.966 u. However, the atomic mass in u of each isotope is quite close to its simple mass number (always within 1%). The only isotope whose atomic mass is exactly a natural number is 12C, which by definition has a mass of exactly 12, because u is defined as 1/12 of the mass of a free neutral carbon-12 atom in the ground state.
The relative atomic mass (historically and commonly also called "atomic weight") of an element is the "average" of the atomic masses of all the chemical element's isotopes as found in a particular environment, weighted by isotopic abundance, relative to the atomic mass unit (u). This number may be a fraction that is "not" close to a whole number, due to the averaging process. For example, the relative atomic mass of chlorine is 35.453 u, which differs greatly from a whole number due to being made of an average of 76% chlorine-35 and 24% chlorine-37. Whenever a relative atomic mass value differs by more than 1% from a whole number, it is due to this averaging effect resulting from significant amounts of more than one isotope being naturally present in the sample of the element in question.
Chemically pure and isotopically pure.
Chemists and nuclear scientists have different definitions of a "pure element". In chemistry, a pure element means a substance whose atoms all (or in practice almost all) have the same atomic number, or number of protons. Nuclear scientists, however, define a pure element as one that consists of only one stable isotope.
For example, a copper wire is 99.99% chemically pure if 99.99% of its atoms are copper, with 29 protons each. However it is not isotopically pure since ordinary copper consists of two isotopes, 69% 63Cu and 31% 65Cu, with different numbers of neutrons.
Allotropes.
Atoms of chemically pure elements may bond to each other chemically in more than one way, allowing the pure element to exist in multiple structures (spacial arrangements of atoms), known as allotropes, which differ in their properties. For example, carbon can be found as diamond, which has a tetrahedral structure around each carbon atom; graphite, which has layers of carbon atoms with a hexagonal structure stacked on top of each other; graphene, which is a single layer of graphite that is very strong; fullerenes, which have nearly spherical shapes; and carbon nanotubes, which are tubes with a hexagonal structure (even these may differ from each other in electrical properties). The ability of an element to exist in one of many structural forms is known as 'allotropy'.
The standard state, also known as reference state, of an element is defined as its thermodynamically most stable state at 1 bar at a given temperature (typically at 298.15 K). In thermochemistry, an element is defined to have an enthalpy of formation of zero in its standard state. For example, the reference state for carbon is graphite, because the structure of graphite is more stable than that of the other allotropes.
Properties.
Several kinds of descriptive categorizations can be applied broadly to the elements, including consideration of their general physical and chemical properties, their states of matter under familiar conditions, their melting and boiling points, their densities, their crystal structures as solids, and their origins.
General properties.
Several terms are commonly used to characterize the general physical and chemical properties of the chemical elements. A first distinction is between metals, which readily conduct electricity, nonmetals, which do not, and a small group, (the "metalloids"), having intermediate properties and often behaving as semiconductors.
A more refined classification is often shown in colored presentations of the periodic table. This system restricts the terms "metal" and "nonmetal" to only certain of the more broadly defined metals and nonmetals, adding additional terms for certain sets of the more broadly viewed metals and nonmetals. The version of this classification used in the periodic tables presented here includes: actinides, alkali metals, alkaline earth metals, halogens, lanthanides, other metals; metalloids, noble gases, polyatomic nonmetals, diatomic nonmetals, and transition metals. In this system, the alkali metals, alkaline earth metals, and transition metals, as well as the lanthanides and the actinides, are special groups of the metals viewed in a broader sense. Similarly, the polyatomic nonmetals, diatomic nonmetals and the noble gases are nonmetals viewed in the broader sense. In some presentations, the halogens are not distinguished, with astatine identified as a metalloid and the others identified as nonmetals.
States of matter.
Another commonly used basic distinction among the elements is their state of matter (phase), whether solid, liquid, or gas, at a selected standard temperature and pressure (STP). Most of the elements are solids at conventional temperatures and atmospheric pressure, while several are gases. Only bromine and mercury are liquids at 0 degrees Celsius (32 degrees Fahrenheit) and normal atmospheric pressure; caesium and gallium are solids at that temperature, but melt at 28.4 °C (83.2 °F) and 29.8 °C (85.6 °F), respectively.
Melting and boiling points.
Melting and boiling points, typically expressed in degrees Celsius at a pressure of one atmosphere, are commonly used in characterizing the various elements. While known for most elements, either or both of these measurements is still undetermined for some of the radioactive elements available in only tiny quantities. Since helium remains a liquid even at absolute zero at atmospheric pressure, it has only a boiling point, and not a melting point, in conventional presentations.
Densities.
The density at a selected standard temperature and pressure (STP) is frequently used in characterizing the elements. Density is often expressed in grams per cubic centimeter (g/cm3). Since several elements are gases at commonly encountered temperatures, their densities are usually stated for their gaseous forms; when liquefied or solidified, the gaseous elements have densities similar to those of the other elements.
When an element has allotropes with different densities, one representative allotrope is typically selected in summary presentations, while densities for each allotrope can be stated where more detail is provided. For example, the three familiar allotropes of carbon (amorphous carbon, graphite, and diamond) have densities of 1.8–2.1, 2.267, and 3.515 g/cm3, respectively.
Crystal structures.
The elements studied to date as solid samples have eight kinds of crystal structures: cubic, body-centered cubic, face-centered cubic, hexagonal, monoclinic, orthorhombic, rhombohedral, and tetragonal. For some of the synthetically produced transuranic elements, available samples have been too small to determine crystal structures.
Occurrence and origin on Earth.
Chemical elements may also be categorized by their origin on Earth, with the first 98 considered naturally occurring, while those with atomic numbers beyond 98 have only been produced artificially as the synthetic products of man-made nuclear reactions.
Of the 98 naturally occurring elements, 84 are considered primordial and either stable or weakly radioactive. The remaining 14 naturally occurring elements possess half lives too short for them to have been present at the beginning of the Solar System, and are therefore considered transient elements. Of these 14 transient elements, 7 (polonium, astatine, radon, francium, radium, actinium, and protactinium) are relatively common decay products of thorium, uranium, and plutonium. The remaining 7 transient elements (technetium, promethium, neptunium, americium, curium, berkelium, and californium) occur only rarely, as products of rare nuclear reaction processes involving uranium or other heavy elements.
Elements with atomic numbers 1 through 40 are all stable, while those with atomic numbers 41 through 82 (except technetium and promethium) are metastable. The half-lives of these metastable "theoretical radionuclides" are so long (at least 100 million times longer than the estimated age of the universe) that their radioactive decay has yet to be detected by experiment. Elements with atomic numbers 83 through 98 are unstable to the point that their radioactive decay can be detected. Some of these elements, notably thorium (atomic number 90) and uranium (atomic number 92), have one or more isotopes with half-lives long enough to survive as remnants of the explosive stellar nucleosynthesis that produced the heavy elements before the formation of our solar system. For example, at over 1.9 years, over a billion times longer than the current estimated age of the universe, bismuth-209 (atomic number 83) has the longest known alpha decay half-life of any naturally occurring element. The very heaviest elements (those beyond californium, atomic number 98) undergo radioactive decay with short half-lives and do not occur in nature.
The periodic table.
The properties of the chemical elements are often summarized using the periodic table, which powerfully and elegantly organizes the elements by increasing atomic number into rows ("periods") in which the columns ("groups") share recurring ("periodic") physical and chemical properties. The current standard table contains 118 confirmed elements as of 10 April 2010.
Although earlier precursors to this presentation exist, its invention is generally credited to the Russian chemist Dmitri Mendeleev in 1869, who intended the table to illustrate recurring trends in the properties of the elements. The layout of the table has been refined and extended over time as new elements have been discovered and new theoretical models have been developed to explain chemical behavior.
Use of the periodic table is now ubiquitous within the academic discipline of chemistry, providing an extremely useful framework to classify, systematize and compare all the many different forms of chemical behavior. The table has also found wide application in physics, geology, biology, materials science, engineering, agriculture, medicine, nutrition, environmental health, and astronomy. Its principles are especially important in chemical engineering.
Nomenclature and symbols.
The various chemical elements are formally identified by their unique atomic numbers, by their accepted names, and by their symbols.
Atomic numbers.
The known elements have atomic numbers from 1 through 118, conventionally presented as Arabic numerals. Since the elements can be uniquely sequenced by atomic number, conventionally from lowest to highest (as in a periodic table), sets of elements are sometimes specified by such notation as "through", "beyond", or "from ... through", as in "through iron", "beyond uranium", or "from lanthanum through lutetium". The terms "light" and "heavy" are sometimes also used informally to indicate relative atomic numbers (not densities!), as in "lighter than carbon" or "heavier than lead", although technically the weight or mass of atoms of an element (their atomic weights or atomic masses) do not always increase monotonically with their atomic numbers.
Element names.
The naming of various substances now known as elements precedes the atomic theory of matter, as names were given locally by various cultures to various minerals, metals, compounds, alloys, mixtures, and other materials, although at the time it was not known which chemicals were elements and which compounds. As they were identified as elements, the existing names for anciently-known elements (e.g., gold, mercury, iron) were kept in most countries. National differences emerged over the names of elements either for convenience, linguistic niceties, or nationalism. For a few illustrative examples: German speakers use "Wasserstoff" (water substance) for "hydrogen", "Sauerstoff" (acid substance) for "oxygen" and "Stickstoff" (smothering substance) for "nitrogen", while English and some romance languages use "sodium" for "natrium" and "potassium" for "kalium", and the French, Italians, Greeks, Portuguese and Poles prefer "azote/azot/azoto" (from roots meaning "no life") for "nitrogen".
For purposes of international communication and trade, the official names of the chemical elements both ancient and more recently recognized are decided by the International Union of Pure and Applied Chemistry (IUPAC), which has decided on a sort of international English language, drawing on traditional English names even when an element's chemical symbol is based on a Latin or other traditional word, for example adopting "gold" rather than "aurum" as the name for the 79th element (Au). IUPAC prefers the British spellings "aluminium" and "caesium" over the U.S. spellings "aluminum" and "cesium", and the U.S. "sulfur" over the British "sulphur". However, elements that are practical to sell in bulk in many countries often still have locally used national names, and countries whose national language does not use the Latin alphabet are likely to use the IUPAC element names.
According to IUPAC, chemical elements are not proper nouns in English; consequently, the full name of an element is not routinely capitalized in English, even if derived from a proper noun, as in californium and einsteinium. Isotope names of chemical elements are also uncapitalized if written out, "e.g.," carbon-12 or uranium-235. Chemical element "symbols" are always capitalized (see below).
In the second half of the twentieth century, physics laboratories became able to produce nuclei of chemical elements with half-lives too short for an appreciable amount of them to exist at any time. These are also named by IUPAC, which generally adopts the name chosen by the discoverer. This practice can lead to the controversial question of which research group actually discovered an element, a question that has delayed naming of elements with atomic number of 104 and higher for a considerable time. (See element naming controversy).
Precursors of such controversies involved the nationalistic namings of elements in the late 19th century. For example, "lutetium" was named in reference to Paris, France. The Germans were reluctant to relinquish naming rights to the French, often calling it "cassiopeium". Similarly, the British discoverer of "niobium" originally named it "columbium," in reference to the New World. It was used extensively as such by American publications prior to international standardization.
Chemical symbols.
Specific chemical elements.
Before chemistry became a science, alchemists had designed arcane symbols for both metals and common compounds. These were however used as abbreviations in diagrams or procedures; there was no concept of atoms combining to form molecules. With his advances in the atomic theory of matter, John Dalton devised his own simpler symbols, based on circles, to depict molecules.
The current system of chemical notation was invented by Berzelius. In this typographical system, chemical symbols are not mere abbreviations—though each consists of letters of the Latin alphabet. They are intended as universal symbols for people of all languages and alphabets.
The first of these symbols were intended to be fully universal. Since Latin was the common language of science at that time, they were abbreviations based on the Latin names of metals. Cu comes from Cuprum, Fe comes from Ferrum, Ag from Argentum. The symbols were not followed by a period (full stop) as with abbreviations. Later chemical elements were also assigned unique chemical symbols, based on the name of the element, but not necessarily in English. For example, sodium has the chemical symbol 'Na' after the Latin "natrium". The same applies to "W" (wolfram) for tungsten, "Fe" (ferrum) for iron, "Hg" (hydrargyrum) for mercury, "Sn" (stannum) for tin, "K" (kalium) for potassium, "Au" (aurum) for gold, "Ag" (argentum) for silver, "Pb" (plumbum) for lead, "Cu" (cuprum) for copper, and "Sb" (stibium) for antimony.
Chemical symbols are understood internationally when element names might require translation. There are sometimes differences. For example, the Germans have used "J" instead of "I" for iodine, so the character would not be confused with a Roman numeral.
The first letter of a chemical symbol is always capitalized, as in the preceding examples, and the subsequent letters, if any, are always lower case (small letters). Thus, the symbols for californium or einsteinium are Cf and Es.
General chemical symbols.
There are also symbols in chemical equations for groups of chemical elements, for example in comparative formulas. These are often a single capital letter, and the letters are reserved and not used for names of specific elements. For example, an "X" indicates a variable group (usually a halogen) in a class of compounds, while "R" is a radical, meaning a compound structure such as a hydrocarbon chain. The letter "Q" is reserved for "heat" in a chemical reaction. "Y" is also often used as a general chemical symbol, although it is also the symbol of yttrium. "Z" is also frequently used as a general variable group. "E" is used in organic chemistry to denote an electron-withdrawing group. "L" is used to represent a general ligand in inorganic and organometallic chemistry. "M" is also often used in place of a general metal. 
At least two additional, two-letter generic chemical symbols are also in informal usage, "Ln" for any lanthanide element and "An" for any actinide element. "Rg" was formerly used for any rare gas element, but the group of rare gases has now been renamed noble gases and the symbol "Rg" has now been assigned to the element roentgenium.
Isotope symbols.
Isotopes are distinguished by the atomic mass number (total protons and neutrons) for a particular isotope of an element, with this number combined with the pertinent element's symbol. IUPAC prefers that isotope symbols be written in superscript notation when practical, for example 12C and 235U. However, other notations, such as carbon-12 and uranium-235, or C-12 and U-235, are also used.
As a special case, the three naturally occurring isotopes of the element hydrogen are often specified as H for 1H (protium), D for 2H (deuterium), and T for 3H (tritium). This convention is easier to use in chemical equations, replacing the need to write out the mass number for each atom. For example, the formula for heavy water may be written D2O instead of 2H2O.
Origin of the elements.
Only about 4% of the total mass of the universe is made of atoms or ions, and thus represented by chemical elements. This fraction is about 15% of the total matter, with the remainder of the matter (85%) being dark matter. The nature of dark matter is unknown, but it is not composed of atoms of chemical elements because it contains no protons, neutrons, or electrons. (The remaining non-matter part of the mass of the universe is composed of the even more mysterious dark energy).
The universe's 98 naturally occurring chemical elements are thought to have been produced by at least four cosmic processes. Most of the hydrogen and helium in the universe was produced primordially in the first few minutes of the Big Bang. Three recurrently occurring later processes are thought to have produced the remaining elements. Stellar nucleosynthesis, an ongoing process, produces all elements from carbon through iron in atomic number, but little lithium, beryllium, or boron. Elements heavier in atomic number than iron, as heavy as uranium and plutonium, are produced by explosive nucleosynthesis in supernovas and other cataclysmic cosmic events. Cosmic ray spallation (fragmentation) of carbon, nitrogen, and oxygen is important to the production of lithium, beryllium and boron.
During the early phases of the Big Bang, nucleosynthesis of hydrogen nuclei resulted in the production of hydrogen-1 (protium, 1H) and helium-4 (4He), as well as a smaller amount of deuterium (2H) and very minuscule amounts (on the order of 10−10) of lithium and beryllium. Even smaller amounts of boron may have been produced in the Big Bang, since it has been observed in some very old stars, while carbon has not. It is generally agreed that no heavier elements than boron were produced in the Big Bang. As a result, the primordial abundance of atoms (or ions) consisted of roughly 75% 1H, 25% 4He, and 0.01% deuterium, with only tiny traces of lithium, beryllium, and perhaps boron. Subsequent enrichment of galactic halos occurred due to stellar nucleosynthesis and supernova nucleosynthesis. However, the element abundance in intergalactic space can still closely resemble primordial conditions, unless it has been enriched by some means.
On Earth (and elsewhere), trace amounts of various elements continue to be produced from other elements as products of natural transmutation processes. These include some produced by cosmic rays or other nuclear reactions (see cosmogenic and nucleogenic nuclides), and others produced as decay products of long-lived primordial nuclides. For example, trace (but detectable) amounts of carbon-14 (14C) are continually produced in the atmosphere by cosmic rays impacting nitrogen atoms, and argon-40 (40Ar) is continually produced by the decay of primordially occurring but unstable potassium-40 (40K). Also, three primordially occurring but radioactive actinides, thorium, uranium, and plutonium, decay through a series of recurrently produced but unstable radioactive elements such as radium and radon, which are transiently present in any sample of these metals or their ores or compounds. Seven other radioactive elements, technetium, promethium, neptunium, americium, curium, berkelium, and californium, occur only incidentally in natural materials, produced as individual atoms by natural fission of the nuclei of various heavy elements or in other rare nuclear processes.
Human technology has produced various additional elements beyond these first 98, with those through atomic number 118 now known.
Abundance.
The following graph (note log scale) shows the abundance of elements in our solar system. The table shows the twelve most common elements in our galaxy (estimated spectroscopically), as measured in parts per million, by mass. Nearby galaxies that have evolved along similar lines have a corresponding enrichment of elements heavier than hydrogen and helium. The more distant galaxies are being viewed as they appeared in the past, so their abundances of elements appear closer to the primordial mixture. As physical laws and processes appear common throughout the visible universe, however, scientist expect that these galaxies evolved elements in similar abundance.
The abundance of elements in the Solar System is in keeping with their origin from nucleosynthesis in the Big Bang and a number of progenitor supernova stars. Very abundant hydrogen and helium are products of the Big Bang, but the next three elements are rare since they had little time to form in the Big Bang and are not made in stars (they are, however, produced in small quantities by the breakup of heavier elements in interstellar dust, as a result of impact by cosmic rays). Beginning with carbon, elements are produced in stars by buildup from alpha particles (helium nuclei), resulting in an alternatingly larger abundance of elements with even atomic numbers (these are also more stable). In general, such elements up to iron are made in large stars in the process of becoming supernovas. Iron-56 is particularly common, since it is the most stable element that can easily be made from alpha particles (being a product of decay of radioactive nickel-56, ultimately made from 14 helium nuclei). Elements heavier than iron are made in energy-absorbing processes in large stars, and their abundance in the universe (and on Earth) generally decreases with their atomic number.
The abundance of the chemical elements on Earth varies from air to crust to ocean, and in various types of life. The abundance of elements in Earth's crust differs from that in the universe (and also in the Sun and heavy planets like Jupiter) mainly in selective loss of the very lightest elements (hydrogen and helium) and also volatile neon, carbon, nitrogen and sulfur, as a result of solar heating in the early formation of the solar system. Aluminum is also far more common in the Earth and Earth's crust than in the universe and solar system, but the composition of Earth's mantle (which has more magnesium and iron in place of aluminum) more closely mirrors that of the universe, save for the noted loss of volatile elements.
The composition of the human body, by contrast, more closely follows the composition of seawater—save that the human body has additional stores of carbon and nitrogen necessary to form the proteins and nucleic acids characteristic of living organisms. Certain kinds of organisms require particular additional elements, for example the magnesium in chlorophyll in green plants, the calcium in mollusc shells, or the iron in the hemoglobin in vertebrate animals' red blood cells.
History.
Evolving definitions.
The concept of an "element" as an undivisible substance has developed through three major historical phases: Classical definitions (such as those of the ancient Greeks), chemical definitions, and atomic definitions.
Classical definitions.
Ancient philosophy posited a set of classical elements to explain observed patterns in nature. These "elements" originally referred to "earth", "water", "air" and "fire" rather than the chemical elements of modern science.
The term 'elements' ("stoicheia") was first used by the Greek philosopher Plato in about 360 BCE in his dialogue Timaeus, which includes a discussion of the composition of inorganic and organic bodies and is a speculative treatise on chemistry. Plato believed the elements introduced a century earlier by Empedocles were composed of small polyhedral forms: tetrahedron (fire), octahedron (air), icosahedron (water), and cube (earth).
Aristotle, c. 350 BCE, also used the term "stoicheia" and added a fifth element called aether, which formed the heavens. Aristotle defined an element as:
Chemical definitions.
In 1661, Robert Boyle proposed his theory of corpuscularism which favoured the analysis of matter as constituted by irreducible units of matter (atoms) and, choosing to side with neither Aristotle's view of the four elements nor Paracelsus' view of three fundamental elements, left open the question of the number of elements. The first modern list of chemical elements was given in Antoine Lavoisier's 1789 "Elements of Chemistry", which contained thirty-three elements, including light and caloric. By 1818, Jöns Jakob Berzelius had determined atomic weights for forty-five of the forty-nine then-accepted elements. Dmitri Mendeleev had sixty-six elements in his periodic table of 1869.
From Boyle until the early 20th century, an element was defined as a pure substance that could not be decomposed into any simpler substance. Put another way, a chemical element cannot be transformed into other chemical elements by chemical processes. Elements during this time were generally distinguished by their atomic weights, a property measurable with fair accuracy by available analytical techniques.
Atomic definitions.
The 1913 discovery by English physicist Henry Moseley that the nuclear charge is the physical basis for an atom's atomic number, further refined when the nature of protons and neutrons became appreciated, eventually led to the current definition of an element based on atomic number (number of protons per atomic nucleus). The use of atomic numbers, rather than atomic weights, to distinguish elements has greater predictive value (since these numbers are integers), and also resolves some ambiguities in the chemistry-based view due to varying properties of isotopes and allotropes within the same element. Currently, IUPAC defines an element to exist if it has isotopes with a lifetime longer than the 10−14 seconds it takes the nucleus to form an electronic cloud.
By 1914, seventy-two elements were known, all naturally occurring. The remaining naturally occurring elements were discovered or isolated in subsequent decades, and various additional elements have also been produced synthetically, with much of that work pioneered by Glenn T. Seaborg. In 1955, element 101 was discovered and named mendelevium in honor of D.I. Mendeleev, the first to arrange the elements in a periodic manner. Most recently, the synthesis of element 118 was reported in October 2006, and the synthesis of element 117 was reported in April 2010.
Discovery and recognition of various elements.
Ten materials familiar to various prehistoric cultures are now known to be chemical elements: Carbon, copper, gold, iron, lead, mercury, silver, sulfur, tin, and zinc. Three additional materials now accepted as elements, arsenic, antimony, and bismuth, were recognized as distinct substances prior to 1500 AD. Phosphorus, cobalt, and platinum were isolated before 1750.
Most of the remaining naturally occurring chemical elements were identified and characterized by 1900, including:
Elements isolated or produced since 1900 include:
Recently discovered elements.
The first transuranium element (element with atomic number greater than 92) discovered was neptunium in 1940. Since 1999 claims for the discovery of new elements have been considered by the IUPAC/IUPAP Joint Working Party. As of May 2012, only the elements up to 112, copernicium, as well as element 114 Flerovium and element 116 Livermorium have been confirmed as discovered by IUPAC, while claims have been made for synthesis of elements 113, 115, 117 and 118. The discovery of element 112 was acknowledged in 2009, and the name 'copernicium' and the atomic symbol 'Cn' were suggested for it. The name and symbol were officially endorsed by IUPAC on 19 February 2010. The heaviest element that is believed to have been synthesized to date is element 118, ununoctium, on 9 October 2006, by the Flerov Laboratory of Nuclear Reactions in Dubna, Russia. Element 117 was the latest element claimed to be discovered, in 2009. IUPAC officially recognized flerovium and livermorium, elements 114 and 116, in June 2011 and approved their names in May 2012.
List of the 118 known chemical elements.
The following sortable table includes the 118 known chemical elements, with the names linking to the "Wikipedia" articles on each.

</doc>
<doc id="5661" url="http://en.wikipedia.org/wiki?curid=5661" title="Centime">
Centime

Centime (from Latin "centesimus") is French for "cent", and is used in English as the name of the fraction currency in several Francophone countries (including Switzerland, Algeria, Belgium, Morocco and France).
In France the usage of "centime" goes back to the introduction of the decimal monetary system under Napoleon. This system aimed at replacing non-decimal fractions of older coins. A five-centime coin was known as a "sou", i.e. a solidus or shilling.
Conversely in francophone Canada, one hundredth of a Canadian dollar is informally called a "sou" (penny), though "cent" is official in both English and French.
Subdivision of euro: cent or centime?
In the European community "cent" is the official name for one hundredth of a euro. However, in French-speaking countries the word "centime " is the preferred term. Indeed, the Superior Council of the French language of Belgium recommended in 2001 the use of "centime", since "cent" is also the French word for "hundred". An analogous decision was published in the "Journal officiel" in France (December 2, 1997).
In Morocco, dirhams are divided into 100 "centime"s and one may find prices in the country quoted in "centime"s rather than in dirhams. Sometimes "centime"s are known as francs or in former Spanish areas, pesetas.
Usage.
A centime is one-hundredth of the following basic monetary units:

</doc>
<doc id="5662" url="http://en.wikipedia.org/wiki?curid=5662" title="Calendar year">
Calendar year

Generally speaking, a calendar year begins on the New Year's Day of the given calendar system and ends on the day before the following New Year's Day. By convention, a calendar year consists of a natural number of days. To reconcile the calendar year with an astronomical cycle (which could not possibly be reckoned in a whole number of days), certain years contain extra days. Through further insertion of non-day units of time, the calendar year can be further synchronized; however, these extra units of time are not considered part of the calendar.
The Gregorian year begins on January 1 and ends on December 31 with a length of 365 days in an ordinary year with 8760 hours, 525600 minutes, and 31536000 seconds and 366 days in a leap year with 8784 hours, 527040 minutes, and 31622400 seconds giving an average length of 365.2425 days. Other formula-based calendars can have lengths which are further out of step with the solar cycle, for example, the Julian calendar has an average length of 365.25 days, and the Hebrew calendar has an average length of 365.2468 days.
The astronomer's mean tropical year which is averaged over equinoxes and solstices is currently 365.24219 days, slightly shorter than the average length of the year in most calendars, but the astronomer's value changes over time, so William Herschel's suggested correction to the Gregorian calendar may become unnecessary by the year 4000.
Quarters.
The calendar year can be divided into 4 quarters, often abbreviated Q1, Q2, Q3 and Q4:

</doc>
<doc id="5663" url="http://en.wikipedia.org/wiki?curid=5663" title="CFA franc">
CFA franc

The CFA franc (in French: "franc CFA" , or colloquially "franc") is the name of two currencies used in Africa which are guaranteed by the French treasury. The two CFA franc currencies are the West African CFA franc and the Central African CFA franc. Although theoretically separate, the two CFA franc currencies are effectively interchangeable.
Both CFA Francs currently have a fixed exchange rate to the euro: 100 CFA francs = 1 former French (nouveau) franc = 0.152449 euro; or 1 euro = 655.957 CFA francs exactly.
Although Central African CFA francs and West African CFA francs have always been at parity and have therefore always had the same monetary value against other currencies, they are in principle separate currencies. They could theoretically have different values from any moment if one of the two CFA monetary authorities, or France, decided it. Therefore West African CFA coins and banknotes are "theoretically" not accepted in countries using Central African CFA francs, and vice versa. However, in practice, the permanent parity of the two CFA franc currencies is widely assumed.
CFA Francs are used in fourteen countries: twelve formerly French-ruled African countries, as well as in Guinea-Bissau (a former Portuguese colony) and in Equatorial Guinea (a former Spanish colony). These fourteen countries have a combined population of 147.5 million people (as of 2013), and a combined GDP of US$166.6 billion (as of 2012). The ISO currency codes are XAF for the Central African CFA franc and XOF for the West African CFA franc.
The currency has been criticized for making economic planning for the developing countries of French West-Africa all but impossible since the CFA's value is pegged to the Euro (whose monetary policy is set by the European Central Bank). Others disagree and argue that the CFA "helps stabilize the national currencies of Franc Zone member-countries and greatly facilitates the flow of exports and imports between France and the member-countries."The European Union's own assessment of the CFA's link to the Euro, carried out in 2008, noted that "benefits from economic integration within each of the two monetary unions of the CFA franc zone, and even more so between them, remained remarkably low" but that "the peg to the French franc and, since 1999, to the euro as exchange rate anchor is usually found to have had favourable effects in the region in terms of macroeconomic stability.
Name.
Between 1945 and 1958, CFA stood for ' ("French colonies of Africa"); then for ' ("French Community of Africa") between 1958 (establishment of the French Fifth Republic) and the independence of these African countries at the beginning of the 1960s. Since independence, CFA is taken to mean "" (African Financial Community), but in actual use, the term can have two meanings (see Institutions below).
History.
Creation.
The CFA franc was created on 26 December 1945, along with the CFP franc. The reason for their creation was the weakness of the French franc immediately after World War II. When France ratified the Bretton Woods Agreement in December 1945, the French franc was devalued in order to set a fixed exchange rate with the US dollar. New currencies were created in the French colonies to spare them the strong devaluation, thereby facilitating exports to France. French officials presented the decision as an act of generosity. René Pleven, the French minister of finance, was quoted as saying:
Exchange rate.
The CFA franc was created with a fixed exchange rate versus the French franc. This exchange rate was changed only twice: in 1948 and in 1994.
Exchange rate:
The 1960 and 1999 events were merely changes in the currency in use in France: the relative value of the CFA franc versus the French franc / euro changed only in 1948 and 1994.
The value of the CFA franc has been widely criticized as being too high, which many economists believe favours the urban elite of the African countries, which can buy imported manufactured goods cheaply at the expense of farmers who cannot easily export agricultural products. The devaluation of 1994 was an attempt to reduce these imbalances.
Changes in countries using the franc.
Over time, the number of countries and territories using the CFA franc has changed as some countries began introducing their own separate currencies. A couple of nations in West Africa have also chosen to adopt the CFA franc since its introduction, despite the fact that they were never French colonies.
European Monetary Union.
In 1998, in anticipation of Economic and Monetary Union of the European Union, the Council of the European Union addressed the monetary agreements France has with the CFA Zone and Comoros and ruled that:
Institutions.
Strictly speaking, there are actually two different currencies called the CFA franc: the West African CFA franc (ISO 4217 currency code XOF), and the Central Africa CFA franc (ISO 4217 currency code XAF). They are distinguished in French by the meaning of the abbreviation CFA. These two CFA francs have the same exchange rate with the euro (1 euro = 655.957 XOF = 655.957 XAF), and they are both guaranteed by the French treasury (), but the West African CFA franc cannot be used in Central African countries, and the Central Africa CFA franc cannot be used in West African countries.
West African.
The West African CFA franc (XOF) is just known in French as the , where CFA stands for "Communauté financière d'Afrique" ("Financial Community of Africa") or ("African Financial Community"). It is issued by the BCEAO (, i.e. "Central Bank of the West African States"), located in Dakar, Senegal, for the 8 countries of the UEMOA (, i.e. "West African Economic and Monetary Union"):
These 8 countries have a combined population of 102.5 million people (as of 2013), and a combined GDP of US$78.4 billion (as of 2012).
Central African.
The Central Africa CFA franc (XAF) is known in French as the , where CFA stands for ("Financial Cooperation in Central Africa"). It is issued by the BEAC (, i.e. "Bank of the Central African States"), located in Yaoundé, Cameroon, for the 6 countries of the CEMAC (, i.e. "Economic and Monetary Community of Central Africa"):
These 6 countries have a combined population of 45.0 million people (as of 2013), and a combined GDP of US$88.2 billion (as of 2012).
In 1975, Central African CFA banknotes were issued with an obverse unique to each participating country, and common reverse, in a fashion similar to euro coins.
Equatorial Guinea, the only former Spanish colony in the zone, adopted the CFA in 1984.

</doc>
<doc id="5664" url="http://en.wikipedia.org/wiki?curid=5664" title="Consciousness">
Consciousness

Consciousness is the quality or state of awareness, or, of being aware of an external object or something within oneself. It has been defined as: sentience, awareness, subjectivity, the ability to experience or to feel, wakefulness, having a sense of selfhood, and the executive control system of the mind. Despite the difficulty in definition, many philosophers believe that there is a broadly shared underlying intuition about what consciousness is. As Max Velmans and Susan Schneider wrote in "The Blackwell Companion to Consciousness": "Anything that we are aware of at a given moment forms part of our consciousness, making conscious experience at once the most familiar and most mysterious aspect of our lives."
Philosophers since the time of Descartes and Locke have struggled to comprehend the nature of consciousness and pin down its essential properties. Issues of concern in the philosophy of consciousness include whether the concept is fundamentally coherent; whether consciousness can ever be explained mechanistically; whether non-human consciousness exists and if so how it can be recognized; how consciousness relates to language; whether consciousness can be understood in a way that does not require a dualistic distinction between mental and physical states or properties; and whether it may ever be possible for computing machines like computers or robots to be conscious, a topic studied in the field of artificial intelligence.
At one time consciousness was viewed with skepticism by many scientists, but in recent years it has become a significant topic of research in psychology, neuropsychology, and neuroscience. The primary focus is on understanding what it means biologically and psychologically for information to be present in consciousness—that is, on determining the neural and psychological correlates of consciousness. The majority of experimental studies assess consciousness by asking human subjects for a verbal report of their experiences (e.g., "tell me if you notice anything when I do this"). Issues of interest include phenomena such as subliminal perception, blindsight, denial of impairment, and altered states of consciousness produced by drugs and alcohol, or spiritual or meditative techniques.
In medicine, consciousness is assessed by observing a patient's arousal and responsiveness, and can be seen as a continuum of states ranging from full alertness and comprehension, through disorientation, delirium, loss of meaningful communication, and finally loss of movement in response to painful stimuli. Issues of practical concern include how the presence of consciousness can be assessed in severely ill, comatose, or anesthetized people, and how to treat conditions in which consciousness is impaired or disrupted.
Etymology and early history.
The origin of the modern concept of consciousness is often attributed to John Locke's "Essay Concerning Human Understanding", published in 1690. Locke defined consciousness as "the perception of what passes in a man's own mind". His essay influenced the 18th-century view of consciousness, and his definition appeared in Samuel Johnson's celebrated "Dictionary" (1755).
The earliest English language uses of "conscious" and "consciousness" date back, however, to the 1500s. The English word "conscious" originally derived from the Latin "conscius" ("con-" "together" and "scio" "to know"), but the Latin word did not have the same meaning as our word—it meant "knowing with", in other words "having joint or common knowledge with another". There were, however, many occurrences in Latin writings of the phrase "conscius sibi", which translates literally as "knowing with oneself", or in other words "sharing knowledge with oneself about something". This phrase had the figurative meaning of "knowing that one knows", as the modern English word "conscious" does. In its earliest uses in the 1500s, the English word "conscious" retained the meaning of the Latin "conscius". For example, Thomas Hobbes in "Leviathan" wrote: "Where two, or more men, know of one and the same fact, they are said to be Conscious of it one to another." The Latin phrase "conscius sibi", whose meaning was more closely related to the current concept of consciousness, was rendered in English as "conscious to oneself" or "conscious unto oneself". For example, Archbishop Ussher wrote in 1613 of "being so conscious unto myself of my great weakness". Locke's definition from 1690 illustrates that a gradual shift in meaning had taken place.
A related word was "conscientia", which primarily means moral conscience. In the literal sense, "conscientia" means knowledge-with, that is, shared knowledge. The word first appears in Latin juridical texts by writers such as Cicero. Here, "conscientia" is the knowledge that a witness has of the deed of someone else. René Descartes (1596–1650) is generally taken to be the first philosopher to use "conscientia" in a way that does not fit this traditional meaning. Descartes used "conscientia" the way modern speakers would use "conscience". In "Search after Truth" (""; publ. 1701) he says "conscience or internal testimony" ("conscientiâ, vel interno testimonio").
In the dictionary.
The dictionary meaning of the word "consciousness" extends through several centuries and associated cognate meanings which have ranged from formal definitions to somewhat more skeptical definitions. One formal definition indicating the range of these cognate meanings is given in "Webster's Third New International Dictionary" stating that "consciousness" is: "(1) a. awareness or perception of an inward psychological or spiritual fact: intuitively perceived knowledge of something in one's inner self. b. inward awareness of an external object, state, or fact. c: concerned awareness: INTEREST, CONCERN -- often used with an attributive noun. (2): the state or activity that is characterized by sensation, emotion, volition, or thought: mind in the broadest possible sense: something in nature that is distinguished from the physical. (3): the totality in psychology of sensations, perceptions, ideas, attitudes and feelings of which an individual or a group is aware at any given time or within a particular time span -- compare STREAM OF CONSCIOUSNESS."
Philosophy of mind.
The philosophy of mind has given rise to many stances regarding consciousness. The "Routledge Encyclopedia of Philosophy" in 1998 defines consciousness as follows:
In a more skeptical definition of "consciousness", Stuart Sutherland has exemplified some of the difficulties in fully ascertaining all of its cognate meanings in his entry for the 1989 version of the "Macmillan Dictionary of Psychology":
Most writers on the philosophy of consciousness have been concerned to defend a particular point of view, and have organized their material accordingly. For surveys, the most common approach is to follow a historical path by associating stances with the philosophers who are most strongly associated with them, for example Descartes, Locke, Kant, etc. An alternative is to organize philosophical stances according to basic issues.
The coherence of the concept.
Philosophers and non-philosophers differ in their intuitions about what consciousness is. While most people have a strong intuition for the existence of what they refer to as consciousness, skeptics argue that this intuition is false, either because the concept of consciousness is intrinsically incoherent, or because our intuitions about it are based in illusions. Gilbert Ryle, for example, argued that traditional understanding of consciousness depends on a Cartesian dualist outlook that improperly distinguishes between mind and body, or between mind and world. He proposed that we speak not of minds, bodies, and the world, but of individuals, or persons, acting in the world. Thus, by speaking of "consciousness" we end up misleading ourselves by thinking that there is any sort of thing as consciousness separated from behavioral and linguistic understandings. More generally, many philosophers and scientists have been unhappy about the difficulty of producing a definition that does not involve circularity or fuzziness.
Types of consciousness.
Many philosophers have argued that consciousness is a unitary concept that is understood intuitively by the majority of people in spite of the difficulty in defining it. Others, though, have argued that the level of disagreement about the meaning of the word indicates that it either means different things to different people (for instance, the objective versus subjective aspects of consciousness), or else is an umbrella term encompassing a variety of distinct meanings with no simple element in common.
Ned Block proposed a distinction between two types of consciousness that he called "phenomenal" (P-consciousness) and "access" (A-consciousness). P-consciousness, according to Block, is simply raw experience: it is moving, colored forms, sounds, sensations, emotions and feelings with our bodies and responses at the center. These experiences, considered independently of any impact on behavior, are called "qualia". A-consciousness, on the other hand, is the phenomenon whereby information in our minds is accessible for verbal report, reasoning, and the control of behavior. So, when we perceive, information about what we perceive is access conscious; when we introspect, information about our thoughts is access conscious; when we remember, information about the past is access conscious, and so on. Although some philosophers, such as Daniel Dennett, have disputed the validity of this distinction, others have broadly accepted it. David Chalmers has argued that A-consciousness can in principle be understood in mechanistic terms, but that understanding P-consciousness is much more challenging: he calls this the "hard problem of consciousness".
Some philosophers believe that Block's two types of consciousness are not the end of the story. William Lycan, for example, argued in his book "Consciousness and Experience" that at least eight clearly distinct types of consciousness can be identified (organism consciousness; control consciousness; consciousness "of"; state/event consciousness; reportability; introspective consciousness; subjective consciousness; self-consciousness)—and that even this list omits several more obscure forms.
Mind–body problem.
The first influential philosopher to discuss this question specifically was Descartes, and the answer he gave is known as Cartesian dualism. Descartes proposed that consciousness resides within an immaterial domain he called res cogitans (the realm of thought), in contrast to the domain of material things, which he called res extensa (the realm of extension). He suggested that the interaction between these two domains occurs inside the brain, perhaps in a small midline structure called the pineal gland.
Although it is widely accepted that Descartes explained the problem cogently, few later philosophers have been happy with his solution, and his ideas about the pineal gland have especially been ridiculed. Alternative solutions, however, have been very diverse. They can be divided broadly into two categories: dualist solutions that maintain Descartes' rigid distinction between the realm of consciousness and the realm of matter but give different answers for how the two realms relate to each other; and monist solutions that maintain that there is really only one realm of being, of which consciousness and matter are both aspects. Each of these categories itself contains numerous variants. The two main types of dualism are substance dualism (which holds that the mind is formed of a distinct type of substance not governed by the laws of physics) and property dualism (which holds that the laws of physics are universally valid but cannot be used to explain the mind). The three main types of monism are physicalism (which holds that the mind consists of matter organized in a particular way), idealism (which holds that only thought or experience truly exists, and matter is merely an illusion), and neutral monism (which holds that both mind and matter are aspects of a distinct essence that is itself identical to neither of them). There are also, however, a large number of idiosyncratic theories that cannot cleanly be assigned to any of these camps.
Since the dawn of Newtonian science with its vision of simple mechanical principles governing the entire universe, some philosophers have been tempted by the idea that consciousness could be explained in purely physical terms. The first influential writer to propose such an idea explicitly was Julien Offray de La Mettrie, in his book "Man a Machine" ("L'homme machine"). His arguments, however, were very abstract. The most influential modern physical theories of consciousness are based on psychology and neuroscience. Theories proposed by neuroscientists such as Gerald Edelman and Antonio Damasio, and by philosophers such as Daniel Dennett, seek to explain consciousness in terms of neural events occurring within the brain. Many other neuroscientists, such as Christof Koch, have explored the neural basis of consciousness without attempting to frame all-encompassing global theories. At the same time, computer scientists working in the field of artificial intelligence have pursued the goal of creating digital computer programs that can simulate or embody consciousness.
A few theoretical physicists have argued that classical physics is intrinsically incapable of explaining the holistic aspects of consciousness, but that quantum theory may provide the missing ingredients. Several theorists have therefore proposed quantum mind (QM) theories of consciousness. Notable theories falling into this category include the holonomic brain theory of Karl Pribram and David Bohm, and the Orch-OR theory formulated by Stuart Hameroff and Roger Penrose. Some of these QM theories offer descriptions of phenomenal consciousness, as well as QM interpretations of access consciousness. None of the quantum mechanical theories has been confirmed by experiment. Recent publications by G. Guerreshi, J. Cia, S. Popescu, and H. Briegel could falsify proposals such as those of Hameroff, which rely on quantum entanglement in protein. At the present time many scientists and philosophers consider the arguments for an important role of quantum phenomena to be unconvincing.
Apart from the general question of the "hard problem" of consciousness, roughly speaking, the question of how mental experience arises from a physical basis, a more specialized question is how to square the subjective notion that we are in control of our decisions (at least in some small measure) with the customary view of causality that subsequent events are caused by prior events. The topic of free will is the philosophical and scientific examination of this conundrum.
Problem of other minds.
Many philosophers consider experience to be the essence of consciousness, and believe that experience can only fully be known from the inside, subjectively. But if consciousness is subjective and not visible from the outside, why do the vast majority of people believe that other people are conscious, but rocks and trees are not? This is called the problem of other minds. It is particularly acute for people who believe in the possibility of philosophical zombies, that is, people who think it is possible in principle to have an entity that is physically indistinguishable from a human being and behaves like a human being in every way but nevertheless lacks consciousness. Related issues have also been studied extensively by Greg Littmann of the University of Illinois. and Colin Allen a professor at Indiana University regarding the literature and research studying artificial intelligence in androids.
The most commonly given answer is that we attribute consciousness to other people because we see that they resemble us in appearance and behavior: we reason that if they look like us and act like us, they must be like us in other ways, including having experiences of the sort that we do. There are, however, a variety of problems with that explanation. For one thing, it seems to violate the principle of parsimony, by postulating an invisible entity that is not necessary to explain what we observe. Some philosophers, such as Daniel Dennett in an essay titled "The Unimagined Preposterousness of Zombies", argue that people who give this explanation do not really understand what they are saying. More broadly, philosophers who do not accept the possibility of zombies generally believe that consciousness is reflected in behavior (including verbal behavior), and that we attribute consciousness on the basis of behavior. A more straightforward way of saying this is that we attribute experiences to people because of what they can "do", including the fact that they can tell us about their experiences.
Animal consciousness.
The topic of animal consciousness is beset by a number of difficulties. It poses the problem of other minds in an especially severe form, because animals, lacking the ability to express human language, cannot tell us about their experiences. Also, it is difficult to reason objectively about the question, because a denial that an animal is conscious is often taken to imply that it does not feel, its life has no value, and that harming it is not morally wrong. Descartes, for example, has sometimes been blamed for mistreatment of animals due to the fact that he believed only humans have a non-physical mind. Most people have a strong intuition that some animals, such as cats and dogs, are conscious, while others, such as insects, are not; but the sources of this intuition are not obvious, and are often based on personal interactions with pets and other animals they have observed.
Philosophers who consider subjective experience the essence of consciousness also generally believe, as a correlate, that the existence and nature of animal consciousness can never rigorously be known. Thomas Nagel spelled out this point of view in an influential essay titled "What Is it Like to Be a Bat?". He said that an organism is conscious "if and only if there is something that it is like to be that organism — something it is like "for" the organism"; and he argued that no matter how much we know about an animal's brain and behavior, we can never really put ourselves into the mind of the animal and experience its world in the way it does itself. Other thinkers, such as Douglas Hofstadter, dismiss this argument as incoherent. Several psychologists and ethologists have argued for the existence of animal consciousness by describing a range of behaviors that appear to show animals holding beliefs about things they cannot directly perceive — Donald Griffin's 2001 book "Animal Minds" reviews a substantial portion of the evidence.
Artifact consciousness.
The idea of an artifact made conscious is an ancient theme of mythology, appearing for example in the Greek myth of Pygmalion, who carved a statue that was magically brought to life, and in medieval Jewish stories of the Golem, a magically animated homunculus built of clay. However, the possibility of actually constructing a conscious machine was probably first discussed by Ada Lovelace, in a set of notes written in 1842 about the Analytical Engine invented by Charles Babbage, a precursor (never built) to modern electronic computers. Lovelace was essentially dismissive of the idea that a machine such as the Analytical Engine could think in a humanlike way. She wrote:
One of the most influential contributions to this question was an essay written in 1950 by pioneering computer scientist Alan Turing, titled "Computing Machinery and Intelligence". Turing disavowed any interest in terminology, saying that even "Can machines think?" is too loaded with spurious connotations to be meaningful; but he proposed to replace all such questions with a specific operational test, which has become known as the Turing test. To pass the test, a computer must be able to imitate a human well enough to fool interrogators. In his essay Turing discussed a variety of possible objections, and presented a counterargument to each of them. The Turing test is commonly cited in discussions of artificial intelligence as a proposed criterion for machine consciousness; it has provoked a great deal of philosophical debate. For example, Daniel Dennett and Douglas Hofstadter argue that anything capable of passing the Turing test is necessarily conscious, while David Chalmers argues that a philosophical zombie could pass the test, yet fail to be conscious. A third group of scholars have argued that with technological growth once machines begin to display any substantial signs of human-like behavior then the dichotomy (of human consciousness compared to human-like consciousness) becomes passé and issues of machine autonomy begin to prevail even as observed in its nascent form within contemporary industry and technology.
In a lively exchange over what has come to be referred to as "the Chinese room argument", John Searle sought to refute the claim of proponents of what he calls "strong artificial intelligence (AI)" that a computer program can be conscious, though he does agree with advocates of "weak AI" that computer programs can be formatted to "simulate" conscious states. His own view is that consciousness has subjective, first-person causal powers by being essentially intentional due simply to the way human brains function biologically; conscious persons can perform computations, but consciousness is not inherently computational the way computer programs are. To make a Turing machine that speaks Chinese, Searle imagines a room stocked with computers and algorithms programmed to respond to Chinese questions, i.e., Turing machines, programmed to correctly answer in Chinese any questions asked in Chinese. Searle argues that with such a machine, he would be able to process the inputs to outputs perfectly without having any understanding of Chinese, nor having any idea what the questions and answers could possibly mean. And this is all a current computer program would do. If the experiment were done in English, since Searle knows English, he would be able to take questions and give answers without any algorithms for English questions, and he would be affectively aware of what was being said and the purposes it might serve. Searle would pass the Turing test of answering the questions in both languages, but he is only conscious of what he is doing when he speaks English. Another way of putting the argument is to say that computational computer programs can pass the Turing test for processing the syntax of a language, but that semantics cannot be reduced to syntax in the way strong AI advocates hoped. Processing semantics is conscious and intentional because we use semantics to consciously produce meaning by what we say.
In the literature concerning artificial intelligence, Searle's essay has been second only to Turing's in the volume of debate it has generated. Searle himself was vague about what extra ingredients it would take to make a machine conscious: all he proposed was that what was needed was "causal powers" of the sort that the brain has and that computers lack. But other thinkers sympathetic to his basic argument have suggested that the necessary (though perhaps still not sufficient) extra conditions may include the ability to pass not just the verbal version of the Turing test, but the robotic version, which requires grounding the robot's words in the robot's sensorimotor capacity to categorize and interact with the things in the world that its words are about, Turing-indistinguishably from a real person. Turing-scale robotics is an empirical branch of research on embodied cognition and situated cognition.
Scientific study.
For many decades, consciousness as a research topic was avoided by the majority of mainstream scientists, because of a general feeling that a phenomenon defined in subjective terms could not properly be studied using objective experimental methods. In 1975 George Mandler published an influential psychological study which distinguished between slow, serial, and limited conscious processes and fast, parallel and extensive unconscious ones. Starting in the 1980s, an expanding community of neuroscientists and psychologists have associated themselves with a field called "Consciousness Studies", giving rise to a stream of experimental work published in books, journals such as "Consciousness and Cognition", and methodological work published in journals such as the "Journal of Consciousness Studies", along with regular conferences organized by groups such as the Association for the Scientific Study of Consciousness.
Modern medical and psychological investigations into consciousness are based on psychological experiments (including, for example, the investigation of priming effects using subliminal stimuli), and on case studies of alterations in consciousness produced by trauma, illness, or drugs. Broadly viewed, scientific approaches are based on two core concepts. The first identifies the content of consciousness with the experiences that are reported by human subjects; the second makes use of the concept of consciousness that has been developed by neurologists and other medical professionals who deal with patients whose behavior is impaired. In either case, the ultimate goals are to develop techniques for assessing consciousness objectively in humans as well as other animals, and to understand the neural and psychological mechanisms that underlie it.
Measurement.
Experimental research on consciousness presents special difficulties, due to the lack of a universally accepted operational definition. In the majority of experiments that are specifically about consciousness, the subjects are human, and the criterion that is used is verbal report: in other words, subjects are asked to describe their experiences, and their descriptions are treated as observations of the contents of consciousness. For example, subjects who stare continuously at a Necker cube usually report that they experience it "flipping" between two 3D configurations, even though the stimulus itself remains the same. The objective is to understand the relationship between the conscious awareness of stimuli (as indicated by verbal report) and the effects the stimuli have on brain activity and behavior. In several paradigms, such as the technique of response priming, the behavior of subjects is clearly influenced by stimuli for which they report no awareness.
Verbal report is widely considered to be the most reliable indicator of consciousness, but it raises a number of issues. For one thing, if verbal reports are treated as observations, akin to observations in other branches of science, then the possibility arises that they may contain errors—but it is difficult to make sense of the idea that subjects could be wrong about their own experiences, and even more difficult to see how such an error could be detected. Daniel Dennett has argued for an approach he calls heterophenomenology, which means treating verbal reports as stories that may or may not be true, but his ideas about how to do this have not been widely adopted. Another issue with verbal report as a criterion is that it restricts the field of study to humans who have language: this approach cannot be used to study consciousness in other species, pre-linguistic children, or people with types of brain damage that impair language. As a third issue, philosophers who dispute the validity of the Turing test may feel that it is possible, at least in principle, for verbal report to be dissociated from consciousness entirely: a philosophical zombie may give detailed verbal reports of awareness in the absence of any genuine awareness.
Although verbal report is in practice the "gold standard" for ascribing consciousness, it is not the only possible criterion. In medicine, consciousness is assessed as a combination of verbal behavior, arousal, brain activity and purposeful movement. The last three of these can be used as indicators of consciousness when verbal behavior is absent. The scientific literature regarding the neural bases of arousal and purposeful movement is very extensive. Their reliability as indicators of consciousness is disputed, however, due to numerous studies showing that alert human subjects can be induced to behave purposefully in a variety of ways in spite of reporting a complete lack of awareness. Studies of the neuroscience of free will have also shown that the experiences that people report when they behave purposefully sometimes do not correspond to their actual behaviors or to the patterns of electrical activity recorded from their brains.
Another approach applies specifically to the study of self-awareness, that is, the ability to distinguish oneself from others. In the 1970s Gordon Gallup developed an operational test for self-awareness, known as the mirror test. The test examines whether animals are able to differentiate between seeing themselves in a mirror versus seeing other animals. The classic example involves placing a spot of coloring on the skin or fur near the individual's forehead and seeing if they attempt to remove it or at least touch the spot, thus indicating that they recognize that the individual they are seeing in the mirror is themselves. Humans (older than 18 months) and other great apes, bottlenose dolphins, killer whales, pigeons, European magpies and elephants have all been observed to pass this test.
Neural correlates.
A major part of the scientific literature on consciousness consists of studies that examine the relationship between the experiences reported by subjects and the activity that simultaneously takes place in their brains—that is, studies of the neural correlates of consciousness. The hope is to find that activity in a particular part of the brain, or a particular pattern of global brain activity, will be strongly predictive of conscious awareness. Several brain imaging techniques, such as EEG and fMRI, have been used for physical measures of brain activity in these studies.
One idea that has drawn attention for several decades is that consciousness is associated with high-frequency (gamma band) oscillations in brain activity. This idea arose from proposals in the 1980s, by Christof von der Malsburg and Wolf Singer, that gamma oscillations could solve the so-called binding problem, by linking information represented in different parts of the brain into a unified experience. Rodolfo Llinás, for example, proposed that consciousness results from recurrent thalamo-cortical resonance where the specific thalamocortical systems (content) and the non-specific (centromedial thalamus) thalamocortical systems (context) interact in the gamma band frequency via synchronous oscillations.
A number of studies have shown that activity in primary sensory areas of the brain is not sufficient to produce consciousness: it is possible for subjects to report a lack of awareness even when areas such as the primary visual cortex show clear electrical responses to a stimulus. Higher brain areas are seen as more promising, especially the prefrontal cortex, which is involved in a range of higher cognitive functions collectively known as executive functions. There is substantial evidence that a "top-down" flow of neural activity (i.e., activity propagating from the frontal cortex to sensory areas) is more predictive of conscious awareness than a "bottom-up" flow of activity. The prefrontal cortex is not the only candidate area, however: studies by Nikos Logothetis and his colleagues have shown, for example, that visually responsive neurons in parts of the temporal lobe reflect the visual perception in the situation when conflicting visual images are presented to different eyes (i.e., bistable percepts during binocular rivalry).
In 2011 Graziano and Kastner proposed the "attention schema" theory of awareness. In that theory specific cortical machinery, notably in the superior temporal sulcus and the temporo-parietal junction, is used to build the construct of awareness and attribute it to other people. The same cortical machinery is also used to attribute awareness to oneself. Damage to this cortical machinery can lead to deficits in consciousness such as hemispatial neglect. In the attention schema theory, the value of constructing the feature of awareness and attributing it to a person is to gain a useful predictive model of that person's attentional processing. Attention is a style of information processing in which a brain focuses its resources on a limited set of interrelated signals. Awareness, in this theory, is a useful, simplified schema that represents attentional state. To be aware of X is to construct a model of one's attentional focus on X.
Biological function and evolution.
Regarding the primary function of conscious processing, a recurring idea in recent theories is that phenomenal states somehow integrate neural activities and information-processing that would otherwise be independent. This has been called the "integration consensus". Another example has been proposed by Gerald Edelman called dynamic core hypothesis which puts emphasis on reentrant connections that reciprocally link areas of the brain in a massively parallel manner. These theories of integrative function present solutions to two classic problems associated with consciousness: differentiation and unity. They show how our conscious experience can discriminate between a virtually unlimited number of different possible scenes and details (differentiation) because it integrates those details from our sensory systems, while the integrative nature of consciousness in this view easily explains how our experience can seem unified as one whole despite all of these individual parts. However, it remains unspecified which kinds of information are integrated in a conscious manner and which kinds can be integrated without consciousness. Nor is it explained what specific causal role conscious integration plays, nor why the same functionality cannot be achieved without consciousness. Obviously not all kinds of information are capable of being disseminated consciously (e.g., neural activity related to vegetative functions, reflexes, unconscious motor programs, low-level perceptual analyses, etc.) and many kinds of information can be disseminated and combined with other kinds without consciousness, as in intersensory interactions such as the ventriloquism effect. Hence it remains unclear why any of it is conscious. For a review of the differences between conscious and unconscious integrations, see the article of E. Morsella.
As noted earlier, even among writers who consider consciousness to be a well-defined thing, there is widespread dispute about which animals other than humans can be said to possess it. Thus, any examination of the evolution of consciousness is faced with great difficulties. Nevertheless, some writers have argued that consciousness can be viewed from the standpoint of evolutionary biology as an adaptation in the sense of a trait that increases fitness. In his article "Evolution of consciousness", John Eccles argued that special anatomical and physical properties of the mammalian cerebral cortex gave rise to consciousness. Bernard Baars proposed that once in place, this "recursive" circuitry may have provided a basis for the subsequent development of many of the functions that consciousness facilitates in higher organisms. Peter Carruthers has put forth one such potential adaptive advantage gained by conscious creatures by suggesting that consciousness allows an individual to make distinctions between appearance and reality. This ability would enable a creature to recognize the likelihood that their perceptions are deceiving them (e.g. that water in the distance may be a mirage) and behave accordingly, and it could also facilitate the manipulation of others by recognizing how things appear to them for both cooperative and devious ends.
Other philosophers, however, have suggested that consciousness would not be necessary for any functional advantage in evolutionary processes. No one has given a causal explanation, they argue, of why it would not be possible for a functionally equivalent non-conscious organism (i.e., a philosophical zombie) to achieve the very same survival advantages as a conscious organism. If evolutionary processes are blind to the difference between function "F" being performed by conscious organism "O" and non-conscious organism "O*", it is unclear what adaptive advantage consciousness could provide. As a result, an exaptive explanation of consciousness has gained favor with some theorists that posit consciousness did not evolve as an adaptation but was an exaptation arising as a consequence of other developments such as increases in brain size or cortical rearrangement.
States of consciousness.
There are some brain states in which consciousness seems to be abolished, including dreamless sleep, coma, and death. There are also a variety of circumstances that can change the relationship between the mind and the world in less drastic ways, producing what are known as altered states of consciousness. Some altered states occur naturally; others can be produced by drugs or brain damage. Altered states can be accompanied by changes in thinking, disturbances in the sense of time, feelings of loss of control, changes in emotional expression, alternations in body image and changes in meaning or significance.
The two most widely accepted altered states are sleep and dreaming. Although dream sleep and non-dream sleep appear very similar to an outside observer, each is associated with a distinct pattern of brain activity, metabolic activity, and eye movement; each is also associated with a distinct pattern of experience and cognition. During ordinary non-dream sleep, people who are awakened report only vague and sketchy thoughts, and their experiences do not cohere into a continuous narrative. During dream sleep, in contrast, people who are awakened report rich and detailed experiences in which events form a continuous progression, which may however be interrupted by bizarre or fantastic intrusions. Thought processes during the dream state frequently show a high level of irrationality. Both dream and non-dream states are associated with severe disruption of memory: it usually disappears in seconds during the non-dream state, and in minutes after awakening from a dream unless actively refreshed.
A variety of psychoactive drugs and alcohol have notable effects on consciousness. These range from a simple dulling of awareness produced by sedatives, to increases in the intensity of sensory qualities produced by stimulants, cannabis, or most notably by the class of drugs known as psychedelics. LSD, mescaline, psilocybin, and others in this group can produce major distortions of perception, including hallucinations; some users even describe their drug-induced experiences as mystical or spiritual in quality. The brain mechanisms underlying these effects are not as well understood as those induced by use of alcohol, but there is substantial evidence that alterations in the brain system that uses the chemical neurotransmitter serotonin play an essential role.
There has been some research into physiological changes in yogis and people who practise various techniques of meditation. Some research with brain waves during meditation has reported differences between those corresponding to ordinary relaxation and those corresponding to meditation. It has been disputed, however, whether there is enough evidence to count these as physiologically distinct states of consciousness.
The most extensive study of the characteristics of altered states of consciousness was made by psychologist Charles Tart in the 1960s and 1970s. Tart analyzed a state of consciousness as made up of a number of component processes, including exteroception (sensing the external world); interoception (sensing the body); input-processing (seeing meaning); emotions; memory; time sense; sense of identity; evaluation and cognitive processing; motor output; and interaction with the environment. Each of these, in his view, could be altered in multiple ways by drugs or other manipulations. The components that Tart identified have not, however, been validated by empirical studies. Research in this area has not yet reached firm conclusions, but a recent questionnaire-based study identified eleven significant factors contributing to drug-induced states of consciousness: experience of unity; spiritual experience; blissful state; insightfulness; disembodiment; impaired control and cognition; anxiety; complex imagery; elementary imagery; audio-visual synesthesia; and changed meaning of percepts.
Phenomenology.
Phenomenology is a method of inquiry that attempts to examine the structure of consciousness in its own right, putting aside problems regarding the relationship of consciousness to the physical world. This approach was first proposed by the philosopher Edmund Husserl, and later elaborated by other philosophers and scientists. Husserl's original concept gave rise to two distinct lines of inquiry, in philosophy and psychology. In philosophy, phenomenology has largely been devoted to fundamental metaphysical questions, such as the nature of intentionality (""aboutness""). In psychology, phenomenology largely has meant attempting to investigate consciousness using the method of introspection, which means looking into one's own mind and reporting what one observes. This method fell into disrepute in the early twentieth century because of grave doubts about its reliability, but has been rehabilitated to some degree, especially when used in combination with techniques for examining brain activity.
Introspectively, the world of conscious experience seems to have considerable structure. Immanuel Kant asserted that the world as we perceive it is organized according to a set of fundamental "intuitions", which include "object" (we perceive the world as a set of distinct things); "shape"; "quality" (color, warmth, etc.); "space" (distance, direction, and location); and "time". Some of these constructs, such as space and time, correspond to the way the world is structured by the laws of physics; for others the correspondence is not as clear. Understanding the physical basis of qualities, such as redness or pain, has been particularly challenging. David Chalmers has called this the "hard problem of consciousness". Some philosophers have argued that it is intrinsically unsolvable, because qualities (""qualia"") are ineffable; that is, they are "raw feels", incapable of being analyzed into component processes. Most psychologists and neuroscientists reject these arguments — nevertheless it is clear that the relationship between a physical entity such as light and a perceptual quality such as color is extraordinarily complex and indirect, as demonstrated by a variety of optical illusions such as neon color spreading.
In neuroscience, a great deal of effort has gone into investigating how the perceived world of conscious awareness is constructed inside the brain. The process is generally thought to involve two primary mechanisms: (1) hierarchical processing of sensory inputs, and (2) memory. Signals arising from sensory organs are transmitted to the brain and then processed in a series of stages, which extract multiple types of information from the raw input. In the visual system, for example, sensory signals from the eyes are transmitted to the thalamus and then to the primary visual cortex; inside the cerebral cortex they are sent to areas that extract features such as three-dimensional structure, shape, color, and motion. Memory comes into play in at least two ways. First, it allows sensory information to be evaluated in the context of previous experience. Second, and even more importantly, working memory allows information to be integrated over time so that it can generate a stable representation of the world—Gerald Edelman expressed this point vividly by titling one of his books about consciousness "The Remembered Present".
Despite the large amount of information available, the most important aspects of perception remain mysterious. A great deal is known about low-level signal processing in sensory systems, but the ways by which sensory systems interact with each other, with "executive" systems in the frontal cortex, and with the language system are very incompletely understood. At a deeper level, there are still basic conceptual issues that remain unresolved. Many scientists have found it difficult to reconcile the fact that information is distributed across multiple brain areas with the apparent unity of consciousness: this is one aspect of the so-called "binding problem". There are also some scientists who have expressed grave reservations about the idea that the brain forms representations of the outside world at all: influential members of this group include psychologist J. J. Gibson and roboticist Rodney Brooks, who both argued in favor of "intelligence without representation".
Medical aspects.
The medical approach to consciousness is practically oriented. It derives from a need to treat people whose brain function has been impaired as a result of disease, brain damage, toxins, or drugs. In medicine, conceptual distinctions are considered useful to the degree that they can help to guide treatments. Whereas the philosophical approach to consciousness focuses on its fundamental nature and its contents, the medical approach focuses on the "amount" of consciousness a person has: in medicine, consciousness is assessed as a "level" ranging from coma and brain death at the low end, to full alertness and purposeful responsiveness at the high end.
Consciousness is of concern to patients and physicians, especially neurologists and anesthesiologists. Patients may suffer from disorders of consciousness, or may need to be anesthetized for a surgical procedure. Physicians may perform consciousness-related interventions such as instructing the patient to sleep, administering general anesthesia, or inducing medical coma. Also, bioethicists may be concerned with the ethical implications of consciousness in medical cases of patients such as Karen Ann Quinlan, while neuroscientists may study patients with impaired consciousness in hopes of gaining information about how the brain works.
Assessment.
In medicine, consciousness is examined using a set of procedures known as neuropsychological assessment. There are two commonly used methods for assessing the level of consciousness of a patient: a simple procedure that requires minimal training, and a more complex procedure that requires substantial expertise. The simple procedure begins by asking whether the patient is able to move and react to physical stimuli. If so, the next question is whether the patient can respond in a meaningful way to questions and commands. If so, the patient is asked for name, current location, and current day and time. A patient who can answer all of these questions is said to be "alert and oriented times four" (sometimes denoted "A&Ox4" on a medical chart), and is usually considered fully conscious.
The more complex procedure is known as a neurological examination, and is usually carried out by a neurologist in a hospital setting. A formal neurological examination runs through a precisely delineated series of tests, beginning with tests for basic sensorimotor reflexes, and culminating with tests for sophisticated use of language. The outcome may be summarized using the Glasgow Coma Scale, which yields a number in the range 3—15, with a score of 3 indicating brain death (the lowest defined level of consciousness), and 15 indicating full consciousness. The Glasgow Coma Scale has three subscales, measuring the "best motor response" (ranging from "no motor response" to "obeys commands"), the "best eye response" (ranging from "no eye opening" to "eyes opening spontaneously") and the "best verbal response" (ranging from "no verbal response" to "fully oriented"). There is also a simpler pediatric version of the scale, for children too young to be able to use language.
In 2013, an experimental procedure was developed to measure degrees of consciousness, the procedure involving stimulating the brain with a magnetic pulse, measuring resulting waves of electrical activity, and developing a consciousness score based on the complexity of the brain activity.
Disorders of consciousness.
Medical conditions that inhibit consciousness are considered disorders of consciousness. This category generally includes minimally conscious state and persistent vegetative state, but sometimes also includes the less severe locked-in syndrome and more severe chronic coma. Differential diagnosis of these disorders is an active area of biomedical research. Finally, brain death results in an irreversible disruption of consciousness. While other conditions may cause a moderate deterioration (e.g., dementia and delirium) or transient interruption (e.g., grand mal and petit mal seizures) of consciousness, they are not included in this category.
Anosognosia.
One of the most striking disorders of consciousness goes by the name anosognosia, a Greek-derived term meaning "unawareness of disease". This is a condition in which patients are disabled in some way, most commonly as a result of a stroke, but either misunderstand the nature of the problem or deny that there is anything wrong with them. The most frequently occurring form is seen in people who have experienced a stroke damaging the parietal lobe in the right hemisphere of the brain, giving rise to a syndrome known as hemispatial neglect, characterized by an inability to direct action or attention toward objects located to the right with respect to their bodies. Patients with hemispatial neglect are often paralyzed on the right side of the body, but sometimes deny being unable to move. When questioned about the obvious problem, the patient may avoid giving a direct answer, or may give an explanation that doesn't make sense. Patients with hemispatial neglect may also fail to recognize paralyzed parts of their bodies: one frequently mentioned case is of a man who repeatedly tried to throw his own paralyzed right leg out of the bed he was lying in, and when asked what he was doing, complained that somebody had put a dead leg into the bed with him. An even more striking type of anosognosia is Anton–Babinski syndrome, a rarely occurring condition in which patients become blind but claim to be able to see normally, and persist in this claim in spite of all evidence to the contrary.
Stream of consciousness.
William James is usually credited with popularizing the idea that human consciousness flows like a stream, in his "Principles of Psychology" of 1890. According to James, the "stream of thought" is governed by five characteristics: "(1) Every thought tends to be part of a personal consciousness. (2) Within each personal consciousness thought is always changing. (3) Within each personal consciousness thought is sensibly continuous. (4) It always appears to deal with objects independent of itself. (5) It is interested in some parts of these objects to the exclusion of others". A similar concept appears in Buddhist philosophy, expressed by the Sanskrit term "Citta-saṃtāna", which is usually translated as mindstream or "mental continuum". In the Buddhist view, though, the "mindstream" is viewed primarily as a source of noise that distracts attention from a changeless underlying reality.
In the west, the primary impact of the idea has been on literature rather than science: stream of consciousness as a narrative mode means writing in a way that attempts to portray the moment-to-moment thoughts and experiences of a character. This technique perhaps had its beginnings in the monologues of Shakespeare's plays, and reached its fullest development in the novels of James Joyce and Virginia Woolf, although it has also been used by many other noted writers.
Here for example is a passage from Joyce's Ulysses about the thoughts of Molly Bloom:
Spiritual approaches.
To most philosophers, the word "consciousness" connotes the relationship between the mind and the world. To writers on spiritual or religious topics, it frequently connotes the relationship between the mind and God, or the relationship between the mind and deeper truths that are thought to be more fundamental than the physical world. Krishna consciousness, for example, is a term used to mean an intimate linkage between the mind of a worshipper and the god Krishna. The mystical psychiatrist Richard Maurice Bucke distinguished between three types of consciousness: "Simple Consciousness", awareness of the body, possessed by many animals; "Self Consciousness", awareness of being aware, possessed only by humans; and "Cosmic Consciousness", awareness of the life and order of the universe, possessed only by humans who are enlightened. Many more examples could be given. The most thorough account of the spiritual approach may be Ken Wilber's book "The Spectrum of Consciousness", a comparison of western and eastern ways of thinking about the mind. Wilber described consciousness as a spectrum with ordinary awareness at one end, and more profound types of awareness at higher levels.

</doc>
<doc id="5665" url="http://en.wikipedia.org/wiki?curid=5665" title="Currency">
Currency

A currency (from , "in circulation", from ) in the most specific use of the word refers to money in any form when in actual use or circulation, as a medium of exchange, especially circulating paper money. This use is synonymous with banknotes, or (sometimes) with banknotes plus coins, meaning the physical tokens used for money by a government.
A much more general use of the word currency is anything that is used in any circumstances, as a medium of exchange. In this use, "currency" is a synonym for the concept of money.
A definition of intermediate generality is that a currency is a "system" of money (monetary units) in common use, especially in a nation. Under this definition, British pounds, U.S. dollars, and European euros are different types of currency, or currencies. Currencies in this definition need not be physical objects, but as stores of value are subject to trading between nations in foreign exchange markets, which determine the relative values of the different currencies. Currencies in the sense used by foreign exchange markets, are defined by governments, and each type has limited boundaries of acceptance.
The former definitions of the term "currency" are discussed in their respective synonymous articles banknote, coin, and money. The latter definition, pertaining to the currency systems of nations, is the topic of this article. Currencies can be classified into two monetary systems: fiat money and commodity money, depending on what guarantees the value (the economy at large vs. the government's physical metal reserves). Some currencies are legal tender in certain jurisdictions, which means they cannot be refused as payment for debt. Others are simply traded for their economic value. Digital currency arose with the popularity of computers and the Internet.
History.
Early currency.
Currency evolved from two basic innovations, both of which had occurred by 2000 BC. Originally money was a form of receipt, representing grain stored in temple granaries in Sumer in ancient Mesopotamia, then Ancient Egypt.
In this first stage of currency, metals were used as symbols to represent value stored in the form of commodities. This formed the basis of trade in the Fertile Crescent for over 1500 years. However, the collapse of the Near Eastern trading system pointed to a flaw: in an era where there was no place that was safe to store value, the value of a circulating medium could only be as sound as the forces that defended that store. Trade could only reach as far as the credibility of that military. By the late Bronze Age, however, a series of treaties had established safe passage for merchants around the Eastern Mediterranean, spreading from Minoan Crete and Mycenae in the northwest to Elam and Bahrain in the southeast. It is not known what was used as a currency for these exchanges, but it is thought that ox-hide shaped ingots of copper, produced in Cyprus, may have functioned as a currency. It is thought that the increase in piracy and raiding associated with the Bronze Age collapse, possibly produced by the Peoples of the Sea, brought this trading system to an end. It was only with the recovery of Phoenician trade in the 10th and 9th centuries BC that saw a return to prosperity, and the appearance of real coinage, possibly first in Anatolia with Croesus of Lydia and subsequently with the Greeks and Persians. In Africa many forms of value store have been used, including beads, ingots, ivory, various forms of weapons, livestock, the manilla currency, and ochre and other earth oxides. The manilla rings of West Africa were one of the currencies used from the 15th century onwards to sell slaves. African currency is still notable for its variety, and in many places various forms of barter still apply.
Coinage.
These factors led to the metal itself being the store of value: first silver, then both silver and gold, and at one point also bronze. Now we have copper coins and other non-precious metals as coins. Metals were mined, weighed, and stamped into coins. This was to assure the individual taking the coin that he was getting a certain known weight of precious metal. Coins could be counterfeited, but they also created a new unit of account, which helped lead to banking. Archimedes' principle provided the next link: coins could now be easily tested for their fine weight of metal, and thus the value of a coin could be determined, even if it had been shaved, debased or otherwise tampered with (see Numismatics).
Most major economies using coinage had three tiers of coins: copper, silver and gold. Gold coins were used for large purchases, payment of the military and backing of state activities. Silver coins were used for midsized transactions, and as a unit of account for taxes, dues, contracts and fealty, while copper coins were used for everyday transactions. This system had been used in ancient India since the time of the Mahajanapadas. In Europe, this system worked through the medieval period because there was virtually no new gold, silver or copper introduced through mining or conquest. Thus the overall ratios of the three coinages remained roughly equivalent.
Paper money.
In premodern China, the need for credit and for a medium of exchange that was less physically cumbersome than large numbers of copper coins led to the introduction of paper money, i.e. banknotes. Their introduction was a gradual process which lasted from the late Tang Dynasty (618–907) into the Song Dynasty (960–1279). It began as a means for merchants to exchange heavy coinage for receipts of deposit issued as promissory notes by wholesalers' shops. These notes that were valid for temporary use in a small regional territory. In the 10th century, the Song Dynasty government began to circulate these notes amongst the traders in its monopolized salt industry. The Song government granted several shops the right to issue banknotes, and in the early 12th century the government finally took over these shops to produce state-issued currency. Yet the banknotes issued were still only locally and temporarily valid: it was not until the mid 13th century that a standard and uniform government issue of paper money became an acceptable nationwide currency. The already widespread methods of woodblock printing and then Pi Sheng's movable type printing by the 11th century were the impetus for the mass production of paper money in premodern China.
At around the same time in the medieval Islamic world, a vigorous monetary economy was created during the 7th–12th centuries on the basis of the expanding levels of circulation of a stable high-value currency (the dinar). Innovations introduced by Muslim economists, traders and merchants include the earliest uses of credit, cheques, promissory notes, savings accounts, transactional accounts, loaning, trusts, exchange rates, the transfer of credit and debt, and banking institutions for loans and deposits.
In Europe, paper money was first introduced on a regular basis in Sweden in 1661 (although Washington Irving records an earlier emergency use of it, by the Spanish in a siege during the Conquest of Granada). Sweden was rich in copper, thus, because of copper's low value, extraordinarily big coins (often weighing several kilograms) had to be made.
The advantages of paper currency were numerous: it reduced the need to transport gold and silver, which was risky; it facilitated loans of gold or silver at interest, since the underlying specie (gold or silver) never left the possession of the lender until someone else redeemed the note; and it allowed a division of currency into credit and specie backed forms. It enabled the sale of stock in joint stock companies, and the redemption of those shares in paper.
But there were also disadvantages. First, since a note has no intrinsic value, there was nothing to stop issuing authorities printing more notes than they had specie to back them with. Second, because it increased the money supply, it increased inflationary pressures, a fact observed by David Hume in the 18th century. Thus paper money would often lead to an inflationary bubble, which could collapse if people began demanding hard money, causing the demand for paper notes to fall to zero. The printing of paper money was also associated with wars, and financing of wars, and therefore regarded as part of maintaining a standing army. For these reasons, paper currency was held in suspicion and hostility in Europe and America. It was also addictive, since the speculative profits of trade and capital creation were quite large. Major nations established mints to print money and mint coins, and branches of their treasury to collect taxes and hold gold and silver stock.
At that time, both silver and gold were considered legal tender, and accepted by governments for taxes. However, the instability in the ratio between the two grew over the course of the 19th century, with the increases both in supply of these metals, particularly silver, and in trade. The parallel use of both metals is called bimetallism, and the attempt to create a bimetallic standard where both gold and silver backed currency remained in circulation occupied the efforts of inflationists. Governments at this point could use currency as an instrument of policy, printing paper currency such as the United States Greenback, to pay for military expenditures. They could also set the terms at which they would redeem notes for specie, by limiting the amount of purchase, or the minimum amount that could be redeemed.
By 1900, most of the industrializing nations were on some form of gold standard, with paper notes and silver coins constituting the circulating medium. Private banks and governments across the world followed Gresham's Law: keeping the gold and silver they received, but paying out in notes. This did not happen all around the world at the same time, but occurred sporadically, generally in times of war or financial crisis, beginning in the early part of the 20th century and continuing across the world until the late 20th century, when the regime of floating fiat currencies came into force. One of the last countries to break away from the gold standard was the United States in 1971.
No country anywhere in the world today has an enforceable gold standard or silver standard currency system.
Banknote era.
A banknote (more commonly known as a bill in the United States and Canada) is a type of currency, and commonly used as legal tender in many jurisdictions. With coins, banknotes make up the cash form of all money. Banknotes are mostly paper, but Australia's Commonwealth Scientific and Industrial Research Organisation developed the world's first polymer currency in the 1980s that went into circulation on the nation's bicentenary in 1988. Now used in some 22 countries (over 40 if counting commemorative issues), polymer currency dramatically improves the life span of banknotes and prevents counterfeiting.
Modern currencies.
Currency use is based on the concept of lex monetae; that a sovereign state decides which currency it shall use. Currently, the International Organization for Standardization has introduced a three-letter system of codes (ISO 4217) to define currency (as opposed to simple names or currency signs), in order to remove the confusion that there are dozens of currencies called the dollar and many called the franc. Even the pound is used in nearly a dozen different countries; most of these are tied to the Pound Sterling, while the remainder have varying values. In general, the three-letter code uses the ISO 3166-1 country code for the first two letters and the first letter of the name of the currency (D for dollar, for instance) as the third letter. United States currency, for instance is globally referred to as USD.
The International Monetary Fund uses a variant system when referring to national currencies.
Alternative currencies.
Distinct from centrally controlled government-issued currencies, private decentralized trust networks support alternative currencies such as Bitcoin, Litecoin, Peercoin or Dogecoin, as well as branded currencies, for example 'obligation' based stores of value, such as quasi-regulated BarterCard, Loyalty Points (Credit Cards, Airlines) or Game-Credits (MMO games) that are based on reputation of commercial products, or highly regulated 'asset backed' 'alternative currencies' such as mobile-money schemes like MPESA (called E-Money Issuance).
Currency may be Internet-based and digital, for instance, Bitcoin and not tied to any specific country, or the IMF's SDR that is based on a basket of currencies (and assets held).
Control and production.
In most cases, a central bank has a monopoly right to issue of coins and banknotes (fiat money) for its own area of circulation (a country or group of countries); it regulates the production of currency by banks (credit) through monetary policy.
An exchange rate is the price at which two currencies can be exchanged against each other. This is used for trade between the two currency zones. Exchange rates can be classified as either floating or fixed. In the former, day-to-day movements in exchange rates are determined by the market; in the latter, governments intervene in the market to buy or sell their currency to balance supply and demand at a fixed exchange rate.
In cases where a country has control of its own currency, that control is exercised either by a central bank or by a Ministry of Finance. The institution that has control of monetary policy is referred to as the monetary authority. Monetary authorities have varying degrees of autonomy from the governments that create them. In the United States, the Federal Reserve System operates without direct oversight by the legislative or executive branches. A monetary authority is created and supported by its sponsoring government, so independence can be reduced by the legislative or executive authority that creates it.
Several countries can use the same name for their own separate currencies (for example, "dollar" in Australia, Canada and the United States). By contrast, several countries can also use the same currency (for example, the euro), or one country can declare the currency of another country to be legal tender. For example, Panama and El Salvador have declared U.S. currency to be legal tender, and from 1791 to 1857, Spanish silver coins were legal tender in the United States. At various times countries have either re-stamped foreign coins, or used currency board issuing one note of currency for each note of a foreign government held, as Ecuador currently does.
Each currency typically has a main currency unit (the dollar, for example, or the euro) and a fractional unit, often defined as of the main unit: 100 cents = 1 dollar, 100 centimes = 1 franc, 100 pence = 1 pound, although units of or occasionally also occur. Some currencies do not have any smaller units at all, such as the Icelandic króna.
Mauritania and Madagascar are the only remaining countries that do not use the decimal system; instead, the Mauritanian ouguiya is in theory divided into 5 khoums, while the Malagasy ariary is theoretically divided into 5 iraimbilanja. In these countries, words like "dollar" or "pound" "were simply names for given weights of gold." Due to inflation khoums and iraimbilanja have in practice fallen into disuse. (See non-decimal currencies for other historic currencies with non-decimal divisions.)
Currency convertibility.
Convertibility of a currency determines the ability of an individual, corporate or government to convert its local currency to another currency or vice versa with or without central bank/government intervention. Based on the above restrictions or free and readily conversion features, currencies are classified as:
Local currencies.
In economics, a local currency is a currency not backed by a national government, and intended to trade only in a small area. Advocates such as Jane Jacobs argue that this enables an economically depressed region to pull itself up, by giving the people living there a medium of exchange that they can use to exchange services and locally produced goods (in a broader sense, this is the original purpose of all money). Opponents of this concept argue that local currency creates a barrier which can interfere with economies of scale and comparative advantage, and that in some cases they can serve as a means of tax evasion.
Local currencies can also come into being when there is economic turmoil involving the national currency. An example of this is the Argentinian economic crisis of 2002 in which IOUs issued by local governments quickly took on some of the characteristics of local currencies.
One of the best examples of a local currency is the original LETS currency, founded on Vancouver Island in the early 1980s. In 1982, the Canadian Central Bank’s lending rates ran up to 14% which drove chartered bank lending rates as high as 19%. The resulting currency and credit scarcity left island residents with few options other than to create a local currency.
See also.
Related concepts
Accounting units
Lists

</doc>
