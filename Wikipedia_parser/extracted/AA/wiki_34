<doc id="2781" url="http://en.wikipedia.org/wiki?curid=2781" title="Atari 7800">
Atari 7800

The Atari 7800 ProSystem, or simply the Atari 7800, is a video game console officially released by Atari Corporation in January 1986. The 1986 launch is sometimes referred to as a "re-release" or "relaunch" because the Atari 7800 had originally been announced in May 1984, to replace Atari Inc.'s Atari 5200, but a general release was shelved due to the sale of the company. In January 1986, the 7800 was relaunched and would compete that year with the Nintendo Entertainment System and the Sega Master System. It had simple digital joysticks and was almost fully backward-compatible with the Atari 2600, the first console to have backward compatibility without the use of additional modules. It was considered affordable at a price of US$140.
In 2009, IGN chose the 7800 to be their 17th best video game console of all time. They justified this relatively low ranking (though higher than every other Atari console save the 2600) with the summary statement: "Its delayed release, its cancelled peripherals, and a lack of financial backing from the company's new owners all combined to ensure that Atari 7800 would never see any success beyond being a sexier way of playing Atari 2600 titles."
History.
The Atari 7800 ProSystem was the first game system from Atari Inc. designed by an outside company, General Computer Corporation (GCC). The system had been designed in 1983 through 1984 with an intended mass market rollout in June 1984, but was canceled shortly thereafter due to the sale of the company to Tramel Technology Ltd on July 2, 1984. The project was originally called the Atari 3600, though was later renamed the Atari 7800.
Several key factors influenced the design of the 7800. First, Atari had been facing mounting pressure from the ColecoVision, which boasted graphics that more closely mirrored arcade games of the time than Atari’s reigning 2600 VCS system. Second, the Atari 5200 (the original intended successor to the Atari 2600 VCS) had been widely criticized for not being able to play Atari 2600 VCS games without an adapter. Finally, dropping prices of home computers like the Commodore 64 had caused many to believe that buying a home computer was a better investment because it provided more detailed gameplay and could be used for other purposes such as word processing.
Previous game consoles sometimes had a difficult time replicating the arcade experience in home versions of popular arcade games. In particular, home versions of arcade games sometimes had problems with flickering and slow down when more than a few moving objects appeared on the screen at once. GCC, which had a background in creating arcade games, designed their new system with a graphical architecture similar to arcade machines of the time. The 7800 featured the ability to move around a tremendous amount of objects (75-to-100) that far exceeded previous consoles. Powering the system was an Atari SALLY 6502 (Atari's slightly custom 6502, sometimes described as a "6502C") processor running at 1.79 MHz, similar to the processor found in home computers (Atari 8-bit, Apple II, Commodore 64) and other consoles (Atari 5200 and Nintendo Entertainment System).
In response to the criticisms of the Atari 5200, the Atari 7800 could play almost all Atari 2600 games out of the box, without the need for an adapter. In addition, it featured a return to a digital controller.
To address the concerns of parents that home computers were a better investment than consoles, the system was designed to be upgraded to a full-fledged home computer. A keyboard was developed, and the keyboard had an expansion port (which was the SIO port from Atari's 8-bit computer line, though the 7800 could not run Atari computer programs) that allowed for the addition of peripherals such as disk drives and printers.
To further enhance the gaming experience, GCC had also designed a "high score cartridge", a battery-backed RAM cartridge designed for storing game scores. On the side of the 7800 was an expansion port, reportedly for a planned connection with a laserdisc player.
Launch.
The 7800 was initially released in southern California in June 1984, following an announcement on May 21, 1984 at the Summer Consumer Electronics Show. 13 games were announced for the system's launch, including "Ms. Pac-Man", "Pole Position II", "Centipede", "Joust", "Dig Dug", "Desert Falcon", "", "Galaga", "Xevious", "Food Fight", "Ballblazer", "Rescue on Fractalus!", and "Track and Field". Atari was a sponsor of the 1984 Summer Olympics and planned to push the 7800 aggressively in time for Christmas that year. 
On July 2, 1984, Warner Communications sold Atari's Consumer Division to Jack Tramiel. All projects were halted during an initial evaluation period. Modern publications have often incorrectly asserted that Jack Tramiel mothballed the Atari 7800 feeling video games were a past fad and subsequently asserted that he dusted off the Atari 7800 once the Nintendo NES became successful. The reality was that a contractual issue arose in that GCC had not been paid for their development of the 7800. Warner and Tramiel battled back and forth over who was accountable, with Tramiel believing that the 7800 should have been covered as part of his acquisition deal. In May 1985, Jack relented and paid GCC the overdue payment. This led to additional negotiations regarding the initial launch titles that GCC had developed and then an effort to find someone to lead their new video game division, which was completed in November 1985.
The original production run of the Atari 7800 languished on warehouse shelves until it was re-introduced in January 1986 after strong 2600 sales the previous Christmas.
Atari's launch of the 7800 under Tramiel was far more subdued than Warner had planned for the system in 1984 with a marketing budget of just $300,000. Additionally, the keyboard and high score cartridge were canceled, the expansion port was removed from later production runs of the system and, in lieu of new titles, the system was launched with titles intended for the 7800's debut in 1984.
Marketplace challenges.
Atari's lineup for the 7800 emphasized high-quality versions of popular arcade games like "Joust" and "Asteroids". This had been a primary reason for the success of the Atari 2600 VCS against systems like the Intellivision.
During the Atari 7800’s life cycle, Atari found themselves struggling to get developers to create 7800 versions of then-popular arcade titles because of a controversial policy employed by Nintendo. When Nintendo revived the industry, they signed up software development companies to create Nintendo Entertainment System games under a strict license agreement which imposed serious restrictions on what they were allowed to do. One of the key clauses was that companies who made Nintendo games were not allowed to make that game on a competing system for a period of two years. Because of the market success of the Nintendo Entertainment System, companies chose to develop for it first and were thus barred from developing the same games on competing systems for two years. The software libraries of the Atari 7800 and Sega Master System suffered tremendously as a result.
Eleven titles were developed and sold by three third-party companies under their own labels for the 7800 (Absolute Entertainment, Activision, and Froggo) with the rest published by Atari themselves. However, most Atari development was contracted out.
Some NES titles were developed by companies who had licensed their title from a different arcade manufacturer. While the creator of the NES version would be restricted from making a competitive version of an NES game, the original arcade copyright holder was not precluded from licensing out rights for a home version of an arcade game to multiple systems. Through this loophole, Atari 7800 conversions of "Mario Bros.", "Double Dragon", "Commando", "Rampage", "Xenophobe", "Ikari Warriors" and "Kung Fu Master" were licensed and developed.
Demise.
The Atari 7800 remained officially active in the United States between 1986 and 1991 and in Europe between 1989 and 1991. On January 1, 1992, Atari Corp. formally announced that production of the Atari 7800, the Atari 2600, the Atari 8-bit computer line, and the Atari XE Game System would cease. By the time of the cancellation, Nintendo's NES dominated the North American market, controlling 80% while Atari Corp. controlled just 12%. 
Despite trailing the Nintendo Entertainment System in terms of number of units sold, the 7800 was a profitable enterprise for Atari Corp., benefiting largely from Atari’s name and the system's 2600 compatibility. Profits were strong owing to low investment in game development and marketing. Nonetheless, the 7800 failed to help Atari regain its dominance in the videogame industry.
Technical specifications.
Graphics.
The graphics are generated by a custom graphics chip called MARIA which is very different from other second and third generation consoles, and made it more difficult for game programmers to make the transition. Instead of a limited number of hardware sprites, the MARIA allows for a much larger number of sprites described in a list of display lists. Each display list contains sprite entries with pointers to graphics data, color information, and horizontal positioning. The same display list is used for multiple rasters with the pointers being automatically adjusted. However, managing and displaying a large number of sprites required much more CPU time (both directly and indirectly since the MARIA would halt the CPU when drawing sprites) than consoles with hardware sprites and backgrounds.
MARIA has a number of different graphics modes which are either 160 pixels wide or 320 pixels wide. While the 320 pixel modes theoretically enable the 7800 to create games at higher resolution than the 256 pixel wide graphics found in the Nintendo Entertainment System and Sega Master System, the intense processing demands of MARIA typically meant that programmers created their games using the lower 160 pixel modes.
The 7800 features a broad (for its time) palette of 256 colors. Depending on various parameters, each individual sprite can use from 1 to 12 colors, with 3 colors (plus a 4th "transparency" color) being the most common. In this format, the sprite is referenced to one of 8 palettes, where each palette holds 3 assignable colors. There is also an assignable background color, which will be visible wherever another object has not covered it up. In total the system can utilize 25 colors on a scanline at one time.
The graphics resolution, color palette assignments, and background color can be adjusted in between scanlines. This advanced programming technique is documented in the original 1983 "Atari 3600 Software Guide". Games often used this feature to render high resolution text in one area of the screen, while displaying more colorful graphics with less resolution in the gameplay area. Demos also exist which use this feature to place all 256 colors on the screen at the same time.
The MARIA’s approach had advantages and disadvantages when it came to generating graphics in software during the lifespan of the 7800. It excelled at moving around large numbers of sprites on a static screen without the screen flickering that plagued other 8-bit systems. Its flexible design enabled it to play games which used display list manipulation to generate a pseudo 3D appearance such as "Ballblazer" (1987) and "F-18 Hornet" (1988). While side-scrolling games in the vein of "Super Mario Bros." are possible on the system (1990's "Scrapyard Dog" is the best example), it is significantly harder to develop such a title than on a tile-based system such as the Nintendo Entertainment System.
Sound.
A common criticism of the 7800 regards its use of the TIA to provide 2-channel sound effects and music, resulting in sound quality that is virtually identical to the Atari 2600 VCS from 1977. While the inclusion of 2600 hardware is required to maintain compatibility with the older system, this drove up production costs and reduced available space on the 7800’s motherboard. As such, the 7800 does not include additional hardware for generating sound as it does with graphics and the sound hardware is considered the weakest part of the system.
To compensate for this, GCC’s engineers allowed games to include a POKEY audio chip in the cartridge which substantially improved the audio quality. To ensure software developers had an economical means of producing better sound than TIA, GCC had originally planned to make a low-cost, high performance sound chip, GUMBY, which could also be placed in 7800 cartridges to enhance its sound capabilities further. This project was cancelled when Atari was sold to Jack Tramiel.
Despite having the capability to support sound chips in cartridges, almost no 7800 cartridges feature POKEY hardware for enhanced sound. "Ballblazer", released in 1987, uses the POKEY to generate all music and sound effects. Similarly, "Commando", released in 1989, uses a POKEY to generate in-game music while the TIA generates the game's sound effects for a total of 6 channels of sound.
Lockout features.
Following the debate over "Custer's Revenge", an Atari 2600 VCS title with adult themes, Atari had concerns over similar adult titles finding their way onto the 7800 and displaying adult graphics on the significantly improved graphics of the MARIA chip. To combat this, they included a digital signature protection method which prevented unauthorized 7800 games from being played on the system.
When a cartridge was inserted into the system, the 7800 BIOS included code which would generate a digital signature of the cartridge ROM and compare it to the signature stored on the cartridge. If a correct signature was located on the cartridge, the 7800 would operate in 7800 mode, granting the game access to MARIA and other features. If a signature was not located, the 7800 remained in 2600 mode and MARIA was unavailable. All 7800 games released in North America had to be digitally signed by Atari. This digital signature code is not present in PAL 7800s, which use various heuristics to detect 2600 cartridges, due to export restrictions. The signing utility was found and released by Classic Gaming Expo in 2001.
Backward compatibility.
The Atari 7800 differs from the 2600 in several key areas. It features a full Atari SALLY 6502 processor whereas the 2600 VCS has a stripped down 6507 processor running at a slower speed. It has additional RAM (Random Access Memory) and the ability to access more cartridge data at one time than the 2600. The most substantial difference, however, is a graphics architecture which differs markedly from either the Atari 2600 VCS or Atari’s 8-bit line of computers.
The 7800's compatibility with the Atari 2600 is made possible by including many of the same chips used in the Atari 2600. When operating in “2600” mode to play Atari 2600 titles, the 7800 uses a Television Interface Adapter (TIA) chip to generate graphics and sound. The processor is slowed to 1.19 MHz, enabling the 7800 to mirror the performance of the 2600s stripped-down 6507 processor. RAM is limited to 128 bytes found in the RIOT and game data is accessed in 4K blocks.
When in “7800” mode (signified by the appearance of the full screen Atari logo), the graphics are generated entirely by the MARIA graphics processing unit, all system RAM is available and game data is accessed in larger 48K blocks. The system’s SALLY 6502 runs at its normal 1.79 MHz instead of the reduced speed of 2600 mode. The 2600 chips are used in 7800 mode to generate sound as well as switch and controller interfaces.
The Atari 7800 does not support backward compatibility for Atari 5200 games or accessories.
System revisions.
Prototypes:
Production:
Peripherals.
The Atari 7800 came bundled with the Atari Proline Joystick, a two button controller with a joystick for movement. In response to criticism over ergonomic issues in the 7800’s Pro-Line controllers, Atari later released joypad controllers with European 7800s, which were similar in style to controllers found on Nintendo and Sega Systems. The Joypad was not available in the United States.
Unlike the NES or Sega Master System, there were few add-on peripherals for the 7800, though its backwards compatibility feature allowed it to be compatible with most Atari 2600 peripherals.
The most notable exception was the XG-1 lightgun, which came bundled with the Atari XE Game System. The XG-1 was fully compatible with the 7800 and was sold separately for other Atari systems. Atari released four 7800 light gun games: "Alien Brigade", "Crossbow", "Meltdown", and "Barnyard Blaster".
Canceled peripherals.
Due to the acquisition of the Atari Consumer Division by Jack Tramiel in 1984, a number of planned peripherals for the system were canceled.
Software library.
While the 7800 can actually play hundreds of titles due to its compatibility with the Atari 2600, there was limited third party support for the 7800 and fewer than 100 titles were specifically designed for it.
Unreleased games.
As with most game consoles, there were many more games in development for the 7800 than were actually released. However, very few prototypes have been located, due to Tramiel Atari’s reluctance to make them in the first place. Atari 7800 prototypes tend to be highly coveted by collectors, often fetching hundreds of dollars when sold. Some collectors are unwilling to share the rare items publicly as doing so is assumed to decrease the value of their prototype.
Nonetheless, some unreleased Atari 7800 games, as well as early versions of released games have been released to the public. A few have been manufactured and sold.
These include
Engineering Notes list "Tempest" as a game that was between 15–20% completed for the Atari 7800; no code to date has been found. The Atari Museum located and posted unreleased box art and notes for a 7800 version of "Crystal Castles", but no code to date has been found for that game, either. Atari's earlier 7800 games listing showed "Millipede" as one of the games in the line up; however, it does not appear that it was ever started or worked on.
Source code release.
The source code for 13 games, as well as the OS and development tools (for the Atari ST computer system) were discovered in a dumpster behind the Atari building in Sunnyvale, California. Commented assembly language source code was made available for "Centipede", "Commando", "Crossbow", "Desert Falcon", "Dig Dug", "Food Fight", "Galaga", "Hat Trick", "Joust", "Ms. Pac-Man", "Super Stunt Cycle", "Robotron: 2084" and "Xevious" game titles.
Emulation and homebrew.
When emulators of 1980s video game consoles began to appear on home computers in the late 1990s, the Atari 7800 was one of the last to be emulated. The lack of awareness of the system, the lack of understanding of the hardware, and fears about the digital signature lockout initially caused concerns. Since that time, however, the 7800 has been emulated successfully and is now common on emulation sites. One such program is ProSystem, written in C/C++ for the Microsoft Windows operating system. It uses the Windows API and DirectX to display what it emulates in both PAL and NTSC.
The digital signature long prevented homebrew games from being developed until the original encryption generating software was discovered. When the original digital signature generating software was turned over to the Atari community, development of new Atari 7800 titles began. In addition, the Atari community has slowly uncovered the original 7800 development tools and released them into the public domain. New tools, documentation, source code and utilities for development have since been created which has sponsored additional homebrew development. Several new commercial Atari 7800 titles such as Beef Drop, B*nQ, Pac Man Collection, Combat 1990, Santa Simon, and Space War have been created and released.
System compatible hardware has also been produced for the system. Among these was the Cuttle Cart II, a device that allowed the Atari 7800 to read MMC cards containing binary files of Atari 7800 programs. The Cuttle Cart II has enabled more people to play the entire 2600 and 7800 library on an original system as well as binaries of unreleased games and new homebrew titles. The Cuttle Cart II was a success by homebrew standards, selling out both production runs and commanding high prices on eBay.
A more recent development is the Atari 7800 expansion module developed by Legacy Engineering with a high scores save feature (with compatible games), additional RAM capabilities, as well as vastly improved sound capabilities using the POKEY and YM2151 sound chips. The Expansion module is designed as a pass-through device that sits on top of the console and requires no system modding.
Atari Flashback.
In 2004, Atari (now owned by Infogrames) released the first Atari Flashback console. This system resembled a miniature Atari 7800 and joysticks and had 20 built in games (five 7800 and fifteen 2600 titles). While the unit sold well, it was controversial among Atari fans. Atari had given the engineering firm, Legacy Engineering, extremely limited development timelines. The firm was forced to build the Flashback using NES-On-A-Chip hardware instead of recreating the Atari 7800 hardware. As a result, the Flashback has been criticized for failing to properly replicate the actual Atari gaming experience.
Legacy Engineering was later commissioned to create another 7800 project that was subsequently cancelled after prototypes were made.

</doc>
<doc id="2782" url="http://en.wikipedia.org/wiki?curid=2782" title="Atari Jaguar">
Atari Jaguar

The Atari Jaguar is a video game console that was released by Atari Corporation in 1993. It was the last to be marketed under the Atari brand until the release of the Atari Flashback in 2004. It was designed to surpass the Mega Drive/Genesis, Super Nintendo Entertainment System, and the Panasonic 3DO in processing power. Although launched one year earlier, it was eventually in competition with the Sega Saturn, the PlayStation, and other consoles that made up the fifth generation of video game consoles. The console was first released in New York City and San Francisco in 1993, and the rest of the US in early 1994. Although it was marketed as the first 64-bit gaming system, the Jaguar proved to be a commercial failure and prompted Atari to leave the home video game console market. Despite its commercial failure, the Jaguar has a dedicated fan base that produces homebrew games for it. It was the last console from an American company until the 2001 introduction of Microsoft's Xbox.
Development.
The Jaguar was developed by the members of Flare Technology, a company formed by Martin Brennan and John Mathieson. The team had claimed that they could not only make a console superior to the Sega Genesis or the Super Nintendo Entertainment System, but they could also be cost-effective.
Impressed by their work on the Konix Multisystem, Atari persuaded them to close Flare and form a new company called Flare II, with Atari providing the funding. Flare II initially set to work designing two consoles for Atari Corp. One was a 32-bit architecture (codenamed "Panther"), and the other was a 64-bit system (codenamed "Jaguar"); however, work on the Jaguar design progressed faster than expected, so Atari Corp. canceled the Panther project to focus on the more promising Jaguar.
Reception.
The Jaguar was introduced in 1993 at a price of $249.99, under a $500 million manufacturing deal with IBM. The system was initially marketed only in the New York City and the San Francisco Bay areas, under the slogan "Do the Math", claiming superiority over competing 16-bit and 32-bit systems. A US-wide release followed in early 1994.
The Atari Jaguar struggled to attain a substantial user base. In 1993, Atari reported that they had shipped 17,000 units as part of the system's initial test market. By the end of 1994, Atari reported that they had sold approximately 100,000 systems and had reduced the price to improve the competitive nature of the console. By the end of 1995, Sony and Sega had entered the marketplace with competing consoles and Atari's sales declined rapidly. In their 1995 annual report, they noted:
"Jaguar sales were substantially below Atari's expectations, and Atari's business and financial results were materially adversely affected in 1995 as Atari continued to invest heavily in Jaguar game development, entered into arrangements to publish certain licensed titles and reduced the retail price for its Jaguar console unit. Atari attributes the poor performance of Jaguar to a number of factors including (i) extensive delays in development of software for the Jaguar which resulted in reduced orders due to consumer concern as to when titles for the platform would be released and how many titles would ultimately be available, and (ii) the introduction of competing products by Sega and Sony in May 1995 and September 1995, respectively."
Jaguar did earn praise with titles such as "Tempest 2000", "Doom", and "Wolfenstein 3D". The most successful title during the Jaguar's first year was "Alien vs. Predator". Both it and "Tempest 2000" were named among the system's defining titles by "GamePro" in 2007. With such a small library of games to challenge the incumbent 16-bit game consoles, Jaguar's appeal never grew beyond a small gaming audience.
In 2006 IGN editor Craig Harris rated the Jaguar controller as the worst ever, criticizing the complexity of the "phone keypad" and the VGA-style D-subminiature connector. A version that has six action buttons, the Pro Controller, is suggested for certain games.
Lack of titles was attributable to two main factors: the Jaguar's questionable long-term prospects among third-party game-publishers and the problematic nature of developing games for the Jaguar. Atari had one opportunity to convince third-party developers, vital for the diversity of Jaguar's game library, with a solid retail-performance, but as things played out, post-holiday sales figures questioned the viability of Atari's business; Atari failed to attract many third-party developers already committed to other game platforms. In addition, the Jaguar's underlying hardware was crippled by a flaw in the CPU's memory controller, which prevented code execution out of system RAM. Less severe, but still annoying defects included a buggy UART. The memory controller flaw could have been mitigated by a mature code-development environment, to unburden the programmer from having to micromanage small chunks of code. Jaguar's development tools left much to the programmer's own implementation, as documentation was incomplete. Writing game-code was often an endurance exercise in the tedious assembler.
In a July 1995 interview with "Next Generation", then-CEO Sam Tramiel declared that the Jaguar was as powerful, if not more powerful, than the Sega Saturn, and slightly weaker than the PlayStation.
By the end of 1995, Atari's revenues declined by more than half, from US$38.7 million in 1994 to $14.6 million in 1995. In late 1995, Atari Corp. ran early-morning infomercial advertisements with enthusiastic salesmen touting the powerful game system. The infomercials ran most of the year, but did not significantly sell the remaining stock of Jaguar systems. In its 10-K405 SEC Filing, filed April 12, 1996, Atari informed their stockholders of the truly dire nature of the Jaguar business: Atari had already suffered an ill-fated crash in the mid-1980s as a result of the oversaturation of the video game market by third-party developers.
Production of the Jaguar ceased after Atari Corp. merged with JT Storage in a reverse takeover. In a last-ditch effort to revive the Jaguar, Atari Corp. tried to play down the other two consoles by proclaiming the Jaguar was the only "64-bit" system. This claim is questioned by some, because the CPU (68000) and GPU executed a 32-bit instruction-set, but sent control signals to the 64-bit graphics co-processors (or "graphics accelerators"). Atari Corp.'s position was that the mere presence of 64-bit ALUs for graphics was sufficient to validate the claim. Design specs for the console allude to the GPU or DSP being capable of acting as a CPU, leaving the Motorola 68000 to read controller inputs. In practice, however, many developers used the Motorola 68000 to drive gameplay logic.
Over the short life of the console, several add-on peripherals were announced. However, only the ProController, the Atari Jaguar CD drive, and the JagLink (a simple two-console networking device) reached retail shelves. A voice modem and VR headset (with infrared head-tracking) existed in prototype form, but were never commercialized (see Loki and Konix Multisystem for early development).
Legacy.
After the Atari Corporation properties were bought out by Hasbro Interactive in the late 1990s, Hasbro released the rights to the Jaguar, declaring the console an open platform and opening the doors for homebrew development. A few developers, including Telegames and Songbird Productions, have not only released previously unfinished materials from the Jaguar's past, but also several brand new titles to satisfy the system's cult following.
In the United Kingdom in 2001, a deal was struck between Telegames and retailer Game to bring the Jaguar to Game's retail outlets. The machine was initially sold for £29.99 brand new and the software was ranged between £9.99 for more common games such as "Doom" and "Ruiner Pinball", and up to £39.99 for more sought-after releases such as "Defender 2000" and "Checkered Flag". The machine had a presence in the stores until 2007 when remaining consoles were sold off for £9.99 and games were sold for as low as 97p.
This deal was seen as a move to remain competitive with Game's rival at the time, Gamestation, who were well known for stocking retro formats.
Imagin Systems, a manufacturer of dental imaging equipment, has since purchased the molding plates for the Jaguar's casing as with minor modification they were found to be the right size for housing their HotRod camera. The game cartridge molds were reused to create an optional memory expansion card.
The Jaguar continues to have a small and dedicated game development circle.
Technical specifications.
From the Jaguar Software Reference manual, page 1:
Jaguar is a custom chip set primarily intended to be the heart of a very high-performance games/leisure computer. It may also be used as a graphics accelerator in more complex systems, and applied to workstation and business uses. As well as a general purpose CPU, Jaguar contains four processing units. These are the Object Processor, Graphics Processor, Blitter, and Digital Sound Processor. Jaguar provides these blocks with a 64-bit data path to external memory devices, and is capable of a very high data transfer rate into external dynamic RAM.
Accessories.
Atari Jaguar CD.
The Atari Jaguar CD or Jag CD is a CD-ROM peripheral that sat on top the Atari Jaguar video game console and plugged in through the cartridge slot.
Memory Track.
Released in 1995, the Atari Jaguar CD "Memory Track" is a cartridge that contains a 128 K EEPROM, allowing Atari Jaguar CD games to save persistent data such as preferences and saved games. The Memory Track Program Manager is accessed by pushing the option button while the system is starting, and exited by pushing the * and # keys simultaneously.
Team Tap.
The Atari Jaguar "Team Tap" is a multi-player adapter for use on certain Jaguar games. It was available by itself, or as a pack-in with the four player game "White Men Can't Jump". Each Team Tap supports up to 4 players; by plugging Team Taps into both Jaguar cartridge ports, up to 8 players are supported. The only games compatible with the Team Tap are "White Men Can't Jump" and "NBA Jam Tournament Edition".
Jaglink Interface.
The Atari Jaguar "Jaglink Interface" enables players to hook two Atari Jaguars connected to two TVs together with a phone cable, allowing multiplayer gaming on two separate screens. The only games to support this add-on are "DOOM", "BattleSphere", and "AirCars". The Jaglink is also used to connect the Jaguar to a PC for game development using the JUGS DD (Jaguar Unmodified Game Server Dev Disc) dev system disk for Jaguar CD that was included in "BattleSphere Gold", which also came with the "JUGS Device" (a DB9 to phone cord adapter) to connect a PC to a Jaguar via Jaglink. Jaglink is also compatible with ICD's CatBox. Jaglink came in sets of two.
Atari Jaguar CD Bypass Cartridge.
The Atari Jaguar CD Bypass Cartridge from B&C Computervisions allows players to boot unencrypted discs on an Atari Jaguar CD system.
CatBox by ICD.
The "CatBox" is an unofficial expansion peripheral for the Atari Jaguar released in 1996 by the Rockford, Illinois company ICD. The ICD CatBox plugs directly into the AV/DSP connectors located in the rear of the Jaguar console and provides three main functions. These are audio, video, and communications. It features six output formats, three for audio (line level stereo, RGB monitor, headphone jack with volume control) and three for video (composite, S-Video, and RGB analog component video) making the Jaguar compatible with multiple high quality monitor systems and multiple monitors at the same time. It is capable of communications methods known as CatNet and RS-232 as well as DSP pass through, allowing the user to connect two or more Jaguars together for multi player games either directly or with modems. The ICD CatBox features a polished steel casing and red LEDs in the jaguar's eyes on the logo that indicate communications activity. An IBM AT type null modem cable may be used to connect two Jaguars together. The CatBox is also compatible with Atari's Jaglink Interface peripheral.
Atari Jaguar VR Headset Prototype.
Atari developed several Virtual Reality headset prototypes for the Jaguar. There were two models, a blue and grey high resolution unit, and a red and grey low resolution unit. After Atari merged with JTS in 1996, most of the headsets were reportedly destroyed, but not all of them. Supposedly, there are only two working units known to be left in existence. The only game released that was compatible with the Atari Jaguar VR Headset was Missile Command 3D.
Atari Jaguar Cortina Web TV Adapter Prototype.
An adaptor for the Jaguar that allows for WebTV access was revealed in 1998, one prototype is known to exist.
Jaguar Voice/Data Communicator aka Jaguar Voice Modem (JVM) Prototype.
In 1994 at the CES, Atari announced that it partnered up with Phylon, Inc. to create the Jaguar Voice/Data Communicator. The unit was delayed and eventually in 1995 mass production was canceled all together, but not before an estimated 100 or so were made. The JVM as it became known, utilized a 19.9kbit/s dial up modem and had the ability to answer incoming phone calls and store up to 18 phone numbers. Players were required to directly dial each other for online game play. The only Jaguar game that supports the JVM is Ultra Vortek, the modem is initialized in the Ultra Vortek start up screen by entering 911 on the key pad.
COJAG Arcade Games.
Atari Games licensed the Atari Jaguar's chipset for use in its arcade games. The system, named COJAG (for "Coin-Op Jaguar"), replaced the 68000 with a 68020 or MIPS R3000-based CPU (depending on the board version), and added a hard drive and more RAM. It ran the lightgun games "Area 51" and "Maximum Force", which were released by Atari as dedicated cabinets or as the Area 51/Maximum Force combo machine. Other games ("3 On 3 Basketball"; "Fishin' Frenzy"; "Freeze"; "Vicious Circle") were developed but never released.
Atari Jaguar Duo.
The Atari Jaguar Duo was a proposed console similar to the Sega Neptune. It was an attempt by Atari to combine the Atari Jaguar and Atari Jaguar CD to make a new console. It was never completed and was thus never released. After cancelling the console, Atari was bought by Hasbro and ceased all console development.

</doc>
<doc id="2783" url="http://en.wikipedia.org/wiki?curid=2783" title="Atari Lynx">
Atari Lynx

The Atari Lynx is a 16-bit handheld game console that was released by Atari Corporation in September 1989. The Lynx holds the distinction of being the world's first handheld electronic game with a color LCD. The system is also notable for its forward-looking features, advanced graphics, and ambidextrous layout. As part of the fourth generation of gaming, the Lynx competed with Nintendo's Game Boy (released just a month earlier), the Sega Game Gear and NEC's TurboExpress, both released the following year. 
As with many classic consoles, there has been a modern retrogaming community, creating and selling games for the system.
Features.
The Atari Lynx has several innovative features including it being the first color handheld, with a backlit display, a switchable right-handed/left-handed (upside down) configuration, and the ability to network with up to 17 other units via its "Comlynx" system (though most games would network eight or fewer players). Comlynx was originally developed to run over infrared links (and was codenamed RedEye). This was changed to a cable-based networking system before the final release.
The Lynx was cited as the "first gaming console with hardware support for zooming and distortion of sprites". Featuring a 4096 color palette and integrated math and graphics co-processors (including a blitter unit), its pseudo-3D color graphics display was said to be the key defining feature in the system's competition against Nintendo's monochromatic Game Boy. The fast pseudo-3D graphics features were made possible on a minimal hardware system by Needle having "invented the technique for planar expansion/shrinking capability" and using stretched, textured, triangles instead of full polygons. These particular features were achieved over a year prior to the launch of the Super Nintendo Entertainment System, whose stock hardware features the comparable Mode 7 but which can't scale sprites.
History.
The Lynx was the second handheld game system to be released with the Atari name. The first was Atari Inc.'s handheld electronic game "Touch Me". Atari Inc. had previously worked on several other handheld projects including the "Breakout", "Space Invaders", and the Atari Cosmos portable/tabletop console. However, those projects were shut down during development, some just short of their intended commercial release.
The Lynx system was originally developed by Epyx as the Handy Game. In 1986, two former Amiga designers, R.J. Mical and Dave Needle, had been asked by former manager at Amiga, David Morse, if they could come up a design for a portable gaming system. Morse now worked at Epyx, a game software company that had a recent string of hit games. Morse's son had asked him if he could make a portable gaming system, prompting the lunch with Mical and Needle to discuss the idea. Morse convinced Mical and Needle to develop the idea and they were hired by Epyx to be apart of the design team. Planning and design of the console began in 1986 and was completed in 1987. Epyx first showed the Handy system at the Winter Consumer Electronics Show (CES) in January 1989. Facing financial difficulties, Epyx sought out partners. Atari Corp. and Epyx eventually agreed that Atari Corp. would handle production and marketing, while Epyx would handle software development.
The Handy was designed to run games from the cartridge format, and the game data must be copied from ROM to RAM before it can be used, so less memory was available and the games initially load relatively slowly. There are trace remnants of a cassette tape interface physically capable of being programmed to read a tape. Lynx developers have noted that "there is still reference of the tape and some hardware addresses" and an updated vintage Epyx manual describes the bare existence of what could be utilized for tape support. A 2009 retrospective interview clarifies: "Although there had been reports that games were going to be loaded from tape, Mical says there was no truth in them. 'We did think about hard disk a little…'"
Atari Corp. changed the internal speaker and removed the thumb-stick on the control pad before releasing it as the Lynx, initially retailing in the US at . Atari Corp. then showed the Lynx to the press at the Summer 1989 CES as the "Portable Color Entertainment System", which was changed to "Lynx" when actual consoles were distributed to resellers.
The Lynx started off successful. With Atari reported that they had sold 90% of the 50,000 units it shipped in its launch month in the U.S. with a limited launch in New York. US sales in 1990 were approximately 500,000 units according to the Associated Press In late 1991, it was reported that Atari sales estimates were about 800,000, which Atari claimed was within their expected projections. Lifetime sales by 1995 amounted to less than 7 million units when combined with the Game Gear. In comparison, the Game Boy sold 16 million units by 1995. 
As with the actual console units, the game cartridges themselves evolved over the first year of the console's release. The first generation of cartridges were flat, and were designed to be stackable for ease of storage. However, this design proved to be very difficult to remove from the console and was replaced by a second design. This style, called "tabbed" or "ridged", used the same basic design as the original cartridges with the addition of two small tabs on the cartridge's underside to aid in removal. The original flat style cartridges could be stacked on top of the newer cartridges, but the newer cartridges could not be easily stacked on each other, nor were they stored easily. Thus a third style, the "curved lip" style was produced, and all official and third-party cartridges during the console's lifespan were released (or re-released) using this style.
In May 1991, Sega launched its Game Gear portable gaming handheld. Also a color handheld, in comparison to the Lynx it had a higher cost and shorter battery life (3–4 hours as opposed to 4-5 for the Lynx), but it was slightly smaller and was backed up by significantly more games. In North America the Game Gear took second place, and while in Europe sales of the Lynx were initially quite strong on the back of the popular Atari ST, it still could not compete with the software library of the Game Gear and was eventually pushed into third place. Retailers such as Game and Toys R Us continued to sell the Lynx well into the mid-90s on the back of the Atari Jaguar launch, helped by magazines such as Ultimate Future Games who continued to cover the Lynx alongside the new generation of 32-bit and 64-bit consoles.
During 1990, the Lynx had moderate sales. In 1991, Atari Corp. introduced the Lynx II with a new marketing campaign, new packaging, slightly improved hardware, better battery life and a new sleeker look. The new system (referred to within Atari Corp. as the "Lynx II") featured rubber hand grips and a clearer backlit color screen with a power save option (which turned off the LCD panel's backlighting). It also replaced the monaural headphone jack of the original Lynx with one wired for stereo. The new packaging made the Lynx available without any accessories, dropping the price to $99. Although sales improved, Nintendo still dominated the handheld market.
In 1994, Atari Corp. shifted its focus away from the Lynx. As Nintendo's Super Nintendo and Sega's Sega Genesis filled retailers' shelves, Atari Corp. refocused its efforts on its Jaguar console. A handful of games were released during this time, including "Battlezone 2000". In 1996, Atari shut down its internal game development.
Telegames released a number of games in the second half of the 1990s, including a port of "Raiden" and a platformer called "Fat Bobby" in 1997, as well as an action sports game called "Hyperdrome" in 1999. At the end of the 1990s, Hasbro, the owners of the Atari properties at the time, released the rights to develop for the system to the public domain. Since then a number of independent developers released games into the new decade, like "Championship Rally", "CyberVirus", and "Alpine Games". Some of the late 90s/early 2000s games were under development by other companies at one time, but rights to the game programs and all of the existing code was bought and finished by other developers.
In 2008 Atari was honored at the 59th Annual Technology & Engineering Emmy Awards for pioneering the development of handheld games with its Lynx game unit.
On October 24, 2009, North American company Super Fighter Team released "Zaku", a horizontal shooter for the Lynx developed by PenguiNet. It was the first new game for the system since the 1990s whose game card has an authentic "curved lip" plastic shell instead of a custom bare circuit board.
Reception.
Although it was innovative and unique for its time, gamers found the Atari Lynx to be quite large and bulky, even the second version of the unit.
However, the game system was reviewed in 1990 in "Dragon" #155 by Hartley, Patricia, and Kirk Lesser in "The Role of Computers" column. The reviewers gave the Lynx 5 out of 5 stars, stating that it "throws the Gameboy into the prehistoric age", and praising the built-in object scaling capabilities, the multiplayer feature of the ComLynx cable, and the strong set of launch games.

</doc>
<doc id="2784" url="http://en.wikipedia.org/wiki?curid=2784" title="Ahimsa">
Ahimsa

Ahimsa (; IAST: , Pāli: ) is a term meaning 'to not injure'. The word is derived from the Sanskrit root "hiṃs" – to strike; "hiṃsā" is injury or harm, "a-hiṃsā" is the opposite of this, i.e. cause no injury, do no harm. Ahimsa is also referred to as nonviolence, and it applies to all living beings including animals according to many Indian religions.
Ahimsa is one of the cardinal virtues and an important tenet of major Indian religions (Buddhism, Hinduism, and Jainism). Ahimsa is a multidimensional concept, inspired by the premise that all living beings have the spark of the divine spiritual energy, to hurt another being is to hurt oneself. Ahimsa has also been related to the notion that any violence has karmic consequences. While ancient scholars of Hinduism pioneered and over time perfected the principles of Ahimsa, the concept reached an extraordinary status in the ethical philosophy of Jainism. Most popularly, Mahatma Gandhi strongly believed in the principle of "ahimsa".
Ahimsa's precept of 'cause no injury' includes one's deeds, words, and thoughts. Classical literature of Hinduism such as Mahabharata and Ramayana, as well as modern scholars debate principles of Ahimsa when one is faced with war and situations requiring self-defense. The historic literature from India and modern discussions have contributed to theories of Just War, and theories of appropriate self-defense.
Etymology.
The word "Ahimsa" - sometimes spelled as "Ahinsa" - is derived from the Sanskrit root "hiṃs" – to strike; "hiṃsā" is injury or harm, "a-hiṃsā" is the opposite of this, i.e. "non harming" or "nonviolence".
There is a debate on the origins of the word "Ahimsa", and how its meaning evolved. Mayrhofer as well as Dumot suggest the root word may be "han" which means kill, which leads to the interpretation that "ahimsa" means "do not kill". Schmidt as well as Bodewitz explain the proper root word is "hiṃs" and the Sanskrit verb "hinasti", which leads to the interpretation "ahimsa" means "do not injure", or "do not hurt". Wackernagel-Debrunner concur with the latter explanation.
Ancient texts use ahimsa to mean non-injury, a broader concept than non-violence. Non-injury implies not killing others, as well as not hurting others mentally or verbally; it includes avoiding all violent means - including physical violence - anything that injures others. In classical Sanskrit literature of Hinduism, another word "Adrohi" is sometimes used instead of "Ahimsa", as one of the cardinal virtues necessary for moral life. One example is in Baudhayana Dharmasutra 2.6.23: वाङ्-मनः-कर्म-दण्डैर् भूतानाम् अद्रोही (One who does not injure others with words, thoughts or acts is named "Adrohi").
Hinduism.
Ancient Vedic Texts.
Ahimsa as an ethical concept evolves in Vedic texts. The oldest scripts, dated to be before 1700 BC, along with discussing ritual animal sacrifices, indirectly mention Ahimsa, but do not emphasize it. Over time, the Hindu scripts revise ritual practices and the concept of Ahimsa is increasingly refined and emphasized, ultimately Ahimsa becomes the concept that describes the highest virtue by the late Vedic era (about 500 BC). For example, one passage in the oldest Rig Veda reads, "do not harm anything"; later, the Yajur Veda dated to be between 1000 BC and 600 BC, states, "may all beings look at me with a friendly eye, may I do likewise, and may we look at each other with the eyes of a friend".
The term "Ahimsa" appears in the text Taittiriya Shakha of the Yajurveda (TS 5.2.8.7), where it refers to non-injury to the sacrificer himself. It occurs several times in the "Shatapatha Brahmana" in the sense of "non-injury" without a moral connotation. The Ahimsa doctrine is a late development in Brahmanical culture. The earliest reference to the idea of non-violence to animals ("pashu-Ahimsa"), apparently in a moral sense, is in the Kapisthala Katha Samhita of the Yajurveda (KapS 31.11), which may have been written in about the 8th century BCE.
Bowker claims the word scarcely appears in the principal Upanishads. Kaneda gives examples of the word "Ahimsa" in Upanishads. Other scholars suggest "Ahimsa" as an ethical concept that started evolving in the Vedas, became an increasingly central concept in Upanishads.
The Chāndogya Upaniṣad, dated to the 8th or 7th century BCE, one of the oldest Upanishads, has the earliest evidence for the use of the word "Ahimsa" in the sense familiar in Hinduism (a code of conduct). It bars violence against "all creatures" ("sarvabhuta") and the practitioner of Ahimsa is said to escape from the cycle of metempsychosis (CU 8.15.1). A few scholars are of the opinion that this noted concept started being recognized in mainstream Hinduism, after it was largely advocated by Jainism, Buddhism.
Chāndogya Upaniṣad also names Ahimsa, along with Satyavacanam (truthfulness), Arjavam (sincerity), Danam (charity), Tapo (penance/meditation), as one of five essential virtues (CU 3.17.4).
The Sandilya Upanishad lists ten forbearances: Ahimsa, Satya, Asteya, Brahmacharya, Daya, Arjava,Kshama, Dhriti, Mitahara and Saucha. According to Kaneda, the term Ahimsa is an important spiritual doctrine shared by Hinduism, Buddhism and Jainism. It literally means 'non-injury' and 'non-killing'. It implies the total avoidance of harming of any kind of living creatures not only by deeds, but also by words and in thoughts.
The Epics.
The Mahabharata, one of the epics of Hinduism, has multiple mentions of the phrase "Ahimsa Paramo Dharma" (अहिंसा परमॊ धर्मः), which literally means: non-violence is the highest moral virtue. For example, Mahaprasthanika Parva has the verse:
<poem>
अहिंसा परमं दानम अहिंसा परमस तपः।
अहिंसा परमं मित्रम अहिंसा परमं सुखम।
</poem>
The above passage from Mahabharata emphasizes the cardinal importance of Ahimsa in Hinduism, and literally means: Ahimsa is the highest virtue, Ahimsa is the highest self-control, Ahimsa is the greatest gift, Ahimsa is the best suffering, Ahimsa is the highest sacrifice, Ahimsa is the finest strength, Ahimsa is the greatest friend, Ahimsa is the greatest happiness, Ahimsa is the highest truth, and Ahimsa is the greatest teaching. Some other examples where the phrase "Ahimsa Paramo Dharma" are discussed include Adi Parva, Vana Parva and Anushasana Parva. The Bhagavad Gita, among other things, discusses the doubts and questions about appropriate response when one faces systematic violence or war. These verses develop the concepts of lawful violence in self-defense and the theories of just war. However, there is no consensus on this interpretation. Gandhi, for example, considers this debate about non-violence and lawful violence as a mere metaphor for the internal war within each human being, when he or she faces moral questions.
Self-defense, criminal law, and war.
The classical texts of Hinduism devote numerous chapters discussing what people who practice the virtue of Ahimsa, can and must do when they are faced with war, violent threat or need to sentence someone convicted of a crime. These discussions have led to theories of just war, theories of reasonable self-defense and theories of proportionate punishment. Arthashastra discusses, among other things, why and what constitutes proportionate response and punishment.
The precepts of Ahimsa under Hinduism require that war must be avoided, with sincere and truthful dialogue. Force must be the last resort. If war becomes necessary, its cause must be just, its purpose virtuous, its objective to restrain the wicked, its aim peace, its method lawful. War can only be started and stopped by a legitimate authority. Weapons used must be proportionate to the opponent and the aim of war, not indiscriminate tools of destruction. All strategies and weapons used in the war must be to defeat the opponent, not designed to cause misery to the opponent; for example, use of arrows is allowed, but use of arrows smeared with painful poison is not allowed. Warriors must use judgment in the battlefield. Cruelty to the opponent during war is forbidden. Wounded, unarmed opponent warriors must not be attacked or killed, they must be brought to your realm and given medical treatment. Children, women and civilians must not be injured. While the war is in progress, sincere dialogue for peace must continue.
In matters of self-defense, different interpretations of ancient Hindu texts have been offered. For example, Tähtinen suggests self-defense is appropriate, criminals are not protected by the rule of Ahimsa, and Hindu scriptures support the use of violence against an armed attacker. Ahimsa is not meant to imply pacifism.
Alternate theories of self-defense, inspired by Ahimsa, build principles similar to theories of just war. Aikido, pioneered in Japan, illustrates one such principles of self-defense. Morihei Ueshiba, the founder of Aikido, described his inspiration as Ahimsa. According to this interpretation of Ahimsa in self-defense, one must not assume that the world is free of aggression. One must presume that some people will, out of ignorance, error or fear, attack other persons or intrude into their space, physically or verbally. The aim of self-defense, suggested Ueshiba, must be to neutralize the aggression of the attacker, and avoid the conflict. The best defense is one where the victim is protected, as well as the attacker is respected and not injured if possible. Under Ahimsa and Aikido, there are no enemies, and appropriate self-defense focuses on neutralizing the immaturity, assumptions and aggressive strivings of the attacker.
Tähtinen concludes that Hindus have no misgivings about death penalty; their position is that evil-doers who deserve death should be killed, and that a king in particular is obliged to punish criminals and should not hesitate to kill them, even if they happen to be his own brothers and sons.
Other scholars conclude that the scriptures of Hinduism suggest sentences for any crime must be fair, proportional and not cruel.
There is no universal consensus on pacifism among Hindu scholars of modern times. The conflict between pacifistic interpretations of Ahimsa and the theories of just war prescribed by the Gita has been resolved by some scholars such as Mohandas Karamchand Gandhi, as being an allegory, wherein the battlefield is the soul and Arjuna, the war is within each human being, where man's higher impulses struggle against his own evil impulses.
Non-human life.
The Hindu precept of 'cause no injury' applies to animals and all life forms. This precept isn’t found in the oldest verses of Vedas, but increasingly becomes one of the central ideas between 500 BC and 400 AD. In the oldest texts, numerous ritual sacrifices of animals, including cows and horses, are highlighted and hardly any mention is made of Ahimsa to non-human life.
Hindu scriptures, dated to between 5th century and 1st century BC, while discussing human diet, initially suggest ‘‘kosher’’ meat may be eaten, evolving it with the suggestion that only meat obtained through ritual sacrifice can be eaten, then that one should eat no meat because it hurts animals, with verses describing the noble life as one that lives on flowers, roots and fruits alone.
Later texts of Hinduism declare Ahimsa one of the primary virtues, declare any killing or harming any life as against ‘‘dharma’’ (moral life). Finally, the discussion in Upanishads and Hindu Epics shifts to whether a human being can ever live his or her life without harming animal and plant life in some way; which and when plants or animal meat may be eaten, whether violence against animals causes human beings to become less compassionate, and if and how one may exert least harm to non-human life consistent with ahimsa precept, given the constraints of life and human needs. The Mahabharata permits hunting by warriors, but opposes it in the case of hermits who must be strictly non-violent. Sushruta Samhita, a Hindu text written in the 3rd or 4th century, in Chapter XLVI suggests proper diet as a means of treating certain illnesses, and recommends various fishes and meats for different ailments and for pregnant women, and the Charaka Samhita describes meat as superior to all other kinds of food for convalescents.
Across the texts of Hinduism, there is a profusion of ideas about the virtue of Ahimsa when applied to non-human life, but without a universal consensus. Alsdorf claims the debate and disagreements between supporters of vegetarian lifestyle and meat eaters was significant. Even suggested exceptions – ritual slaughter and hunting – were challenged by advocates of Ahimsa. In the Mahabharata both sides present various arguments to substantiate their viewpoints. Moreover, a hunter defends his profession in a long discourse.
Many of the arguments proposed in favor of non-violence to animals refer to the bliss one feels, the rewards it entails before or after death, the danger and harm it prevents, as well as to the karmic consequences of violence.
The ancient Hindu texts discuss Ahimsa and non-animal life. They discourage wanton destruction of nature including of wild and cultivated plants. Hermits (sannyasins) were urged to live on a fruitarian diet so as to avoid the destruction of plants. Scholars claim the principles of ecological non-violence is innate in the Hindu tradition, and its conceptual fountain has been Ahimsa as their cardinal virtue.
The classical literature of Hinduism exists in many Indian languages. For example, "Tirukkuṛaḷ" written between 200 BC and 400 AD, and sometimes called the Tamil Veda, is one of the most cherished classics on Hinduism written in a South Indian language. Tirukkuṛaḷ dedicates Chapter 32 and 33 of Book 1 to the virtue of Ahimsa. "Tirukkuṛaḷ" suggests that Ahimsa applies to all life forms.
Modern times.
In the 19th and 20th centuries, prominent figures of Indian spirituality such as Swami Vivekananda, Ramana Maharshi, Swami Sivananda, A. C. Bhaktivedanta Swami and in the present time Vijaypal Baghel emphasized the importance of Ahimsa.
Mohandas Karamchand Gandhi promoted the principle of Ahimsa, very successful by applying it to all spheres of life, particularly to politics (Swaraj). His non-violent resistance movement satyagraha had an immense impact on India, impressed public opinion in Western countries, and influenced the leaders of various civil and political rights movements such as the American civil rights movement's Martin Luther King, Jr. and James Bevel. In Gandhi’s thought, Ahimsa precludes not only the act of inflicting a physical injury, but also mental states like evil thoughts and hatred, unkind behavior such as harsh words, dishonesty and lying, all of which he saw as manifestations of violence incompatible with Ahimsa. Gandhi believed Ahimsa to be a creative energy force, encompassing all interactions leading one's self to find satya, "Divine Truth". Sri Aurobindo criticized the Gandhian concept of Ahimsa as unrealistic and not universally applicable; he adopted a pragmatic non-pacifist position, saying that the justification of violence depends on the specific circumstances of the given situation. Sri Aurobindo also indicated that Gandhi's Ahimsa led to partition of India as it blocked the forceful action that the Indian people were engaged in during the 1920s and 30s, which caused delay in independence, allowing other forces to take root, including those who wanted India divided.
A thorough historical and philosophical study of Ahimsa was instrumental in the shaping of Albert Schweitzer's principle of "reverence for life". Schweitzer criticized Indian philosophical and religious traditions for having conceived Ahimsa as the negative principle of avoiding violence instead of emphasizing the importance of positive action (helping injured beings).
Yoga.
Ahimsa is imperative for practitioners of Patañjali’s "classical" Yoga (Raja Yoga). It is one of the five Yamas (restraints) which make up the code of conduct, the first of the eight limbs of which this path consists. In the schools of Bhakti Yoga, the devotees who worship Vishnu or Krishna are particularly keen on Ahimsa. Another Bhakti Yoga school, Radha Soami Satsang Beas observes vegetarianism and moral living as aspects of "Ahimsa."
Ahimsa is also an obligation in Hatha Yoga according to the classic manual Hatha Yoga Pradipika (1.1.17). But it is important to note that Ahmisa as used here is distinct from that in Jainism. Yagnas and mantras are prescribed for attaining glory in Vedas.
Jainism.
In Jainism, the understanding and implementation of Ahimsa is more radical, scrupulous, and comprehensive than in any other religion. Non-violence is seen as the most essential religious duty for everyone ("", a statement often inscribed on Jain temples). Like in Hinduism, the aim is to prevent the accumulation of harmful karma. When Mahavira revived and reorganized the Jain movement in the 6th or 5th century BCE, Ahimsa was already an established, strictly observed rule. Parshva, the earliest Jain Tirthankara, whom modern Western historians consider to be a historical figure, lived in about the 8th century BCE. He founded the community to which Mahavira’s parents belonged. Ahimsa was already part of the "Fourfold Restraint" ("Caujjama"), the vows taken by Parshva’s followers. In the times of Mahavira and in the following centuries, Jains were at odds with both Buddhists and followers of the Vedic religion or Hindus, whom they accused of negligence and inconsistency in the implementation of Ahimsa. There is some evidence, however, that ancient Jain ascetics accepted meat as alms if the animal had not been specifically killed for them. Modern Jains deny this vehemently, especially with regard to Mahavira himself. According to the Jain tradition either lacto vegetarianism or veganism is mandatory.
The Jain concept of Ahimsa is characterized by several aspects. It does not make any exception for ritual sacrificers and professional warrior-hunters. Killing of animals for food is absolutely ruled out. Jains also make considerable efforts not to injure plants in everyday life as far as possible. Though they admit that plants must be destroyed for the sake of food, they accept such violence only inasmuch as it is indispensable for human survival, and there are special instructions for preventing unnecessary violence against plants. Jains go out of their way so as not to hurt even small insects and other minuscule animals. For example, Jains often do not go out at night, when they are more likely to step upon an insect. In their view, injury caused by carelessness is like injury caused by deliberate action. Eating honey is strictly outlawed, as it would amount to violence against the bees. Some Jains abstain from farming because it inevitably entails unintentional killing or injuring of many small animals, such as worms and insects, but agriculture is not forbidden in general and there are Jain farmers. Additionally, because they consider harsh words to be a form of violence, they often keep a cloth to ritually cover their mouth, as a reminder not to allow violence in their speech.
In contrast, Jains agree with Hindus that violence in self-defense can be justified, and they agree that a soldier who kills enemies in combat is performing a legitimate duty. Jain communities accepted the use of military power for their defense, there were Jain monarchs, military commanders, and soldiers.
Though, theoretically, all life forms are said to deserve full protection from all kinds of injury, Jains admit that this ideal cannot be completely implemented in practice. Hence, they recognize a hierarchy of life. Mobile beings are given higher protection than immobile ones. For the mobile beings, they distinguish between one-sensed, two-sensed, three-sensed, four-sensed and five-sensed ones; a one-sensed animal has touch as its only sensory modality. The more senses a being has, the more they care about its protection. Among the five-sensed beings, the rational ones (humans) are most strongly protected by Jain Ahimsa. In the practice of Ahimsa, the requirements are less strict for the lay persons who have undertaken "anuvrata" (Lesser Vows) than for the monastics who are bound by the Mahavrata "Great Vows".
Buddhism.
Unlike in Hindu and Jain sources, in ancient Buddhist texts "Ahimsa" (or its Pāli cognate ) is not used as a technical term. The traditional Buddhist understanding of non-violence is not as rigid as the Jain one, but like the Jains, Buddhists have always condemned the killing of all living beings. In some Buddhist traditions vegetarianism is not mandatory. In these traditions, monks and lay persons may eat meat and fish on condition that the animal was not killed specifically for them. For some monks, specifically monks of some Mahayana traditions, the eating of meat is strictly forbidden. Laypeople are also encouraged to eat vegetarian.
Since the beginnings of the Buddhist community, monks and nuns have had to commit themselves to the Five Precepts of moral conduct. In ancient Buddhism, lay persons were encouraged, but not obliged, to commit themselves to observe the Five Precepts of morality (). In both codes the first rule is to abstain from taking the life of a sentient being (). Buddhist monks should avoid cutting or burning trees, because some sentient beings rely on them.
War.
Unlike the Vedic religion, ancient Buddhism had strong misgivings about violent ways of punishing criminals and war. Neither was explicitly condemned, but peaceful ways of conflict resolution and punishment with the least amount of injury were encouraged. The early texts condemn the mental states that lead to violent behavior.
Non-violence is an overriding concern of the Pali Canon. While the early texts condemn killing in the strongest terms, and portray the ideal king as a pacifist, such a king is nonetheless flanked by an army. It seems that the Buddha's teaching on non-violence was not interpreted or put into practice in an uncompromisingly pacifist or anti-military-service way by early Buddhists. The early texts assume war to be a fact of life, and well-skilled warriors are viewed as necessary for defensive warfare. In Pali texts, injunctions to abstain from violence and involvement with military affairs are directed at members of the sangha; later Mahayana texts, which often generalize monastic norms to laity, require this of lay people as well.
The early texts do not contain just-war ideology as such. Some argue that a sutta in the "Gamani Samyuttam" rules out all military service. In this passage, a soldier asks the Buddha if it is true that, as he has been told, soldiers slain in battle are reborn in a heavenly realm. The Buddha reluctantly replies that if he is killed in battle while his mind is seized with the intention to kill, he will undergo an unpleasant rebirth. In the early texts, a person's mental state at the time of death is generally viewed as having an inordinate impact on the next birth.
Some Buddhists point to other early texts as justifying defensive war. One example is the "Kosala Samyutta", in which King Pasenadi, a righteous king favored by the Buddha, learns of an impending attack on his kingdom. He arms himself in defense, and leads his army into battle to protect his kingdom from attack. He lost a battle but won the war. King Pasenadi defeated King Ajatasattu and captured him alive. He thought that although this King of Magadha has transgressed against his kingdom, he had not transgressed against him personally, and Ajatasattu is still his nephew. He released Ajatasattu and did not harm him. Upon his return, the Buddha says, among other things, that Pasenadi is "a friend of virtue, acquainted with virtue, intimate with virtue", while the opposite is said of the aggressor, King Ajatasattu.
According to Theravada commentaries, there are five requisite factors that must all be fulfilled for an act to be both an act of killing and to be karmically negative. These are: (1) the presence of a living being, human or animal; (2) the knowledge that the being is a living being; (3) the intent to kill; (4) the act of killing by some means; and (5) the resulting death. Some Buddhists have argued on this basis that the act of killing is complicated, and its ethicization is predicated upon intent. Some have argued that in defensive postures, for example, the primary intention of a soldier is not to kill, but to save, and the act of killing in that situation would have minimal negative karmic repercussions.
According to Dr. Babasaheb Ambedkar, the doctrine of Ahimsa does not state "kill not" but rather "love all". Buddha said ""Love all, so that you may not wish to kill any."" This is a positive way of stating the principle of Ahimsa. The Buddha's Ahimsa is quite in keeping with his middle path. To put it differently, the Buddha made a distinction between a principle and a rule. He did not make Ahimsa a matter of rule. He enunciated it as a matter of principle. A principle leaves you freedom to act; a rule does not.
Laws.
The emperors of Sui dynasty, Tang dynasty and early Song dynasty banned killing in Lunar calendar 1st, 5th, and 9th month. Empress Wu Tse-Tien banned killing for more than half a year in 692. Some also banned fishing for some time each year.
The King Bayinnaung of Burma, after conquering the Bago in 1559, the Buddhist King prohibited the practice of halal, specifically, killing food animals in the name of God. He also disallowed the Eid al-Adha religious sacrifice of cattle. Halal food was also forbidden by King Alaungpaya in the late 18th century.
There were bans after death of emperors, Buddhist and Taoist prayers,
an 8 days ban from August 12, 1959 after the August 7 flood (八七水災), the last big flood before the 88 Taiwan Flood. There was a 3-day ban after the death of Chiang Kai-shek.
People avoid killing during some festivals, like the Taoist Ghost Festival, the Nine Emperor Gods Festival, the Vegetarian Festival and many others.

</doc>
<doc id="2785" url="http://en.wikipedia.org/wiki?curid=2785" title="Annals of Mathematics">
Annals of Mathematics

The Annals of Mathematics is a bimonthly mathematical journal published by Princeton University and the Institute for Advanced Study.
History.
The journal was established as "The Analyst" in 1874 and with Joel E. Hendricks as the founding editor-in-chief. It was "intended to afford a medium for the presentation and analysis of any and all questions of interest or importance in pure and applied Mathematics, embracing especially all new and interesting discoveries in theoretical and practical astronomy, mechanical philosophy, and engineering". It was published in Des Moines, Iowa, and was the earliest American mathematics journal to be published continuously for more than a year or two. This incarnation of the journal ceased publication after its tenth year, in 1883, giving as an explanation Hendricks' declining health, but Hendricks made arrangements to have it taken over by new management, and it was continued from March 1884 as the "Annals of Mathematics". The new incarnation of the journal was edited by Ormond Stone (University of Virginia). It moved to Harvard in 1899 before reaching its current home in Princeton in 1911.
An important period for the journal was 1928–1958 with Solomon Lefschetz as editor. During this time, it became an increasingly well-known and respected journal. Its rise, in turn, stimulated American mathematics. Norman Steenrod characterized Lefschetz' impact as editor as follows: ""The importance to American mathematicians of a first-class journal is that it sets high standards for them to aim at. In this somewhat indirect manner, Lefschetz profoundly affected the development of mathematics in the United States.""
Princeton University continued to publish the annals on its own until 1933, when the Institute for Advanced Study took joint editorial control. Since 1998 it has been available in an electronic edition, alongside its regular print edition. The electronic edition was available without charge, as an open access journal, but since 2008 this is no longer the case. Issues from before 2003 were transferred to the non-free JSTOR archive, and articles are not freely available until 5 years after publication.
Editors.
The current editors of the "Annals of Mathematics" are Jean Bourgain, Richard Taylor (both from the Institute for Advanced Study), David Gabai, Nick Katz, Sergiu Klainerman, and Gang Tian (all from Princeton University).
Abstracting and indexing.
The journal is abstracted and indexed in the Science Citation Index, Current Contents/Physical, Chemical & Earth Sciences, and Scopus. According to the "Journal Citation Reports", the journal has a 2012 impact factor of 3.027, ranking it third out of 296 journals in the category "Mathematics".

</doc>
<doc id="2786" url="http://en.wikipedia.org/wiki?curid=2786" title="Andrei Sakharov">
Andrei Sakharov

Andrei Dmitrievich Sakharov (; May 21, 1921December 14, 1989) was a Russian nuclear physicist, Soviet dissident and human rights activist.
He became renowned as the designer of the Soviet Union's Third Idea, a codename for Soviet development of thermonuclear weapons. Sakharov was an advocate of civil liberties and civil reforms in the Soviet Union. He was awarded the Nobel Peace Prize in 1975. The Sakharov Prize, which is awarded annually by the European Parliament for people and organizations dedicated to human rights and freedoms, is named in his honor.
Biography.
Sakharov was born in Moscow on May 21, 1921. His father was Dmitri Ivanovich Sakharov, a private school physics teacher and an amateur pianist. His father later taught at the Second Moscow State University. Dmitri's grandfather Ivan had been a prominent lawyer in imperial Russia who had displayed respect for social awareness and humanitarian principles (including advocating the abolition of capital punishment) that would later influence his grandson. Sakharov's mother was Yekaterina Alekseyevna Sakharova, a great-granddaughter of the prominent military commander Alexey Semenovich Sofiano (who was of Greek ancestry). His parents and his paternal grandmother, Maria Petrovna, largely shaped Sakharov's personality. Although his paternal great-grandfather had been a priest in the Russian Orthodox Church, and his pious mother did have him baptised, he was an atheist in later life. However, he did believe that a non-scientific "guiding principle" governed the universe and human life.
Education and career.
Sakharov entered Moscow State University in 1938. Following evacuation in 1941 during the Great Patriotic War (World War II), he graduated in Aşgabat, in today's Turkmenistan. He was then assigned laboratory work in Ulyanovsk. During this period, in 1943, he married Klavdia Alekseyevna Vikhireva, with whom he raised two daughters and a son before she died in 1969. He returned to Moscow in 1945 to study at the Theoretical Department of FIAN (the Physical Institute of the Soviet Academy of Sciences). He received his Ph.D. in 1947.
Development of thermonuclear devices.
After the end of World War II, he researched cosmic rays. In mid-1948 he participated in the Soviet atomic bomb project under Igor Kurchatov and Igor Tamm. The first Soviet atomic device was tested on August 29, 1949. After moving to Sarov in 1950, Sakharov played a key role in the development of the first megaton-range Soviet hydrogen bomb using a design known as "Sakharov's Third Idea" in Russia and the Teller-Ulam design in the United States. Before his "Third Idea", Sakharov tried a "layer cake" of alternating layers of fission and fusion fuel. The results were disappointing, yielding no more than a typical fission bomb. However the design was worth pursuing because deuterium is abundant and uranium is scarce, and he had no idea how powerful the US design was. One of the Bikini atomic experiments changed that, because the magnitude of the explosion became public knowledge, because of the dispute between Japan and the US over the contamination of a large area of ocean. Sakharov was surprised by the size of the explosion and immediately realized that the Americans had harnessed the power of a separate fission explosion to compress the fusion fuel. Sakharov realised that in order to cause the explosion of one side of the fuel to symmetrically compress the fusion fuel, a mirror could be used to reflect the radiation. The details had not been officially declassified in Russia when Sakharov was writing his memoirs, but we know that in the Teller-Ulam design soft X-rays emitted by the fission bomb were focused onto a cylinder of lithium deuteride to compress it symmetrically. This is called Radiation implosion. (The X-rays cause rapid heating of the surface of the fusion fuel, causing particles to be ejected at a high velocity. The conservation of momentum causes a recoil that compresses the fuel.) The Teller-Ulam design also had a secondary fission device inside the fusion cylinder to assist with the compression of the fusion fuel and generate neutrons to convert some of the lithium to tritium, producing a mixture of deuterium and tritium. Sakarov's idea was first tested as RDS-37 in 1955. A larger variation of the same design which Sakharov worked on was the 50 Mt Tsar Bomba of October 1961, which was the most powerful nuclear device ever detonated.
Sakharov saw "striking parallels" between his fate and those of J. Robert Oppenheimer and Edward Teller in the USA. Sakharov believed that in this "tragic confrontation of two outstanding people", both deserved respect, because "each of them was certain he had right on his side and was morally obligated to go to the end in the name of truth." While Sakharov strongly disagreed with Teller over nuclear testing in the atmosphere and Strategic Defense Initiative, he believed that American academics had been unfair to Teller’s resolve to get the H-bomb for the United States since "all steps by the Americans of a temporary or permanent rejection of developing thermonuclear weapons would have been seen either as a clever feint, or as the manifestation of stupidity. In both cases, the reaction would have been the same – avoid the trap and immediately take advantage of the enemy’s stupidity."
Sakharov never felt that by creating nuclear weapons he had "known sin", in Oppenheimer’s expression. He later wrote: "After more than forty years, we have had no third world war, and the balance of nuclear terror ... may have helped to prevent one. But I am not at all sure of this; back then, in those long-gone years, the question didn’t even arise. What most troubles me now is the instability of the balance, the extreme peril of the current situation, the appalling waste of the arms race ... Each of us has a responsibility to think about this in global terms, with tolerance, trust, and candor, free from ideological dogmatism, parochial interests, or national egotism."
Support for peaceful use of nuclear technology.
In 1950 he also proposed an idea for a controlled nuclear fusion reactor, the tokamak, which is still the basis for the majority of work in the area. Sakharov, in association with Igor Tamm, proposed confining extremely hot ionized plasma by torus shaped magnetic fields for controlling thermonuclear fusion that led to the development of the tokamak device.
Efforts to improve nuclear reactor technology.
In 1951 he invented and tested the first explosively pumped flux compression generators, compressing magnetic fields by explosives. He called these devices MC or MK (for "magnetocumulative") generators. The radial MK-1 produced a pulsed magnetic field of 25 megagauss (2500 teslas). The following helical MK-2 generated 100 million amperes in 1953.
Sakharov then tested a MK-driven "plasma cannon" where a small aluminium ring was vaporized by huge eddy currents into a stable, self-confined toroidal plasmoid and was accelerated to 100 km/s. Sakharov later suggested to replace the copper coil in MK generators by a big superconductor solenoid to magnetically compress and focus underground nuclear explosions into a shaped charge effect. He theorized this could focus 1023 protons per second on a 1 mm2 surface, then envisaged making two such beams collide. But it is not known if any experiment based on this idea has been ever achieved.
Research and physics.
After 1965 Sakharov returned to fundamental science and began working on particle physics and cosmology.
He especially tried to explain the baryon asymmetry of the universe, being the first scientist to introduce two universes called "sheets", linked by the Big Bang. Sakharov achieved there a complete CPT symmetry since the second sheet is enantiomorph (P-symmetry), has an opposite arrow of time (T-symmetry) and is mainly populated by antimatter (C-symmetry) because of an opposite CP-violation. In this model the two universes do not interact, except via local matter accumulation whose density and pressure would become high enough to connect the two sheets through a bridge without spacetime between them, but with geodesics continuity beyond the radius limit allowing an exchange of matter. Sakharov called such singularities a "collapse" and an "anticollapse", which are an alternative to the couple black hole and white hole in the wormhole theory. Sakharov also proposed the idea of induced gravity as an alternative theory of quantum gravity.
Turn to activism.
Since the late 1950s Sakharov had become concerned about the moral and political implications of his work. Politically active during the 1960s, Sakharov was against nuclear proliferation. Pushing for the end of atmospheric tests, he played a role in the 1963 Partial Test Ban Treaty, signed in Moscow.
The major turn in Sakharov’s political evolution came in 1967, when anti-ballistic missile defense became a key issue in US–Soviet relations. In a secret detailed letter to the Soviet leadership of July 21, 1967, Sakharov explains the need to "take the Americans at their word" and accept their proposal "for a bilateral rejection by the USA and the Soviet Union of the development of antiballistic missile defense", because otherwise an arms race in this new technology would increase the likelihood of nuclear war. He also asked permission to publish his manuscript (which accompanied the letter) in a newspaper to explain the dangers posed by this kind of defense. The government ignored his letter and refused to let him initiate a public discussion of ABMs in the Soviet press.
In May 1968 he completed an essay, "Reflections on Progress, Peaceful Coexistence, and Intellectual Freedom", where the anti-ballistic missile defense is described as a major threat of world nuclear war. After this essay was circulated in "samizdat" and then published outside the Soviet Union (initially on July 6, 1968, in the Dutch newspaper "Het Parool" through intermediary of the Dutch academic and writer Karel van het Reve, followed by "The New York Times"), Sakharov was banned from all military-related research and returned to FIAN to study fundamental theoretical physics. In 1970 he, along with Valery Chalidze and Andrei Tverdokhlebov, was one of the founders of the Committee on Human Rights in the USSR and came under increasing pressure from the government. He married a fellow human rights activist, Yelena Bonner, in 1972.
In 1973 and 1974, the Soviet media campaign targeted both Andrei Sakharov and Aleksandr Solzhenitsyn. While Sakharov disagreed with Solzhenitsyn’s Slavophile vision of Russian revival, he deeply respected him for his courage. Only a few individuals in the Soviet Union dared to defend 'traitors' like Sakharov and Solzhenitsyn, and those who had dared were inevitably punished.
Sakharov later described that "it took years" for him "to understand how much substitution, deceit, and lack of correspondence with reality there was" in the Soviet ideals. "At first I thought, despite everything that I saw with my own eyes, that the Soviet state was a breakthrough into the future, a kind of prototype for all countries". Then he came, in his words, to "the theory of symmetry: all governments and regimes to a first approximation are bad, all peoples are oppressed, and all are threatened by common dangers." After that he realized that there is not much "symmetry between a cancer cell and a normal one. Yet our state is similar to a cancer cell – with its messianism and expansionism, its totalitarian suppression of dissent, the authoritarian structure of power, with a total absence of public control in the most important decisions in domestic and foreign policy, a closed society that does not inform its citizens of anything substantial, closed to the outside world, without freedom of travel or the exchange of information." Sakharov's ideas on social development led him to put forward the principle of human rights as a new basis of all politics. In his works he declared that "the principle 'what is not prohibited is allowed' should be understood literally", defying the unwritten ideological rules imposed by the Communist ruling elite on the society in spite of the seemingly democratic USSR Constitution.
In 1973, Andrei Sakharov was nominated for the Nobel Peace Prize and in 1974 was awarded the Prix mondial Cino Del Duca. He was awarded the Nobel Peace Prize in 1975, although he was not allowed to leave the Soviet Union to collect it. His wife read his speech at the ceremony in Oslo, Norway. The Norwegian Nobel Committee called him "a spokesman for the conscience of mankind". In the words of the Nobel Committee’s citation: "In a convincing manner Sakharov has emphasised that Man's inviolable rights provide the only safe foundation for genuine and enduring international cooperation."
In no way did Sakharov consider himself a prophet or the like: "I am no volunteer priest of the idea, but simply a man with an unusual fate. I am against all kinds of self-immolation (for myself and for others, including the people closest to me)." In a letter written from his exile, he cheered up a fellow physicist and human rights activist with the words: "Fortunately, the future is unpredictable and also – because of quantum effects – uncertain." For Sakharov the indeterminacy of the future supported his belief that he could, and should, take personal responsibility for it.
Internal exile.
Sakharov was arrested on January 22, 1980, following his public protests against the Soviet intervention in Afghanistan in 1979, and was sent to internal exile in the city of Gorky, now Nizhny Novgorod, a city that was off-limits to foreigners.
Between 1980 and 1986, Sakharov was kept under tight Soviet police surveillance. In his memoirs he mentions that their apartment in Gorky was repeatedly subjected to searches and heists. Sakharov was named the 1980 Humanist of the Year by the American Humanist Association.
In May 1984, Sakharov's wife, Yelena Bonner, was detained and Sakharov began a hunger strike, demanding permission for his wife to travel to the United States for heart surgery. He was forcibly hospitalized and force-fed. He was held in isolation for four months. In August 1984 Yelena Bonner was sentenced by a court to five years of exile in Gorky.
In April 1985, Sakharov started a new hunger strike for his wife to travel abroad for medical treatment. He again was taken to a hospital and force-fed. He remained in the hospital until October 1985 when his wife finally was allowed to travel to the United States. She had heart surgery in the United States and returned to Gorky in June 1986.
Most of Sakharov's friends in the human rights movement failed to appreciate the motivation for his hunger strikes and blamed Bonner for his sufferings. Sakharov, meanwhile, claimed his human right to make decisions that he felt to be morally necessary for him personally.
In December 1985, the European Parliament established the Sakharov Prize for Freedom of Thought, to be given annually for outstanding contributions to human rights.
On December 19, 1986, Mikhail Gorbachev, who had initiated the policies of perestroika and glasnost, called Sakharov to tell him that he and his wife may return to Moscow.
Political leader.
In 1988, Sakharov was given the International Humanist Award by the International Humanist and Ethical Union. He helped to initiate the first independent legal political organizations and became prominent in the Soviet Union's growing political opposition. In March 1989, Sakharov was elected to the new parliament, the All-Union Congress of People's Deputies and co-led the democratic opposition, the Inter-Regional Deputies Group.
Death.
Soon after 21:00 on December 14, 1989, Sakharov went to his study to take a nap before preparing an important speech he was to deliver the next day in the Congress. His wife went to wake him at 23:00 as he had requested but she found Sakharov dead on the floor. He died of a heart attack at the age of 68. He was interred in the Vostryakovskoye Cemetery in Moscow.
Influence.
The Sakharov Prize, established in 1988 and awarded annually by the European Parliament for people and organizations dedicated to human rights and freedoms, was named in his honor.
An Andrei Sakharov prize is also to be awarded by the American Physical Society every second year from 2006, "to recognize outstanding leadership and/or achievements of scientists in upholding human rights".
The Andrei Sakharov Prize For Writer's Civic Courage was established in October 1990.
Andrei Sakharov Archives and Human Rights Center.
The Andrei Sakharov Archives and Human Rights Center, established at Brandeis University in 1993, are now housed at Harvard University.
The documents from that archive were published by the Yale University Press in 2005. These documents are available online.
Most of documents of the archive are letters from the head of the KGB to the Central Committee about activities of Soviet dissidents and recommendations about the interpretation in newspapers. The letters cover the period from 1968 to 1991 (Brezhnev stagnation). The documents characterize not only the Sakharov's activity, but that of other dissidents, as well as that of highest-position apparatchiks, and the KGB. No Russian equivalent of the KGB archive is available.
Sakharov Prize for Freedom of Thought.
Established in 1988 in honour of Russian nuclear scientist and human rights activist Andrei Sakharov, the Sakharov Prize for Freedom of Thought is the highest tribute to human rights endeavours the European Union accords. It is awarded to those who carry the spirit of Soviet dissident Andrei Sakharov. With this in mind, the Parliament selects Laureates who, like Sakharov, dedicate their lives to peaceful struggle for human rights.
Honours and awards.
In 1980, Sakharov was stripped of all Soviet awards for "anti-Soviet activities". Later, during glasnost, he declined the return of his awards and, consequently, Mikhail Gorbachev did not sign the necessary decree.

</doc>
<doc id="2787" url="http://en.wikipedia.org/wiki?curid=2787" title="Astrobiology">
Astrobiology

Astrobiology is the study of the origin, evolution, distribution, and future of life in the universe: extraterrestrial life and life on Earth. This interdisciplinary field encompasses the search for habitable environments in our Solar System and habitable planets outside our Solar System, the search for evidence of prebiotic chemistry, laboratory and field research into the origins and early evolution of life on Earth, and studies of the potential for life to adapt to challenges on Earth and in outer space. Astrobiology addresses the question of whether life exists beyond Earth, and how humans can detect it if it does. (The term exobiology is similar but more specific — it covers the search for life beyond Earth, and the effects of extraterrestrial environments on living things.)
Astrobiology makes use of physics, chemistry, astronomy, biology, molecular biology, ecology, planetary science, geography, and geology to investigate the possibility of life on other worlds and help recognize biospheres that might be different from the biosphere on Earth. Astrobiology concerns itself with interpretation of existing scientific data; given more detailed and reliable data from other parts of the universe, the roots of astrobiology itself—physics, chemistry and biology—may have their theoretical bases challenged. Although speculation is entertained to give context, astrobiology concerns itself primarily with hypotheses that fit firmly into existing scientific theories.
Earth is the only place in the universe known to harbor life. However, recent advances in planetary science have changed fundamental assumptions about the possibility of life in the universe, raising the estimates of habitable zones around other stars, along with the discovery of hundreds of extrasolar planets and new insights into the extreme habitats here on Earth, suggesting that there may be many more habitable places in the universe than considered possible until very recently. On 4 November 2013, astronomers reported, based on "Kepler" space mission data, that there could be as many as 40 billion Earth-sized planets orbiting in the habitable zones of sun-like stars and red dwarf stars within the Milky Way Galaxy. 11 billion of these estimated planets may be orbiting sun-like stars. The nearest such planet may be 12 light-years away, according to the scientists.
It has been proposed that viruses are likely to be encountered on other life-bearing planets. Efforts to discover current or past life on Mars, is an active area of research. On 24 January 2014, NASA reported that current studies on the planet Mars by the "Curiosity" and "Opportunity" rovers will now be searching for evidence of ancient life, including a biosphere based on autotrophic, chemotrophic and/or chemolithoautotrophic microorganisms, as well as ancient water, including fluvio-lacustrine environments (plains related to ancient rivers or lakes) that may have been habitable. The search for evidence of habitability, taphonomy (related to fossils), and organic carbon on the planet Mars is now a primary NASA objective.
Overview.
"Astrobiology" is etymologically derived from the Greek , "astron", "constellation, star"; , "bios", "life"; and , "-logia", "study". The synonyms of astrobiology are diverse; however, the synonyms were structured in relation to the most important sciences implied in its development: astronomy and biology. A close synonym is "exobiology" from the Greek , "external"; Βίος, "bios", "life"; and λογία, -logia, "study". The term exobiology was first coined by molecular biologist Joshua Lederberg. Exobiology is considered to have a narrow scope limited to search of life external to earth, whereas subject area of astrobiology is wider and investigates the link between life and the universe, which includes the search for extraterrestrial life, but also includes the study of life on earth, its origin, evolution and limits. Exobiology as a term has tended to be replaced by astrobiology.
Another term used in the past is xenobiology, ("biology of the foreigners") a word used in 1954 by science fiction writer Robert Heinlein in his work The Star Beast.
The term "xenobiology" is now used in a more specialized sense, to mean "biology based on foreign chemistry", whether of extraterrestrial or terrestrial (possibly synthetic) origin. Since alternate chemistry analogs to some life-processes have been created in the laboratory, xenobiology is now considered as an extant subject.
While it is an emerging and developing field, the question of whether life exists elsewhere in the universe is a verifiable hypothesis and thus a valid line of scientific inquiry. Though once considered outside the mainstream of scientific inquiry, astrobiology has become a formalized field of study. Planetary scientist David Grinspoon calls astrobiology a field of natural philosophy, grounding speculation on the unknown, in known scientific theory. NASA's interest in exobiology first began with the development of the U.S. Space Program. In 1959, NASA funded its first exobiology project, and in 1960, NASA founded an Exobiology Program; Exobiology research is now one of four elements of NASA's current Astrobiology Program. In 1971, NASA funded the Search for Extra-Terrestrial Intelligence (SETI) to search radio frequencies of the electromagnetic spectrum for signals being transmitted by extraterrestrial life outside the Solar System. NASA's Viking missions to Mars, launched in 1976, included three biology experiments designed to look for possible signs of present life on Mars. The Mars Pathfinder lander in 1997 carried a scientific payload intended for exopaleontology in the hopes of finding microbial fossils entombed in the rocks.
In the 21st century, astrobiology is a focus of a growing number of NASA and European Space Agency Solar System exploration missions. The first European workshop on astrobiology took place in May 2001 in Italy, and the outcome was the Aurora programme. Currently, NASA hosts the NASA Astrobiology Institute and a growing number of universities in the United States (e.g., University of Arizona, Penn State University, Montana State University – Bozeman, University of Washington, and Arizona State University), Britain (e.g., The University of Glamorgan, Buckingham University), Canada, Ireland, and Australia (e.g., The University of New South Wales) now offer graduate degree programs in astrobiology. The International Astronomical Union regularly organizes international conferences through its Bioastronomy Commission.
Advancements in the fields of astrobiology, observational astronomy and discovery of large varieties of extremophiles with extraordinary capability to thrive in the harshest environments on Earth, have led to speculation that life may possibly be thriving on many of the extraterrestrial bodies in the universe. A particular focus of current astrobiology research is the search for life on Mars due to its proximity to Earth and geological history. There is a growing body of evidence to suggest that Mars has previously had a considerable amount of water on its surface, water being considered an essential precursor to the development of carbon-based life.
Missions specifically designed to search for life include the Viking program and Beagle 2 probes, both directed to Mars. The Viking results were inconclusive, and Beagle 2 failed to transmit from the surface and is assumed to have crashed. A future mission with a strong astrobiology role would have been the Jupiter Icy Moons Orbiter, designed to study the frozen moons of Jupiter—some of which may have liquid water—had it not been cancelled. In late 2008, the Phoenix lander probed the environment for past and present planetary habitability of microbial life on Mars, and to research the history of water there.
In November 2011, NASA launched the Mars Science Laboratory (MSL) rover, nicknamed "Curiosity", which landed on Mars at Gale Crater in August 2012. "Curiosity" rover is currently probing the environment for past and present planetary habitability of microbial life on Mars. On 9 December 2013, NASA reported that, based on evidence from "Curiosity" studying Aeolis Palus, Gale Crater contained an ancient freshwater lake which could have been a hospitable environment for microbial life.
The European Space Agency is currently collaborating with the Russian Federal Space Agency (Roscosmos) and developing the ExoMars astrobiology rover, which is to be launched in 2018.
Methodology.
Planetary habitability.
When looking for life on other planets like the earth, some simplifying assumptions are useful to reduce the size of the task of the astrobiologist. One is to assume that the vast majority of life forms in our galaxy are based on carbon chemistries, as are all life forms on Earth. Carbon is well known for the unusually wide variety of molecules that can be formed around it. Carbon is the fourth most abundant element in the universe and the energy required to make or break a bond is just at an appropriate level for building molecules which are not only stable, but also reactive. The fact that carbon atoms bond readily to other carbon atoms allows for the building of arbitrarily long and complex molecules.
The presence of liquid water is a useful assumption, as it is a common molecule and provides an excellent environment for the formation of complicated carbon-based molecules that could eventually lead to the emergence of life. Some researchers posit environments of ammonia, or more likely, water-ammonia mixtures.
A third assumption is to focus on sun-like stars. This comes from the idea of planetary habitability. Very big stars have relatively short lifetimes, meaning that life would not likely have time to evolve on planets orbiting them. Very small stars provide so little heat and warmth that only planets in very close orbits around them would not be frozen solid, and in such close orbits these planets would be tidally "locked" to the star. Without a thick atmosphere, one side of the planet would be perpetually baked and the other perpetually frozen. In 2005, the question was brought back to the attention of the scientific community, as the long lifetimes of red dwarfs could allow some biology on planets with thick atmospheres. This is significant, as red dwarfs are extremely common. (See Habitability of red dwarf systems).
It is estimated that 10% of the stars in our galaxy are sun-like; there are about a thousand such stars within 100 light-years of our Sun. These stars would be useful primary targets for interstellar listening. Since Earth is the only planet known to harbor life, there is no evident way to know if any of the simplifying assumptions are correct.
Communication attempts.
Research on communication with extraterrestrial intelligence (CETI) focuses on composing and deciphering messages that could theoretically be understood by another technological civilization. Communication attempts by humans have included broadcasting mathematical languages, pictorial systems such as the Arecibo message and computational approaches to detecting and deciphering 'natural' language communication. The SETI program, for example, uses both radio telescopes and optical telescopes to search for deliberate signals from extraterrestrial intelligence.
While some high-profile scientists, such as Carl Sagan, have advocated the transmission of messages, scientist Stephen Hawking has warned against it, suggesting that aliens might simply raid Earth for its resources and then move on.
Elements of astrobiology.
Astronomy.
Most astronomy-related astrobiological research falls into the category of extrasolar planet (exoplanet) detection, the hypothesis being that if life arose on Earth, then it could also arise on other planets with similar characteristics. To that end, a number of instruments designed to detect Earth-sized exoplanets have been considered, most notably NASA's Terrestrial Planet Finder (TPF) and ESA's Darwin programs, both of which have been cancelled. Additionally, NASA has launched the Kepler mission in March 2009, and the French Space Agency has launched the COROT space mission in 2006. There are also several less ambitious ground-based efforts underway. (See exoplanet).
The goal of these missions is not only to detect Earth-sized planets, but also to directly detect light from the planet so that it may be studied spectroscopically. By examining planetary spectra, it would be possible to determine the basic composition of an extrasolar planet's atmosphere and/or surface; given this knowledge, it may be possible to assess the likelihood of life being found on that planet. A NASA research group, the Virtual Planet Laboratory, is using computer modeling to generate a wide variety of virtual planets to see what they would look like if viewed by TPF or Darwin. It is hoped that once these missions come online, their spectra can be cross-checked with these virtual planetary spectra for features that might indicate the presence of life. The photometry temporal variability of extrasolar planets may also provide clues to their surface and atmospheric properties.
An estimate for the number of planets with "intelligent" extraterrestrial life can be gleaned from the Drake equation, essentially an equation expressing the probability of intelligent life as the product of factors such as the fraction of planets that might be habitable and the fraction of planets on which life might arise:
where:
However, whilst the rationale behind the equation is sound, it is unlikely that the equation will be constrained to reasonable error limits any time soon. The first term, "N", Number of Stars, is generally constrained within a few orders of magnitude. The second and third terms, "fp", Stars with Planets and "fe", Planets with Habitable Conditions, are being evaluated for the sun's neighborhood. The problem with the formula is that it is not usable to generate or support hypotheses because it contains units that can never be verified. Drake originally formulated the equation merely as an agenda for discussion at the Green Bank conference, but some applications of the formula had been taken literally and related to simplistic or pseudoscientific arguments. Another associated topic is the Fermi paradox, which suggests that if intelligent life is common in the universe, then there should be obvious signs of it. This is the purpose of projects like SETI, which tries to detect signs of radio transmissions from intelligent extraterrestrial civilizations.
Another active research area in astrobiology is planetary system formation. It has been suggested that the peculiarities of our Solar System (for example, the presence of Jupiter as a protective shield) may have greatly increased the probability of intelligent life arising on our planet. No firm conclusions have been reached so far.
Biology.
Unlike in physics, biology cannot state that a process or phenomenon, by being mathematically possible, have to exist forcibly in an extraterrestrial body. Biologists specify what is speculative and what is not.
Until the 1970s, life was thought to be entirely dependent on energy from the Sun. Plants on Earth's surface capture energy from sunlight to photosynthesize sugars from carbon dioxide and water, releasing oxygen in the process, and are then eaten by oxygen-respiring animals, passing their energy up the food chain. Even life in the ocean depths, where sunlight cannot reach, was thought to obtain its nourishment either from consuming organic detritus rained down from the surface waters or from eating animals that did. A world's ability to support life was thought to depend on its access to sunlight. However, in 1977, during an exploratory dive to the Galapagos Rift in the deep-sea exploration submersible "Alvin", scientists discovered colonies of giant tube worms, clams, crustaceans, mussels, and other assorted creatures clustered around undersea volcanic features known as black smokers. These creatures thrive despite having no access to sunlight, and it was soon discovered that they comprise an entirely independent food chain. Instead of plants, the basis for this food chain is a form of bacterium that derives its energy from oxidization of reactive chemicals, such as hydrogen or hydrogen sulfide, that bubble up from the Earth's interior. This chemosynthesis revolutionized the study of biology by revealing that life need not be sun-dependent; it only requires water and an energy gradient in order to exist.
Extremophiles (organisms able to survive in extreme environments) are a core research element for astrobiologists. Such organisms include biota which are able to survive several kilometers below the ocean's surface near hydrothermal vents and microbes that thrive in highly acidic environments. It is now known that extremophiles thrive in ice, boiling water, acid, the water core of nuclear reactors, salt crystals, toxic waste and in a range of other extreme habitats that were previously thought to be inhospitable for life. It opened up a new avenue in astrobiology by massively expanding the number of possible extraterrestrial habitats. Characterization of these organisms—their environments and their evolutionary pathways—is considered a crucial component to understanding how life might evolve elsewhere in the universe. According to astrophysicist Dr. Steinn Sigurdsson, "There are viable bacterial spores that have been found that are 40 million years old on Earth - and we know they're very hardened to radiation." Some organisms able to withstand exposure to the vacuum and radiation of space include the lichen fungi "Rhizocarpon geographicum" and "Xanthoria elegans", the bacterium "Bacillus safensis", "Deinococcus radiodurans", "Bacillus subtilis", yeast "Saccharomyces cerevisiae", seeds from "Arabidopsis thaliana" ('mouse-ear cress'), as well as the invertebrate animal Tardigrade. On 29 April 2013, French scientists, funded by NASA, reported that, during spaceflight, microbes (like Pseudomonas aeruginosa) seem to adapt to the space environment in ways "not observed on Earth" and can increase in "virulence". On 27 June 2011, it was reported that a new E. coli bacterium was produced from an engineered DNA in which approximately 90% of its thymine was replaced with the synthetic building block 5-chlorouracil, a substance "toxic to other organisms".
Jupiter's moon, Europa, and Saturn's moon, Enceladus, are now considered the most likely locations for extant extraterrestrial life in the solar system.
The origin of life, known as abiogenesis, distinct from the evolution of life, is another ongoing field of research. Oparin and Haldane postulated that the conditions on the early Earth were conducive to the formation of organic compounds from inorganic elements and thus to the formation of many of the chemicals common to all forms of life we see today. The study of this process, known as prebiotic chemistry, has made some progress, but it is still unclear whether or not life could have formed in such a manner on Earth. The alternative hypothesis of panspermia is that the first elements of life may have formed on another planet with even more favorable conditions (or even in interstellar space, asteroids, etc.) and then have been carried over to Earth by a variety of means. Somewhat related to such a hypothesis, NIH scientists reported studies that life began billion years ago, billions of years before the Earth was formed, based on extrapolating the "genetic complexity of organisms" "major phylogenetic lineages" to earlier times. (also see Abiogenesis#Primitive extraterrestrial life and Panspermia#Complexity)
In October 2011, scientists found that the cosmic dust permeating the universe contains complex organic matter ("amorphous organic solids with a mixed aromatic-aliphatic structure") that could be created naturally, and rapidly, by stars. As one of the scientists noted, "Coal and kerogen are products of life and it took a long time for them to form ... How do stars make such complicated organics under seemingly unfavorable conditions and it so rapidly?" Further, the scientist suggested that these compounds may have been related to the development of life on earth and said that, "If this is the case, life on Earth may have had an easier time getting started as these organics can serve as basic ingredients for life." In September 2012, NASA scientists reported that polycyclic aromatic hydrocarbons (PAHs), subjected to interstellar medium (ISM) conditions, are transformed, through hydrogenation, oxygenation and hydroxylation, to more complex organics - "a step along the path toward amino acids and nucleotides, the raw materials of proteins and DNA, respectively". Further, as a result of these transformations, the PAHs lose their spectroscopic signature which could be one of the reasons "for the lack of PAH detection in interstellar ice grains, particularly the outer regions of cold, dense clouds or the upper molecular layers of protoplanetary disks."
On 29 August 2012, and in a world first, astronomers at Copenhagen University reported the detection of a specific sugar molecule, glycolaldehyde, in a distant star system. The molecule was found around the protostellar binary "IRAS 16293-2422", which is located 400 light years from Earth. Glycolaldehyde is needed to form ribonucleic acid, or RNA, which is similar in function to DNA. This finding suggests that complex organic molecules may form in stellar systems prior to the formation of planets, eventually arriving on young planets early in their formation.
On 21 February 2014, NASA announced a greatly upgraded database for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. According to scientists, more than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.
Astroecology.
Astroecology concerns the interactions of life with space environments and resources, in planets, asteroids and comets. On a larger scale, astroecology concerns resources for life about stars in the galaxy through the cosmological future. Astroecology attempts to quantify future life in space, addressing this area of astrobiology.
Experimental astroecology investigates resources in planetary soils, using actual space materials in meteorites. The results suggest that Martian and carbonaceous chondrite materials can support bacteria, algae and plant (asparagus, potato) cultures, with high soil fertilities. The results support that life could have survived in early aqueous asteroids and on similar materials imported to Earth by dust, comets and meteorites, and that such asteroid materials can be used as soil for future space colonies.
On the largest scale, cosmoecology concerns life in the universe over cosmological times. The main sources of energy may be red giant stars and white and red dwarf stars, sustaining life for 1020 years. Astroecologists suggest that their mathematical models may quantify the immense potential amounts of future life in space, allowing a comparable expansion in biodiversity, potentially leading to diverse intelligent life-forms.
Astrogeology.
Astrogeology is a planetary science discipline concerned with the geology of the celestial bodies such as the planets and their moons, asteroids, comets, and meteorites. The information gathered by this discipline allows the measure of a planet's or a natural satellite's potential to develop and sustain life, or planetary habitability.
An additional discipline of astrogeology is geochemistry, which involves study of the chemical composition of the Earth and other planets, chemical processes and reactions that govern the composition of rocks and soils, the cycles of matter and energy and their interaction with the hydrosphere and the atmosphere of the planet. Specializations include cosmochemistry, biochemistry and organic geochemistry.
The fossil record provides the oldest known evidence for life on Earth. By examining the fossil evidence, paleontologists are able to better understand the types of organisms that arose on the early Earth. Some regions on Earth, such as the Pilbara in Western Australia and the McMurdo Dry Valleys of Antarctica, are also considered to be geological analogs to regions of Mars, and as such, might be able to provide clues on how to search for past life on Mars.
Consistent with the above, the earliest evidence for life on Earth are graphite found to be biogenic in 3.7 billion-year-old metasedimentary rocks discovered in Western Greenland and microbial mat fossils found in 3.48 billion-year-old sandstone discovered in Western Australia. Nonetheless, several studies suggest that life on Earth may have started even earlier, as early as 4.25 billion years ago according to one study.
Life in the Solar System.
People have long speculated about the possibility of life in settings other than Earth, however, speculation on the nature of life elsewhere often has paid little heed to constraints imposed by the nature of biochemistry. The likelihood that life throughout the universe is probably carbon-based is encouraged by the fact that carbon is one of the most abundant of the higher elements. Only two of the natural atoms, carbon and silicon, are known to serve as the backbones of molecules sufficiently large to carry biological information. As the structural basis for life, one of carbon's important features is that unlike silicon it can readily engage in the formation of chemical bonds with many other atoms, thereby allowing for the chemical versatility required to conduct the reactions of biological metabolism and propagation.
The various organic functional groups, composed of hydrogen, oxygen, nitrogen, phosphorus, sulfur, and a host of metals, such as iron, magnesium, and zinc, provide the enormous diversity of chemical reactions necessarily catalyzed by a living organism. Silicon, in contrast, interacts with only a few other atoms, and the large silicon molecules are monotonous compared with the combinatorial universe of organic macromolecules. Indeed, it seems likely that the basic building blocks of life anywhere will be similar to our own, in the generality if not in the detail. Although terrestrial life and life that might arise independently of Earth are expected to use many similar, if not identical, building blocks, they also are expected to have some biochemical qualities that are unique. If life has had a comparable impact elsewhere in the solar system, the relative abundances of chemicals key for its survival - whatever they may be - could betray its presence. Whatever extraterrestrial life may be, its tendency to chemically alter its environment might just give it away.
Thought on where in the Solar System life might occur was limited historically by the belief that life relies ultimately on light and warmth from the Sun and, therefore, is restricted to the surfaces of planets. The three most likely candidates for life in the Solar System are the planet Mars, the Jovian moon Europa, and Saturn's moon Titan. More recently, Saturn's moon Enceladus may be considered a likely candidate as well. This speculation of likely candidates of life is primarily based on the fact that (in the cases of Mars and Europa) the planetary bodies may have liquid water, a molecule essential for life as we know it, for its use as a solvent in cells.
Water on Mars is found in its polar ice caps, and newly carved gullies recently observed on Mars suggest that liquid water may exist, at least transiently, on the planet's surface. At the Martian low temperatures and low pressure, liquid water is likely to be highly saline. As for Europa, liquid water likely exists beneath the moon's icy outer crust. This water may be warmed to a liquid state by volcanic vents on the ocean floor (an especially intriguing theory considering the various types of extremophiles that live near Earth's volcanic vents), but the primary source of heat is probably tidal heating. On 11 December 2013, NASA reported the detection of "clay-like minerals" (specifically, phyllosilicates), often associated with organic materials, on the icy crust of Europa. The presence of the minerals may have been the result of a collision with an asteroid or comet according to the scientists.
Another planetary body that could potentially sustain extraterrestrial life is Saturn's largest moon, Titan. Titan has been described as having conditions similar to those of early Earth. On its surface, scientists have discovered the first liquid lakes outside Earth, but they seem to be composed of ethane and/or methane, not water. After Cassini data was studied, it was reported on March 2008 that Titan may also have an underground ocean composed of liquid water and ammonia. Additionally, Saturn's moon Enceladus may have an ocean below its icy surface and, according to NASA scientists in May 2011, "is emerging as the most habitable spot beyond Earth in the Solar System for life as we know it".
On 26 April 2012, scientists reported that lichen survived and showed remarkable results on the adaptation capacity of photosynthetic activity within the simulation time of 34 days under Martian conditions in the Mars Simulation Laboratory (MSL) maintained by the German Aerospace Center (DLR). In June, 2012, scientists reported that measuring the ratio of hydrogen and methane levels on Mars may help determine the likelihood of life on Mars. According to the scientists, "...low H2/CH4 ratios (less than approximately 40) indicate that life is likely present and active." Other scientists have recently reported methods of detecting hydrogen and methane in extraterrestrial atmospheres.
Rare Earth hypothesis.
This hypothesis states that based on astrobiological findings, multi-cellular life forms found on Earth may actually be more of a rarity than scientists initially assumed. It provides a possible answer to the Fermi paradox which suggests, "If extraterrestrial aliens are common, why aren't they obvious?" It is apparently in opposition to the principle of mediocrity, assumed by famed astronomers Frank Drake, Carl Sagan, and others. The Principle of Mediocrity suggests that life on Earth is not exceptional, but rather that life is more than likely to be found on innumerable other worlds.
The anthropic principle states that fundamental laws of the universe work specifically in a way that life would be possible. The anthropic principle supports the Rare Earth Hypothesis by arguing the overall elements that are needed to support life on Earth are so fine-tuned that it is nearly impossible for another just like it to exist by random chance (note that these terms are used by scientists in a different way from the vernacular conception of them). However, Stephen Jay Gould compared the claim that the universe is fine-tuned for the benefit of our kind of life to saying that sausages were made long and narrow so that they could fit into modern hot dog buns, or saying that ships had been invented to house barnacles.
Research.
The systematic search for possible life outside Earth is a valid multidisciplinary scientific endeavor. The University of Glamorgan, UK, started just such a degree in 2006, and the American government funds the NASA Astrobiology Institute. However, characterization of non-Earth life is unsettled; hypotheses and predictions as to its existence and origin vary widely, but at the present, the development of theories to inform and support the exploratory search for life may be considered astrobiology's most concrete practical application.
Biologist Jack Cohen and mathematician Ian Stewart, amongst others, consider xenobiology separate from astrobiology. Cohen and Stewart stipulate that astrobiology is the search for Earth-like life outside our solar system and say that xenobiologists are concerned with the possibilities open to us once we consider that life need not be carbon-based or oxygen-breathing, so long as it has the defining characteristics of life. (See carbon chauvinism).
Research outcomes.
, no evidence of extraterrestrial life has been identified. Examination of the Allan Hills 84001 meteorite, which was recovered in Antarctica in 1984 and originated from Mars, is thought by David McKay, Chief Scientist for Astrobiology at NASA's Johnson Space Center, as well as other scientists, to contain microfossils of extraterrestrial origin; this interpretation is controversial.
Yamato 000593 is the second largest meteorite from Mars, and was found on Earth in 2000. At a microscopic level, spheres are found in the meteorite that are rich in carbon compared to surrounding areas that lack such spheres. The carbon-rich spheres may have been formed by biotic activity according to NASA scientists.
On 5 March 2011, Richard B. Hoover, a scientist with the Marshall Space Flight Center, speculated on the finding of alleged microfossils similar to cyanobacteria in CI1 carbonaceous meteorites. However, NASA formally distanced itself from Hoover's claim. According to American astrophysicist Neil deGrasse Tyson: "At the moment, life on Earth is the only known life in the Universe, but there are compelling arguments to suggest we are not alone."
On 17 March 2013, researchers reported data that suggested microbial life forms thrive in the Mariana Trench, the deepest spot on the Earth. Other researchers reported related studies that microbes thrive inside rocks up to 1900 feet below the sea floor under 8500 feet of ocean off the coast of the northwestern United States. According to one of the researchers,"You can find microbes everywhere — they're extremely adaptable to conditions, and survive wherever they are."
In 2004, the spectral signature of methane was detected in the Martian atmosphere by both Earth-based telescopes as well as by the Mars Express probe. Because of solar radiation and cosmic radiation, methane is predicted to disappear from the Martian atmosphere within several years, so the gas must be actively replenished in order to maintain the present concentration. The Mars Science Laboratory rover will perform precision measurements of oxygen and carbon isotope ratios in carbon dioxide (CO2) and methane (CH4) in the atmosphere of Mars in order to distinguish between a geochemical and a biological origin.
It is possible that some planets, like the gas giant Jupiter in our solar system, may have moons with solid surfaces or liquid oceans that are more hospitable. Most of the planets so far discovered outside our solar system are hot gas giants thought to be inhospitable to life, so it is not yet known whether our solar system, with a warm, rocky, metal-rich inner planet such as Earth, is of an aberrant composition. Improved detection methods and increased observing time will undoubtedly discover more planetary systems, and possibly some more like ours. For example, NASA's Kepler Mission seeks to discover Earth-sized planets around other stars by measuring minute changes in the star's light curve as the planet passes between the star and the spacecraft. Progress in infrared astronomy and submillimeter astronomy has revealed the constituents of other star systems. Infrared searches have detected belts of dust and asteroids around distant stars, underpinning the formation of planets.
Efforts to answer questions such as the abundance of potentially habitable planets in habitable zones and chemical precursors have had much success. Numerous extrasolar planets have been detected using the wobble method and transit method, showing that planets around other stars are more numerous than previously postulated. The first Earth-sized extrasolar planet to be discovered within its star's habitable zone is Gliese 581 c, which was found using radial velocity.
Missions.
Research into the environmental limits of life and the workings of extreme ecosystems is ongoing, enabling researchers to better predict what planetary environments might be most likely to harbor life. Missions such as the Phoenix lander, Mars Science Laboratory, ExoMars to Mars, and the Cassini probe to Saturn's moon Titan hope to further explore the possibilities of life on other planets in our solar system.
Viking program.
The two Viking spacecraft each carried four types of biological experiments to the surface of Mars in the late 1970s. These were the only Mars landers to carry out experiments to look specifically for biosignatures of life on Mars. The landers used a robotic arm to put soil samples into sealed test containers on the craft. The two landers were identical, so the same tests were carried out at two places on Mars' surface; Viking 1 near the equator and Viking 2 further north. The result was inconclusive, and is still disputed by some scientists.
Beagle 2.
"Beagle 2" was an unsuccessful British Mars lander that formed part of the European Space Agency's 2003 Mars Express mission. Its primary purpose was to search for signs of life on Mars, past or present. All contact with it was lost upon its entry into the atmosphere.
EXPOSE.
EXPOSE was a multi-user facility mounted in 2008 outside the International Space Station dedicated to astrobiology. EXPOSE was developed by the European Space Agency (ESA) for long-term spaceflights that allowed to expose organic chemicals and biological samples to outer space for one and a half years in low Earth orbit.
Mars Science Laboratory.
The Mars Science Laboratory (MSL) mission landed a rover that is currently in operation on Mars. It was launched 26 November 2011, and landed at Gale Crater on 6 August 2012. Mission objectives are to help assess Mars' habitability and in doing so, determine whether Mars is or has ever been able to support life, collect data for a future manned mission, study Martian geology, its climate, and further assess the role that water, an essential ingredient for life as we know it, played in forming minerals on Mars.
ExoMars.
ExoMars is a robotic mission to Mars to search for possible biosignatures of Martian life, past or present. This astrobiological mission is currently under development by the European Space Agency (ESA) with likely collaboration by the Russian Federal Space Agency (Roscosmos); it is planned for a 2018 launch.
Mars 2020 rover mission.
The 'Mars 2020 rover mission' is a concept under study by NASA with a possible launch in 2020. It is intended to investigate astrobiologically relevant environments on Mars, investigate its surface geological processes and history, including the assessment of its past habitability and potential for preservation of biosignatures within accessible geological materials. The Science Definition Team is proposing the rover collect and package as many as 31 samples of rock cores and soil for a later mission to bring back for more definitive analysis in laboratories on Earth. The rover could make measurements and technology demonstrations to help designers of a human expedition understand any hazards posed by Martian dust and demonstrate how to collect carbon dioxide (CO2), which could be a resource for making oxygen (O2) and rocket fuel. Improved precision landing technology that enhances the scientific value of robotic missions also will be critical for eventual human exploration on the surface.
Red Dragon.
Red Dragon is a proposed concept for a low-cost Mars lander mission that would utilize a SpaceX Falcon Heavy launch vehicle, and a modified Dragon capsule to enter the Martian atmosphere. The lander's primary mission would be to search for evidence of life on Mars (biosignatures), past or present. The concept had been scheduled to propose for funding on 2012/2013 as a NASA Discovery mission, for launch in 2018.
Icebreaker Life.
"Icebreaker Life" is a lander mission that is being proposed for NASA's Discovery Program for the 2018 launch opportunity. If selected and funded, the stationary lander would be a near copy of the successful 2008 "Phoenix" and it would carry an upgraded astrobiology scientific payload, including a 1 meter-long drill to sample ice-cemented ground in the northern plains to conduct a search for organic molecules and evidence of current or past life on Mars. One of the key goals of the "Icebreaker Life" mission is to test the hypothesis that the ice-rich ground in the polar regions has significant concentrations of organics due to protection by the ice from oxidants and radiation.
Europa Clipper.
Europa Clipper is a mission concept under study by NASA that would conduct detailed reconnaissance of Jupiter's moon Europa and would investigate whether the icy moon could harbor conditions suitable for life. It would also aid in the selection of future landing sites.

</doc>
<doc id="2790" url="http://en.wikipedia.org/wiki?curid=2790" title="Air show">
Air show

An air show, (also airshow, Air Fair, or Air Tattoo) is a public event at which aviators display their flying skills and the capabilities of their aircraft to spectators, usually by means of aerobatics. Air shows without aerobatic displays, having only aircraft displayed parked on the ground, are called "static air shows".
Outline.
Some air shows are held as a business venture or as a trade event where aircraft, avionics and other services are promoted to potential customers. Many air shows are held in support of local, national or military charities. Military air firms often organise air shows at military airfields as a public relations exercise to thank the local community, promote military careers and raise the profile of the military.
Air show "seasons" vary around the world. The United States enjoys a long season that generally runs from March to November, covering the spring, summer, and fall seasons. Other countries often have much shorter seasons. The European season usually starts in late April or Early May and is usually over by mid October. The Middle East, Australia and New Zealand hold their events between January and March. However, for many acts, the "off-season" does not mean a period of inactivity; they use this time for maintenance and practice.
The type of displays seen at an event are constrained by a number of factors, including the weather and visibility. Most aviation authorities now publish rules and guidance on minimum display heights and criteria for differing conditions. In addition to the weather, pilots and organizers must also consider local airspace restrictions. Most exhibitors will plan "full," "rolling" and "flat" display for varying weather and airspace conditions.
The types of shows vary greatly. Some are large scale military events with large flying displays and ground exhibitions while others held at small local airstrips can often feature just one or two hours of flying with just a few stalls on the ground. Air Displays can be held during day or night with the latter becoming increasingly popular. Shows don't always take place over airfields; some have been held over the grounds of stately homes or castles and over the sea at coastal resorts.
Attractions.
Before the Second World War, air shows were associated with long distance air races, often lasting many days and covering thousands of miles. While the Reno Air Races keep this tradition alive, most air shows today primarily feature a series of aerial demos of short duration.
Most air shows feature warbirds, aerobatics, and demonstrations of modern military aircraft, and many air shows offer a variety of other aeronautical attractions as well, such as wing-walking, radio-controlled aircraft, water/slurry drops from firefighting aircraft, simulated helicopter rescues and sky diving.
Specialist aerobatic aircraft have powerful piston engines, light weight and big control surfaces, making them capable of very high roll rates and accelerations. A skilled pilot will be able to climb vertically, perform very tight turns, tumble his aircraft end-over-end and perform manoeuvres during loops.
Solo military jet demos, also known as tactical demo, feature one aircraft, usually a strike fighter or an advanced trainer. The demonstration focuses on the capabilities of modern aircraft used in combat operations. The display will usually demonstrate the aircraft's very short (and often very loud) takeoff rolls, fast speeds, slow approach speeds, as well as their ability to quickly make tight turns, to climb quickly, and their ability to be precisely controlled at a large range of speeds. Manoeuvres include aileron rolls, barrel rolls, hesitation rolls, Cuban-8s, tight turns, high-alpha flight, a high-speed pass, double Immelmans, and touch-and-gos. Tactical demos may include simulated bomb drops, sometimes with pyrotechnics on the ground for effect. Aircraft with special characteristics that give them unique capabilities will often display those in their demos; For example, Russian fighters with Thrust vectoring may be used to perform Pugachev's Cobra or the Kulbit, among other difficult manoeuvers that cannot be performed by other aircraft. Similarly, an F-22 pilot may hover his jet in the air with the nose pointed straight up, a Harrier or Osprey pilot may perform a vertical landing or vertical takeoff, and so on.
Safety.
Air shows may present some risk to spectators and aviators. Accidents have occurred, sometimes with a large loss of life, such as the 1988 disaster at Ramstein Air Base in Germany and the 2002 air show crash at Lviv, Ukraine. Because of these accidents, the various aviation authorities around the world have created set rules and guidance for those running and participating in air displays. Air displays are often monitored by aviation authorities to ensure safe procedures.
In the United Kingdom, local authorities will first need to approve any application for an event to which the public is admitted. No approval, no event. The first priority must be to arrange insurance cover and details can be obtained from your local authority. An added complication is a whole new raft of legislation concerning Health & Safety in particular Corporate Manslaughter, which can involve the event organiser being charged with a criminal office if any of the insurances and risk assessments are not fully completed well in advance of the event. If this very basic step isn't completed then any further activity should be halted until it is.
Rules govern the distance from the crowds that aircraft must fly. These vary according to the rating of the pilot/crew, the type of aircraft and the way the aircraft is being flown. For instance, slower lighter aircraft are usually allowed closer and lower to the crowd than larger, faster types. Also, a fighter jet flying straight and level will be able to do so closer to the crowd and lower than if it were performing a roll or a loop.
Pilots can get authorizations for differing types of displays (i.e. limbo flying, basic aerobatics to unlimited aerobatics) and to differing minimum base heights above the ground. To gain such authorizations, the pilots will have to demonstrate to an examiner that they can perform to those limits without endangering themselves, ground crew or spectators.
Despite display rules and guidances, accidents have continued to happen. However, air show accidents are rare and where there is proper supervision air shows have impressive safety records. Each year, organisations such as The International Council of Air Shows and The European Airshow Council meet and discuss various subjects including air show safety where accidents are discussed and lessons learnt.
Weather.
Air shows, and other big shows such as agricultural shows, on grassy land, are vulnerable to continued heavy rain waterlogging the ground, and making the cloudbase too low for flying, forcing cancellation, or the show ending early, costing much money for the show's organizers, as people and parking cars have difficulty moving about and turn the land into a morass, and the organizers may be tempted to put straw or cinders down to make movement easier, and the owner of the land cannot accept the resulting damage.

</doc>
<doc id="2792" url="http://en.wikipedia.org/wiki?curid=2792" title="Anthropic principle">
Anthropic principle

In astrophysics and cosmology, the anthropic principle () is the philosophical consideration that observations of the physical Universe must be compatible with the conscious life that observes it. Some proponents of the anthropic principle reason that it explains why the Universe has the age and the fundamental physical constants necessary to accommodate conscious life. As a result, they believe it is unremarkable that the universe's fundamental constants happen to fall within the narrow range thought to be compatible with life.
The strong anthropic principle (SAP) as explained by Barrow and Tipler (see variants) states that this is all the case because the Universe is compelled, in some sense, for conscious life to eventually emerge. Critics of the SAP argue in favor of a weak anthropic principle (WAP) similar to the one defined by Brandon Carter, which states that the universe's ostensible fine tuning is the result of selection bias: i.e., only in a universe capable of eventually supporting life will there be living beings capable of observing any such fine tuning, while a universe less compatible with life will go unbeheld. Such arguments are closely related to some multiverse ideas and can link to the Fermi paradox.
Definition and basis.
The principle was formulated as a response to a series of observations that the laws of nature and parameters of the Universe take on values that are consistent with conditions for life as we know it rather than a set of values that would not be consistent with life on Earth. The anthropic principle states that this is a necessity, because if life were impossible, no living entity would be there to observe it, and thus would not be known. That is, it must be possible to observe "some" Universe, and hence, the laws and constants of any such universe must accommodate that possibility.
The term "anthropic" in "anthropic principle" has been argued to be a misnomer. While singling out our kind of carbon-based life, none of the finely tuned phenomena require human life or some kind of carbon chauvinism. Any form of life or any form of heavy atom, stone, star or galaxy would do; nothing specifically human or anthropic is involved.
The anthropic principle has given rise to some confusion and controversy, partly because the phrase has been applied to several distinct ideas. All versions of the principle have been accused of discouraging the search for a deeper physical understanding of the universe. The anthropic principle is often criticized for lacking falsifiability and therefore critics of the anthropic principle may point out that the anthropic principle is a non-scientific concept, even though the weak anthropic principle, ""conditions that are observed in the universe must allow the observer to exist"," is "easy" to support in mathematics and philosophy, i.e. it is a tautology or truism. However, building a substantive argument based on a tautological foundation is problematic. Stronger variants of the anthropic principle are not tautologies and thus make claims considered controversial by some and that are contingent upon empirical verification.
Anthropic coincidences.
In 1961, Robert Dicke noted that the age of the universe, as seen by living observers, cannot be random. Instead, biological factors constrain the universe to be more or less in a "golden age," neither too young nor too old. If the universe were one tenth as old as its present age, there would not have been sufficient time to build up appreciable levels of
metallicity (levels of elements besides hydrogen and helium) especially carbon, by nucleosynthesis. Small rocky planets did not yet exist. If the universe were 10 times older than it actually is, most stars would be too old to remain on the main sequence and would have turned into white dwarfs, aside from the dimmest red dwarfs, and stable planetary systems would have already come to an end. Thus, Dicke explained the coincidence between large dimensionless numbers constructed from the constants of physics and the age of the universe, a coincidence which had inspired Dirac's varying-G theory.
Dicke later reasoned that the density of matter in the universe must be almost exactly the critical density needed to prevent the Big Crunch (the "Dicke coincidences" argument). The most recent measurements may suggest that the observed density of baryonic matter, and some theoretical predictions of the amount of dark matter account for about 30% of this critical density, with the rest contributed by a cosmological constant. Steven Weinberg gave an anthropic explanation for this fact: he noted that the cosmological constant has a remarkably low value, some 120 orders of magnitude smaller than the value particle physics predicts (this has been described as the "worst prediction in physics"). However, if the cosmological constant were only one order of magnitude larger than its observed value, the universe would suffer catastrophic inflation, which would preclude the formation of stars, and hence life.
The observed values of the dimensionless physical constants (such as the fine-structure constant) governing the four fundamental interactions are balanced as if fine-tuned to permit the formation of commonly found matter and subsequently the emergence of life. A slight increase in the strong nuclear force would bind the dineutron and the diproton, and nuclear fusion would have converted all hydrogen in the early universe to helium. Water, as well as sufficiently long-lived stable stars, both essential for the emergence of life as we know it, would not exist. More generally, small changes in the relative strengths of the four fundamental interactions can greatly affect the universe's age, structure, and capacity for life.
Origin.
The phrase "anthropic principle" first appeared in Brandon Carter's contribution to a 1973 Kraków symposium honouring Copernicus's 500th birthday. Carter, a theoretical astrophysicist, articulated the Anthropic Principle in reaction to the Copernican Principle, which states that humans do not occupy a privileged position in the Universe. As Carter said: "Although our situation is not necessarily "central", it is inevitably privileged to some extent." Specifically, Carter disagreed with using the Copernican principle to justify the Perfect Cosmological Principle, which states that all large regions "and times" in the universe must be statistically identical. The latter principle underlay the steady-state theory, which had recently been falsified by the 1965 discovery of the cosmic microwave background radiation. This discovery was unequivocal evidence that the universe has changed radically over time (for example, via the Big Bang).
Carter defined two forms of the anthropic principle, a "weak" one which referred only to anthropic selection of privileged spacetime locations in the universe, and a more controversial "strong" form which addressed the values of the fundamental constants of physics.
Roger Penrose explained the weak form as follows:
One reason this is plausible is that there are many other places and times in which we can imagine finding ourselves. But when applying the strong principle, we only have one Universe, with one set of fundamental parameters, so what exactly is the point being made? Carter offers two possibilities: First, we can use our own existence to make "predictions" about the parameters. But second, "as a last resort", we can convert these predictions into "explanations" by assuming that there "is" more than one Universe, in fact a large and possibly infinite collection of universes, something that is now called a multiverse ("world ensemble" was Carter's term), in which the parameters (and perhaps the laws of physics) vary across universes. The strong principle then becomes an example of a selection effect, exactly analogous to the weak principle. Postulating a multiverse is certainly a radical step, but taking it could provide at least a partial answer to a question which had seemed to be out of the reach of normal science: "why do the fundamental laws of physics take the particular form we observe and not another?"
Since Carter's 1973 paper, the term "anthropic principle" has been extended to cover a number of ideas which differ in important ways from those he espoused. Particular confusion was caused in 1986 by the book "The Anthropic Cosmological Principle" by John D. Barrow and Frank Tipler, published that year which distinguished between "weak" and "strong" anthropic principle in a way very different from Carter's, as discussed in the next section.
Carter was not the first to invoke some form of the anthropic principle. In fact, the evolutionary biologist Alfred Russel Wallace anticipated the anthropic principle as long ago as 1904: "Such a vast and complex universe as that which we know exists around us, may have been absolutely required ... in order to produce a world that should be precisely adapted in every detail for the orderly development of life culminating in man." In 1957, Robert Dicke wrote: "The age of the Universe 'now' is not random but conditioned by biological factors ... in the values of the fundamental constants of physics would preclude the existence of man to consider the problem."
Variants.
Weak anthropic principle (WAP) (Carter): "we must be prepared to take account of the fact that our location in the universe is "necessarily" privileged to the extent of being compatible with our existence as observers." Note that for Carter, "location" refers to our location in time as well as space.
Strong anthropic principle (SAP) (Carter): "the Universe (and hence the fundamental parameters on which it depends) must be such as to admit the creation of observers within it at some stage. To paraphrase Descartes, "cogito ergo mundus talis est"."<br>The Latin tag ("I think, therefore the world is such it is") makes it clear that "must" indicates a deduction from the fact of our existence; the statement is thus a truism.
In their 1986 book, "The Anthropic Cosmological Principle", John Barrow and Frank Tipler depart from Carter and define the WAP and SAP as follows:
Weak anthropic principle (WAP) (Barrow and Tipler): "The observed values of all physical and cosmological quantities are not equally probable but they take on values restricted by the requirement that there exist sites where carbon-based life can evolve and by the requirements that the Universe be old enough for it to have already done so."<br>Unlike Carter they restrict the principle to carbon-based life, rather than just "observers." A more important difference is that they apply the WAP to the fundamental physical constants, such as the fine structure constant, the number of spacetime dimensions, and the cosmological constant — topics that fall under Carter's SAP.
Strong anthropic principle (SAP) (Barrow and Tipler): "The Universe must have those properties which allow life to develop within it at some stage in its history."<br>This looks very similar to Carter's SAP, but unlike the case with Carter's SAP, the "must" is an imperative, as shown by the following three possible elaborations of the SAP, each proposed by Barrow and Tipler:
Modified anthropic principle (MAP) (Schmidhuber): The 'problem' of existence is only relevant to a species capable of formulating the question. Prior to "Homo sapiens" intellectual evolution to the point where the nature of the observed universe - and humans' place within same - spawned deep inquiry into its origins, the 'problem' simply did not exist.
The philosophers John Leslie and Nick Bostrom reject the Barrow and Tipler SAP as a fundamental misreading of Carter. For Bostrom, Carter's anthropic principle just warns us to make allowance for anthropic bias, that is, the bias created by anthropic selection effects (which Bostrom calls "observation" selection effects) — the necessity for observers to exist in order to get a result. He writes:
Strong self-sampling assumption (SSSA) (Bostrom): "Each observer-moment should reason as if it were randomly selected from the class of all observer-moments in its reference class."
Playwright and novelist Michael Frayn describes a form of the Strong Anthropic Principle in his 2006 book "The Human Touch", which explores what he characterises as "the central oddity of the Universe":
Character of anthropic reasoning.
Carter chose to focus on a tautological aspect of his ideas, which has resulted in much confusion. In fact, anthropic reasoning interests scientists because of something that is only implicit in the above formal definitions, namely that we should give serious consideration to there being other universes with different values of the "fundamental parameters" — that is, the dimensionless physical constants and initial conditions for the Big Bang. Carter and others have argued that life as we know it would not be possible in most such universes. In other words, the universe we are in is fine tuned to permit life. Collins & Hawking (1973) characterized Carter's then-unpublished big idea as the postulate that "there is not one universe but a whole infinite ensemble of universes with all possible initial conditions". If this is granted, the anthropic principle provides a plausible explanation for the fine tuning of our universe: the "typical" universe is not fine-tuned, but given enough universes, a small fraction thereof will be capable of supporting intelligent life. Ours must be one of these, and so the observed fine tuning should be no cause for wonder.
Although philosophers have discussed related concepts for centuries, in the early 1970s the only genuine physical theory yielding a multiverse of sorts was the many-worlds interpretation of quantum mechanics. This would allow variation in initial conditions, but not in the truly fundamental constants. Since that time a number of mechanisms for producing a multiverse have been suggested: see the review by Max Tegmark. An important development in the 1980s was the combination of inflation theory with the hypothesis that some parameters are determined by symmetry breaking in the early universe, which allows parameters previously thought of as "fundamental constants" to vary over very large distances, thus eroding the distinction between Carter's weak and strong principles. At the beginning of the 21st century, the string landscape emerged as a mechanism for varying essentially all the constants, including the number of spatial dimensions.
The anthropic idea that fundamental parameters are selected from a multitude of different possibilities (each actual in some universe or other) contrasts with the traditional hope of physicists for a theory of everything having no free parameters: as Einstein said, "What really interests me is whether God had any choice in the creation of the world." In 2002, proponents of the leading candidate for a "theory of everything", string theory, proclaimed "the end of the anthropic principle" since there would be no free parameters to select. Ironically, string theory now seems to offer no hope of predicting fundamental parameters, and now some who advocate it invoke the anthropic principle as well (see below).
The modern form of a design argument is put forth by Intelligent design. Proponents of intelligent design often cite the fine-tuning observations that (in part) preceded the formulation of the anthropic principle by Carter as a proof of an intelligent designer. Opponents of intelligent design are not limited to those who hypothesize that other universes exist; they may also argue, anti-anthropically, that the universe is less fine-tuned than often claimed, or that accepting fine tuning as a brute fact is less astonishing than the idea of an intelligent creator. Furthermore, even accepting fine tuning, Sober (2005) and Ikeda and Jefferys, argue that the Anthropic Principle as conventionally stated actually undermines intelligent design; see fine-tuned universe.
Paul Davies's book "The Goldilocks Enigma" (2006) reviews the current state of the fine tuning debate in detail, and concludes by enumerating the following responses to that debate:
Omitted here is Lee Smolin's model of cosmological natural selection, also known as "fecund universes," which proposes that universes have "offspring" which are more plentiful if they resemble our universe. Also see Gardner (2005).
Clearly each of these hypotheses resolve some aspects of the puzzle, while leaving others unanswered. Followers of Carter would admit only option 3 as an anthropic explanation, whereas 3 through 6 are covered by different versions of Barrow and Tipler's SAP (which would also include 7 if it is considered a variant of 4, as in Tipler 1994).
The anthropic principle, at least as Carter conceived it, can be applied on scales much smaller than the whole universe. For example, Carter (1983) inverted the usual line of reasoning and pointed out that when interpreting the evolutionary record, one must take into account cosmological and astrophysical considerations. With this in mind, Carter concluded that given the best estimates of the age of the universe, the evolutionary chain culminating in "Homo sapiens" probably admits only one or two low probability links. Antonio Feoli and Salvatore Rampone dispute this conclusion, arguing instead that the estimated size of our universe and the number of planets in it allows for a higher bound, so that there is no need to invoke intelligent design to explain evolution.
Observational evidence.
No possible observational evidence bears on Carter's WAP, as it is merely advice to the scientist and asserts nothing debatable. The obvious test of Barrow's SAP, which says that the Universe is "required" to support life, is to find evidence of life in universes other than ours. Any other universe is, by most definitions, unobservable (otherwise it would be included in "our" portion of "this" universe). Thus, in principle Barrow's SAP cannot be falsified by observing a universe in which an observer cannot exist.
Philosopher John Leslie states that the Carter SAP (with multiverse) predicts the following:
Hogan has emphasised that it would be very strange if all fundamental constants were strictly determined, since this would leave us with no ready explanation for apparent fine tuning. In fact we might have to resort to something akin to Barrow and Tipler's SAP: there would be no option for such a universe "not" to support life.
Probabilistic predictions of parameter values can be made given:
The probability of observing value "X" is then proportional to "N"("X") "P"("X").
(A more sophisticated analysis is that of Nick Bostrom.) A generic feature of an analysis of this nature is that the expected values of the fundamental physical constants should not be "over-tuned," i.e. if there is some perfectly tuned predicted value (e.g. zero), the observed value need be no closer to that predicted value than what is required to make life possible. The small but finite value of the cosmological constant can be regarded as a successful prediction in this sense.
One thing that would "not" count as evidence for the Anthropic Principle is evidence that the Earth or the solar system occupied a privileged position in the universe, in violation of the Copernican principle (for possible counterevidence to this principle, see Copernican principle), unless there was some reason to think that that position was a necessary condition for our existence as observers.
Applications of the principle.
The nucleosynthesis of carbon-12.
Fred Hoyle may have invoked anthropic reasoning to predict an astrophysical phenomenon. He is said to have reasoned from the prevalence on earth of life forms whose chemistry was based on carbon-12 atoms, that there must be an undiscovered resonance in the carbon-12 nucleus facilitating its synthesis in stellar interiors via the triple-alpha process. He then calculated the energy of this undiscovered resonance to be 7.6 million electron-volts. Willie Fowler's research group soon found this resonance, and its measured energy was close to Hoyle's prediction.
However, a recently released paper argues that Hoyle did not use anthropic reasoning to make this prediction.
Cosmic inflation.
Don Page criticized the entire theory of cosmic inflation as follows. He emphasized that initial conditions which made possible a thermodynamic arrow of time in a universe with a Big Bang origin, must include the assumption that at the initial singularity, the entropy of the universe was low and therefore extremely improbable. Paul Davies rebutted this criticism by invoking an inflationary version of the anthropic principle. While Davies accepted the premise that the initial state of the visible Universe (which filled a microscopic amount of space before inflating) had to possess a very low entropy value — due to random quantum fluctuations — to account for the observed thermodynamic arrow of time, he deemed this fact an advantage for the theory. That the tiny patch of space from which our observable Universe grew had to be extremely orderly, to allow the post-inflation universe to have an arrow of time, makes it unnecessary to adopt any "ad hoc" hypotheses about the initial entropy state, hypotheses other Big Bang theories require.
String theory.
String theory predicts a large number of possible universes, called the "backgrounds" or "vacua." The set of these vacua is often called the "multiverse" or "anthropic landscape" or "string landscape." Leonard Susskind has argued that the existence of a large number of vacua puts anthropic reasoning on firm ground: only universes whose properties are such as to allow observers to exist are observed, while a possibly much larger set of universes lacking such properties go unnoticed.
Steven Weinberg believes the Anthropic Principle may be appropriated by cosmologists committed to nontheism, and refers to that Principle as a "turning point" in modern science because applying it to the string landscape "...may explain how the constants of nature that we observe can take values suitable for life without being fine-tuned by a benevolent creator." Others, most notably David Gross but also Lubos Motl, Peter Woit, and Lee Smolin, argue that this is not predictive. Max Tegmark, Mario Livio, and Martin Rees argue that only some aspects of a physical theory need be observable and/or testable for the theory to be accepted, and that many well-accepted theories are far from completely testable at present.
Jürgen Schmidhuber (2000–2002) points out that Ray Solomonoff's theory of universal inductive inference and its extensions already provide a framework for maximizing our confidence in any theory, given a limited sequence of physical observations, and some prior distribution on the set of possible explanations of the universe.
Ice density.
When water freezes into ice, the ice floats because ice is less dense than liquid water. This is one possible example of the anthropic principle, because if ice did not float, it might have been difficult or impossible for living organisms to have existed in water; without the insulating properties of a top ice layer, lakes and ponds would tend to freeze solid and thaw very little during warmer periods. This principle has been criticized as neglecting the existence of the tropical zone and other warmer climates.
Ice is unusual in that it is approximately 9% less dense than liquid water. Water is the only known non-metallic substance to expand when it freezes. The density of ice is 0.9167 g/cm3 at 0°C, whereas water has a density of 0.9998 g/cm3 at the same temperature. Liquid water is densest, essentially 1.00 g/cm3, at 4°C and becomes less dense as the water molecules begin to form the hexagonal crystals of ice as the freezing point is reached. This is due to hydrogen bonding dominating the intermolecular forces, which results in a packing of molecules less compact in the solid.
Spacetime.
In 1920, Paul Ehrenfest showed that if there is only one time dimension and greater than three spatial dimensions, the orbit of a planet about its sun cannot remain stable. The same is true of a star's orbit around the center of its galaxy. Ehrenfest also showed that if there are an even number of spatial dimensions, then the different parts of a wave impulse will travel at different speeds. If there are formula_1 spatial dimensions, where "k" is a whole number, then wave impulses become distorted. In 1922, Hermann Weyl showed that Maxwell's theory of electromagnetism works only when with three dimensions of space and one of time. Finally, Tangherlini showed in 1963 that when there are more than three spatial dimensions, electron orbitals around nuclei cannot be stable; electrons would either fall into the nucleus or disperse.
Max Tegmark expands on the preceding argument in the following anthropic manner. If "T" differs from 1, the behavior of physical systems could not be predicted reliably from knowledge of the relevant partial differential equations. In such a universe, intelligent life capable of manipulating technology could not emerge. Moreover, if "T" > 1, Tegmark maintains that protons and electrons would be unstable and could decay into particles having greater mass than themselves (This is not a problem if the particles have a sufficiently low temperature).
"The Anthropic Cosmological Principle".
A thorough extant study of the anthropic principle is the book "The Anthropic Cosmological Principle" by John D. Barrow, a cosmologist, and Frank J. Tipler, a theosophist and mathematical physicist. This book sets out in detail the many known anthropic coincidences and constraints, including many found by its authors. While the book is primarily a work of theoretical astrophysics, it also touches on quantum physics, chemistry, and earth science. An entire chapter argues that "Homo sapiens" is, with high probability, the only intelligent species in the Milky Way.
The book begins with an extensive review of many topics in the history of ideas the authors deem relevant to the anthropic principle, because the authors believe that principle has important antecedents in the notions of teleology and intelligent design. They discuss the writings of Fichte, Hegel, Bergson, and Alfred North Whitehead, and the Omega Point cosmology of Teilhard de Chardin. Barrow and Tipler carefully distinguish teleological reasoning from "eutaxiological" reasoning; the former asserts that order must have a consequent purpose; the latter asserts more modestly that order must have a planned cause. They attribute this important but nearly always overlooked distinction to an obscure 1883 book by L. E. Hicks.
Seeing little sense in a principle requiring intelligent life to emerge while remaining indifferent to the possibility of its eventual extinction, Barrow and Tipler propose the final anthropic principle (FAP): Intelligent information-processing must come into existence in the Universe, and, once it comes into existence, it will never die out.
Barrow and Tipler submit that the FAP is both a valid physical statement and "closely connected with moral values." FAP places strong constraints on the structure of the universe, constraints developed further in Tipler's "The Physics of Immortality". One such constraint is that the universe must end in a big crunch, which seems unlikely in view of the tentative conclusions drawn since 1998 about dark energy, based on observations of very distant supernovas.
In his review of Barrow and Tipler, Martin Gardner ridiculed the FAP by quoting the last two sentences of their book as defining a Completely Ridiculous Anthropic Principle (CRAP):
Criticisms.
Carter has frequently regretted his own choice of the word "anthropic," because it conveys the misleading impression that the principle involves humans specifically, rather than intelligent observers in general. Others have criticised the word "principle" as being too grandiose to describe straightforward applications of selection effects.
A common criticism of Carter's SAP is that it is an easy deus ex machina which discourages searches for physical explanations. To quote Penrose again: "it tends to be invoked by theorists whenever they do not have a good enough theory to explain the observed facts."
Carter's SAP and Barrow and Tipler's WAP have been dismissed as truisms or trivial tautologies, that is, statements true solely by virtue of their logical form (the conclusion is identical to the premise) and not because a substantive claim is made and supported by observation of reality. As such, they are criticized as an elaborate way of saying "if things were different, they would be different," which is a valid statement, but does not make a claim of some factual alternative over another.
Critics of the Barrow and Tipler SAP claim that it is neither testable nor falsifiable, and thus is not a scientific statement but rather a philosophical one. The same criticism has been leveled against the hypothesis of a multiverse, although some argue that it does make falsifiable predictions. A modified version of this criticism is that we understand so little about the emergence of life, especially intelligent life, that it is effectively impossible to calculate the number of observers in each universe. Also, the prior distribution of universes as a function of the fundamental constants is easily modified to get any desired result.
Many criticisms focus on versions of the strong anthropic principle, such as Barrow and Tipler's "anthropic cosmological principle", which are teleological notions that tend to describe the existence of life as a "necessary prerequisite" for the observable constants of physics. Similarly, Stephen Jay Gould, Michael Shermer, and others claim that the stronger versions of the anthropic principle seem to reverse known causes and effects. Gould compared the claim that the universe is fine-tuned for the benefit of our kind of life to saying that sausages were made long and narrow so that they could fit into modern hotdog buns, or saying that ships had been invented to house barnacles. These critics cite the vast physical, fossil, genetic, and other biological evidence consistent with life having been fine-tuned through natural selection to adapt to the physical and geophysical environment in which life exists. Life appears to have adapted to the universe, and not vice versa.
Some applications of the anthropic principle have been criticized as an argument by lack of imagination, for tacitly assuming that carbon compounds and water are the only possible chemistry of life (sometimes called "carbon chauvinism", see also alternative biochemistry). The range of fundamental physical constants consistent with the evolution of carbon-based life may also be wider than those who advocate a fine tuned universe have argued. For instance, Harnik et al. propose a weakless universe in which the weak nuclear force is eliminated. They show that this has no significant effect on the other fundamental interactions, provided some adjustments are made in how those interactions work. However, if some of the fine-tuned details of our universe were violated, that would rule out complex structures of any kind — stars, planets, galaxies, etc.
Lee Smolin has offered a theory designed to improve on the lack of imagination that anthropic principles have been accused of. He puts forth his fecund universes theory, which assumes universes have "offspring" through the creation of black holes whose offspring universes have values of physical constants that depend on those of the mother universe. 
Some versions of the anthropic principle are only interesting if the range of physical constants that allow certain kinds of life are unlikely in a landscape of possible universes. But Lee Smolin assumes that conditions for carbon based life are similar to conditions for black hole creation, which would change the a priori distribution of universes such that universes containing life would be likely. In Smolin vs. Susskind: The Anthropic Principle the string theorist Leonard Susskind disagrees about some assumptions in Lee Smolin's theory, while Smolin defends his theory.
The philosophers of cosmology John Earman, Ernan McMullin, and Jesús Mosterín contend that "in its weak version, the anthropic principle is a mere tautology, which does not allow us to explain anything or to predict anything that we did not already know. In its strong version, it is a gratuitous speculation". A further criticism by Mosterín concerns the flawed "anthropic" inference from the assumption of an infinity of worlds to the existence of one like ours:

</doc>
<doc id="2795" url="http://en.wikipedia.org/wiki?curid=2795" title="Australian Army">
Australian Army

The Australian Army is Australia's military land force. It is part of the Australian Defence Force (ADF) along with the Royal Australian Navy and the Royal Australian Air Force. While the Chief of the Defence Force (CDF) commands the ADF, the Army is commanded by the Chief of Army (CA). The CA is therefore subordinate to the CDF, but is also directly responsible to the Minister for Defence. Although Australian soldiers have been involved in a number of minor and major conflicts throughout its history, only in World War II has Australian territory come under direct attack.
History.
The history of the Australian Army can be divided into two periods:
During its history the Australian Army has fought a large number of major wars, including: Second Boer War (1899–1902), First World War (1914–1918), the Second World War (1939–1945), Korea War (1950–1953), Malayan Emergency (1950–1960), Indonesia-Malaysia Confrontation (1962–1966), Vietnam War (1962–1973), and more recently in Afghanistan (2001 – present) and Iraq (2003–2009). Since 1947 the Australian Army has also been involved in many peacekeeping operations, usually under the auspices of the United Nations, however the non United Nations sponsored Multinational Force and Observers in the Sinai is a notable exception. Australia's largest peacekeeping deployment began in 1999 in East Timor, while other ongoing operations include peacekeeping on Bougainville, in the Sinai, and in the Solomon Islands. Humanitarian relief after 2004 Indian Ocean earthquake in Aceh Province, Indonesia, Operation Sumatra Assist, ended on 24 March 2005.
Current organisation.
The 1st Division comprises a deployable headquarters, while 2nd Division under the command of Forces Command is the main home-defence formation, containing Army Reserve units. 2nd Division's headquarters only performs administrative functions. The Australian Army has not deployed a divisional-sized formation since 1945 and does not expect to do so in the future.
1st Division.
1st Division carries out high-level training activities and deploys to command large-scale ground operations. It does not have any combat units permanently assigned.
Forces Command.
Forces Command controls for administrative purposes all non-special-forces assets of the Australian Army. It is neither an operational nor a deployable command.
Additionally, Forces Command includes the following training establishments:
Special Forces.
Special Operations Command comprises a command formation of equal status to the other commands in the ADF. It is a brigade-sized formation responsible for all of Australia's special-forces assets.
Planned restructuring.
Under a restructuring program known as Plan Beersheba announced in late 2011, the 1st, 3rd and 7th Brigades will be re-formed as combined-arms multi-role manoeuvre brigades with the 2nd Battalion, Royal Australian Regiment (part of the 3rd Brigade) forming the core of a future amphibious force The force will be known as an Amphibious Ready Element and will utilise the former Royal Navy 16,000-tonne auxiliary Bay class landing ship RFA Largs Bay (L3006), bought for $100 million to become HMAS Choules.
Colours, standards and guidons.
Infantry, and some other combat units of the Australian Army carry flags called the Queen's Colour and the Regimental Colour, known as "the Colours". Armoured units carry Standards and Guidons – flags smaller than Colours and traditionally carried by Cavalry, Lancer, Light Horse and Mounted Infantry units. The 1st Armoured Regiment is the only unit in the Australian Army to carry a Standard, in the tradition of heavy armoured units. Artillery units' guns are considered to be their Colours, and on parade are provided with the same respect. Non-combat units (combat service support corps) do not have Colours, as Colours are battle flags and so are only available to combat units. As a substitute, many have Standards or Banners. Units awarded battle honours have them emblazoned on their Colours, Standards and Guidons. They are a link to the unit's past and a memorial to the fallen. Artillery do not have Battle Honours – their single Honour is "Ubique" which means "Everywhere" – although they can received Honour Titles.
The Army is the guardian of the National Flag and as such, unlike the Royal Australian Air Force, does not have a flag or Colours. The Army, instead, has a banner, known as the Army Banner. To commemorate the centenary of the Army, the Governor General Sir William Deane, presented the Army with a new Banner at a parade in front of the Australian War Memorial on 10 March 2001. The Banner was presented to the Regimental Sergeant Major of the Army (RSM-A), Warrant Officer Peter Rosemond.
The Army Banner bears the Australian Coat of Arms on the obverse, with the dates "1901–2001" in gold in the upper hoist. The reverse bears the "rising sun" badge of the Australian Army, flanked by seven campaign honours on small gold-edged scrolls: South Africa, World War I, World War II, Korea, Malaya-Borneo, South Vietnam, and Peacekeeping. The banner is trimmed with gold fringe, has gold and crimson cords and tassels, and is mounted on a pike with the usual British royal crest finial.
Personnel.
Strength.
In the 2010–11 financial year the Army had an average strength of 47,135 personnel: 30,235 permanent (regular) and 16,900 active reservists (part-time). In addition there are another 12,496 members of the Standby Reserve. The regular Army is targeted to expand to 31,000 personnel by 2014–15.
Rank and insignia.
The ranks of the Australian Army are based on the ranks of the British Army, and carry mostly the same actual insignia. For officers the ranks are identical except for the shoulder title "Australia". The Non-Commissioned Officer insignia are the same up until Warrant Officer ranks, where they are stylised for Australia (for example, using the Australian, rather than the British coat of arms).
The ranks of the Australian Army are as follows:
Army bases.
The Army's operational headquarters, Forces Command, is located at Victoria Barracks in Sydney. The Australian Army's three regular brigades are based at Robertson Barracks near Darwin, Lavarack Barracks in Townsville and Gallipoli Barracks in Brisbane. The Deployable Joint Force Headquarters is also located at Gallipoli Barracks.
Other important Army bases include the Army Aviation Centre near Oakey, Queensland, Holsworthy Barracks near Sydney, Lone Pine Barracks in Singleton, New South Wales and Woodside Barracks near Adelaide, South Australia. The SASR is based at Campbell Barracks Swanbourne, a suburb of Perth, Western Australia.
Puckapunyal north of Melbourne houses the Australian Army's Combined Arms Training Centre, Land Warfare Development Centre, and three of the five principal Combat Arms schools. Further barracks include Steele Barracks in Sydney, Keswick Barracks in Adelaide, and Irwin Barracks at Karrakatta in Perth. Dozens of Australian Army Reserve depots are located across Australia.

</doc>
<doc id="2799" url="http://en.wikipedia.org/wiki?curid=2799" title="American Registry for Internet Numbers">
American Registry for Internet Numbers

The American Registry for Internet Numbers (ARIN) is the Regional Internet Registry (RIR) for Canada, the United States, and many Caribbean and North Atlantic islands. ARIN manages the distribution of Internet number resources, including IPv4 and IPv6 address space and AS numbers. ARIN opened its doors for business on December 22, 1997 after incorporating on April 18, 1997. ARIN is a nonprofit corporation with headquarters in Chantilly, Virginia, USA.
ARIN is one of five Regional Internet Registries (RIRs) in the world. Like the other RIRs, ARIN:
Services.
ARIN provides services related to the technical coordination and management of Internet number resources. The nature of these services is described in ARIN's mission statement:
These services are grouped in three areas: Registration, Organization, and Policy Development.
Registration services.
Registration services pertain to the technical coordination and inventory management of Internet number resources. Services include:
For information on requesting Internet number resources from ARIN, see https://www.arin.net/resources/index.html. This section includes the request templates, specific distribution policies, and guidelines for requesting and managing Internet number resources.
Organization services.
Organization services pertain to interaction between stakeholders, ARIN members, and ARIN. Services include:
Policy development services.
Policy development services facilitate the development of policy for the technical coordination and management of Internet number resources.
All ARIN policies are set by the community. Everyone is encouraged to participate in the policy development process at public policy meetings and on the Public Policy Mailing List. The ARIN Board of Trustees ratifies policies only after:
The community develops policies by following a formal Policy Development Process as outlined at https://www.arin.net/policy/pdp.html. The Number Resource Policy Manual, ARIN’s complete set of current policies, is available at https://www.arin.net/policy/nrpm.html.
Membership is not required to participate in ARIN’s policy development process or to apply for Internet number resources.
Services include:
Organizational structure.
ARIN consists of the Internet community within its region, its members, a 7-member Board of Trustees, a 15-member Advisory Council, and a professional staff of about 50. The Board of Trustees and Advisory Council are elected by ARIN members for three-year terms.
Board of trustees.
The ARIN membership elects the Board of Trustees (BoT), which has ultimate responsibility for the business affairs and financial health of ARIN, and manages ARIN's operations in a manner consistent with the guidance received from the Advisory Council and the goals set by the registry's members. The BoT is responsible for determining the disposition of all revenues received to ensure all services are provided in an equitable manner. The BoT ratifies proposals generated from the membership and submitted through the Advisory Council. Executive decisions are carried out following approval by the BoT.
The BoT consists of 7 members:
Advisory council.
In addition to the BoT, ARIN has an advisory council that advises ARIN and the BoT on IP address allocation policy and related matters. Adhering to the procedures in the Internet Resource Policy Evaluation Process, the advisory council forwards consensus-based policy proposals to the BoT for ratification.
The advisory council consists of 15 elected members:
History.
The organization was formed in December 1997 to "provide IP registration services as an independent, nonprofit corporation." Until this time, IP address registration (outside of RIPE and APNIC regions) was done in accordance with policies set by the IETF by Network Solutions corporation as part of the InterNIC project. The National Science Foundation approved the plan for the creation of the not-for-profit organization to ""give the users of IP numbers (mostly Internet service providers, corporations and other large institutions) a voice in the policies by which they are managed and allocated within the North American region."". As part of the transition, Network Solutions corporation transitioned these tasks as well as initial staff and computer infrastructure to ARIN.
The initial Board of Trustees consisted of Scott Bradner, John Curran, Kim Hubbard, Don Telage, Randy Bush, Raymundo Vega Aguilar, and Jon Postel (IANA) as an ex-officio member.
The first president of ARIN was Kim Hubbard, from 1997 until 2000. Kim was succeeded by Raymond "Ray" Plzak until the end of 2008. Trustee John Curran was acting President until July 1 of 2009 when he assumed the CEO role permanently.
Until late 2002 it served Mexico, Central America, South America and all of the Caribbean. LACNIC now handles parts of the Caribbean, Mexico, Central America, and South America. Also, Sub-Saharan Africa was part of its region until April 2005, when AfriNIC was officially recognized by ICANN as the fifth Regional Internet Registry.
Service Region.
The countries in the ARIN service region are:
Former service regions.
ARIN formerly covered Angola, Botswana, Burundi, Republic of Congo, Democratic Republic of Congo, Malawi, Mozambique, Namibia, Rwanda, South Africa, Swaziland, Tanzania, Zambia, and Zimbabwe until AfriNIC was formed.
ARIN formerly covered Argentina, Aruba, Belize, Bolivia, Brazil, Chile, Colombia, Costa Rica, Cuba, Dominican Republic, Dutch West Indies, Ecuador, El Salvador, Falkland Islands (UK), French Guiana, Guatemala, Guyana, Haiti, Honduras, Mexico, Nicaragua, Panama, Paraguay, Peru, South Georgia and the South Sandwich Islands, Suriname, Trinidad and Tobago, Uruguay, and Venezuela until LACNIC was formed.

</doc>
<doc id="2800" url="http://en.wikipedia.org/wiki?curid=2800" title="Asimov (disambiguation)">
Asimov (disambiguation)

Asimov usually refers to Isaac Asimov, the science fiction writer.
Asimov may also refer to:

</doc>
<doc id="2802" url="http://en.wikipedia.org/wiki?curid=2802" title="Akihabara">
Akihabara

Akihabara gained the nickname shortly after World War II for being a major shopping center for household electronic goods and the post-war black market. Nowadays, Akihabara is considered by many to be an otaku cultural center and a shopping district for video games, anime, manga, and computer goods. Icons from popular anime and manga are displayed prominently on the shops in the area, and numerous maid cafés are found throughout the district. 
Geography.
The main area of Akihabara is located on a street just west of Akihabara Station, where most of the major shops are situated. Most of the electronics shops are just west of the station, and the anime and manga shops and the cosplay cafés are north of them.
History.
The area that is now Akihabara was once near a city gate of Edo and served as a passage between the city and northwestern Japan. This made the region a home to many craftsmen and tradesmen, as well as some low class samurai. One of Tokyo’s frequent fires destroyed the area in 1869, and the people decided to replace the buildings of the area with a shrine called Chinkasha, meaning fire extinguisher shrine, in an attempt to prevent the spread of future fires. The locals nicknamed the shrine Akiba after a deity that could control fire, and the area around it became known as Akibagahara and later Akihabara.
In 1890, the Akihabara Station became a major freight transit point, which allowed a vegetable and fruit market to spring up in the district. Then, in the 1920s, the station saw a large volume of passengers after opening for public transport, and after World War II, the black market thrived in the absence of a strong government. This disconnection of Akihabara from government authority has allowed the district to grow as a market city and given rise to an excellent atmosphere for entrepreneurship. In the 1930s, this climate turned Akihabara into a future-oriented market region specializing in household electronics, such as washing machines, refrigerators, televisions, and stereos, earning Akihabara the nickname "Electric Town".
As household electronics began to lose their futuristic appeal in about the 1980s, the shops of Akihabara shifted their focus to home computers at a time when they were only used by specialists and hobbyists. This new specialization brought in a new type of consumer, computer nerds or "otaku". The market in Akihabara naturally latched onto their new customer base that was focused on anime, manga, and video games. The connection between Akihabara and otaku has survived and grown to the point that the region is now known worldwide as a center for otaku culture, and most otaku even consider Akihabara to be a sacred place.
Akihabara massacre.
On Sunday, June 8, 2008 at 12:33 JST, a man drove into a crowd with a truck, then stabbed at least 12 people using a dagger. Seven died and ten were injured. Tokyo Metropolitan Police Department arrested , 25, on suspicion of attempted murder. Kato was eventually sentenced to death by the Tokyo District Court in 2011, and the sentence was upheld on appeal in 2012.
"Otaku" culture.
The influence of "otaku" culture has shaped Akihabara's businesses and buildings to reflect the interests of "otaku" and gained the district worldwide fame for its distinctive imagery. Akihabara tries to create an atmosphere as much like the game and anime worlds the customers are used to as is possible. The streets of Akihabara are covered with anime and manga icons, and cosplayers line the sidewalks handing out advertisements, especially for maid cafés.
Release events, special events, and conventions in Akihabara give anime and manga fans frequent opportunities to meet the creators of the works they follow so closely and strengthen the connection between the region and "otaku" culture. The design of many of the buildings serves to create the sort of atmosphere that draws in "otaku". Architects design the stores of Akihabara to be more opaque and closed to reflect the general desire of many "otaku" to live in their anime worlds rather than display their interests to the world at large.
Because "otaku" culture is considered by some to be unusual, some controversies have sprung up as a result of Akihabara's public display of the culture. Since "otaku" are primarily male, and because of the nature of the medium, Akihabara contains some depictions of sexualized female characters. The abundance of cosplay and maid cafés also puts young women in a position of taking requests from the male customers they serve, though they fully consent to do so.
Akihabara's role as a free market has also allowed a large amount of amateur work to find a passionate audience in the otaku who frequent the area. "Doujinshi" (amateur manga) has been growing in Akihabara since the 1970s when publishers began to drop manga that were not ready for large markets.

</doc>
<doc id="2807" url="http://en.wikipedia.org/wiki?curid=2807" title="Active Directory">
Active Directory

Active Directory (AD) is a directory service that Microsoft developed for Windows domain networks and included in most Windows Server operating systems as a set of processes and services.
An AD domain controller authenticates and authorizes all users and computers in a Windows domain type network—assigning and enforcing security policies for all computers and installing or updating software. For example, when a user logs into a computer that is part of a Windows domain, Active Directory checks the submitted password and determines whether the user is a system administrator or normal user.
Active Directory makes use of Lightweight Directory Access Protocol (LDAP) versions 2 and 3, Microsoft's version of Kerberos, and DNS.
History.
Active Directory, like many information-technology efforts, originated out of a democratization of design using Request for Comments or RFCs. The Internet Engineering Task Force (IETF), which oversees the RFC process, has accepted numerous RFCs initiated by widespread participants. Active Directory incorporates decades of communication technologies into the overarching Active Directory concept then makes improvements upon them.
For example, Lightweight Directory Access Protocol (LDAP), a long-standing directory technology, underpins Active Directory. Also X.500 directories and the Organizational Unit preceded the Active Directory concept that makes use of those methods. The LDAP concept began to emerge even before the founding of Microsoft in April 1975, with RFCs as early as 1971. RFCs contributing to LDAP include RFC 1823 (on the LDAP API, August 1995), RFC 2307, RFC 3062, and RFC 4533.
Microsoft previewed Active Directory in 1999, released it first with Windows 2000 Server edition, and revised it to extend functionality and improve administration in Windows Server 2003. Additional improvements came with Windows Server 2003 R2, Windows Server 2008, and Windows Server 2008 R2. With the release of the last, Microsoft renamed the "domain controller" role (see below) as "Active Directory Domain Services" (AD DS). It is also included in Windows Server 2012 and Windows Server 2012 R2.
Logical structure.
As a directory service, an Active Directory instance consists of a database and corresponding executable code responsible for servicing requests and maintaining the database. The executable part, known as Directory System Agent, is a collection of Windows services and processes that run on Windows 2000 and later. Objects in Active Directory databases can be accessed via LDAP protocol, ADSI (a component object model interface), messaging API and Security Accounts Manager services. 
Objects.
An Active Directory structure is an arrangement of information about objects. The objects fall into two broad categories: resources (e.g., printers) and security principals (user or computer accounts and groups). Security principals are assigned unique security identifiers (SIDs).
Each object represents a single entity—whether a user, a computer, a printer, or a group—and its attributes. Certain objects can contain other objects. An object is uniquely identified by its name and has a set of attributes—the characteristics and information that the object represents— defined by a schema, which also determines the kinds of objects that can be stored in Active Directory.
The schema object lets administrators extend or modify the schema when necessary. However, because each schema object is integral to the definition of Active Directory objects, deactivating or changing these objects can fundamentally change or disrupt a deployment. Schema changes automatically propagate throughout the system. Once created, an object can only be deactivated—not deleted. Changing the schema usually requires planning. Sites are implemented as a set of well-connected subnets.
Forests, trees, and domains.
The Active Directory framework that holds the objects can be viewed at a number of levels. The forest, tree, and domain are the logical divisions in an Active Directory network.
Within a deployment, objects are grouped into domains. The objects for a single domain are stored in a single database (which can be replicated). Domains are identified by their DNS name structure, the namespace.
A domain is defined as a logical group of network objects (computers, users, devices) that share the same active directory database.
A tree is a collection of one or more domains and domain trees in a contiguous namespace, linked in a transitive trust hierarchy.
At the top of the structure is the "forest." A forest is a collection of trees that share a common global catalog, directory schema, logical structure, and directory configuration. The forest represents the security boundary within which users, computers, groups, and other objects are accessible.
Organizational units.
The objects held within a domain can be grouped into Organizational Units (OUs). OUs can provide hierarchy to a domain, ease its administration, and can resemble the organization's structure in managerial or geographical terms. OUs can contain other OUs—domains are containers in this sense. Microsoft recommends using OUs rather than domains for structure and to simplify the implementation of policies and administration. The OU is the recommended level at which to apply group policies, which are Active Directory objects formally named Group Policy Objects (GPOs), although policies can also be applied to domains or sites (see below). The OU is the level at which administrative powers are commonly delegated, but delegation can be performed on individual objects or attributes as well.
Organizational Units are an arrangement for the administrator and do not function as containers; the underlying domain is the true container. It is not possible, for example, to create user accounts with an identical username (sAMAccountName) in separate OUs, such as "fred.staff-ou.domain" and "fred.student-ou.domain", where "staff-ou" and "student-ou" are the OUs. This is so because sAMAccountName, a user object attribute, must be unique within the domain.
In general the reason for this lack of allowance for duplicate names through hierarchical directory placement, is that Microsoft primarily relies on the principles of NetBIOS, which is a flat-file method of network object management that for Microsoft software, goes all the way back to Windows NT 3.1 and MS-DOS LAN Manager. Allowing for duplication of object names in the directory, or completely removing the use of NetBIOS names, would prevent backward compatibility with legacy software and equipment.
As the number of users in a domain increases, conventions such as "first initial, middle initial, last name" (Western order) or the reverse (Eastern order) fail for common family names like "Li (李)", "Smith" or "Garcia". Workarounds include adding a digit to the end of the username. Alternatives include creating a separate ID system of unique employee/student id numbers to use as account names in place of actual user's names, and allowing users to nominate their preferred word sequence within an acceptable use policy.
Because duplicate usernames cannot exist within a domain, account name generation poses a significant challenge for large organizations that cannot be easily subdivided into separate domains, such as students in a public school system or university who must be able to use any computer across the network.
Shadow groups.
In Microsoft's Active Directory, OUs do not confer access permissions, and objects placed within OUs are not automatically assigned access privileges based on their containing OU. This is a design limitation specific to Active Directory. Other competing directories such as Novell NDS are able to assign access privileges through object placement within an OU.
Active Directory requires a separate step for an administrator to assign an object in an OU as a member of a group also within that OU. Relying on OU location alone to determine access permissions is unreliable, because the object may not have been assigned to the group object for that OU.
A common workaround for an Active Directory administrator is to write a custom PowerShell or Visual Basic script to automatically create and maintain a "user group" for each OU in their directory. The scripts are run periodically to update the group to match the OU's account membership, but are unable to instantly update the security groups anytime the directory changes, as occurs in competing directories where security is directly implemented into the directory itself. Such groups are known as "Shadow Groups". Once created, these shadow groups are selectable in place of the OU in the administrative tools.
Microsoft refers to shadow groups in the Server 2008 Reference documentation, but does not explain how to create them. There are no built-in server methods or console snap-ins for managing shadow groups.
The division of an organization's information infrastructure into a hierarchy of one or more domains and top-level OUs is a key decision. Common models are by business unit, by geographical location, by IT Service, or by object type and hybrids of these. OUs should be structured primarily to facilitate administrative delegation, and secondarily, to facilitate group policy application. Although OUs form an administrative boundary, the only true security boundary is the forest itself and an administrator of any domain in the forest must be trusted across all domains in the forest.
Partitions.
The Active Directory database is organized in "partitions", each holding specific object types and following a specific replication pattern. Microsoft often refers to these partitions as 'naming contexts'. The 'Schema' partition contains the definition of object classes and attributes within the Forest. The 'Configuration' partition contains information on the physical structure and configuration of the forest (such as the site topology). Both replicate to all domains in the Forest. The 'Domain' partition holds all objects created in that domain and replicates only within its domain.
Physical structure.
"Sites" are physical (rather than logical) groupings defined by one or more IP subnets. AD also holds the definitions of connections, distinguishing low-speed (e.g., WAN, VPN) from high-speed (e.g., LAN) links. Site definitions are independent of the domain and OU structure and are common across the forest. Sites are used to control network traffic generated by replication and also to refer clients to the nearest domain controllers (DCs). Microsoft Exchange Server 2007 uses the site topology for mail routing. Policies can also be defined at the site level.
Physically, the Active Directory information is held on one or more peer domain controllers, replacing the NT PDC/BDC model. Each DC has a copy of the Active Directory. Servers joined to Active Directory that are not domain controllers are called Member Servers. A subset of objects in the domain partition replicate to domain controllers that are configured as global catalogs. Global catalog (GC) servers provide a global listing of all objects in the Forest. Global Catalog servers replicate to themselves all objects from all domains and hence, provide a global listing of objects in the forest. However, to minimize replication traffic and keep the GC's database small, only selected attributes of each object are replicated. This is called the partial attribute set (PAS). The PAS can be modified by modifying the schema and marking attributes for replication to the GC. Earlier versions of Windows used NetBIOS to communicate. Active Directory is fully integrated with DNS and requires TCP/IP—DNS. To be fully functional, the DNS server must support SRV resource records, also known as service records.
Replication.
Active Directory synchronizes changes using "multi-master replication". Replication by default is 'pull' rather than 'push', meaning that replicas pull changes from the server where the change was effected. The "Knowledge Consistency Checker" (KCC) creates a replication topology of "site links" using the defined "sites" to manage traffic. Intrasite replication is frequent and automatic as a result of change notification, which triggers peers to begin a pull replication cycle. Intersite replication intervals are typically less frequent and do not use change notification by default, although this is configurable and can be made identical to intrasite replication.
Each link can have a 'cost' (e.g., DS3, T1, ISDN etc.) and the KCC alters the site link topology accordingly. Replication may occur transitively through several site links on same-protocol "site link bridges", if the cost is low, although KCC automatically costs a direct site-to-site link lower than transitive connections. Site-to-site replication can be configured to occur between a "bridgehead server" in each site, which then replicates the changes to other DCs within the site. Replication for Active Directory zones is automatically configured when DNS is activated in the domain based by site.
Replication of Active Directory uses Remote Procedure Calls (RPC) over IP (RPC/IP). Between Sites (SMTP) can be used for replication, but only for changes in the Schema, Configuration, or Partial Attribute Set (Global Catalog) NCs. SMTP cannot be used for replicating the default Domain partition.
Implementation.
In general, a network utilizing Active Directory has more than one licensed Windows server computer. Backup and restore of Active Directory is possible for a network with a single domain controller, but Microsoft recommends more than one domain controller to provide automatic failover protection of the directory. Domain controllers are also ideally single-purpose for directory operations only, and should not run any other software or role.
Certain Microsoft products such as SQL Server and Exchange can interfere with the operation of a domain controller, necessitating isolation of these products on additional Windows servers. Combining them can make configuration or troubleshooting of either the domain controller or the other installed software more difficult. A business intending to implement Active Directory is therefore recommended to purchase a number of Windows server licenses, to provide for at least two separate domain controllers, and optionally, additional domain controllers for performance or redundancy, a separate file server, a separate Exchange server, a separate SQL Server, and so forth to support the various server roles.
Physical hardware costs for the many separate servers can be reduced through the use of virtualization, although for proper failover protection, Microsoft recommends not running multiple virtualized domain controllers on the same physical hardware.
Database.
The Active-Directory database, the "directory store", in Windows 2000 Server uses the JET Blue-based Extensible Storage Engine (ESE98) and is limited to 16 terabytes and 2 billion objects (but only 1 billion security principals) in each domain controller's database. Microsoft has created NTDS databases with more than 2 billion objects. (NT4's Security Account Manager could support no more than 40,000 objects). Called NTDS.DIT, it has two main tables: the "data table" and the "link table". Windows Server 2003 added a third main table for security descriptor single instancing.
Programs may access the features of Active Directory via the COM interfaces provided by "Active Directory Service Interfaces".
Single server operations.
Flexible Single Master Operations Roles (FSMO, sometimes pronounced "fizz-mo") operations are also known as operations master roles. Although domain controllers allow simultaneous updates in multiple places, certain operations are supported only on a single server. These operations are performed using the roles listed below:
Trusting.
To allow users in one domain to access resources in another, Active Directory uses trusts.
Trusts inside a forest are automatically created when domains are created. The forest sets the default boundaries of trust, and implicit, transitive trust is automatic for all domains within a forest.
Terminology.
Forest trusts.
Windows Server 2003 introduced the "forest root trust". This trust can be used to connect Windows Server 2003 forests if they are operating at the 2003 forest functional level. Authentication across this type of trust is Kerberos-based (as opposed to NTLM).
Forest trusts are transitive for all the domains within the trusted forests. However, forest trusts are "not" transitive between forests.
Example: Suppose that a two-way transitive forest trust exists between the forest root domains in Forest A and Forest B, and another two-way transitive forest trust exists between the forest root domains in Forest B and Forest C. Such a configuration lets users in Forest B access resources in any domain in either Forest A or Forest C, and users in Forest A or C can access resources in any domain in Forest B. However, it does "not" let users in Forest A access resources in Forest C, or vice versa. To let users in Forest A and Forest C share resources, a two-way transitive trust must exist between both forests.
Unix integration.
Varying levels of interoperability with Active Directory can be achieved on most Unix-like operating systems (including Unix, Linux, Mac OS X or Java and Unix-based programs) through standards-compliant LDAP clients, but these systems usually do not interpret many attributes associated with Windows components, such as Group Policy and support for one-way trusts.
Third parties offer Active Directory integration for Unix-like platforms, including:
The schema additions shipped with Windows Server 2003 R2 include attributes that map closely enough to RFC 2307 to be generally usable. The reference implementation of RFC 2307, nss_ldap and pam_ldap provided by PADL.com, support these attributes directly. The default schema for group membership complies with RFC 2307bis (proposed). Windows Server 2003 R2 includes a Microsoft Management Console snap-in that creates and edits the attributes.
An alternate option is to use another directory service such as 389 Directory Server (formerly Fedora Directory Server, FDS), ViewDS Identity Solutions - ViewDS v7.2 XML Enabled Directory or Sun Microsystems Sun Java System Directory Server, with the latter two both being able to perform two-way synchronization with AD and thus provide a "deflected" integration, as non-Windows clients authenticate to this while Windows Clients authenticate to AD. Another option is to use OpenLDAP with its "translucent" overlay, which can extend entries in any remote LDAP server with additional attributes stored in a local database. Clients pointed at the local database see entries containing both the remote and local attributes, while the remote database remains completely untouched.
Administration (querying, modifying, and monitoring) of Active Directory can be achieved via many scripting languages, including PowerShell, VBScript, JScript/JavaScript, Perl, Python, and Ruby. Using free AD administration tools can help to simplify AD management tasks.

</doc>
<doc id="2809" url="http://en.wikipedia.org/wiki?curid=2809" title="Arian (disambiguation)">
Arian (disambiguation)

Arian may refer to:

</doc>
<doc id="2810" url="http://en.wikipedia.org/wiki?curid=2810" title="Aldona of Lithuania">
Aldona of Lithuania

Aldona (baptized "Ona" or "Anna"; her pagan name, Aldona, is known only from the writings of Maciej Stryjkowski; c. 1309 – 26 May 1339) was Queen consort of Poland (1333–1339), and a princess of the Grand Duchy of Lithuania. She was the daughter of Gediminas, Grand Duke of Lithuania.
Biography.
Aldona married Casimir III of Poland, when he was 15 or 16 years old. The bride was probably of about the same age. The marriage took place on 30 April or 16 October 1325 and was a purely political maneuver to strengthen the first Polish–Lithuanian coalition against the Teutonic Knights. Casimir was seeking allies in the dispute over Pomerania with the Order. Gediminas had just undertaken an unsuccessful attempt to Christianize Lithuania. This coalition was a prelude to the Union of Krewo in 1385, and the Union of Lublin in 1569, which resulted in the creation of a new state, the Polish–Lithuanian Commonwealth. The details of the agreement are not known; however, it is known that Gediminas released all Polish captives, numbered at around 25,000, who returned to Poland. The importance of the marriage was attested by the fact that Władysław abandoned his earlier plans to marry his son to Jutta of Bohemia. The alliance was put into effect when joint Polish-Lithuanian forces organized an attack against the Margraviate of Brandenburg in 1326. However, the coalition was not strong and collapsed c. 1330. Yet, there is no evidence of fighting between Poland and Lithuania while Aldona was alive. Aldona died suddenly at the end of May 1339, and was buried in Kraków.
Aldona was remembered for her piety and devotion to music. She was accompanied by court musicians wherever she went. It was even suggested by Jan Długosz that the cymbals which were played in procession before her represented a pagan Lithuanian tradition. Her husband Casimir is known for his romantic affairs: after Aldona's death he married three more times. Aldona had two daughters:

</doc>
<doc id="2812" url="http://en.wikipedia.org/wiki?curid=2812" title="Aron Nimzowitsch">
Aron Nimzowitsch

Aron Nimzowitsch (, , "Aron Isayevich Nimtsovich"; born Aron Niemzowitsch; 7 November 1886 – 16 March 1935) was a Russian-born, Danish leading chess master and a very influential chess writer. He was the foremost figure amongst the "hypermoderns".
Life.
Born in part of the Russian Empire, the Jewish German-speaking Nimzowitsch came from a wealthy family, where he learned chess from his father, who was a merchant. In 1904, he travelled to Berlin to study philosophy, but set aside his studies soon and began a career as a professional chess player that same year. He won his first international tournament at Munich 1906. Then, he tied for first with Alexander Alekhine at St. Petersburg 1913/14 (the eighth All-Russian Masters' Tournament).
During the 1917 Russian Revolution, Nimzowitsch was in the Baltic war zone. He escaped being drafted into one of the armies by feigning madness, insisting that a fly was on his head. He then escaped to Berlin, and gave his first name as Arnold, possibly to avoid anti-Semitic persecution.
Nimzowitsch eventually moved to Copenhagen in 1922, which coincided with his rise to the world chess elite, where he lived for the rest of his life in one small rented room. In Copenhagen, he twice won the Nordic Chess Championship, in 1924 and 1934. He obtained Danish citizenship and lived in Denmark until his death in 1935. Although he had long suffered from heart trouble, his early death was unexpected; taken ill suddenly at the end of 1934, he lay bedridden for three months before dying of pneumonia. He is buried in Bispebjerg Cemetery in Copenhagen.
Chess career.
The height of Nimzowitsch's career was the late 1920s and early 1930s. Chessmetrics places him as the third best player in the world from 1927 to 1931, behind Alexander Alekhine and José Capablanca. His most notable successes were first-place finishes at Copenhagen 1923, Marienbad 1925, Dresden 1926, Hanover 1926, the Carlsbad 1929 chess tournament, and second place behind Alekhine at the San Remo 1930 chess tournament. Nimzowitsch never developed a knack for match play, though; his best match success was a draw with Alekhine, but the match consisted of only two games and took place in 1914, thirteen years before Alekhine became world champion.
Nimzowitsch never beat Capablanca, but fared better against Alekhine. He even beat Alekhine with the black pieces, in their short 1914 match at St. Petersburg. One of Nimzowitsch's most famous games is his celebrated immortal zugzwang game against Sämisch at Copenhagen 1923. Another game on this theme is his win over Paul Johner at Dresden 1926. When in form, Nimzowitsch was very dangerous with the black pieces, scoring many fine wins over top players.
Legacy.
Nimzowitsch is considered one of the most important players and writers in chess history. His works influenced numerous other players, including Savielly Tartakower, Milan Vidmar, Richard Réti, Akiba Rubinstein, Bent Larsen and Tigran Petrosian, and his influence is still felt today.
He wrote three books on chess strategy: "Mein System (My System)", 1925, "Die Praxis meines Systems (The Practice of My System)", 1929, commonly known as "Chess Praxis", and "Die Blockade" ("The Blockade"), 1925, though much in the latter book is generally held to be a rehash of material already presented in "Mein System". It is said that 99 out of 100 chess masters have read "Mein System"; consequently, most consider it to be Nimzowitsch's greatest contribution to chess. It sets out Nimzowitsch's most important ideas, while his second most influential work, "Chess Praxis", elaborates upon these ideas, adds a few new ones, and has immense value as a stimulating collection of Nimzowitsch's own games accompanied by his idiosyncratic, hyperbolic commentary which is often as entertaining as instructive.
Nimzowitsch's chess theories, when first propounded flew in the face of widely held orthodoxies enunciated by the dominant theorist of the era, Siegbert Tarrasch, and his disciples. Tarrasch's rigid generalizations drew on the earlier work of Wilhelm Steinitz, and were upheld by Tarrasch's sharp tongue when dismissing the opinions of doubters. While the greatest players of the time, among them Alekhine, Emanuel Lasker and Capablanca, clearly did not allow their play to be hobbled by blind adherence to general concepts that the center had to be controlled by pawns, that development had to happen in support of this control, that rooks always belong on open files, that wing openings were unsound—core ideas of Tarrasch's chess philosophy as popularly understood—beginners were taught to think of these generalizations as unalterable principles.
Nimzowitsch supplemented many of the earlier simplistic assumptions about chess strategy by enunciating in his turn a further number of general concepts of defensive play aimed at achieving one's own goals by preventing realization of the opponent's plans. Notable in his "system" were concepts such as overprotection of pieces and pawns under attack, control of the center by pieces instead of pawns, blockading of opposing pieces (notably the passed pawns) and prophylaxis. He was also a leading exponent of the fianchetto development of bishops. Perhaps most importantly, he formulated the terminology still in use for various complex chess strategies. Others had used these ideas in practice, but he was the first to present them systematically as a lexicon of themes accompanied by extensive taxonomical observations.
Grandmaster (GM) Raymond Keene writes that Nimzowitsch "was one of the world's leading grandmasters for a period extending over a quarter of a century, and for some of that time he was the obvious challenger for the world championship. ... was also a great and profound chess thinker second only to Steinitz, and his works – "Die Blockade", "My System" and "Chess Praxis" – established his reputation as one of the father figures of modern chess." GM Robert Byrne called him "perhaps the most brilliant theoretician and teacher in the history of the game." GM Jan Hein Donner called Nimzowitsch "a man who was too much of an artist to be able to prove he was right and who was regarded as something of a madman in his time. He would be understood only long after his death."
Many chess openings and variations are named after Nimzowitsch, the most famous being the Nimzo-Indian Defence (1.d4 Nf6 2.c4 e6 3.Nc3 Bb4) and the less often played Nimzowitsch Defence (1.e4 Nc6). Nimzowitsch biographer GM Raymond Keene and others have referred to 1.f4 followed by 2.b3 as the Nimzowitsch–Larsen Attack. Keene wrote a book about the opening with that title. These openings all exemplify Nimzowitsch's ideas about controlling the center with pieces instead of pawns. He was also vital in the development of two important systems in the French Defence, the (in some places called the Nimzowitsch Variation; its moves are 1.e4 e6 2.d4 d5 3.Nc3 Bb4) and the (1.e4 e6 2.d4 d5 3.e5). He also pioneered two provocative variations of the Sicilian Defence: the , 1.e4 c5 2.Nf3 Nf6, which invites 3.e5 Nd5 (similar to Alekhine's Defence) and 1.e4 c5 2.Nf3 Nc6 3.d4 cxd4 4.Nxd4 d5?! (the latter regarded as dubious today). International Master John L. Watson has dubbed the line 1.c4 Nf6 2.Nc3 e6 3.Nf3 Bb4 the "Nimzo-English", employing this designation in Chapter 11 of his recent book "Mastering the Chess Openings, Volume 3".
Personality.
There are many entertaining anecdotes regarding Nimzowitsch—some less savory than others. For example, he once missed first prize in a tournament in Berlin by losing to Sämisch, and when it became clear he was going to lose the game, Nimzowitsch stood up on the table and shouted, "Gegen diesen Idioten muss ich verlieren!" ("That I should lose to this idiot!").
Nimzowitsch was annoyed by his opponents' smoking. A popular, but probably apocryphal, story is that once when an opponent laid an unlit cigar on the table, he complained to the tournament arbiters, "He is threatening to smoke, and as an old player you must know that the threat is stronger than the execution."
Nimzowitsch had lengthy and somewhat bitter dogmatic conflicts with Tarrasch over whose ideas constituted 'proper' chess.
Nimzowitsch's vanity and faith in his ideas of overprotection provoked Hans Kmoch to write a parody about him in February 1928 in the "Wiener Schachzeitung". This consisted of a mock game against the fictional player "Systemsson", supposedly played and annotated by Nimzowitsch himself. The annotations gleefully exaggerate the idea of overprotection, as well as asserting the true genius of the wondrous idea. Kmoch was in fact a great admirer of Nimzowitsch, and the subject of the parody himself was amused at the effort.
Kmoch also wrote an article about his nine years with Nimzowitsch:
Nimzovitsch's colleague Tartakower observed of him, "He pretends to be crazy in order to drive us all crazy."

</doc>
<doc id="2813" url="http://en.wikipedia.org/wiki?curid=2813" title="Aragonese language">
Aragonese language

Aragonese (; "aragonés" in Aragonese) is a Romance language spoken by between 10,000 and 30,000 people throughout the valleys of the Pyrenees in Aragon, Spain, mainly in the comarcas of Somontano de Barbastro, Jacetania, Alto Gállego, Sobrarbe, and Ribagorza. It is the only modern language that developed from medieval Navarro-Aragonese.
While informally known as "fabla" ("talk" or "speech"), Aragonese is also commonly referred to by the names of its numerous local dialects which arose from fragmentation the language underwent over the centuries.
History.
Aragonese originated in the early Middle Ages, as one of many Latin dialects developed in the Pyrenees on top of a strong Basque-like substratum. The original Kingdom of Aragon (formed by the counties of Aragon, Sobrarbe and Ribagorza) was progressively expanded from the mountain ranges towards the South, pushing the Moors farther south in the "Reconquista" and spreading the Aragonese language.
The dynastic union of the Catalan Counties and the Kingdom of Aragon—which formed the Aragonese Crown in the twelfth century—did not result in a merging of the language forms of the two territories; Catalan continued to be spoken in the east, and Navarro-Aragonese in the west, although with blurred boundaries because of dialectal continuity. The Aragonese "Reconquista" to the south ended in the kingdom of Murcia, which was ceded by James I of Aragon to the Kingdom of Castile as a dowry for an Aragonese princess.
The main character of the Aragonese language was undoubtedly Johan Ferrandez d'Heredia, founder of the lineage and Grand Master of the Order of the Hospital of St. John of Jerusalem based in Rhodes. He wrote an extensive catalog of works in Aragonese and also several works translated from Greek into Aragonese, the first in medieval Europe.
The spread of Castilian—now more commonly known as Spanish—the Castilian origin of the Trastámara dynasty, and a strong similarity between Castilian and Aragonese meant that further recession was to follow. One of the key moments in the history of Aragonese was when a king of Castilian origin was appointed in the fifteenth century: Ferdinand I of Aragon, also known as Ferdinand of Antequera.
The mutual union of the crowns of Aragon and Castile and the progressive suspension of all capacity of self-rule from the sixteenth century meant that Aragonese, while still widely spoken, was limited to a rural and colloquial use, as the nobility chose Spanish as their symbol of power.
During the rule of Francisco Franco in the twentieth century and the spreading of compulsory schooling, Aragonese was regarded as a mere dialect of Spanish, and therefore was frowned upon (for example, pupils were punished in schools for using it).
Then, the constitutional democracy voted by the people in 1978 also meant the debut of literary works and studies conducted in and about the Aragonese language.
Modern Aragonese.
Today, Aragonese is still spoken natively within its core area, the Aragonese mountain ranges of the Pyrenees, in the comarcas of Somontano, Jacetania, Sobrarbe, and Ribagorza.
These are the major cities and towns where Aragonese speakers can still be found: Huesca, Graus, Monzón, Barbastro, Bielsa, Chistén, Fonz, Echo, Estadilla, Benasque, Campo, Sabiñánigo, Jaca, Plan, Ansó, Ayerbe, Broto, and El Grado.
Aragonese is also learnt as a second language by other inhabitants of the country in areas like Huesca, Zaragoza, Ejea de los Caballeros, and Teruel. According to recent polls, altogether they only make up around 10,000 active speakers and about 30,000 passive speakers.
In 2009 the Languages Act of Aragon gives recognition of "native language, original and historic" of Aragon, therefore there are a number of linguistic rights, as the utilisation of Aragonese language in the public administrations of Aragon.
Phonology.
Phonological characteristics.
Aragonese has many historical traits that join it to Catalan rather than Spanish. Some of these are conservative features that are also shared with Astur-Leonese and Portuguese, where Spanish innovated in ways that did not spread to nearby languages.
Orthography.
In 2010, the Academia de l'Aragonés, formed in 2006, established a single orthographic standard in order to modernize medieval orthography and look for a more etymological language. This new orthography is used by the Aragonese Wikipedia.
Previously, Aragonese had two orthographic standards:
In the sixteenth century, Aragonese Moriscoes wrote some Romance texts in Arabic writing, probably because of their inability to write in Arabic; the language in these texts shows a mixture of Aragonese and Castilian traits, and they can be considered among the last written examples of the Aragonese formerly spoken in Central and Southern Aragón.
Grammar.
Aragonese grammar is similar to the grammar of other Iberian Romance languages, such as Spanish and Catalan.
Article.
Definite article.
The definite article in Aragonese has undergone certain changes that have become characteristics of dialectal differentiation. Articles in old Aragonese were similar to Castillan articles. The most widespread articles in Aragonese dialects are similar to those in Galician and Portuguese, as they lack the initial "l":
The second article, auxiliary, after a vowel, is used with an R, whose pronunciation is the soft "r".
Lexicology.
Aragonese lexicology shows the origin of Aragonese words from different languages which have influenced it.
The influence of other Romance languages on Aragonese is evident, especially from neighboring languages. Catalan and Occitan influenced Medieval Aragonese, and the Catalan influence continued, under the Crown of Aragon, in the territory where the languages are in contact (Ribagorça), a fact that explains the main characteristics of Eastern Aragonese. Since the 15th century, the Romance language which has influenced Aragonese the most is Spanish, having been adopted in almost all of Aragon as the first language and reducing Aragonese to the northern region around the Pyrenees.
Another Romance language with certain influence is French, the only official language in the neighboring country. Italian loans have come through other languages such as Catalan, and words from Portuguese, through Spanish.
Germanic words came through the conquest of the region by Germanic peoples in the 5th century AD.
Aragonese has also loans from Arabic and Mozarabic languages due to the Umayyad conquest of Hispania in the 8th century. Arabic brought words from other languages such as Persian and Sanskrit.
English, with its international importance, has introduced a lot of new words into the language.
Gender.
Words that derive from the second declension or assimilated to the second declension are usually masculine:
Words derived from the first declension are usually feminine:
There are neutral plurals assimilated to the first declension that become feminine-gender singular: 
Words that end in -or are feminine:
The names of fruit trees usually end in "-era", a suffix derived from Latin -ARIA and they are usually feminine:
The genders of the names of rivers vary according to the river:
Pronouns.
Aragonese preserves the system of clitic pronouns derived from the Latin forms 'inde' and 'ibi', as 'en/ne' and ".
This feature is shared with other Romance languages (Catalan, Occitan, French, Italian), and makes Aragonese different from other Ibero-Romance languages without those clitics (Spanish, Asturian, Portuguese).
'En/ne' is used for:
" is used for:
Dialects.
There are about 25-30 dialectal variants of Aragonese, the majority of which are in the province of Huesca, due to its mountainous terrain where natural isoglosses have developed around valley enclaves, and where the highest incidence of spoken Aragonese is found. Ribagorçan, is one such variant: an eastern Aragonese dialect, which is transitional to Gascon Occitan, Catalan, and Spanish.
There is a proposal to classify the language varieties into four main dialects:
For certains linguistics, these groups are complex dialects formed by various varieties. For other, these four groups are Aragonese dialects and Hecho Aragonese and Chistau Valley Aragonese are comarcal varieties.
Literature.
Medieval Ages.
At no point in its history did the Aragonese language acquire the prestige literature developed in other Romance languages from the Iberian Peninsula.
Not until the 12th - 13th centuries did Aragonese begin to be used in written documents. From this period are featured "Liber Regum'" (a book of general history), "Razón feita d'amor", "Libre dels tres reys d'orient" and "Vida de Santa María Egipcíaca".
Early modern period literature.
Spanish was from 1500 the first language of culture in Aragon: a lot of Aragonese stood out with writings in the Spanish language, to the point that in the 17th century the Argensola brothers went to Castile to teach Spanish.
Aragonese became a familiar and village language and each day acquired popular traces. The 16th was obscure: there is no document in Aragonese, just in Aljamia. In the 17th century appeared certain texts that used the language to characterised popular characters.
In a literary contest held in Huesca in 1650, there were three poems submitted in Aragonese, respectively, by Matías Pradas, Isabel de Rodas and "Fileno, montañés".
The first "pastoradas" come from the 16th and the 17th century.
Contemporary literature.
The 19th and 20th centuries have seen a renaissance of the Aragonese literature; however, due to the lack of a standard language, authors write about local topics, each using his own variety of the language. In 1844, the novel "Vida de Pedro Saputo", by Braulio Foz, appeared in Almudévar Aragonese. In the 20th century the costumbrist comedies written by Domingo Miral gained importance, as well as the poetry of Veremundo Méndez Coarasa, both in Hecho Aragonese; in Graus Aragonese, one can highlight Cleto Torrodellas' verses and Tonón de Baldomera's popular writings; in Somontano Aragonese, there are Pedro Arnal Cavero's costumbrist narrations, as well as Juana Coscujuela' novel, "A Lueca, historia d'una moceta d'o Semontano".

</doc>
<doc id="2815" url="http://en.wikipedia.org/wiki?curid=2815" title="Advanced Mobile Phone System">
Advanced Mobile Phone System

Advanced Mobile Phone System (AMPS) is an analog mobile cell phone system standard developed by Bell Labs, and officially introduced in the Americas in 1978, Israel in 1986, and Australia in 1987. It was the primary analog mobile phone system in North America (and other locales) through the 1980s and into the 2000s. As of February 18, 2008, carriers in the United States were no longer required to support AMPS and companies such as AT&T and Verizon have discontinued this service permanently. AMPS was discontinued in Australia in September 2000.
History.
The first cellular network efforts began at Bell Labs (which first proposed the idea of a cellular system in 1947 and continued to petition the FCC for channels through the 1950s and 1960s) and with research conducted at Motorola. 
In 1960, John F. Mitchell,
an electrical engineer who had graduated from the Illinois Institute of Technology, became Motorola's chief engineer for its mobile-communication products. Mitchell oversaw the development and marketing of the first pager to use transistors.
Motorola had long produced mobile telephones for automobiles, but these large and heavy models consumed too much power to allow their use without the automobile's engine running. Mitchell's team, which included the gifted Dr. Martin Cooper, developed portable cellular telephony, and Mitchell was among the Motorola employees granted a patent for this work in 1973; the first call on the prototype connected, reportedly, to a wrong number.
While Motorola was developing a cellular phone, from 1968-1983 Bell Labs worked out a system called Advanced Mobile Phone System (AMPS), which became the first cellular network standard in the U.S. Motorola and others designed and built the cellular phones for this and other cellular systems.
Martin Cooper, a former general manager for the systems division at Motorola, led a team that produced the DynaTAC8000x, the first commercially-available cellular phone small enough to be easily carried, and made the first phone call from it, and later introduced the so-called Bag Phone.
In 1992 the first smartphone used AMPS. Frank Canova led its design at IBM and it was demonstrated that year at the COMDEX computer-industry trade-show. A refined version of the product was marketed to consumers in 1994 by BellSouth under the name Simon Personal Communicator. The Simon was the first device that can be properly referred to as a "smartphone", even though that term was not yet coined.
Technology.
AMPS is a first-generation cellular technology that uses separate frequencies, or "channels", for each conversation (see Frequency-division multiple access(FDMA). It therefore required considerable bandwidth for a large number of users. In general terms, AMPS was very similar to the older "0G" Improved Mobile Telephone Service, but used considerably more computing power in order to select frequencies, hand off conversations to PSTN lines, and handle billing and call setup.
What really separated AMPS from older systems is the "back end" call setup functionality. In AMPS, the cell centers could flexibly assign channels to handsets based on signal strength, allowing the same frequency to be re-used in various locations without interference. This allowed a larger number of phones to be supported over a geographical area. AMPS pioneers coined the term "cellular" because of its use of small hexagonal "cells" within a system.
AMPS suffered from many weaknesses compared to today's digital technologies. As an analog standard, it was susceptible to static and noise, and there was no protection from 'eavesdropping' using a scanner.
Cloning.
In the 1990s an epidemic of "cloning" cost the cellular carriers millions of dollars. An eavesdropper with specialized equipment could intercept a handset's ESN (Electronic Serial Number) and MDN or CTN (Mobile Directory Number or Cellular Telephone Number). The Electronic Serial Number, a 12-digit number sent by the handset to the cellular system for billing purposes, uniquely identified that phone on the network. The system then allowed or disallowed calls and or features based on its customer file. A person intercepting an ESN/MDN pair could clone the combination onto a different phone and use it in other areas for making calls without paying.
Cell-phone cloning became possible with off-the-shelf technology in the 1990s. Would-be cloners required three key items :
The radio, when tuned to the proper frequency, would receive the signal transmitted by the cell phone to be cloned, containing the phone's ESN/MDN pair. This signal would feed into the sound-card audio-input of the PC, and Banpaia would decode the ESN/MDN pair from this signal and display it on the screen. The hacker could then copy that data into the Oki 900 phone and reboot it, after which the phone network could not distinguish the Oki from the original phone whose signal had been received. This gave the cloner, through the Oki phone, the ability to use the mobile-phone service of the legitimate subscriber whose phone was cloned - just as if that phone had been physically stolen, except that the subscriber retained his or her phone, unaware that the phone had been cloned—at least until that subscriber received his or her next bill.
The problem became so large that some carriers required the use of a PIN before making calls. Eventually, the cellular companies initiated a system called RF Fingerprinting, whereby it could determine subtle differences in the signal of one phone from another and shut down some cloned phones. Some legitimate customers had problems with this though if they made certain changes to their own phone, such as replacing the battery and/or antenna. The Oki 900, the ultimate tool of cell-phone hackers, could listen in to AMPS phone-calls right out-of-the-box with no hardware modifications.
Standards.
AMPS was originally standardized by American National Standards Institute(ANSI) as EIA/TIA/IS-3. EIA/TIA/IS-3 was superseded by EIA/TIA-553 and TIA interim standard with digital technologies, the cost of wireless service is so low that the problem of cloning has virtually disappeared.
Frequency bands.
AMPS cellular service operated in the 850 MHz Cellular band. For each market area, the United States Federal Communications Commission (FCC) allowed two licensees (networks) known as "A" and "B" carriers. Each carrier within a market used a specified "block" of frequencies consisting of 21 control channels and 395 voice channels. Originally, the B (wireline) side license was usually owned by the local phone company, and the A (non-wireline) license was given to wireless telephone providers.
At the inception of cellular in 1983, the FCC had granted each carrier within a market 333 channel pairs (666 channels total). By the late 1980s, the cellular industry's subscriber base had grown into the millions across America and it became necessary to add channels for additional capacity. In 1989, the FCC granted carriers an expansion from the previous 666 channels to the final 832 (416 pairs per carrier). The additional frequencies were from the band held in reserve for future (inevitable) expansion. These frequencies were immediately adjacent to the existing cellular band. These bands had previously been allocated to UHF TV channels 70–83.
Each duplex channel was composed of 2 frequencies. 416 of these were in the 824–849 MHz range for transmissions from mobile stations to the base stations, paired with 416 frequencies in the 869–894 MHz range for transmissions from base stations to the mobile stations. Each cell site used a different subset of these channels than its neighbors to avoid interference. This significantly reduced the number of channels available at each site in real-world systems. Each AMPS channel had a one way bandwidth of 30 kHz, for a total of 60 kHz for each duplex channel.
Laws were passed in the US which prohibited the FCC type acceptance and sale of any receiver which could tune the frequency ranges occupied by analog AMPS cellular services. Though the service is no longer offered, these laws remain in force.
Digital AMPS.
Later, many AMPS networks were partially converted to D-AMPS, often referred to as TDMA (though TDMA is a generic term that applies to many cellular systems). D-AMPS was a digital, 2G standard used mainly by AT&T Mobility and U.S. Cellular in the United States, Rogers Wireless in Canada, Telcel in Mexico, Telecom Italia Mobile (TIM) in Brazil, VimpelCom in Russia, Movilnet in Venezuela, and Cellcom in Israel. In most areas, D-AMPS is no longer offered and has been replaced by more advanced digital wireless networks.
Successor technologies.
AMPS and D-AMPS have now been phased out in favor of either CDMA2000 or Global System for Mobile Communications (GSM) which allow for higher capacity data transfers for services such as WAP, Multimedia Messaging System (MMS), and wireless Internet access. There are some phones capable of supporting AMPS, D-AMPS and GSM all in one phone (using the GAIT standard).
Analog AMPS being replaced by digital.
In 2002, the FCC decided to no longer require A and B carriers to support AMPS service as of February 18, 2008. All AMPS carriers have converted to a digital standard such as CDMA2000 or GSM. Digital technologies such as GSM and CDMA2000 support multiple voice calls on the same channel and offer enhanced features such as two-way text messaging and data services.
Unlike in the United States, the Canadian Radio-television and Telecommunications Commission (CRTC) and Industry Canada have not set any requirement for maintaining AMPS service in Canada. Rogers Wireless has dismantled their AMPS (along with IS-136) network; the networks were shut down May 31, 2007. Bell Mobility and Telus Mobility, who operated AMPS networks in Canada, announced that they would observe the same timetable as outlined by the FCC in the United States, and as a result would not begin to dismantle their AMPS networks until after February 2008.
OnStar relied heavily on North American AMPS service for its subscribers because, when the system was developed, AMPS offered the most comprehensive wireless coverage in the US. In 2006, ADT asked the FCC to extend the AMPS deadline due to many of their alarm systems still using analog technology to communicate with the control centers. Cellular companies who own an A or B license (such as Verizon and Alltel) were required to provide analog service until February 18, 2008. After that point, however, most cellular companies were eager to shut down AMPS and use the remaining channels for digital services. OnStar transitioned to digital service with the help of data transport technology developed by Airbiquity, but warned customers who could not be upgraded to digital that their service would permanently expire on January 1, 2008.

</doc>
<doc id="2819" url="http://en.wikipedia.org/wiki?curid=2819" title="Aerodynamics">
Aerodynamics

Aerodynamics, from Greek ἀήρ "aer" (air) + δυναμική (dynamics), is a branch of dynamics concerned with studying the motion of air, particularly when it interacts with a solid object, such as an airplane wing. Aerodynamics is a sub-field of fluid dynamics and gas dynamics, and many aspects of aerodynamics theory are common to these fields. The term "aerodynamics" is often used synonymously with gas dynamics, with the difference being that "gas dynamics" applies to the study of the motion of all gases, not limited to air.
Formal aerodynamics study in the modern sense began in the eighteenth century, although observations of fundamental concepts such as aerodynamic drag have been recorded much earlier. Most of the early efforts in aerodynamics worked towards achieving heavier-than-air flight, which was first demonstrated by Wilbur and Orville Wright in 1903. Since then, the use of aerodynamics through mathematical analysis, empirical approximations, wind tunnel experimentation, and computer simulations has formed the scientific basis for ongoing developments in heavier-than-air flight and a number of other technologies. Recent work in aerodynamics has focused on issues related to compressible flow, turbulence, and boundary layers, and has become increasingly computational in nature.
History.
Modern aerodynamics only dates back to the seventeenth century, but aerodynamic forces have been harnessed by humans for thousands of years in sailboats and windmills, and images and stories of flight appear throughout recorded history, such as the Ancient Greek legend of Icarus and Daedalus. Fundamental concepts of continuum, drag, and pressure gradients, appear in the work of Aristotle and Archimedes.
In 1726, Sir Isaac Newton became the first person to develop a theory of air resistance, making him one of the first aerodynamicists. Dutch-Swiss mathematician Daniel Bernoulli followed in 1738 with "Hydrodynamica", in which he described a fundamental relationship between pressure, density, and velocity for incompressible flow known today as Bernoulli's principle, which provides one method for calculating aerodynamic lift. In 1757, Leonhard Euler published the more general Euler equations, which could be applied to both compressible and incompressible flows. The Euler equations were extended to incorporate the effects of viscosity in the first half of the 1800s, resulting in the Navier-Stokes equations. The Navier-Stokes equations are the most general governing equations of fluid flow and are difficult to solve.
In 1799, Sir George Cayley became the first person to identify the four aerodynamic forces of flight (weight, lift, drag, and thrust), as well as the relationships between them, outlining the work towards achieving heavier-than-air flight for the next century. In 1871, Francis Herbert Wenham constructed the first wind tunnel, allowing precise measurements of aerodynamic forces. Drag theories were developed by Jean le Rond d'Alembert, Gustav Kirchhoff, and Lord Rayleigh. In 1889, Charles Renard, a French aeronautical engineer, became the first person to reasonably predict the power needed for sustained flight. Otto Lilienthal, the first person to become highly successful with glider flights, was also the first to propose thin, curved airfoils that would produce high lift and low drag. Building on these developments as well as research carried out in their own wind tunnel, the Wright brothers flew the first powered aircraft on December 17, 1903.
During the time of the first flights, Frederick W. Lanchester, Martin Wilhelm Kutta, and Nikolai Zhukovsky independently created theories that connected circulation of a fluid flow to lift. Kutta and Zhukovsky went on to develop a two-dimensional wing theory. Expanding upon the work of Lanchester, Ludwig Prandtl is credited with developing the mathematics behind thin-airfoil and lifting-line theories as well as work with boundary layers.
As aircraft speed increased, designers began to encounter challenges associated with air compressibility at speeds near or greater than the speed of sound. The differences in air flows under these conditions led to problems in aircraft control, increased drag due to shock waves, and structural dangers due to aeroelastic flutter. The ratio of the flow speed to the speed of sound was named the Mach number after Ernst Mach, who was one of the first to investigate the properties of supersonic flow. William John Macquorn Rankine and Pierre Henri Hugoniot independently developed the theory for flow properties before and after a shock wave, while Jakob Ackeret led the initial work on calculating the lift and drag of supersonic airfoils. Theodore von Kármán and Hugh Latimer Dryden introduced the term transonic to describe flow speeds around Mach 1 where drag increases rapidly. This rapid increase in drag led aerodynamicists and aviators to disagree on whether supersonic flight was achievable. The sound barrier was broken for the first time in 1947 using the Bell X-1 aircraft.
By the time the sound barrier was broken, much of the subsonic and low supersonic aerodynamics knowledge had matured. The Cold War fueled an ever evolving line of high performance aircraft. Computational fluid dynamics began as an effort to solve for flow properties around complex objects and has rapidly grown to the point where entire aircraft can be designed using a computer, with wind-tunnel tests followed by flight tests to confirm the computer predictions. Knowledge of supersonic and hypersonic aerodynamics has also matured since the 1960s, and the goals of aerodynamicists have shifted from understanding the behavior of fluid flow to understanding how to engineer a vehicle to interact appropriately with the fluid flow. Designing aircraft for supersonic and hypersonic conditions, as well as the desire to improve the aerodynamic efficiency of current aircraft and propulsion systems, continues to fuel new research in aerodynamics, while work continues to be done on important problems in basic aerodynamic theory related to flow turbulence and the existence and uniqueness of analytical solutions to the Navier-Stokes equations.
Fundamental concepts.
Understanding the motion of air around an object (often called a flow field) enables the calculation of forces and moments acting on the object. In many aerodynamics problems, the forces of interest are the fundamental forces of flight: lift, drag, thrust, and weight. Of these, lift and drag are aerodynamic forces, i.e. forces due to air flow over a solid body. Calculation of these quantities is often founded upon the assumption that the flow field behaves as a continuum. Continuum flow fields are characterized by properties such as velocity, pressure, density and temperature, which may be functions of spatial position and time. These properties may be directly or indirectly measured in aerodynamics experiments, or calculated from equations for the conservation of mass, momentum, and energy in air flows. Density, velocity, and an additional property, viscosity, are used to classify flow fields.
Flow classification.
Flow velocity is used to classify flows according to speed regime. Subsonic flows are flow fields in which air velocity throughout the entire flow is below the local speed of sound. Transonic flows include both regions of subsonic flow and regions in which the flow speed is greater than the speed of sound. Supersonic flows are defined to be flows in which the flow speed is greater than the speed of sound everywhere. A fourth classification, hypersonic flow, refers to flows where the flow speed is much greater than the speed of sound. Aerodynamicists disagree on the precise definition of hypersonic flow.
Compressibility refers to whether or not the flow in a problem can have a varying density. Subsonic flows are often assumed to be incompressible, i.e. the density is assumed to be constant. Transonic and supersonic flows are compressible, and neglecting to account for the changes in density in these flow fields when performing calculations will yield inaccurate results.
Viscosity is associated with the frictional forces in a flow. In some flow fields, viscous effects are very small, and solutions may neglect to account for viscous effects. These approximations are called inviscid flows. Flows for which viscosity is not neglected are called viscous flows. Finally, aerodynamic problems may also be classified by the flow environment. External aerodynamics is the study of flow around solid objects of various shapes (e.g. around an airplane wing), while internal aerodynamics is the study of flow through passages in solid objects (e.g. through a jet engine).
Continuum assumption.
Unlike liquids and solids, gases are composed of discrete molecules which occupy only a small fraction of the volume filled by the gas. On a molecular level, flow fields are made up of many individual collisions between gas molecules and between gas molecules and solid surfaces. In most aerodynamics applications, however, this discrete molecular nature of gases is ignored, and the flow field is assumed to behave as a continuum. This assumption allows fluid properties such as density and velocity to be defined anywhere within the flow.
Validity of the continuum assumption is dependent on the density of the gas and the application in question. For the continuum assumption to be valid, the mean free path length must be much smaller than the length scale of the application in question. For example, many aerodynamics applications deal with aircraft flying in atmospheric conditions, where the mean free path length is on the order of micrometers. In these cases, the length scale of the aircraft ranges from a few meters to a few tens of meters, which is much larger than the mean free path length. For these applications, the continuum assumption holds. The continuum assumption is less valid for extremely low-density flows, such as those encountered by vehicles at very high altitudes (e.g. 300,000 ft/90 km) or satellites in Low Earth orbit. In these cases, statistical mechanics is a more valid method of solving the problem than continuous aerodynamics. The Knudsen number can be used to guide the choice between statistical mechanics and the continuous formulation of aerodynamics.
Conservation laws.
Aerodynamic problems are typically solved using fluid dynamics conservation laws as applied to a fluid continuum. Three conservation principles are used: 
The ideal gas law or another equation of state is often used in conjunction with these equations to form a determined system to solve for the unknown variables.
Branches of aerodynamics.
Aerodynamic problems are classified by the flow environment or properties of the flow, including flow speed, compressibility, and viscosity. "External" aerodynamics is the study of flow around solid objects of various shapes. Evaluating the lift and drag on an airplane or the shock waves that form in front of the nose of a rocket are examples of external aerodynamics. "Internal" aerodynamics is the study of flow through passages in solid objects. For instance, internal aerodynamics encompasses the study of the airflow through a jet engine or through an air conditioning pipe.
Aerodynamic problems can also be classified according to whether the flow speed is below, near or above the speed of sound. A problem is called subsonic if all the speeds in the problem are less than the speed of sound, transonic if speeds both below and above the speed of sound are present (normally when the characteristic speed is approximately the speed of sound), supersonic when the characteristic flow speed is greater than the speed of sound, and hypersonic when the flow speed is much greater than the speed of sound. Aerodynamicists disagree over the precise definition of hypersonic flow; a rough definition considers flows with Mach numbers above 5 to be hypersonic.
The influence of viscosity in the flow dictates a third classification. Some problems may encounter only very small viscous effects on the solution, in which case viscosity can be considered to be negligible. The approximations to these problems are called inviscid flows. Flows for which viscosity cannot be neglected are called viscous flows.
Incompressible aerodynamics.
An incompressible flow is a flow in which density is constant in both time and space. Although all real fluids are compressible, a flow problem is often considered incompressible if the effect of the density changes in the problem on the outputs of interest is small. This is more likely to be true when the flow speeds are significantly lower than the speed of sound. Effects of compressibility are more significant at speeds close to or above the speed of sound. The Mach number is used to evaluate whether the incompressibility can be assumed or the flow must be solved as compressible.
Subsonic flow.
Subsonic (or low-speed) aerodynamics studies fluid motion in flows which are much lower than the speed of sound everywhere in the flow. There are several branches of subsonic flow but one special case arises when the flow is inviscid, incompressible and irrotational. This case is called potential flow and allows the differential equations used to be a simplified version of the governing equations of fluid dynamics, thus making available to the aerodynamicist a range of quick and easy solutions.
In solving a subsonic problem, one decision to be made by the aerodynamicist is whether to incorporate the effects of compressibility. Compressibility is a description of the amount of change of density in the problem. When the effects of compressibility on the solution are small, the aerodynamicist may choose to assume that density is constant. The problem is then an incompressible low-speed aerodynamics problem. When the density is allowed to vary, the problem is called a compressible problem. In air, compressibility effects are usually ignored when the Mach number in the flow does not exceed 0.3 (about 335 feet (102m) per second or 228 miles (366 km) per hour at 60 °F). Above 0.3, the problem should be solved by using compressible aerodynamics.
Compressible aerodynamics.
According to the theory of aerodynamics, a flow is considered to be compressible if its change in density with respect to pressure is non-zero along a streamline. This means that - unlike incompressible flow - changes in density must be considered. In general, this is the case where the Mach number in part or all of the flow exceeds 0.3. The Mach .3 value is rather arbitrary, but it is used because gas flows with a Mach number below that value demonstrate changes in density with respect to the change in pressure of less than 5%. Furthermore, that maximum 5% density change occurs at the stagnation point of an object immersed in the gas flow and the density changes around the rest of the object will be significantly lower. Transonic, supersonic, and hypersonic flows are all compressible.
Transonic flow.
The term Transonic refers to a range of velocities just below and above the local speed of sound (generally taken as Mach 0.8–1.2). It is defined as the range of speeds between the critical Mach number, when some parts of the airflow over an aircraft become supersonic, and a higher speed, typically near Mach 1.2, when all of the airflow is supersonic. Between these speeds, some of the airflow is supersonic, and some is not.
Supersonic flow.
Supersonic aerodynamic problems are those involving flow speeds greater than the speed of sound. Calculating the lift on the Concorde during cruise can be an example of a supersonic aerodynamic problem.
Supersonic flow behaves very differently from subsonic flow. Fluids react to differences in pressure; pressure changes are how a fluid is "told" to respond to its environment. Therefore, since sound is in fact an infinitesimal pressure difference propagating through a fluid, the speed of sound in that fluid can be considered the fastest speed that "information" can travel in the flow. This difference most obviously manifests itself in the case of a fluid striking an object. In front of that object, the fluid builds up a stagnation pressure as impact with the object brings the moving fluid to rest. In fluid traveling at subsonic speed, this pressure disturbance can propagate upstream, changing the flow pattern ahead of the object and giving the impression that the fluid "knows" the object is there and is avoiding it. However, in a supersonic flow, the pressure disturbance cannot propagate upstream. Thus, when the fluid finally does strike the object, it is forced to change its properties -- temperature, density, pressure, and Mach number—in an extremely violent and irreversible fashion called a shock wave. The presence of shock waves, along with the compressibility effects of high-velocity (see Reynolds number) fluids, is the central difference between supersonic and subsonic aerodynamics problems.
Hypersonic flow.
In aerodynamics, hypersonic speeds are speeds that are highly supersonic. In the 1970s, the term generally came to refer to speeds of Mach 5 (5 times the speed of sound) and above. The hypersonic regime is a subset of the supersonic regime. Hypersonic flow is characterized by high temperature flow behind a shock wave, viscous interaction, and chemical dissociation of gas.
Associated terminology.
The incompressible and compressible flow regimes produce many associated phenomena, such as boundary layers and turbulence.
Boundary layers.
The concept of a boundary layer is important in many aerodynamic problems. The viscosity and fluid friction in the air is approximated as being significant only in this thin layer. This principle makes aerodynamics much more tractable mathematically.
Turbulence.
In aerodynamics, turbulence is characterized by chaotic, stochastic property changes in the flow. This includes low momentum diffusion, high momentum convection, and rapid variation of pressure and velocity in space and time. Flow that is not turbulent is called laminar flow.
Aerodynamics in other fields.
Aerodynamics is important in a number of applications other than aerospace engineering. It is a significant factor in any type of vehicle design, including automobiles. It is important in the prediction of forces and moments in sailing. It is used in the design of mechanical components such as hard drive heads. Structural engineers also use aerodynamics, and particularly aeroelasticity, to calculate wind loads in the design of large buildings and bridges. Urban aerodynamics seeks to help town planners and designers improve comfort in outdoor spaces, create urban microclimates and reduce the effects of urban pollution. The field of environmental aerodynamics studies the ways atmospheric circulation and flight mechanics affect ecosystems. The aerodynamics of internal passages is important in heating/ventilation, gas piping, and in automotive engines where detailed flow patterns strongly affect the performance of the engine.
People who do wind turbine design use aerodynamics.
A few aerodynamic equations are used as part of numerical weather prediction.
Further reading.
General aerodynamics
Subsonic aerodynamics
Transonic aerodynamics
Supersonic aerodynamics
Hypersonic aerodynamics
History of aerodynamics
Aerodynamics related to engineering
"Ground vehicles"
"Fixed-wing aircraft"
"Helicopters"
"Missiles"
"Model aircraft"
Related branches of aerodynamics
"Aerothermodynamics"
"Aeroelasticity"
"Boundary layers"
"Turbulence"

</doc>
<doc id="2820" url="http://en.wikipedia.org/wiki?curid=2820" title="Andreas Schlüter">
Andreas Schlüter

Andreas Schlüter (20 May 1664 – May 1714) was a German baroque sculptor and architect associated with the Petrine Baroque style of architecture and decoration.
Biography.
Andreas Schlüter was born in Hamburg His early life is obscure as at least three different persons of that name are documented. The records of St. Michaelis Church, Hamburg show that an Andreas Schlüter, son of sculptor Gerhart Schlüter, had been baptized there on 22 May 1664. Documents from Danzig (Gdańsk) reported that an Andreas Schlüter "(senior)" had worked 1640-1652 in Danzig's Jopengasse lane (today's ulica Piwna). Possibly born in 1640, an "Andres Schliter" is recorded as apprentice on 9 May 1656 by the mason's guild. Other sources state 1659 as year of birth.
He probably did spend several years abroad as Journeyman. His first work, in 1675, may have been epitaphs of the Dukes Sambor and Mestwin in the dome of Pelplin monastery.
Schlüter's first known work was the decoration of the facade of the St. Johannis Chapel, or Danzig Royal Chapel, in 1681. He later created statues for King John III Sobieski's Wilanów Palace in Warsaw and sepulchral sculptures in Żółkiew (Zhovkva). In 1689, he moved to Warsaw and made the pediment reliefs and sculptural work of Krasiński Palace.
Schlüter was invited to Berlin in 1694 by Eberhard von Danckelmann to work as court sculptor at the armory ("Zeughaus") for Elector Frederick III. His sculpted decorations are a masterpiece of baroque expression and pathos. While the more visible reliefs on the outside had to praise fighting, the statues of dying warriors in the interior denounced war and gave an indication of his pacifist religious beliefs (he is said to have been a Mennonite). Travelling through Italy in 1696, he studied the work of masters like Michelangelo Buonarroti and Gian Lorenzo Bernini.
Schlüter also worked as an architect and built many state buildings in Berlin in his role as "Hofbaumeister" (Court Architect), which he lost when one tower showed signs of a weak fundament. He also served as director of the Akademie der Künste from 1702 to 1704, after which he began concentrating on sculpting again, as "Hofbildhauer" (Court Sculptor). His most important equestrian sculpture is that of the "Great Elector", Frederick William of Brandenburg, cast in 1708 and placed at "Lange Brücke" near the Berlin City Palace, now situated in the honor court before Charlottenburg Palace.
The Berlin City Palace, and many of his works, were partially destroyed by bombing in World War II and by the subsequent Communist regime. A similar fate probably befell the Amber Room, made between 1701 and 1709, Schlüter's most famous work of architecture.
In 1713 Schlüter's fame brought him to work for Tsar Peter the Great in Saint Petersburg, where he died of an illness after creating several designs. Together with Johann Friedrich Braunstein, he designed the Grand Palace and Monplaisir Palace in Peterhof. Also the city's oldest building, Kikin's Palace, and the reliefs at the Summer Palace are attributed to him.

</doc>
<doc id="2822" url="http://en.wikipedia.org/wiki?curid=2822" title="Ash">
Ash

Ash may refer to:
Products of fire, incineration or combustion.
The solid remains of fires, such as:

</doc>
<doc id="2823" url="http://en.wikipedia.org/wiki?curid=2823" title="Antiderivative">
Antiderivative

In calculus, an antiderivative, primitive integral or indefinite integral
of a function "f" is a differentiable function "F" whose derivative is equal to "f", i.e., "F" ′ = "f". The process of solving for antiderivatives is called antidifferentiation (or indefinite integration) and its opposite operation is called differentiation, which is the process of finding a derivative. Antiderivatives are related to definite integrals through the fundamental theorem of calculus: the definite integral of a function over an interval is equal to the difference between the values of an antiderivative evaluated at the endpoints of the interval.
The discrete equivalent of the notion of antiderivative is antidifference.
Example.
The function "F"("x") = "x"3/3 is an antiderivative of "f"("x") = "x"2. As the derivative of a constant is zero, "x"2 will have an infinite number of antiderivatives; such as ("x"3/3) + 0, ("x"3/3) + 7, ("x"3/3) − 42, ("x"3/3) + 293 etc. Thus, all the antiderivatives of "x"2 can be obtained by changing the value of C in "F"("x") = ("x"3/3) + "C"; where "C" is an arbitrary constant known as the constant of integration. Essentially, the graphs of antiderivatives of a given function are vertical translations of each other; each graph's vertical location depending upon the value of "C".
In physics, the integration of acceleration yields velocity plus a constant. The constant is the initial velocity term that would be lost upon taking the derivative of velocity because the derivative of a constant term is zero. This same pattern applies to further integrations and derivatives of motion (position, velocity, acceleration, and so on).
Uses and properties.
Antiderivatives are important because they can be used to compute definite integrals, using the fundamental theorem of calculus: if "F" is an antiderivative of the integrable function "f" and "f" is continuous over the interval then:
Because of this, each of the infinitely many antiderivatives of a given function "f" is sometimes called the "general integral" or "indefinite integral" of "f" and is written using the integral symbol with no bounds:
If "F" is an antiderivative of "f", and the function "f" is defined on some interval, then every other antiderivative "G" of "f" differs from "F" by a constant: there exists a number "C" such that "G"("x") = "F"("x") + "C" for all "x". "C" is called the arbitrary constant of integration. If the domain of "F" is a disjoint union of two or more intervals, then a different constant of integration may be chosen for each of the intervals. For instance
is the most general antiderivative of formula_4 on its natural domain formula_5
Every continuous function "f" has an antiderivative, and one antiderivative "F" is given by the definite integral of "f" with variable upper boundary:
Varying the lower boundary produces other antiderivatives (but not necessarily all possible antiderivatives). This is another formulation of the fundamental theorem of calculus.
There are many functions whose antiderivatives, even though they exist, cannot be expressed in terms of elementary functions (like polynomials, exponential functions, logarithms, trigonometric functions, inverse trigonometric functions and their combinations). Examples of these are 
See also differential Galois theory for a more detailed discussion.
Techniques of integration.
Finding antiderivatives of elementary functions is often considerably harder than finding their derivatives. For some elementary functions, it is impossible to find an antiderivative in terms of other elementary functions. See the article on elementary functions for further information.
There are various methods available:
Antiderivatives of non-continuous functions.
Non-continuous functions can have antiderivatives. While there are still open questions in this area, it is known that:
Assuming that the domains of the functions are open intervals:
on the closed interval ["a", "b"]. Then "g" must have either a maximum or minimum "c" in the open interval ("a", "b") and so 

</doc>
<doc id="2824" url="http://en.wikipedia.org/wiki?curid=2824" title="Alphabet song">
Alphabet song

An alphabet song is any of various songs used to teach children an alphabet, used in kindergartens, pre-schools and homes around the world. Alphabet songs typically follow the alphabetic principle (though the phonics method offers variants). In languages such as English with morphophonemic variation (e.g. "cake" is , not ), an alphabet song usually chooses a particular pronunciation for each letter in the alphabet and also typically for some words in the song.
The A.B.C. (Verse 1).
"The A.B.C." or "A.B.C's" is one of the best-known English language alphabet songs, and perhaps the one most frequently referred to as "the alphabet song", especially in the United States.
The song was first copyrighted in 1835 by the Boston-based music publisher Charles Bradlee, and given the title "The A.B.C., a German air with variations for the flute with an easy accompaniment for the piano forte". The musical arrangement was attributed to Louis Le Maire (sometimes Lemaire), an 18th-century composer. This was "Entered according to act of Congress, in the year 1835, by C. Bradlee, in the clerk's office of the District Court of Massachusetts", according to the Newberry Library, which also says, "The theme is that used by Mozart for his piano variations, Ah, vous dirai-je, maman." This tune is the same as the tune for "Twinkle Twinkle Little Star" and "Baa, Baa, Black Sheep".
Lyrics: "(each line represents two measures, or eight beats)"
Zed for Zee.
In the United States, Z is pronounced "zee"; in most other English-speaking countries it is pronounced "zed". Generally, the absent "zee"-rhyme is not missed, although some children use a "zee" pronunciation in the rhyme which they would not use elsewhere. Variants of the song exist to accommodate the "zed" pronunciation. One variation shortens the second line and lengthens the last, to form a near-rhyme between N and zed:
In UK (Nursery Rhymes):
Other variants make significantly more changes in order to rhyme with zed, and even alter the rest of the song to fit a new rhythm. For example:
Phonics songs.
Because the English language has 40 sounds and only 26 letters, children and beginning readers also need to learn the different sounds (phonemes) associated with each letter. Many songs have been written to teach phonemic awareness and they are usually referred to as alphabet songs.
Acrostic songs.
There are also songs that go through the alphabet, making each letter stand for something in the process. An example was recorded in 1948, by Buddy Kaye, Fred Wise, Sidney Lippman, and later Perry Como, called "A, You're Adorable" (also known as "The Alphabet Love Song"):
Backwards song (Verse 2).
The group Wee Sing released an alphabet song with the letters in reverse order. It is called ZYXs. It goes as follows:
Another version ends with "Now I know my ZYXs, let's all go and walk to Texas."
The Canadian children's TV series "The Big Comfy Couch" used a version of the song in the episode "Backwards".
Comedian Soupy Sales released a song in 1966 called "Backwards Alphabet" which contained the reverse alphabet in lyrical style.
In the opening scene of the 1992 episode of "Martin", Martin sings the song in the dark radio station in season 1's "Dead Men Don't Flush".

</doc>
<doc id="2826" url="http://en.wikipedia.org/wiki?curid=2826" title="Antigonid dynasty">
Antigonid dynasty

The Antigonid dynasty (; ) was a dynasty of Hellenistic kings descended from Alexander the Great's general Antigonus I Monophthalmus ("the One-eyed").
History.
Succeeding the Antipatrid dynasty in much of Macedonia, Antigonus ruled mostly over Asia Minor and northern Syria. His attempts to take control of the whole of Alexander's empire led to his defeat and death at the Battle of Ipsus in 301 BC. Antigonus's son Demetrius I Poliorcetes survived the battle, and managed to seize control of Macedon itself a few years later, but eventually lost his throne, dying as a prisoner of Seleucus I Nicator. After a period of confusion, Demetrius's son Antigonus II Gonatas was able to establish the family's control over the old Kingdom of Macedon, as well as over most of the Greek city-states, by 276 BC.
Legacy.
It was one of four dynasties established by Alexander's successors, the others being the Seleucid dynasty, Ptolemaic dynasty and Attalid dynasty. The last scion of the dynasty, Perseus of Macedon, who reigned between 179-168 BC, proved unable to stop the advancing Roman legions and Macedon's defeat at the Battle of Pydna signaled the end of the dynasty.
Dynasty.
The ruling members of the Antigonid dynasty were:
The Greek rebel against Rome and last King of Macedonia, Andriscus, claimed to be the son of Perseus.

</doc>
<doc id="2827" url="http://en.wikipedia.org/wiki?curid=2827" title="Abingdon">
Abingdon

Abingdon may refer to the following places:
In Australia :
In Britain:
In Canada:
In the United States:
In the Galapagos Islands:

</doc>
<doc id="2830" url="http://en.wikipedia.org/wiki?curid=2830" title="Abjuration">
Abjuration

Abjuration is the solemn repudiation, abandonment, or renunciation by or upon oath, often the renunciation of citizenship or some other right or privilege. (It comes from the Latin "abjurare", "to forswear").
Abjuration of the realm.
Abjuration of the realm was a type of abjuration in ancient English law. The person taking the oath swore to leave the country directly and promptly never to return to the kingdom unless by permission of the sovereign. This was often taken by fugitives who had taken sanctuary: 
English Commonwealth.
Near the start of the English Civil War, on 18 August 1643 Parliament passed "An Ordinance for Explanation of a former Ordinance for Sequestration of Delinquents Estates with some Enlargements." The enlargements included an oath which became known as the "Oath of Abjuration":
In 1656, it was reissued in what was for Catholics an even more objectionable form. Everyone was to be "adjudged a Papist" who refused this oath, and the consequent penalties began with the confiscation of two thirds of the recusant's goods, and went on to deprive him of almost every civic right.
The Catholic Encyclopaedia make the point that the oath and the penalties were so severe that it stopped the efforts of the Gallicanizing party among the English Catholics, who had been ready to offer forms of submission similar to the old oath of Allegiance, which was condemned anew about this time by Pope Innocent X.
Great Britain and Ireland.
In England (and after 1707 Great Britain) the Oath of Abjuration denied the royal title of James II's heirs (i.e. the direct Catholic descendent of the House of Stuart exiled after the Glorious Revolution in 1688). In England, an Oath of Abjuration was taken by Members of Parliament, clergy, and laymen, pledging to support the current British monarch and repudiated the right of the Stuarts and other claimants to the throne. This oath was imposed under William III, George I and George III. It was superseded by the oath of allegiance. In Ireland the oath was imposed of state office holders, teachers and lawyers, and on clergy of the established church in from 1703, the following year it was on all Irish voters and from 1709 it could be demanded of any adult male by a magistrate.
The Netherlands.
Another famous abjuration was brought about by the Plakkaat van Verlatinghe of July 26, 1581, the formal declaration of independence of the Low Countries from the Spanish king, Philip II. This oath was the climax of the Eighty Years' War (Dutch Revolt).

</doc>
<doc id="2833" url="http://en.wikipedia.org/wiki?curid=2833" title="Abitibi">
Abitibi

Abitibi may refer to:

</doc>
<doc id="2834" url="http://en.wikipedia.org/wiki?curid=2834" title="A Vindication of the Rights of Woman">
A Vindication of the Rights of Woman

A Vindication of the Rights of Woman: with Strictures on Political and Moral Subjects (1792), written by the 18th-century British feminist Mary Wollstonecraft, is one of the earliest works of feminist philosophy. In it, Wollstonecraft responds to those educational and political theorists of the 18th century who did not believe women should have an education. She argues that women ought to have an education commensurate with their position in society, claiming that women are essential to the nation because they educate its children and because they could be "companions" to their husbands, rather than mere wives. Instead of viewing women as ornaments to society or property to be traded in marriage, Wollstonecraft maintains that they are human beings deserving of the same fundamental rights as men.
Wollstonecraft was prompted to write the "Rights of Woman" after reading Charles Maurice de Talleyrand-Périgord's 1791 report to the French National Assembly, which stated that women should only receive a domestic education; she used her commentary on this specific event to launch a broad attack against sexual double standards and to indict men for encouraging women to indulge in excessive emotion. Wollstonecraft wrote the "Rights of Woman" hurriedly to respond directly to ongoing events; she intended to write a more thoughtful second volume but died before completing it.
While Wollstonecraft does call for equality between the sexes in particular areas of life, such as morality, she does not explicitly state that men and women are equal. Her ambiguous statements regarding the equality of the sexes have since made it difficult to classify Wollstonecraft as a modern feminist, particularly since the word and the concept were unavailable to her. Although it is commonly assumed now that the "Rights of Woman" was unfavourably received, this is a modern misconception based on the belief that Wollstonecraft was as reviled during her lifetime as she became after the publication of William Godwin's "Memoirs of the Author of A Vindication of the Rights of Woman" (1798). The "Rights of Woman" was actually well received when it was first published in 1792. One biographer has called it "perhaps the most original book of [Wollstonecraft's] century".
Historical context.
"A Vindication of the Rights of Woman" was written against the tumultuous background of the French Revolution and the debates that it spawned in Britain. In a lively and sometimes vicious pamphlet war, now referred to as the "Revolution Controversy", British political commentators addressed topics ranging from representative government to human rights to the separation of church and state, many of these issues having been raised in France first. Wollstonecraft first entered this fray in 1790 with "A Vindication of the Rights of Men", a response to Edmund Burke's "Reflections on the Revolution in France" (1790). In his "Reflections", Burke criticised the view of many British thinkers and writers who had welcomed the early stages of the French revolution. While they saw the revolution as analogous to Britain's own Glorious Revolution in 1688, which had restricted the powers of the monarchy, Burke argued that the appropriate historical analogy was the English Civil War (1642–1651) in which Charles I had been executed in 1649. He viewed the French revolution as the violent overthrow of a legitimate government. In "Reflections" he argues that citizens do not have the right to revolt against their government because civilisation is the result of social and political consensus; its traditions cannot be continually challenged—the result would be anarchy. One of the key arguments of Wollstonecraft's "Rights of Men", published just six weeks after Burke's "Reflections", is that rights cannot be based on tradition; rights, she argues, should be conferred because they are reasonable and just, regardless of their basis in tradition.
When Charles Maurice de Talleyrand-Périgord presented his "Rapport sur l'instruction publique" (1791) to the National Assembly in France, Wollstonecraft was galvanised to respond. In his recommendations for a national system of education, Talleyrand had written:
Let us bring up women, not to aspire to advantages which the Constitution denies them, but to know and appreciate those which it guarantees them . . . Men are destined to live on the stage of the world. A public education suits them: it early places before their eyes all the scenes of life: only the proportions are different. The paternal home is better for the education of women; they have less need to learn to deal with the interests of others, than to accustom themselves to a calm and secluded life.
Wollstonecraft dedicated the "Rights of Woman" to Talleyrand: "Having read with great pleasure a pamphlet which you have lately published, I dedicate this volume to you; to induce you to reconsider the subject, and maturely weigh what I have advanced respecting the rights of woman and national education." At the end of 1791, French feminist Olympe de Gouges had published her "Declaration of the Rights of Woman and the Female Citizen", and the question of women's rights became central to political debates in both France and Britain.
The "Rights of Woman" is an extension of Wollstonecraft's arguments in the "Rights of Men". In the "Rights of Men", as the title suggests, she is concerned with the rights of particular men (18th-century British men) while in the "Rights of Woman", she is concerned with the rights afforded to "woman", an abstract category. She does not isolate her argument to 18th-century women or British women. The first chapter of the "Rights of Woman" addresses the issue of natural rights and asks who has those inalienable rights and on what grounds. She answers that since natural rights are given by God, for one segment of society to deny them to another segment is a sin. "The Rights of Woman" thus engages not only specific events in France and in Britain but also larger questions being raised by contemporary political philosophers such as John Locke and Jean-Jacques Rousseau.
Themes.
Wollstonecraft did not employ the formal argumentation or logical prose style common to 18th-century philosophical writing when composing her own works. The "Rights of Woman" is a long essay that introduces all of its major topics in the opening chapters and then repeatedly returns to them, each time from a different point of view. It also adopts a hybrid tone that combines rational argument with the fervent rhetoric of sensibility.
In the 18th century, "sensibility" was a physical phenomenon that came to be attached to a specific set of moral beliefs. Physicians and anatomists believed that the more sensitive people's nerves, the more emotionally affected they would be by their surroundings. Since women were thought to have keener nerves than men, it was also believed that women were more emotional than men. The emotional excess associated with sensibility also theoretically produced an ethic of compassion: those with sensibility could easily sympathise with people in pain. Thus historians have credited the discourse of sensibility and those who promoted it with the increased humanitarian efforts, such as the movement to abolish the slave trade. But sensibility also paralysed those who had too much of it; as scholar G. J. Barker-Benfield explains, "an innate refinement of nerves was also identifiable with greater suffering, with weakness, and a susceptibility to disorder".
By the time Wollstonecraft was writing the "Rights of Woman", sensibility had already been under sustained attack for a number of years. Sensibility, which had initially promised to draw individuals together through sympathy, was now viewed as "profoundly separatist"; novels, plays, and poems that employed the language of sensibility asserted individual rights, sexual freedom, and unconventional familial relationships based only upon feeling. Furthermore, as Janet Todd, another scholar of sensibility, argues, "to many in Britain the cult of sensibility seemed to have feminized the nation, given women undue prominence, and emasculated men."
Rational education.
One of Wollstonecraft's central arguments in the "Rights of Woman" is that women should be educated rationally to give them the opportunity to contribute to society. In the 18th century, it was often assumed by both educational philosophers and conduct book writers, who wrote what one might think of as early self-help books, that women were incapable of rational or abstract thought. Women, it was believed, were too susceptible to sensibility and too fragile to be able to think clearly. Wollstonecraft, along with other female reformers such as Catharine Macaulay and Hester Chapone, maintained that women were indeed capable of rational thought and deserved to be educated. She argued this point in her own conduct book, "Thoughts on the Education of Daughters" (1787), in her children's book, "Original Stories from Real Life" (1788), as well as in the "Rights of Woman".
Stating in her preface that "my main argument is built on this simple principle, that if be not prepared by education to become the companion of man, she will stop the progress of knowledge and virtue; for truth must be common to all", Wollstonecraft contends that society will degenerate without educated women, particularly because mothers are the primary educators of young children. She attributes the problem of uneducated women to men and "a false system of education, gathered from the books written on this subject by men who [consider females rather as women than human creatures". Women are capable of rationality; it only appears that they are not, because men have refused to educate them and encouraged them to be frivolous (Wollstonecraft describes silly women as "spaniels" and "toys"). While stressing it is of the same kind, she entertains the notion that women might not be able to attain the same degree of knowledge that men do.
Wollstonecraft attacks conduct book writers such as James Fordyce and John Gregory as well as educational philosophers such as Jean-Jacques Rousseau who argue that a woman does not need a rational education. (Rousseau famously argues in "" (1762) that women should be educated for the pleasure of men; Wollstonecraft, infuriated by this argument, attacks not only it but also Rousseau himself.) Intent on illustrating the limitations that contemporary educational theory placed upon women, Wollstonecraft writes, "taught from their infancy that beauty is woman's sceptre, the mind shapes itself to the body, and, roaming round its gilt cage, only seeks to adorn its prison", implying that without this damaging ideology, which encourages young women to focus their attention on beauty and outward accomplishments, they could achieve much more. Wives could be the rational "companions" of their husbands and even pursue careers should they so choose: "women might certainly study the art of healing, and be physicians as well as nurses. And midwifery, decency seems to allot to them . . . they might, also, study politics . . . Business of various kinds, they might likewise pursue."
For Wollstonecraft, "the most perfect education" is "an exercise of the understanding as is best calculated to strengthen the body and form the heart. Or, in other words, to enable the individual to attach such habits of virtue as will render it independent." In addition to her broad philosophical arguments, Wollstonecraft lays out a specific plan for national education to counter Talleyrand's. In Chapter 12, "On National Education", she proposes that children be sent to day schools as well as given some education at home "to inspire a love of home and domestic pleasures", and that such schools be free for children "five to nine years of age". She also maintains that schooling should be co-educational, contending that men and women, whose marriages are "the cement of society", should be "educated after the same model".
Feminism.
It is debatable to what extent the "Rights of Woman" is a feminist text; because the definitions of "feminist" vary, different scholars have come to different conclusions. Wollstonecraft would never have referred to her text as feminist because the words "feminist" and "feminism" were not coined until the 1890s. Moreover, there was no feminist movement to speak of during Wollstonecraft's lifetime. In the introduction to her seminal work on Wollstonecraft's thought, Barbara Taylor writes:
Describing philosophy as feminist is problematic, and I do it only after much consideration. The label is of course anachronistic . . . Treating Wollstonecraft's thought as an anticipation of nineteenth and twentieth-century feminist argument has meant sacrificing or distorting some of its key elements. Leading examples of this . . . have been the widespread neglect of her religious beliefs, and the misrepresentation of her as a bourgeois liberal, which together have resulted in the displacement of a religiously inspired utopian radicalism by a secular, class-partisan reformism as alien to Wollstonecraft's political project as her dream of a divinely promised age of universal happiness is to our own. Even more important however has been the imposition on Wollstonecraft of a heroic-individualist brand of politics utterly at odds with her own ethically driven case for women's emancipation. Wollstonecraft's leading ambition for women was that they should attain virtue, and it was to this end that she sought their liberation.
In the "Rights of Woman", Wollstonecraft does not make the claim for gender equality using the same arguments or the same language that late 19th- and 20th century feminists later would. For instance, rather than unequivocally stating that men and women are equal, Wollstonecraft contends that men and women are equal in the eyes of God, which means that they are both subject to the same moral law. For Wollstonecraft, men and women are equal in the most important areas of life. While such an idea may not seem revolutionary to 21st-century readers, its implications were revolutionary during the 18th century. For example, it implied that both men and women—not just women—should be modest and respect the sanctity of marriage. Wollstonecraft's argument exposed the sexual double standard of the late 18th century and demanded that men adhere to the same virtues demanded of women.
However, Wollstonecraft's arguments for equality stand in contrast to her statements respecting the superiority of masculine strength and valour. Wollstonecraft famously and ambiguously states:
Let it not be concluded, that I wish to invert the order of things; I have already granted, that, from the constitution of their bodies, men seem to be designed by Providence to attain a greater degree of virtue. I speak collectively of the whole sex; but I see not the shadow of a reason to conclude that their virtues should differ in respect to their nature. In fact, how can they, if virtue has only one eternal standard? I must therefore, if I reason consequentially, as strenuously maintain that they have the same simple direction, as that there is a God.
Moreover, Wollstonecraft calls on men, rather than women, to initiate the social and political changes she outlines in the "Rights of Woman". Because women are uneducated, they cannot alter their own situation—men must come to their aid. Wollstonecraft writes at the end of her chapter "Of the Pernicious Effects Which Arise from the Unnatural Distinctions Established in Society":
I then would fain convince reasonable men of the importance of some of my remarks; and prevail on them to weigh dispassionately the whole tenor of my observations. – I appeal to their understandings; and, as a fellow-creature, claim, in the name of my sex, some interest in their hearts. I entreat them to assist to emancipate their companion, to make her a help meet for them! Would men but generously snap our chains, and be content with rational fellowship instead of slavish obedience, they would find us more observant daughters, more affectionate sisters, more faithful wives, more reasonable mothers – in a word, better citizens.
It is Wollstonecraft's last novel, "" (1798), the fictionalised sequel to the "Rights of Woman", that is usually considered her most radical feminist work.
Sensibility.
One of Wollstonecraft's most scathing criticisms in the "Rights of Woman" is against false and excessive sensibility, particularly in women. She argues that women who succumb to sensibility are "blown about by every momentary gust of feeling"; because these women are "the prey of their senses", they cannot think rationally. In fact, not only do they do harm to themselves but they also do harm to all of civilisation: these are not women who can refine civilisation – these are women who will destroy it. But reason and feeling are not independent for Wollstonecraft; rather, she believes that they should inform each other. For Wollstonecraft, as for the important 18th-century philosopher David Hume, the passions underpin all reason. This was a theme that she would return to throughout her career, but particularly in her novels ' (1788) and '.
As part of her argument that women should not be overly influenced by their feelings, Wollstonecraft emphasises that they should not be constrained by or made slaves to their bodies or their sexual feelings. This particular argument has led many modern feminists to suggest that Wollstonecraft intentionally avoids granting women any sexual desire. Cora Kaplan argues that the "negative and prescriptive assault on female sexuality" is a ""leitmotif"" of the "Rights of Woman". For example, Wollstonecraft advises her readers to "calmly let passion subside into friendship" in the ideal companionate marriage (that is, in the ideal of a love-based marriage that was developing at the time). It would be better, she writes, when "two virtuous young people marry . . . if some circumstances checked their passion". According to Wollstonecraft, "love and friendship cannot subsist in the same bosom". As Mary Poovey explains, "Wollstonecraft betrays her fear that female desire might in fact court man's lascivious and degrading attentions, that the subordinate position women have been given might even be deserved. Until women can transcend their fleshly desires and fleshly forms, they will be hostage to the body." If women are not interested in sexuality, they cannot be dominated by men. Wollstonecraft worries that women are consumed with "romantic wavering", that is, they are interested only in satisfying their lusts. Because the "Rights of Woman" eliminates sexuality from a woman's life, Kaplan contends, it "expresses a violent antagonism to the sexual" while at the same time "exaggeratthe importance of the sensual in the everyday life of women". Wollstonecraft was so determined to wipe sexuality from her picture of the ideal woman that she ended up foregrounding it by insisting upon its absence. But as Kaplan and others have remarked, Wollstonecraft may have been forced to make this sacrifice: "it is important to remember that the notion of woman as politically enabled and independent [was fatally linked the eighteenth century to the unrestrained and vicious exercise of her sexuality."
Republicanism.
Claudia Johnson, a prominent Wollstonecraft scholar, has called the "Rights of Woman" "a republican manifesto". Johnson contends that Wollstonecraft is hearkening back to the Commonwealth tradition of the 17th century and attempting to reestablish a republican ethos. In Wollstonecraft's version, there would be strong, but separate, masculine and feminine roles for citizens. According to Johnson, Wollstonecraft "denounces the collapse of proper sexual distinction as the leading feature of her age, and as the grievous consequence of sentimentality itself. The problem undermining society in her view is feminized men". If men feel free to adopt both the masculine position and the sentimental feminine position, she argues, women have no position open to them in society. Johnson therefore sees Wollstonecraft as a critic, in both the "Rights of Men" and the "Rights of Woman", of the "masculinization of sensitivity" in such works as Edmund Burke's "Reflections on the Revolution in France".
In the "Rights of Woman" Wollstonecraft adheres to a version of republicanism that includes a belief in the eventual overthrow of all titles, including the monarchy. She also briefly suggests that all men and women should be represented in government. But the bulk of her "political criticism," as Chris Jones, a Wollstonecraft scholar, explains, "is couched predominantly in terms of morality". Her definition of virtue focuses on the individual's happiness rather than, for example, the good of the entire society. This is reflected in her explanation of natural rights. Because rights ultimately proceed from God, Wollstonecraft maintains that there are duties, tied to those rights, incumbent upon each and every person. For Wollstonecraft, the individual is taught republicanism and benevolence within the family; domestic relations and familial ties are crucial to her understanding of social cohesion and patriotism.
Class.
In many ways the "Rights of Woman" is inflected by a bourgeois view of the world, as is its direct predecessor the "Rights of Men". Wollstonecraft addresses her text to the middle class, which she calls the "most natural state". She also frequently praises modesty and industry, virtues which, at the time, were associated with the middle class. From her position as a middle-class writer arguing for a middle-class ethos, Wollstonecraft also attacks the wealthy, criticising them using the same arguments she employs against women. She points out the "false-refinement, immorality, and vanity" of the rich, calling them "weak, artificial beings, raised above the common wants and affections of their race, in a premature unnatural manner undermine the very foundation of virtue, and spread corruption through the whole mass of society".
But Wollstonecraft's criticisms of the wealthy do not necessarily reflect a concomitant sympathy for the poor. For her, the poor are fortunate because they will never be trapped by the snares of wealth: "Happy is it when people have the cares of life to struggle with; for these struggles prevent their becoming a prey to enervating vices, merely from idleness!" Moreover, she contends that charity has only negative consequences because, as Jones puts it, she "sees it as sustaining an unequal society while giving the appearance of virtue to the rich".
In her national plan for education, she retains class distinctions (with an exception for the intelligent), suggesting that: "After the age of nine, girls and boys, intended for domestic employments, or mechanical trades, ought to be removed to other schools, and receive instruction, in some measure appropriated to the destination of each individual . . . The young people of superior abilities, or fortune, might now be taught, in another school, the dead and living languages, the elements of science, and continue the study of history and politics, on a more extensive scale, which would not exclude polite literature."
Rhetoric and style.
In attempting to navigate the cultural expectations of female writers and the generic conventions of political and philosophical discourse, Wollstonecraft, as she does throughout her "oeuvre", constructs a unique blend of masculine and feminine styles in the "Rights of Woman". She utilises the language of philosophy, referring to her work as a "treatise" with "arguments" and "principles". However, Wollstonecraft also uses a personal tone, employing "I" and "you", dashes and exclamation marks, and autobiographical references to create a distinctly feminine voice in the text. The "Rights of Woman" further hybridizes its genre by weaving together elements of the conduct book, the short essay, and the novel, genres often associated with women, while at the same time claiming that these genres could be used to discuss philosophical topics such as rights.
Although Wollstonecraft argues against excessive sensibility, the rhetoric of the "Rights of Woman" is at times heated and attempts to provoke the reader. Many of the most emotional comments in the book are directed at Rousseau. For example, after excerpting a long passage from "" (1762), Wollstonecraft pithily states, "I shall make no other comments on this ingenious passage, than just to observe, that it is the philosophy of lasciviousness." A mere page later, after indicting Rousseau's plan for female education, she writes "I must relieve myself by drawing another picture." These terse exclamations are meant to draw the reader to her side of the argument (it is assumed that the reader will agree with them). While she claims to write in a plain style so that her ideas will reach the broadest possible audience, she actually combines the plain, rational language of the political treatise with the poetic, passionate language of sensibility to demonstrate that one can combine rationality and sensibility in the same self. Wollstonecraft defends her positions not only with reasoned argument but also with ardent rhetoric.
In her efforts to vividly describe the condition of women within society, Wollstonecraft employs several different analogies. She often compares women to slaves, arguing that their ignorance and powerlessness places them in that position. But at the same time, she also compares them to "capricious tyrants" who use cunning and deceit to manipulate the men around them. At one point, she reasons that a woman can become either a slave or tyrant, which she describes as two sides of the same coin. Wollstonecraft also compares women to soldiers; like military men, they are valued only for their appearance. And like the rich, women's "softness" has "debased mankind".
Revision.
Wollstonecraft was forced to write the "Rights of Woman" hurriedly to respond to Talleyrand and ongoing events. Upon completing the work, she wrote to her friend William Roscoe: "I am dissatisfied with myself for not having done justice to the subject. – Do not suspect me of false modesty – I mean to say that had I allowed myself more time I could have written a better book, in every sense of the word . . . I intend to finish the next volume before I begin to print, for it is not pleasant to have the Devil coming for the conclusion of a sheet fore it is written." When Wollstonecraft revised the "Rights of Woman" for the second edition, she took the opportunity not only to fix small spelling and grammar mistakes but also to bolster the feminist claims of her argument. She changed some of her statements regarding female and male difference to reflect a greater equality between the sexes.
Wollstonecraft never wrote the second part to the "Rights of Woman," although William Godwin published her "Hints", which were "chiefly designed to have been incorporated in the second part of the "Vindication of the Rights of Woman"", in the posthumous collection of her works. However, she did begin writing the novel "", which most scholars consider a fictionalised sequel to the "Rights of Woman". It was unfinished at her death and also included in the "Posthumous Works" published by Godwin.
Reception and legacy.
When it was first published in 1792, the "Rights of Woman" was reviewed favourably by the "Analytical Review", the "General Magazine", the "Literary Magazine", "New York Magazine", and the "Monthly Review", although the assumption persists even today that "Rights of Woman" received hostile reviews. It was almost immediately released in a second edition in 1792, several American editions appeared, and it was translated into French. Taylor writes that "it was an immediate success". Moreover, other writers such as Mary Hays and Mary Robinson specifically alluded to Wollstonecraft's text in their own works. Hays cited the "Rights of Woman" in her novel "Memoirs of Emma Courtney" (1796) and modelled her female characters after Wollstonecraft's ideal woman. Although female conservatives such as Hannah More excoriated Wollstonecraft personally, they actually shared many of the same values. As the scholar Anne Mellor has shown, both More and Wollstonecraft wanted a society founded on "Christian virtues of rational benevolence, honesty, personal virtue, the fulfillment of social duty, thrift, sobriety, and hard work". During the early 1790s, many writers within British society were engaged in an intense debate regarding the position of women in society. For example, the respected poet and essayist Anna Laetitia Barbauld and Wollstonecraft sparred back and forth; Barbauld published several poems responding to Wollstonecraft's work and Wollstonecraft commented on them in footnotes to the "Rights of Woman". The work also provoked outright hostility. The bluestocking Elizabeth Carter was unimpressed with the work. Thomas Taylor, the Neoplatonist translator who had been a landlord to the Wollstonecraft family in the late 1770s, swiftly wrote a satire called "A Vindication of the Rights of Brutes": if women have rights, why not animals too?
After Wollstonecraft died in 1797, her husband William Godwin published his "Memoirs of the Author of A Vindication of the Rights of Woman" (1798). He revealed much about her private life that had previously not been known to the public: her illegitimate child, her love affairs, and her attempts at suicide. While Godwin believed he was portraying his wife with love, sincerity, and compassion, contemporary readers were shocked by Wollstonecraft's unorthodox lifestyle and she became a reviled figure. Richard Polwhele targeted her in particular in his anonymous long poem "The Unsex'd Females" (1798), a defensive reaction to women's literary self-assertion: Hannah More is Christ to Wollstonecraft's Satan. His poem was "well known" among the responses "A Vindication". One reviewer comments this "ingenious poem" with its "playful sallies of sarcastic wit" against "our modern ladies," though others found it "a tedious, lifeless piece of writing." Critical responses largely fell along clear-cut political lines.
Wollstonecraft's ideas became associated with her life story and women writers felt that it was dangerous to mention her in their texts. Hays, who had previously been a close friend and an outspoken advocate for Wollstonecraft and her "Rights of Woman", for example, did not include her in the collection of "Illustrious and Celebrated Women" she published in 1803. Maria Edgeworth specifically distances herself from Wollstonecraft in her novel "Belinda" (1802); she caricatures Wollstonecraft as a radical feminist in the character of Harriet Freke. But, like Jane Austen, she does not reject Wollstonecraft's ideas. Both Edgeworth and Austen argue that women are crucial to the development of the nation; moreover, they portray women as rational beings who should choose companionate marriage.
The negative views towards Wollstonecraft persisted for over a century. The "Rights of Woman" was not reprinted until the middle of the 19th century and it still retained an aura of ill-repute. George Eliot wrote "there is in some quarters a vague prejudice against the "Rights of Woman" as in some way or other a reprehensible book, but readers who go to it with this impression will be surprised to find it eminently serious, severely moral, and withal rather heavy".
The suffragist (i.e. moderate reformer, as opposed to suffragette) Millicent Garrett Fawcett wrote the introduction to the centenary edition of the "Rights of Woman", cleansing the memory of Wollstonecraft and claiming her as the foremother of the struggle for the vote. While the "Rights of Woman" may have paved the way for feminist arguments, 20th century feminists have tended to use Wollstonecraft's life story, rather than her texts, for inspiration; her unorthodox lifestyle convinced them to try new "experiments in living", as Virginia Woolf termed it in her famous essay on Wollstonecraft. However, there is some evidence that the "Rights of Woman" may be influencing current feminists. Ayaan Hirsi Ali, a feminist who is critical of Islam's dictates regarding women, cites the "Rights of Woman" in her autobiography "Infidel", writing that she was "inspired by Mary Wollstonecraft, the pioneering feminist thinker who told women they had the same ability to reason as men did and deserved the same rights".

</doc>
<doc id="2835" url="http://en.wikipedia.org/wiki?curid=2835" title="Afghan Hound">
Afghan Hound

The Afghan Hound is a hound that is one of the oldest dog breeds in existence. Distinguished by its thick, fine, silky coat and its tail with a ring curl at the end, the breed acquired its unique features in the cold mountains of Afghanistan. Its local name is Tāžī Spay () or Sag-e Tāzī (Dari Persian: سگ تازی). Other alternate names for this breed are "Kuchi Hound", "Tāzī", "Balkh Hound", "Baluchi Hound", "Barutzy Hound", "Shalgar Hound", "Kabul Hound", "Galanday Hound", or sometimes incorrectly "African Hound".
History.
Sighthounds are among the oldest recognisable types of dogs, and genetic testing has placed the Afghan Hound breed among those with the least genetic divergence from the wolf on some markers; this is taken to mean that such dogs are descended from the oldest dog types, not that the breeds tested had in antiquity their exact modern form. 
Today's modern purebred breed of Afghan Hound descends from dogs brought in the 1920s to Great Britain, and are a blending of types and varieties of long haired sighthounds from across Afghanistan and the surrounding areas. Some had been kept as hunting dogs, others as guardians.
Although demonstrably ancient, verifiable written or visual records that tie today's Afghan Hound breed to specific Afghan owners or places is absent, even though there is much speculation about possible connections with the ancient world among fanciers and in non-scientific breed books and breed websites. Connections with other types and breeds from the same area may provide clues to the history. A name for a desert coursing Afghan hound, Tazi (sag-e-tazi), suggests a shared ancestry with the very similar Tasy breed from the Caspian Sea area of Russia and Turkmenistan.Other types or breeds of similar appearance are the Taigan from the mountainous Tian Shan region on the Chinese border of Afghanistan, and the Barakzay, or Kurram Valley Hound. 
There are at least 13 types known in Afghanistan, and some are being developed (through breeding and recordkeeping) into modern purebred breeds. As the lives of the peoples with whom these dogs developed change in the modern world, often these landrace types of dogs lose their use and disappear; there may have been many more types of longhaired sighthound in the past.
Once out of Afghanistan, the history of the Afghan Hound breed becomes an important part of the history of the very earliest dog shows and The Kennel Club (UK). Various sighthounds were brought to England in the 1800s by army officers returning from British India (which at the time included), Afghanistan, and Persia, and were exhibited at dog shows, which were then just becoming popular, under various names, such as Barukzy hounds. They were also called "Persian Greyhounds" by the English, in reference to their own indigenous sighthound.
One dog in particular, "Zardin", was brought in 1907 from India by Captain Bariff, and became the early ideal of breed type for what was still called the Persian Greyhound. Zardin was the basis of the writing of the first breed standard in 1912, but breeding of the dogs was stopped by World War I.
Out of the longhaired sighthound types known in Afghanistan, two main strains make up the modern Afghan Hound breed. The first were a group of hounds brought to Scotland from Baluchistan by Major and Mrs. G. Bell-Murray and Miss Jean C. Manson in 1920, and are called the "Bell-Murray strain". 
These dogs were of the lowland or steppe type, also called kalagh, and are less heavily coated. The second strain was a group of dogs from a kennel in Kabul owned by Mrs. Mary Amps, which she shipped to England in 1925. She and her husband came to Kabul after the Afghan war in 1919, and the foundation sire of her kennel (named Ghazni) in Kabul was a dog that closely resembled Zardin. Her "Ghazni strain" were the more heavily coated mountain type. Most of the Afghans in the United States were developed from the Ghazni strain from England. The first Afghans in Australia were imported from the United States in 1934, also of the Ghazni strain. The French breed club was formed in 1939 (FALAPA). The mountain and steppe strains became mixed into the modern Afghan Hound breed, and a new standard was written in 1948, which is still used today.
The spectacular beauty of Afghan Hound dogs caused them to become highly desirable showdogs and pets, and they are recognised by all of the major kennel clubs in the English-speaking world. One of the Amps Ghazni, "Sirdar", won BIS at Crufts in 1928 and 1930. An Afghan hound was featured on the cover of Life Magazine, November 26, 1945. "Afghan Hounds were the most popular in Australia in the 1970s…and won most of the major shows". An Afghan Hound won BIS (Best in Show) at the 1996 World Dog Show in Budapest. Afghan hounds were BIS at the Westminster Kennel Club Dog Show in 1957 and again in 1983. That win also marked the most recent win at Westminster for breeder-owner-handler, Chris Terrell.
The Afghan Hound breed is no longer used for hunting, although it can be seen in the sport of lure coursing.
Descriptions.
The Afghan Hound is tall, standing in height 24–29 inches and weighing 45–60 pounds. The coat may be any colour, but white markings, particularly on the head, are discouraged; many individuals have a black facial mask. A specimen may have facial hair that looks like a Fu Manchu moustache. The moustache is called "mandarins." Some Afghan Hounds are almost white, but parti-colour hounds (white with islands of red or black) are not acceptable and may indicate impure breeding. The long, fine-textured coat requires considerable care and grooming. The long topknot and the shorter-haired saddle on the back of the dog are distinctive features of the Afghan Hound coat. The high hipbones and unique small ring on the end of the tail are also characteristics of the breed.
The temperament of the typical Afghan Hound can be aloof and dignified, but happy and clownish when it's playing. This breed, as is the case with many sighthounds, has a high prey drive and may not get along with small animals. The Afghan Hounds' reasoning skills have made it a successful competitor in dog agility trials as well as an intuitive therapy dog and companion. Genomic studies have pointed to the Afghan Hound as one of the oldest of dog breeds.
The breed has a reputation among some dog trainers of having a relatively slow "obedience intelligence" as defined by author Stanley Coren in "The Intelligence of Dogs".
Although seldom used today for hunting in Europe and America where they are popular, Afghan hounds are frequent participants in lure coursing events and are also popular in the sport of conformation showing.
Variants.
The Khalag Tazi is a variety of the Afghan. It was introduced to Europe in 1920 when an Indian Army officer, Major G Bell-Murray, brought some animals back from Afghanistan. "Tazi" is a current and ancient name for hunting dogs of the sighthound type in the Middle East. It has been used to denote the Saluki, Afghan, Taigan, Persian Greyhound, greyhound types of hound.
Health.
Lifespan.
Afghan Hounds in UK surveys had a median lifespan of about 12 years. which is similar to other breeds of their size. In the 2004 UK Kennel Club survey, the most common causes of death were cancer (31%), old age (20%), cardiac (10.5%), and urologic (5%). Those that die of old age had an average lifespan of 13 to 14 1/2 years.
Health concerns.
Major health issues are allergies, cancer, and hip dysplasia. Sensitivity to anesthesia is an issue the Afghan hound shares with the rest of the sighthound group, as sighthounds have relatively low levels of body fat. Afghan hounds are also among the dog breeds most likely to develop chylothorax, a rare condition which causes the thoracic ducts to leak, allowing large quantities of chyle fluid to enter the dog's chest cavity. This condition commonly results in a lung torsion (in which the dog's lung twists within the chest cavity, requiring emergency surgery), due to the breed's typically deep, "barrel"-shaped chest. If not corrected through surgery, chylothorax can ultimately cause fibrosing pleuritis, or a hardening of the organs, due to scar tissue forming around the organs to protect them from the chyle fluid. Chylothorax is not necessarily, but often, fatal.
In popular culture.
Because of its distinctive appearance, the Afghan hound has been represented in animated feature films and TV shows, including Universal Pictures' "Balto" (Sylvie), Disney's "Lady and the Tramp II" (Ruby), and Brainy Barker from "Krypto the Superdog", an Afghan hound also appeared on "101 Dalmatians" as well as in "102 Dalmatians" as one of the dogs in Cruella De Vil's party and the television series What-a-Mess (Prince Amir of Kinjan; based on children's books by Frank Muir) and, as Prissy in the 1961 Disney animated film "One Hundred and One Dalmatians" and "". Afghan hounds have also been featured in television advertisements and in fashion magazines. The Afghan hound is represented in books as well, including being featured in a series of mystery novels by Nina Wright (Abra), and a talking Afghan Hound in David Rothman's "The Solomon Scandals" (2008, Twilight Times Books). In the novel "Between the Acts", Virginia Woolf uses an Afghan hound (named Sohrab) to represent aspects of one of the book's human characters.
On August 3, 2005, Korean scientist Hwang Woo-Suk announced that his team of researchers had become the first team to successfully clone a dog, an Afghan Hound named Snuppy. In 2006 Hwang Woo-Suk was dismissed from his university position for fabricating data in his research. Snuppy, nonetheless, was a genuine clone, and thus the first cloned dog in history.
In the BBC Three Sitcom "Mongrels" the character of Destiny is an Afghan Hound.
In the 2010 comedy "Marmaduke", two Afghan hounds appear in the dog park and are shown to resemble high school girls watching the more athletic dogs as if they were "jocks" and are also shown to be "air heads".
The Afghan Hound features prominently in the avant-garde music video of popular French band M83's, "Set in Stone (M83 Remix)." 
In the FX animated series "Archer", the titular protagonist's mother mourns the death of her Afghan Hound "Duchess" as a recurring gag throughout the show.

</doc>
<doc id="2836" url="http://en.wikipedia.org/wiki?curid=2836" title="Azawakh">
Azawakh

The Azawakh is a sighthound dog breed from Africa.
Description.
Appearance.
Morphology is very similar to that of the Middle Eastern and South Indian sight hounds, all swift, high-bred coursing hounds, although there are several obvious differences. For example, a short, flat back combined with long legs place the hips higher than the withers. The Azawakh is almond eyed and thin. It moves with a distinctly feline gait and can be found in a variety of colors as well as varying degrees of refinement, though format is basically constant.
Height and weight.
The standards call for a hound from ; its height is . The coat is very short and almost absent on the belly. Its bone structure shows clearly through the skin and musculature. Its muscles are "dry", meaning that they lie quite flat, unlike the Greyhound and Whippet. In this respect it is similar in type to the Saluki.
Colors.
In Africa, Azawakh are found in a variety of colors such as red, blue fawn (that is, with a lilac cast), grizzle, and, rarely, blue and black. The Azawakh in its native land also comes with various white markings including Irish marked (white collar) and particolour (mostly white). Because of this wide color variation in the native population, the American standard used by the AKC and UKC allows any color combination found in Africa. In the United States, the FCI standard is modified to have no color restrictions at a minimum and there is a strong sentiment that the FCI standard should be heavily edited or replaced.
Colors permitted by the FCI breed standard are clear sand to dark fawn/brown, red and brindle (with or without a dark mask), with white bib, tail tip, and white on all feet (which can be tips of toes to high stockings). Currently, white stockings that go above the elbow joint are considered disqualifying features in France, as is a white collar or half collar (Irish marked).
Movement.
The Azawakh's light, supple, lissome gait is a notable breed characteristic, as is an upright double suspension gallop.
Health.
Azawakhs are an incredibly sound coursing hound. Serious coursing injuries are rare. The dogs heal very quickly from injury.
Azawakh have no known incidence of hip dysplasia. There is a small occurrence of adult-onset idiopathic epilepsy in the breed. Wobbler disease, or cervical vertebral instability, does rarely occur. Some breeders believe this is largely a developmental problem where puppies grow too quickly due to a high-protein Western diet.
Reproduction.
Like the Basenji and Tibetan Mastiff, the Azawakh often has a single annual estrus. Unassisted birth of healthy puppies is normal. Litter sizes are usually from four to six puppies, but litters as small as one and as large as ten occur.
Care.
Azawakh need a fairly high level of exercise and should have regular runs off lead in large enclosed areas to run off steam. The dogs are very social and emotional. They need a master that provides firm but fair leadership. Azawakh thrive on companionship of other Azawakh.
Temperament.
Unlike other sighthounds, the primary function of the Azawakh in its native land is that of protector. It develops an intense bond with his owner, yet can perform independently from its master. With those they accept, Azawakh are gentle and extremely affectionate. With strangers many are reserved and prefer not to be touched, but are not inherently aggressive. Although raised to protect livestock, they do not have innate aggression toward canines or humans unless they are threatened. 
Azawakh have high energy and tremendous endurance. They are excellent training companions for runners and are nearly impervious to heat. They will happily run in weather over 100 degrees Fahrenheit that would kill a Greyhound. They often dig holes in the garden.
Many Azawakh dislike rain and cold weather.
Azawakh are pack oriented and form complex social hierarchies. They have tremendous memories and are able to recognize each other after long periods of separation. They can often be found sleeping on top of each other for warmth and companionship.
History.
Bred by the Tuareg, Fula and various other nomads of the Sahara and sub-Saharan Sahel in the countries of Mali, Niger, Burkina Faso, and southern Algeria, the breed is used there as a guard dog and to hunt gazelle and hare at speeds up to 40 miles per hour. The austerity of the Sahel environment has ensured that only the most fit dogs survive and has accentuated the breed's ruggedness and independence. Unlike some other sighthounds, the Azawakh is more of a pack hunter and they bump down the quarry with hindquarters when it has been tired out. In role of a guard dog, if an Azawakh senses danger it will bark to alert the other members of the pack, and they will gather together as a pack under the lead of the alpha dog, then chase off or attack the predator. The Sloughi, by comparison, is more of an independent lone hunter and has a high hunting instinct.
They are relatively uncommon in Europe and North America but there is a growing band of devotees. Azawakhs have a range of temperaments from lap dog to quite fierce. Lifelong socialization and firm but gentle handling are critical. Well socialised and trained, they can be good with other dogs, cats, children, and strangers. Azawakh may be registered with the FCI in the USA via the Federación Canófila de Puerto Rico (FCPR). European FCI clubs and the AKC recognize the FCPR as an acceptable registry. The AKC currently recognizes Azawakh as a Foundation Stock Service breed and they are eligible to participate in AKC-sanctioned Companion & Performance events. The breed will enter the AKC Miscellaneous Class on June 30, 2011. The American Azawakh Association (AAA). is the AKC Parent Club for the Azawakh. Azawakh may be registered with the UKC and ARBA. The breed is not yet registered by CKC. Azawakh are eligible for ASFA and AKC lure coursing and NOFCA open field coursing events.
Origin.
Recent genetic, blood protein and archaeological studies, as well as direct observation in the field, offer a glimpse into the origin of the contemporary Azawakh breed. It originated from the pariah dogs of sub-Saharan Africa—also called "bush dogs" or "basenji"—and is also closely related to the Sloughi of the Maghreb. Despite morphological similarities, mitochondrial DNA evidence shows that it is only very distantly related to other sight hounds. Azawakh have a rare glucose isomerase allele (GPIB) that occurs only in foxes, jackals, Italian wolves, Sloughi dogs and a handful of other quite unrelated rare dogs found mostly in Japan. The presence of the GPIB suggests an ancient differentiation of the Azawakh from other dog populations near the base of the dog family tree divergence from wolves or perhaps a uniquely African cross-breeding with local African canids such as jackals. Petroglyph rock art dating from 8,000 to 10,000 years ago during the Green Sahara (also known as the Holocene and Neolithic Subpluvial) shows cursorial dogs in conjunction with hunters. Archaeologists have found dog bones buried in Holocene settlements in the Sahara. At the close of the Holocene Wet Phase in the 4th millennium BCE, the Sahara returned to desert and created a formidable physical barrier to travel. Together, this evidence suggests that the Azawakh population has a unique genetic heritage that has been largely isolated from other dog populations for millennia.
In the common era the Sahel dogs are almost totally isolated from northern dogs by the Sahara, but the ties to the pariah dogs to the south are extremely close. Azawakh are virtually indistinguishable from the Sahel pariah dog population from which they are drawn. In addition to a basic physical structure, the Azawakh share a number of unique traits with the pariah dogs:
Throughout the Sahel, very elegant puppies can be found among rustic siblings. The Sahel nomads do not have the same breed concepts as in the West and, unlike the Bedouin of the North, do not recognize a strict separation of "al hor" (noble) from "kelb" (mongrel) dogs. The nomads act as an extra level of selection on top of the intense natural selection pressure of the Sahel environment. The approach to selection is diametrically opposed to Western breeding. Instead of selecting which dogs to breed upon maturity, they decide which puppies should live. This approach has the advantage of maintaining a large reservoir of genetic variability and resilience.
The peoples of the Sahel control dam lines and cull puppies heavily at birth according to locally held aesthetic criteria that are not yet fully understand. In the Sahel, color is not a selection criterion. The alpha male dog from the local population is usually the sire. Unless it is a wet year, only one puppy from a litter might be selected to live. Females are usually culled unless the family projects a need for more dogs in the future.

</doc>
<doc id="2838" url="http://en.wikipedia.org/wiki?curid=2838" title="Acrylic paint">
Acrylic paint

Acrylic paint is a fast-drying paint containing pigment suspension in acrylic polymer emulsion. Acrylic paints are water soluble, but become water-resistant when dry. Depending on how much the paint is diluted with water or modified with acrylic gels, media, or pastes, the finished acrylic painting can resemble a watercolor or an oil painting, or have its own unique characteristics not attainable with other media.
History.
As early as 1934 the first usable acrylic resin dispersion was developed by German chemical company BASF, which was patented by Rohm and Haas. The synthetic paint was first used in 1940s, combining some of the properties of oil and watercolor. Between 1946 and 1949, Leonard Bocour and Sam Golden invented a solution acrylic paint under the brand Magna paint. These were mineral spirit-based paints. Acrylics were made commercially available in the 1950s. A waterborne acrylic paint called "Aquatec" would soon follow. Otto Rohm invented acrylic resin, which quickly transformed into acrylic paint. In 1953, the year that Rohm and Haas developed the first acrylic emulsions, Jose L. Gutierrez produced Politec Acrylic Artists' Colors in Mexico, and Permanent Pigments Co. of Cincinnati, Ohio, produced Liquitex colors. These two product lines were the very first acrylic emulsion artists' paints. Water-based acrylic paints were subsequently sold as latex house paints, as latex is the technical term for a suspension of polymer microparticles in water. Interior latex house paints tend to be a combination of binder (sometimes acrylic, vinyl, pva, and others), filler, pigment, and water. Exterior latex house paints may also be a co-polymer blend, but the best exterior water-based paints are 100% acrylic, due to elasticity and other factors, but vinyl costs half of what 100 percent acrylic resins cost, and PVA (polyvinyl acetate) is even cheaper, so paint companies make many combinations of them to match the market.
Soon after the water-based acrylic binders were introduced as house paints, artists and companies alike began to explore the potential of the new binders. Water-soluble artists' acrylic paints became commercially available in the 1950s, offered by Liquitex, with high-viscosity paints similar to those made today becoming available in the early 1960s. In 1963, Rowney (now part of Daler-Rowney since 1983) was the first manufacturer to introduce an artist’s acrylic color in Europe, under the brand name Cryla.
Techniques.
Acrylic artist paints may be thinned with water and used as washes in the manner of watercolor paints, but the washes are not re-hydratable once dry. For this reason, acrylics do not lend themselves to color lifting techniques as do gum arabic based watercolor paints.
Acrylic paints with gloss or matte finishes are common, although a satin (semi-matte) sheen is most common; some brands exhibit a range of finish (e.g., heavy-body paints from Golden, Liquitex, Winsor & Newton and Daler-Rowney). Politec acrylics are fully matte. As with oils, pigment amounts and particle size or shape can naturally affect the paint sheen. Matting agents can also be added during manufacture to dull the finish. The artist can mix media with their paints and use topcoats or varnishes to alter or unify sheen if desired.
When dry, acrylic paint is generally non-removable from a solid surface. Water or mild solvents do not re-solubilize it, although isopropyl alcohol can lift some fresh paint films off. Toluene and acetone can remove paint films, but they do not lift paint stains very well and are not selective. The use of a solvent to remove paint may result in removal of all of the paint layers, acrylic gesso, etc. Oils and warm, soapy water can remove acrylic paint from skin.
Only a proper, artist-grade acrylic gesso should be used to prime canvas in preparation for painting with acrylic (however, acrylic paint can be applied to raw canvas if so desired without any negative effect or chemical reaction as would be the case with oils). It is important to avoid adding non-stable or non-archival elements to the gesso upon application. However, the viscosity of acrylic can successfully be reduced by using suitable extenders that maintain the integrity of the paint film. There are retarders to slow drying and extend workability time and flow releases to increase color-blending ability.
Painters and acrylic.
Prior to the 19th century, artists mixed their own paints, which allowed them to achieve the desired color and thickness and to control the use of fillers, if any. While suitable media and raw pigments are available for the individual production of acrylic paint, hand mixing may not be practical due to the fast drying time and other technical issues.
Acrylic painters can modify the appearance, hardness, flexibility, texture, and other characteristics of the paint surface by using acrylic media or simply by adding water. Watercolor and oil painters also use various media, but the range of acrylic media is much greater. Acrylics have the ability to bond to many different surfaces, and media can be used to adjust their binding characteristics. Acrylics can be used on paper, canvas and a range of other materials. However, their use on engineered woods such as medium-density fiberboard can be problematic because of the porous nature of those surfaces. In these cases it is recommended that the surface first be sealed with an appropriate sealer. Acrylics can be applied in thin layers or washes to create effects that resemble watercolors and other water-based media. They can also be used to build thick layers of paint—gel and molding paste media are sometimes used to create paintings with relief features that are literally sculptural. Acrylic paints are also used in hobbies such as train, car, house, and human models. People who make such models use acrylic paint to build facial features on dolls or raised details on other types of models. Acrylic paint is easily removed from paint brushes and skin with water, unlike oil paints that require the use of a hydrocarbon.
Acrylic paints are the most common paints used in grattage. Grattage is a surrealist technique that became popular with the release of acrylic paint. Acrylics are used for this purpose because they easily scrape or peel from a surface.
Grades.
Commercial acrylic paints come in three grades:
Differences between acrylic and oil paint.
The vehicle and binder of oil paints is linseed oil or another drying oil, whereas water serves as the vehicle for an emulsion (suspension) of acrylic polymer that is the binder in acrylic paint. Thus, oil paint is said to be "oil-based", whereas acrylic paint is "water-based" (or sometimes "water-borne").
The main practical difference between most acrylics and oil paints is the inherent drying time. Oils allow for more time to blend colors and apply even glazes over underpaintings. This slow drying aspect of oil can be seen as an advantage for certain techniques, but in other regards it impedes the artist trying to work quickly. The fast evaporation of water from regular acrylic paint films can be slowed with the use of acrylic retarders. Retarders are generally glycol or glycerin-based additives. The addition of a retarder slows the evaporation rate of the water.
Oil paints may require the use of solvents such as mineral spirits or turpentine to thin the paint and clean up; these generally have some level of toxicity and are often found objectionable. Relatively recently, water-miscible oil paints have been developed for artists' use. Oil paint films can become increasingly yellow and brittle with time and lose much of their flexibility in a few decades. Additionally, the rules of "fat over lean" must be employed to ensure the paint films are durable.
Oil paint has a higher pigment load than acrylic paint. As linseed oil has a smaller molecule than acrylic, oil paint is able to absorb substantially more pigment. Oil provides a different (less clear) refractive index than acrylic dispersions, imparting a unique "look and feel" to the resultant paint film. Not all pigments in oil are available in acrylic. Acrylic paints, unlike oil, may also be fluorescent.
Due to acrylic's more flexible nature and more consistent drying time between colors, the painter does not have to follow the "fat over lean" rule of oil painting, where more medium must be applied to each layer to avoid cracking. It usually takes between fifteen to twenty minutes for one to two layer of acrylic paint to dry. Although canvas needs to be properly sized and primed before painting with oil (otherwise it will eventually rot the canvas), acrylic can be safely applied to raw canvas. The rapid drying of the paint tends to discourage the blending of color and use of wet-in-wet technique as in oil painting. Even though acrylic retarders can slow drying time to several hours, it remains a relatively fast-drying medium, and the addition of too much acrylic retarder can prevent the paint from ever drying properly.
Meanwhile, acrylic paint is very elastic, which prevents cracking from occurring. Acrylic paint's binder is acrylic polymer emulsion; as this binder dries the paint remains flexible.
Another difference between oil and acrylic paints is the versatility offered by acrylic paints: acrylic is very useful in mixed media, allowing use of pastel (oil & chalk), charcoal, pen, etc. on top of the dried acrylic painted surface. Mixing other bodies into the acrylic is possible—sand, rice, even pasta may be incorporated in the artwork. Mixing artist or student quality acrylic paint with household acrylic emulsions is possible, allowing the use of pre-mixed tints straight from the tube or tin, so presenting the painter with a vast color range at his or her disposal. This versatility is also illustrated in the wide variety of additional artistic uses that acrylics afford the artist. Specialist acrylics have been manufactured and used for lino block printing (acrylic block printing ink produced by Derivan since the early 1980s), face painting, airbrushing, watercolor techniques, and fabric screen printing.

</doc>
<doc id="2839" url="http://en.wikipedia.org/wiki?curid=2839" title="Angular momentum">
Angular momentum

In physics, angular momentum, moment of momentum, or rotational momentum is a measure of the amount of rotation an object has, taking into account its mass, shape and speed. It is a vector quantity that represents the product of a body's rotational inertia and rotational velocity about a particular axis. The angular momentum of a system of particles (e.g. a rigid body) is the sum of angular momenta of the individual particles. For a rigid body rotating around an axis of symmetry (e.g. the blades of a ceiling fan), the angular momentum can be expressed as the product of the body's moment of inertia, "I", (i.e., a measure of an object's resistance to changes in its rotation velocity) and its angular velocity, ω.
In this way, angular momentum is sometimes described as the rotational analog of linear momentum.
For the case of an object that is small compared with the radial distance to its axis of rotation, such as a rubber ball swinging from a long string or a planet orbiting in an ellipse around the Sun, the angular momentum can be approximated as the cross product of its linear momentum, , and its position relative to the point about which it is rotating, r. Thus, the angular momentum, L, of a particle with respect to some point of origin is as follows.
Angular momentum is conserved in a system where there is no net external torque, and its conservation helps explain many diverse phenomena. For example, the increase in rotational speed of a spinning figure skater as the skater's arms are contracted is a consequence of conservation of angular momentum. The very high rotational rates of neutron stars can also be explained in terms of angular momentum conservation. Moreover, angular momentum conservation has numerous applications in physics and engineering (e.g., the gyrocompass).
Angular momentum in classical mechanics.
Definition.
The angular momentum, L, of a particle about a given origin is defined as:
where r is the position vector of the particle relative to the origin, p is the linear momentum of the particle, and × denotes the cross product.
As seen from the definition, the derived SI units of angular momentum are newton meter seconds (N·m·s or kg·m2/s) or joule seconds (J·s). Because of the cross product, L is a pseudovector perpendicular to both the radial vector r and the momentum vector p and it is assigned a sign by the right-hand rule.
For an object with a fixed mass that is rotating about a fixed symmetry axis, the angular momentum is expressed as the product of the moment of inertia of the object and its angular velocity vector:
where "I" is the moment of inertia of the object (in general, a tensor quantity), and ω is the angular velocity.
The angular momentum of a particle or rigid body in rectilinear motion (pure translation) is a vector with constant magnitude and direction. If the path of the particle or center of mass of the rigid body passes through the given origin, its angular momentum is zero.
Angular momentum is also known as moment of momentum.
Angular momentum of a collection of particles.
If a system consists of multiple particles, the total angular momentum about a point can be obtained by adding all the angular momenta of the constituent particles:
For a continuous mass distribution with mass density "ρ" = "ρ"(r), a differential volume element "dV", centred on the position vector r within the mass continuum, has a mass element "dm" = "ρ"(r)"dV". Therefore the infinitesimal angular momentum of this element is:
and integrating this differential over the volume of the entire mass continuum gives its total angular momentum:
Angular momentum simplified using the center of mass.
It is very often convenient to consider the angular momentum of a collection of particles about their center of mass, since this simplifies the mathematics considerably. The angular momentum of a collection of particles is the sum of the angular momentum of each particle:
where r"i" is the position vector of particle "i" from the reference point, "mi" is its mass, and v"i" is its linear velocity. The center of mass is defined by:
where the total mass of all particles is given by
It follows that the linear velocity of the center of mass is
If we define R"i" as the displacement of particle "i" from the center of mass, and V"i" as the linear velocity of particle "i" with respect to the center of mass, then we have
we can see that
thus the total angular momentum with respect to the reference point is
The first term is just the angular momentum of the center of mass. It is the same angular momentum one would obtain if there were just one particle of mass "M" moving at velocity v located at the center of mass. The second term is the angular momentum that is the result of the particles moving relative to their center of mass. This second term can be even further simplified if the particles form a rigid body, in which case it is the product of moment of inertia and angular velocity of the spinning motion (as above). The same result is true if the discrete point masses discussed above are replaced by a continuous distribution of matter.
Fixed axis of rotation.
For many applications where one is only concerned about rotation around one axis, it is sufficient to discard the pseudovector nature of angular momentum, and treat it like a scalar where it is positive when it corresponds to a counter-clockwise rotation, and negative clockwise. To do this, just take the definition of the cross product and discard the unit vector, so that angular momentum becomes:
where "θ""r","p" is the angle between r and p measured from r to p; an important distinction because without it, the sign of the cross product would be meaningless. From the above, it is possible to reformulate the definition to either of the following:
where formula_19 is called the "lever arm distance" to p.
The easiest way to conceptualize this is to consider the lever arm distance to be the distance from the origin to the line that p travels along. With this definition, it is necessary to consider the direction of p (pointed clockwise or counter-clockwise) to figure out the sign of "L". Equivalently:
where formula_21 is the component of p that is perpendicular to r. As above, the sign is decided based on the sense of rotation.
For an object with a fixed mass that is rotating about a fixed symmetry axis,
the angular momentum is expressed as the product of the moment of inertia of the object and its angular
velocity vector:
where "I" is the moment of inertia of the object (in general, a tensor quantity) and ω is the angular velocity. The kinetic energy "T" of a massive rotating body is given by
which means the kinetic energy is proportional to the square of the angular velocity, just like for translational kinetic energy and its relation to translational velocity.
In general, while the angular velocity vector is directed along the rotation axis, the angular momentum pseudovector is not. This is because "I" depends on how the mass is distributed throughout the object, and the axis of rotation. The general relation between the magnitudes and directions of the ω and L pseudovectors is given through the moment of inertia as a second order tensor:
where tensor index notation is used ("i", "j" = 1, 2, 3), including the summation convention. The general expression for the kinetic energy is
Conservation of angular momentum.
The law of conservation of angular momentum states that when no external torque acts on an object or a closed system of objects, no change of angular momentum can occur. Hence, the angular momentum before an event involving only internal torques or no torques is equal to the angular momentum after the event. This conservation law mathematically follows from isotropy, or continuous directional symmetry of space (no direction in space is any different from any other direction). See Noether's theorem.
The time derivative of angular momentum is called torque:
(The cross-product of velocity and momentum is zero, because these vectors are parallel.) So requiring the system to be "closed" here is mathematically equivalent to zero external torque acting on the system:
where formula_28 is any torque applied to the system of particles.
It is assumed that internal interaction forces obey Newton's third law of motion in its strong form, that is, that the forces between particles are equal and opposite and act along the line between the particles.
In orbits, the angular momentum is distributed between the spin of the planet itself and the angular momentum of its orbit:
If a planet is found to rotate slower than expected, then astronomers suspect that the planet is accompanied by a satellite, because the total angular momentum is shared between the planet and its satellite in order to be conserved.
The conservation of angular momentum is used extensively in analyzing what is called "central force motion". If the net force on some body is directed always toward some fixed point, the "center", then there is no torque on the body with respect to the center, and so the angular momentum of the body about the center is constant. Constant angular momentum is extremely useful when dealing with the orbits of planets and satellites, and also when analyzing the Bohr model of the atom.
The conservation of angular momentum explains the angular acceleration of an ice skater as she brings her arms and legs close to the vertical axis of rotation. By bringing part of mass of her body closer to the axis she decreases her body's moment of inertia. Because angular momentum is constant in the absence of external torques, the angular velocity (rotational speed) of the skater has to increase.
The same phenomenon results in extremely fast spin of compact stars (like white dwarfs, neutron stars and black holes) when they are formed out of much larger and slower rotating stars (indeed, decreasing the size of object 104 times results in increase of its angular velocity by the factor 108).
The conservation of angular momentum in Earth–Moon system results in the transfer of angular momentum from Earth to Moon (due to tidal torque the Moon exerts on the Earth). This in turn results in the slowing down of the rotation rate of Earth (at about 42 ns/day ), and in gradual increase of the radius of Moon's orbit (at ~4.5 cm/year rate ).
Angular momentum (modern definition).
In modern (20th century) theoretical physics, angular momentum (not including any intrinsic angular momentum – see below) is described using a different formalism, instead of a classical pseudovector. In this formalism, angular momentum is the 2-form Noether charge associated with rotational invariance. As a result, angular momentum is not conserved for general curved spacetimes, unless it happens to be asymptotically rotationally invariant.
In classical mechanics, the angular momentum of a particle can be reinterpreted as a plane element:
in which the exterior product ∧ replaces the cross product × (these products have similar characteristics but are nonequivalent). This has the advantage of a clearer geometric interpretation as a plane element, defined from the x and p vectors, and the expression is true in any number of dimensions (two or higher). In Cartesian coordinates:
or more compactly in index notation:
The angular velocity can also be defined as an antisymmetric second order tensor, with components "ωij". The relation between the two antisymmetric tensors is given by the moment of inertia which must now be a fourth order tensor:
Again, this equation in L and ω as tensors is true in any number of dimensions. This equation also appears in the geometric algebra formalism, in which L and ω are bivectors, and the moment of inertia is a mapping between them.
In relativistic mechanics, the relativistic angular momentum of a particle is expressed as an antisymmetric tensor of second order:
in the language of four-vectors, namely the four position "X" and the four momentum "P", and absorbs the above L together with the motion of the centre of mass of the particle.
In each of the above cases, for a system of particles, the total angular momentum is just the sum of the individual particle angular momenta, and the centre of mass is for the system.
Angular momentum in quantum mechanics.
Angular momentum in quantum mechanics differs in many profound respects from angular momentum in classical mechanics. In relativistic quantum mechanics, it differs even more, in which the above relativistic definition becomes a tensorial operator.
Spin, orbital, and total angular momentum.
The classical definition of angular momentum as formula_3 can be carried over to quantum mechanics, by reinterpreting r as the quantum position operator and p as the quantum momentum operator. L is then an operator, specifically called the "orbital angular momentum operator".
However, in quantum physics, there is another type of angular momentum, called "spin angular momentum", represented by the spin operator S. Almost all elementary particles have spin. Spin is often depicted as a particle literally spinning around an axis, but this is a misleading and inaccurate picture: spin is an intrinsic property of a particle, fundamentally different from orbital angular momentum. All elementary particles have a characteristic spin, for example electrons always have "spin 1/2" (this actually means "spin ħ/2") while photons always have "spin 1" (this actually means "spin ħ").
Finally, there is total angular momentum J, which combines both the spin and orbital angular momentum of all particles and fields. (For one particle, J = L + S.) Conservation of angular momentum applies to J, but not to L or S; for example, the spin–orbit interaction allows angular momentum to transfer back and forth between L and S, with the total remaining constant.
Quantization.
In quantum mechanics, angular momentum is quantized – that is, it cannot vary continuously, but only in "quantum leaps" between certain allowed values. For any system, the following restrictions on measurement results apply, where formula_36 is the reduced Planck constant and formula_37 is any direction vector such as x, y, or z:
The reduced Planck constant formula_36 is tiny by everyday standards, about 10−34 J s, and therefore this quantization does not noticeably affect the angular momentum of macroscopic objects. However, it is very important in the microscopic world. For example, the structure of electron shells and subshells in chemistry is significantly affected by the quantization of angular momentum.
Quantization of angular momentum was first postulated by Niels Bohr in his Bohr model of the atom.
Uncertainty.
In the definition formula_3, six operators are involved: The position operators formula_40, formula_41, formula_42, and the momentum operators formula_43, formula_44, formula_45. However, the Heisenberg uncertainty principle tells us that it is not possible for all six of these quantities to be known simultaneously with arbitrary precision. Therefore, there are limits to what can be known or measured about a particle's angular momentum. It turns out that the best that one can do is to simultaneously measure both the angular momentum vector's magnitude and its component along one axis.
The uncertainty is closely related to the fact that different components of an angular momentum operator do not commute, for example formula_46. (For the precise commutation relations, see angular momentum operator.)
Total angular momentum as generator of rotations.
As mentioned above, orbital angular momentum L is defined as in classical mechanics: formula_3, but "total" angular momentum J is defined in a different, more basic way: J is defined as the "generator of rotations". More specifically, J is defined so that the operator
is the rotation operator that takes any system and rotates it by angle formula_49 about the axis formula_50.
The relationship between the angular momentum operator and the rotation operators is the same as the relationship between lie algebras and lie groups in mathematics. The close relationship between angular momentum and rotations is reflected in Noether's theorem that proves that angular momentum is conserved whenever the laws of physics are rotationally invariant.
Angular momentum in electrodynamics.
When describing the motion of a charged particle in an electromagnetic field, the canonical momentum P (derived from the Lagrangian for this system) is not gauge invariant. As a consequence, the canonical angular momentum L = r × P is not gauge invariant either. Instead, the momentum that is physical, the so-called "kinetic momentum" (used throughout this article), is (in SI units)
where "e" is the electric charge of the particle and A the magnetic vector potential of the electromagnetic field. The gauge-invariant angular momentum, that is "kinetic angular momentum", is given by
The interplay with quantum mechanics is discussed further in the article on canonical commutation relations.

</doc>
<doc id="2840" url="http://en.wikipedia.org/wiki?curid=2840" title="Plum pudding model">
Plum pudding model

The plum pudding model, also known as the blueberry muffin model, of the atom by J. J. Thomson, who discovered the electron in 1897, was proposed in 1904 before the discovery of the atomic nucleus in order to add the electron to the atomic model. In this model, the atom is composed of electrons (which Thomson still called "corpuscles", though G. J. Stoney had proposed that atoms of electricity be called "electrons" in 1894) surrounded by a soup of positive charge to balance the electrons' negative charges, like negatively charged "raisins" surrounded by positively charged "pudding". The electrons (as we know them today) were thought to be positioned throughout the atom, but with many structures possible for positioning multiple electrons, particularly rotating rings of electrons (see below). Instead of a soup, the atom was also sometimes said to have had a "cloud" of positive charge. 
With this model, Thomson abandoned his earlier "nebular atom" hypothesis in which the atom was composed of immaterial vortices. Now, at least part of the atom was to be composed of Thomson's particulate negative corpuscles, although the rest of the positively charged part of the atom remained somewhat nebulous and ill-defined.
The 1904 Thomson model was disproved by the 1909 gold foil experiment of Hans Geiger and Ernest Marsden. This was interpreted by Ernest Rutherford in 1911
Thomson's model was compared (though not by Thomson) to a British dessert called plum pudding, hence the name. Thomson's paper was published in the March 1904 edition of the "Philosophical Magazine", the leading British science journal of the day. In Thomson's view:
... the atoms of the elements consist of a number of negatively electrified corpuscles enclosed in a sphere of uniform positive electrification, ...
In this model, the electrons were free to rotate within the blob or cloud of positive substance. These orbits were stabilized in the model by the fact that when an electron moved farther from the center of the positive cloud, it felt a larger net positive inward force, because there was more material of opposite charge, inside its orbit (see Gauss's law). In Thomson's model, electrons were free to rotate in rings which were further stabilized by interactions between the electrons, and spectra were to be accounted for by energy differences of different ring orbits. Thomson attempted to make his model account for some of the major spectral lines known for some elements, but was not notably successful at this. Still, Thomson's model (along with a similar Saturnian ring model for atomic electrons, also put forward in 1904 by Nagaoka after James Clerk Maxwell's model of Saturn's rings), were earlier harbingers of the later and more successful solar-system-like Bohr model of the atom.
Related scientific problems.
A particularly useful mathematics problem related to the plum pudding model is the optimal distribution of equal point charges on a unit sphere called the Thomson problem. The Thomson problem is a natural consequence of the plum pudding model in the absence of its uniform positive background charge.
The classical electrostatic treatment of electrons confined to spherical quantum dots is also similar to their treatment in the plum pudding model.

</doc>
<doc id="2844" url="http://en.wikipedia.org/wiki?curid=2844" title="Atomic theory">
Atomic theory

In chemistry and physics, atomic theory is a scientific theory of the nature of matter, which states that matter is composed of discrete units called atoms. It began as a philosophical concept in ancient Greece and entered the scientific mainstream in the early 19th century when discoveries in the field of chemistry showed that matter did indeed behave as if it were made up of atoms.
The word "atom" comes from the Ancient Greek adjective "atomos", meaning "indivisible". 19th century chemists began using the term in connection with the growing number of irreducible chemical elements. While seemingly apropos, around the turn of the 20th century, through various experiments with electromagnetism and radioactivity, physicists discovered that the so-called "indivisible atom" was actually a conglomerate of various subatomic particles (chiefly, electrons, protons and neutrons) which can exist separately from each other. In fact, in certain extreme environments, such as neutron stars, extreme temperature and pressure prevents atoms from existing at all. Since atoms were found to be divisible, physicists later invented the term "elementary particles" to describe the 'indivisible', though not indestructible, parts of an atom. The field of science which studies subatomic particles is particle physics, and it is in this field that physicists hope to discover the true fundamental nature of matter.
History.
Philosophical atomism.
The idea that matter is made up of discrete units is a very old one, appearing in many ancient cultures such as Greece and India. The principle ancient Greek atomists included Leucippus, Democritus. Ancient Indian atomists included Ajivika and Carvaka Jains. The relationship between Greek and Indian atomism, whether they were arrived at independently or influenced each other, is a matter of debate. However, these ideas were founded in philosophical and theological reasoning rather than evidence and experimentation. As a result, their views on what atoms look like and how they behave were very incorrect. They also couldn't convince everybody, so atomism was but one of a number of competing theories on the nature of matter. It wasn't until the 19th century that the idea was embraced and refined by scientists, as the blossoming science of chemistry produced discoveries that only the concept of atoms could explain.
First evidence-based theory.
Near the end of the 18th century, two laws about chemical reactions emerged without referring to the notion of an atomic theory. The first was the law of conservation of mass, formulated by Antoine Lavoisier in 1789, which states that the total mass in a chemical reaction remains constant (that is, the reactants have the same mass as the products). The second was the law of definite proportions. First proven by the French chemist Joseph Louis Proust in 1799, this law states that if a compound is broken down into its constituent elements, then the masses of the constituents will always have the same proportions, regardless of the quantity or source of the original substance.
John Dalton studied and expanded upon this previous work and developed the law of multiple proportions: if two elements can be combined to form a number of possible compounds, then the ratios of the masses of the second element which combine with a fixed mass of the first element will be ratios of small whole numbers. For example: Proust had studied tin oxides and found that their masses were either 88.1% tin and 11.9% oxygen or 78.7% tin and 21.3% oxygen (these were tin(II) oxide and tin dioxide respectively). Dalton noted from these percentages that 100g of tin will combine either with 13.5g or 27g of oxygen; 13.5 and 27 form a ratio of 1:2. Dalton found that an atomic theory of matter could elegantly explain this common pattern in chemistry. In the case of Proust's tin oxides, one tin atom will combine with either one or two oxygen atoms.
Dalton also believed atomic theory could explain why water absorbed different gases in different proportions - for example, he found that water absorbed carbon dioxide far better than it absorbed nitrogen. Dalton hypothesized this was due to the differences in mass and complexity of the gases' respective particles. Indeed, carbon dioxide molecules (CO2) are heavier and larger than nitrogen molecules (N2).
Dalton proposed that each chemical element is composed of atoms of a single, unique type, and though they cannot be altered or destroyed by chemical means, they can combine to form more complex structures (chemical compounds). This marked the first truly scientific theory of the atom, since Dalton reached his conclusions by experimentation and examination of the results in an empirical fashion.
In 1803 Dalton orally presented his first list of relative atomic weights for a number of substances. This paper was published in 1805, but he did not discuss there exactly how he obtained these figures. The method was first revealed in 1807 by his acquaintance Thomas Thomson, in the third edition of Thomson's textbook, "A System of Chemistry". Finally, Dalton published a full account in his own textbook, "A New System of Chemical Philosophy", 1808 and 1810.
Dalton estimated the atomic weights according to the mass ratios in which they combined, with the hydrogen atom taken as unity. However, Dalton did not conceive that with some elements atoms exist in molecules — e.g. pure oxygen exists as O2. He also mistakenly believed that the simplest compound between any two elements is always one atom of each (so he thought water was HO, not H2O). This, in addition to the crudity of his equipment, flawed his results. For instance, in 1803 he believed that oxygen atoms were 5.5 times heavier than hydrogen atoms, because in water he measured 5.5 grams of oxygen for every 1 gram of hydrogen and believed the formula for water was HO. Adopting better data, in 1806 he concluded that the atomic weight of oxygen must actually be 7 rather than 5.5, and he retained this weight for the rest of his life. Others at this time had already concluded that the oxygen atom must weigh 8 relative to hydrogen equals 1, if one assumes Dalton's formula for the water molecule (HO), or 16 if one assumes the modern water formula.
The flaw in Dalton's theory was corrected in principle in 1811 by Amedeo Avogadro. Avogadro had proposed that equal volumes of any two gases, at equal temperature and pressure, contain equal numbers of molecules (in other words, the mass of a gas's particles does not affect the volume that it occupies). Avogadro's law allowed him to deduce the diatomic nature of numerous gases by studying the volumes at which they reacted. For instance: since two liters of hydrogen will react with just one liter of oxygen to produce two liters of water vapor (at constant pressure and temperature), it meant a single oxygen molecule splits in two in order to form two particles of water. Thus, Avogadro was able to offer more accurate estimates of the atomic mass of oxygen and various other elements, and made a clear distinction between molecules and atoms.
In 1827, the British botanist Robert Brown observed that dust particles inside pollen grains floating in water constantly jiggled about for no apparent reason. In 1905, Albert Einstein theorized that this Brownian motion was caused by the water molecules continuously knocking the grains about, and developed a hypothetical mathematical model to describe it. This model was validated experimentally in 1908 by French physicist Jean Perrin, thus providing additional validation for particle theory (and by extension atomic theory).
Discovery of subatomic particles.
Atoms were thought to be the smallest possible division of matter until 1897 when J.J. Thomson discovered the electron through his work on cathode rays.
A Crookes tube is a sealed glass container in which two electrodes are separated by a vacuum. When a voltage is applied across the electrodes, cathode rays are generated, creating a glowing patch where they strike the glass at the opposite end of the tube. Through experimentation, Thomson discovered that the rays could be deflected by an electric field (in addition to magnetic fields, which was already known). He concluded that these rays, rather than being a form of light, were composed of very light negatively charged particles he called "corpuscles" (they would later be renamed electrons by other scientists).
Thomson suggested that atoms were divisible, and that the corpuscles were their building blocks. To explain the overall neutral charge of the atom, he proposed that the corpuscles were distributed in a uniform sea of positive charge; this was the plum pudding model as the electrons were embedded in the positive charge like plums in a plum pudding (although in Thomson's model they were not stationary).
Discovery of the nucleus.
Thomson's plum pudding model was disproved in 1909 by one of his former students, Ernest Rutherford, who discovered that most of the mass and positive charge of an atom is concentrated in a very small fraction of its volume, which he assumed to be at the very center.
In the gold foil experiment, Hans Geiger and Ernest Marsden (colleagues of Rutherford working at his behest) shot alpha particles at a thin sheet of gold and measured their deflection through the use of a fluorescent screen. Given the very small mass of the electrons, the high momentum of the alpha particles and the uniform distribution of positive charge of the plum pudding model, the experimenters expected all the alpha particles to pass through the gold sheet without significant deflection. To their astonishment, a small fraction of the alpha particles experienced heavy deflection. It was thus evident that most of the mass of the nucleus was concentrated in a very small part of the atom, which could either be electrically neutral or not. The alpha particles passing in close proximity of an electrically neutral mass would move past undeflected, whereas they would be deflected going past a positively charged nucleus according to forces experience in accordance with Coulomb's Law. Rutherford's analysis of the results of the scattering experiment favoured the latter.
This led Rutherford to propose a planetary model in which a cloud of electrons surrounded a small, compact nucleus of positive charge. Only such a concentration of charge could produce the electric field strong enough to cause the heavy deflection.
First steps toward a quantum physical model of the atom.
The planetary model of the atom had two significant shortcomings. The first is that, unlike planets orbiting a sun, electrons are charged particles. An accelerating electric charge is known to emit electromagnetic waves according to the Larmor formula in classical electromagnetism; an orbiting charge should steadily lose energy and spiral toward the nucleus, colliding with it in a small fraction of a second. The second problem was that the planetary model could not explain the highly peaked emission and absorption spectra of atoms that were observed.
Quantum theory revolutionized physics at the beginning of the 20th century, when Max Planck and Albert Einstein postulated that light energy is emitted or absorbed in discrete amounts known as quanta (singular, "quantum"). In 1913, Niels Bohr incorporated this idea into his Bohr model of the atom, in which an electron could only orbit the nucleus in particular circular orbits with fixed angular momentum and energy, its distance from the nucleus (i.e., their radii) being proportional to its energy. Under this model an electron could not spiral into the nucleus because it could not lose energy in a continuous manner; instead, it could only make instantaneous "quantum leaps" between the fixed energy levels. When this occurred, light was emitted or absorbed at a frequency proportional to the change in energy (hence the absorption and emission of light in discrete spectra).
Bohr's model was not perfect. It could only predict the spectral lines of hydrogen; it couldn't predict those of multielectron atoms. Worse still, as spectrographic technology improved, additional spectral lines in hydrogen were observed which Bohr's model couldn't explain. In 1916, Arnold Sommerfeld added elliptical orbits to the Bohr model to explain the extra emission lines, but this made the model very difficult to use, and it still couldn't explain more complex atoms.
Discovery of isotopes.
While experimenting with the products of radioactive decay, in 1913 radiochemist Frederick Soddy discovered that there appeared to be more than one element at each position on the periodic table. The term isotope was coined by Margaret Todd as a suitable name for these elements.
That same year, J.J. Thomson conducted an experiment in which he channeled a stream of neon ions through magnetic and electric fields, striking a photographic plate at the other end. He observed two glowing patches on the plate, which suggested two different deflection trajectories. Thomson concluded this was because some of the neon ions had a different mass. The nature of this differing mass would later be explained by the discovery of neutrons in 1932.
Discovery of nuclear particles.
In 1917 Rutherford bombarded nitrogen gas with alpha particles and observed hydrogen nuclei being emitted from the gas (Rutherford recognized these, because he had previously obtained them bombarding hydrogen with alpha particles, and observing hydrogen nuclei in the products). Rutherford concluded that the hydrogen nuclei emerged from the nuclei of the nitrogen atoms themselves (in effect, he had split a nitrogen).
From his own work and the work of his students Bohr and Henry Moseley, Rutherford knew that the positive charge of any atom could always be equated to that of an integer number of hydrogen nuclei. This, coupled with the atomic mass of many elements being roughly equivalent to an integer number of hydrogen atoms - then assumed to be the lightest particles- led him to conclude that hydrogen nuclei were singular particles and a basic constituent of all atomic nuclei. He named such particles protons. Further experimentation by Rutherford found that the nuclear mass of most atoms exceeded that of the protons it possessed; he speculated that this surplus mass was composed of hitherto unknown neutrally charged particles, which were tentatively dubbed "neutrons".
In 1928, Walter Bothe observed that beryllium emitted a highly penetrating, electrically neutral radiation when bombarded with alpha particles. It was later discovered that this radiation could knock hydrogen atoms out of paraffin wax. Initially it was thought to be high-energy gamma radiation, since gamma radiation had a similar effect on electrons in metals, but James Chadwick found that the ionization effect was too strong for it to be due to electromagnetic radiation, so long as energy and momentum were conserved in the interaction. In 1932, Chadwick exposed various elements, such as hydrogen and nitrogen, to the mysterious "beryllium radiation", and by measuring the energies of the recoiling charged particles, he deduced that the radiation was actually composed of electrically neutral particles which could not be massless like the gamma ray, but instead were required to have a mass similar to that of a proton. Chadwick now claimed these particles as Rutherford's neutrons. For his discovery of the neutron, Chadwick received the Nobel Prize in 1935.
Quantum physical models of the atom.
In 1924, Louis de Broglie proposed that all moving particles — particularly subatomic particles such as electrons — exhibit a degree of wave-like behavior. Erwin Schrödinger, fascinated by this idea, explored whether or not the movement of an electron in an atom could be better explained as a wave rather than as a particle. Schrödinger's equation, published in 1926, describes an electron as a wavefunction instead of as a point particle. This approach elegantly predicted many of the spectral phenomena that Bohr's model failed to explain. Although this concept was mathematically convenient, it was difficult to visualize, and faced opposition. One of its critics, Max Born, proposed instead that Schrödinger's wavefunction described not the electron but rather all its possible states, and thus could be used to calculate the probability of finding an electron at any given location around the nucleus. This reconciled the two opposing theories of particle versus wave electrons and the idea of wave–particle duality was introduced. This theory stated that the electron may exhibit the properties of both a wave and a particle. For example, it can be refracted like a wave, and has mass like a particle.
A consequence of describing electrons as waveforms is that it is mathematically impossible to simultaneously derive the position and momentum of an electron. This became known as the Heisenberg uncertainty principle after the theoretical physicist Werner Heisenberg, who first described it and published it in 1927. This invalidated Bohr's model, with its neat, clearly defined circular orbits. The modern model of the atom describes the positions of electrons in an atom in terms of probabilities. An electron can potentially be found at any distance from the nucleus, but, depending on its energy level, exists more frequently in certain regions around the nucleus than others; this pattern is referred to as its atomic orbital. The orbitals come in a variety of shapes-sphere, dumbbell, torus, etc.-with the nucleus in the middle.

</doc>
<doc id="2846" url="http://en.wikipedia.org/wiki?curid=2846" title="Ai">
Ai

AI, A.I., Ai, or ai may refer to:

</doc>
<doc id="2847" url="http://en.wikipedia.org/wiki?curid=2847" title="Aung San Suu Kyi">
Aung San Suu Kyi

Aung San Suu Kyi AC (, , ; born 19 June 1945) is a Burmese opposition politician and chairperson of the National League for Democracy (NLD) in Burma. In the 1990 general election, the NLD won 59% of the national votes and 81% (392 of 485) of the seats in Parliament. She had, however, already been detained under house arrest before the elections. She remained under house arrest in Burma for almost 15 of the 21 years from 20 July 1989 until her most recent release on 13 November 2010, becoming one of the world's most prominent political prisoners.
Suu Kyi received the Rafto Prize and the Sakharov Prize for Freedom of Thought in 1990 and the Nobel Peace Prize in 1991. In 1992 she was awarded the Jawaharlal Nehru Award for International Understanding by the government of India and the International Simón Bolívar Prize from the government of Venezuela. In 2007, the Government of Canada made her an honorary citizen of that country, the fourth person ever to receive the honour. In 2011, she was awarded the Wallenberg Medal.
On 19 September 2012, Aung San Suu Kyi was also presented with the Congressional Gold Medal, which is, along with the Presidential Medal of Freedom, the highest civilian honour in the United States.
On 1 April 2012, her party, the National League for Democracy, announced that she was elected to the Pyithu Hluttaw, the lower house of the Burmese parliament, representing the constituency of Kawhmu; her party also won 43 of the 45 vacant seats in the lower house. The election results were confirmed by the official electoral commission the following day.
On 6 June 2013, Suu Kyi announced on the World Economic Forum’s website that she wants to run for the presidency in Myanmar's 2015 elections. As of 2014, she is listed as the 61st most powerful woman in the world by "Forbes".
Name.
Aung San Suu Kyi, like other Burmese names, has no family name, but is only a personal name, in her case derived from three relatives: "Aung San" from her father, "Suu" from her paternal grandmother, and "Kyi" from her mother Khin Kyi.
The Burmese refer to her as Daw Aung San Suu Kyi. "Daw" is not part of her name, but is a Burmese honorific like "Madame" for any older and revered woman, literally meaning "aunt". They say Daw Suu and Amay Suu ("Mother Suu"), (and even "Aunty Suu"), and also Dr. Suu Kyi.
The foreign media refer to her as Ms. Suu Kyi, or Miss Suu Kyi.
Personal life.
Aung San Suu Kyi was born on 19 June 1945 in Rangoon (now named Yangon). Her father, Aung San, founded the modern Burmese army and negotiated Burma's independence from the British Empire in 1947; he was assassinated by his rivals in the same year. She grew up with her mother, Khin Kyi, and two brothers, Aung San Lin and Aung San Oo, in Rangoon. Aung San Lin died at the age of eight, when he drowned in an ornamental lake on the grounds of the house. Her elder brother emigrated to San Diego, California, becoming a United States citizen. After Aung San Lin's death, the family moved to a house by Inya Lake where Suu Kyi met people of various backgrounds, political views and religions. She was educated in Methodist English High School (now Basic Education High School No. 1 Dagon) for much of her childhood in Burma, where she was noted as having a talent for learning languages. She is a Theravada Buddhist.
Suu Kyi's mother, Khin Kyi, gained prominence as a political figure in the newly formed Burmese government. She was appointed Burmese ambassador to India and Nepal in 1960, and Aung San Suu Kyi followed her there. She studied in the Convent of Jesus and Mary School in New Delhi, and graduated from Lady Shri Ram College in New Delhi with a degree in politics in 1964. Suu Kyi continued her education at St Hugh's College, Oxford, obtaining a B.A. degree in Philosophy, Politics and Economics in 1969. After graduating, she lived in New York City with a family friend Ma Than E, who was once a popular Burmese pop singer. She worked at the United Nations for three years, primarily on budget matters, writing daily to her future husband, Dr. Michael Aris. In late 1971, Aung San Suu Kyi married Aris, a scholar of Tibetan culture, living abroad in Bhutan. The following year she gave birth to their first son, Alexander Aris, in London; their second son, Kim, was born in 1977. Between 1985 and 1987, Suu Kyi was working toward an M.Phil degree in Burmese literature as a research student at SOAS the School of Oriental and African Studies, University of London. She was elected as an Honorary Fellow of SOAS in 1990. For two years she was a Fellow at the Indian Institute of Advanced Studies (IIAS) in Shimla, India. She also worked for the government of the Union of Burma.
In 1988 Suu Kyi returned to Burma, at first to tend for her ailing mother but later to lead the pro-democracy movement. Aris' visit in Christmas 1995 turned out to be the last time that he and Suu Kyi met, as Suu Kyi remained in Burma and the Burmese dictatorship denied him any further entry visas. Aris was diagnosed with prostate cancer in 1997 which was later found to be terminal. Despite appeals from prominent figures and organizations, including the United States, UN Secretary General Kofi Annan and Pope John Paul II, the Burmese government would not grant Aris a visa, saying that they did not have the facilities to care for him, and instead urged Aung San Suu Kyi to leave the country to visit him. She was at that time temporarily free from house arrest but was unwilling to depart, fearing that she would be refused re-entry if she left, as she did not trust the military junta's assurance that she could return.
Aris died on his 53rd birthday on 27 March 1999. Since 1989, when his wife was first placed under house arrest, he had seen her only five times, the last of which was for Christmas in 1995. She was also separated from her children, who live in the United Kingdom, but starting in 2011, they have visited her in Burma.
On 2 May 2008, after Cyclone Nargis hit Burma, Suu Kyi lost the roof of her house and lived in virtual darkness after losing electricity in her dilapidated lakeside residence. She used candles at night as she was not provided any generator set. Plans to renovate and repair the house were announced in August 2009. Suu Kyi was released from house arrest on 13 November 2010.
Political beginnings.
Coincidentally, when Aung San Suu Kyi returned to Burma in 1988, the long-time military leader of Burma and head of the ruling party, General Ne Win, stepped down. Mass demonstrations for democracy followed that event on 8 August 1988 (8–8–88, a day seen as auspicious), which were violently suppressed in what came to be known as the 8888 Uprising. On 26 August 1988, she addressed half a million people at a mass rally in front of the Shwedagon Pagoda in the capital, calling for a democratic government. However in September, a new military junta took power.
During the crisis, the previous democratically elected Prime Minister of Burma, U Nu initiated to form an interim government and invited opposition leaders to join him. Indian Prime Minister Rajiv Gandhi had signaled his readiness to recognize the interim government and Burmese troops started to change sides with Burmese Navy almost totally siding with the opposition. However, Aung San Suu Kyi categorically rejected U Nu's plan by saying "the future of the opposition would be decided by masses of the people". Ex-Brigadier General Aung Gyi, another opposition politician at the time of the 8888 crisis, followed the suit and rejected the plan after Suu Kyi's refusal. Crucial months were passed on the street and the interim government was not internationally recognized due to lack of support from opposition. Political analyst Susanne Prager-Nyein described Aung San Suu Kyi's refusal as "a major strategic mistake".
Influenced by both Mahatma Gandhi's philosophy of non-violence and more specifically by Buddhist concepts, Aung San Suu Kyi entered politics to work for democratization, helped found the National League for Democracy on 27 September 1988, but was put under house arrest on 20 July 1989. Offered freedom if she left the country, she refused.
One of her most famous speeches was Freedom From Fear", which began: "It is not power that corrupts, but fear. Fear of losing power corrupts those who wield it and fear of the scourge of power corrupts those who are subject to it."
She also believes fear spurs many world leaders to lose sight of their purpose. "Government leaders are amazing", she once said. "So often it seems they are the last to know what the people want."
Political career.
1990 general election.
In 1990, the military junta called a general election, in which the National League for Democracy (NLD) received 59% of the votes, guaranteeing NLD 80% of the parliament seats. Some claim that Aung San Suu Kyi would have assumed the office of Prime Minister; in fact, however, as she was not permitted, she did not stand as a candidate in the elections (although being a MP is not a strict prerequisite for becoming PM in most parliamentary systems). Instead, the results were nullified and the military refused to hand over power, resulting in an international outcry. Aung San Suu Kyi was placed under house arrest at her home on University Avenue () in Rangoon, during which time she was awarded the Sakharov Prize for Freedom of Thought in 1990, and the Nobel Peace Prize the year after. Her sons Alexander and Kim accepted the Nobel Peace Prize on her behalf. Aung San Suu Kyi used the Nobel Peace Prize's 1.3 million USD prize money to establish a health and education trust for the Burmese people. Around this time, Suu Kyi chose non-violence as an expedient political tactic, stating in 2007, "I do not hold to non-violence for moral reasons, but for political and practical reasons," however, nonviolent action as well as civil resistance in lieu of armed conflict are also political tactics in keeping with the overall philosophy of her Theravada Buddhist religion.
1996 attack.
On 9 November 1996, the motorcade that she was traveling in with other National League for Democracy leaders Tin Oo and U Kyi Maung, was attacked in Yangon. About 200 men swooped down on the motorcade, wielding metal chains, metal batons, stones and other weapons. The car that Aung San Suu Kyi was in had its rear window smashed, and the car with Tin Oo and U Kyi Maung had its rear window and two backdoor windows shattered. It is believed the offenders were members of the Union Solidarity and Development Association (USDA) who were allegedly paid 500 kyats (@ USD $0.50) each to participate. The NLD lodged an official complaint with the police, and according to reports the government launched an investigation, but no action was taken. (Amnesty International 120297)
House arrest.
Aung San Suu Kyi has been placed under house arrest for 15 of the past 21 years, on numerous occasions, since she began her political career, during which time she was prevented from meeting her party supporters and international visitors. In an interview, Suu Kyi said that while under house arrest she spent her time reading philosophy, politics and biographies that her husband had sent her. She also passed the time playing the piano, and was occasionally allowed visits from foreign diplomats as well as from her personal physician.
The media were also prevented from visiting Suu Kyi, as occurred in 1998 when journalist Maurizio Giuliano, after photographing her, was stopped by customs officials who then confiscated all his films, tapes and some notes. In contrast, Suu Kyi did have visits from government representatives, such as during her autumn 1994 house arrest when she met the leader of Burma, General Than Shwe and General Khin Nyunt on 20 September in the first meeting since she had been placed in detention. On several occasions during Suu Kyi's house arrest, she had periods of poor health and as a result was hospitalized.
The Burmese government detained and kept Suu Kyi imprisoned because it viewed her as someone "likely to undermine the community peace and stability" of the country, and used both Article 10(a) and 10(b) of the 1975 State Protection Act (granting the government the power to imprison people for up to five years without a trial), and Section 22 of the "Law to Safeguard the State Against the Dangers of Those Desiring to Cause Subversive Acts" as legal tools against her. She continuously appealed her detention, and many nations and figures continued to call for her release and that of 2,100 other political prisoners in the country. On 12 November 2010, days after the junta-backed Union Solidarity and Development Party (USDP) won elections conducted after a gap of 20 years, the junta finally agreed to sign orders allowing Suu Kyi's release, and Suu Kyi's house arrest term came to an end on 13 November 2010.
United Nations involvement.
The United Nations (UN) has attempted to facilitate dialogue between the junta and Suu Kyi. On 6 May 2002, following secret confidence-building negotiations led by the UN, the government released her; a government spokesman said that she was free to move "because we are confident that we can trust each other". Aung San Suu Kyi proclaimed "a new dawn for the country". However on 30 May 2003 in an incident similar to the 1996 attack on her, a government-sponsored mob attacked her caravan in the northern village of Depayin, murdering and wounding many of her supporters. Aung San Suu Kyi fled the scene with the help of her driver, Ko Kyaw Soe Lin, but was arrested upon reaching Ye-U. The government imprisoned her at Insein Prison in Rangoon. After she underwent a hysterectomy in September 2003, the government again placed her under house arrest in Rangoon.
The results from the UN facilitation have been mixed; Razali Ismail, UN special envoy to Burma, met with Aung San Suu Kyi. Ismail resigned from his post the following year, partly because he was denied re-entry to Burma on several occasions. Several years later in 2006, Ibrahim Gambari, UN Undersecretary-General (USG) of Department of Political Affairs, met with Aung San Suu Kyi, the first visit by a foreign official since 2004. He also met with Suu Kyi later the same year. On 2 October 2007 Gambari returned to talk to her again after seeing Than Shwe and other members of the senior leadership in Naypyidaw. State television broadcast Suu Kyi with Gambari, stating that they had met twice. This was Suu Kyi's first appearance in state media in the four years since her current detention began.
The United Nations Working Group for Arbitrary Detention published an Opinion that Aung San Suu Kyi's deprivation of liberty was arbitrary and in contravention of Article 9 of the Universal Declaration of Human Rights 1948, and requested that the authorities in Burma set her free, but the authorities ignored the request at that time. The U.N. report said that according to the Burmese Government’s reply, "Daw Aung San Suu Kyi has not been arrested, but has only been taken into protective custody, for her own safety", and while "it could have instituted legal action against her under the country’s domestic legislation ... it has preferred to adopt a magnanimous attitude, and is providing her with protection in her own interests."
Such claims were rejected by Brig-General Khin Yi, Chief of Myanmar Police Force (MPF). On 18 January 2007, the state-run paper "New Light of Myanmar" accused Suu Kyi of tax evasion for spending her Nobel Prize money outside of the country. The accusation followed the defeat of a US-sponsored United Nations Security Council resolution condemning Burma as a threat to international security; the resolution was defeated because of strong opposition from China, which has strong ties with the military junta (China later voted against the resolution, along with Russia and South Africa).
In November 2007, it was reported that Suu Kyi would meet her political allies National League for Democracy along with a government minister. The ruling junta made the official announcement on state TV and radio just hours after UN special envoy Ibrahim Gambari ended his second visit to Burma. The NLD confirmed that it had received the invitation to hold talks with Suu Kyi. However, the process delivered few concrete results.
On 3 July 2009, UN Secretary General Ban Ki-moon went to Burma to pressure the junta into releasing Suu Kyi and to institute democratic reform. However, on departing from Burma, Ban Ki-moon said he was "disappointed" with the visit after junta leader Than Shwe refused permission for him to visit Suu Kyi, citing her ongoing trial. Ban said he was "deeply disappointed that they have missed a very important opportunity."
2007 anti-government protests.
Protests led by Buddhist monks began on 19 August 2007 following steep fuel price increases, and continued each day, despite the threat of a crackdown by the military.
On 22 September 2007, although still under house arrest, Suu Kyi made a brief public appearance at the gate of her residence in Yangon to accept the blessings of Buddhist monks who were marching in support of human rights. It was reported that she had been moved the following day to Insein Prison (where she had been detained in 2003), but meetings with UN envoy Ibrahim Gambari near her Rangoon home on 30 September and 2 October established that she remained under house arrest.
2009 trespass incident.
On 3 May 2009, an American man, identified as John Yettaw, swam across Inya Lake to her house uninvited and was arrested when he made his return trip three days later. He had attempted to make a similar trip two years earlier, but for unknown reasons was turned away. He later claimed at trial that he was motivated by a divine vision requiring him to notify her of an impending terrorist assassination attempt. On 13 May, Suu Kyi was arrested for violating the terms of her house arrest because the swimmer, who pleaded exhaustion, was allowed to stay in her house for two days before he attempted the swim back. Suu Kyi was later taken to Insein Prison, where she could have faced up to five years confinement for the intrusion. The trial of Suu Kyi and her two maids began on 18 May and a small number of protesters gathered outside. Diplomats and journalists were barred from attending the trial; however, on one occasion, several diplomats from Russia, Thailand and Singapore and journalists were allowed to meet Suu Kyi. The prosecution had originally planned to call 22 witnesses. It also accused John Yettaw of embarrassing the country. During the ongoing defence case, Suu Kyi said she was innocent. The defence was allowed to call only one witness (out of four), while the prosecution was permitted to call 14 witnesses. The court rejected two character witnesses, NLD members Tin Oo and Win Tin, and permitted the defence to call only a legal expert. According to one unconfirmed report, the junta was planning to, once again, place her in detention, this time in a military base outside the city. In a separate trial, Yettaw said he swam to Suu Kyi's house to warn her that her life was "in danger". The national police chief later confirmed that Yettaw was the "main culprit" in the case filed against Suu Kyi. According to aides, Suu Kyi spent her 64th birthday in jail sharing biryani rice and chocolate cake with her guards.
Her arrest and subsequent trial received worldwide condemnation by the UN Secretary General Ban Ki-moon, the United Nations Security Council, Western governments, South Africa, Japan and the Association of Southeast Asian Nations, of which Burma is a member. The Burmese government strongly condemned the statement, as it created an "unsound tradition" and criticised Thailand for meddling in its internal affairs. The Burmese Foreign Minister Nyan Win was quoted in the state-run newspaper "New Light of Myanmar" as saying that the incident "was trumped up to intensify international pressure on Burma by internal and external anti-government elements who do not wish to see the positive changes in those countries' policies toward Burma". Ban responded to an international campaign by flying to Burma to negotiate, but Than Shwe rejected all of his requests.
On 11 August 2009 the trial concluded with Suu Kyi being sentenced to imprisonment for three years with hard labour. This sentence was commuted by the military rulers to further house arrest of 18 months. On 14 August, U.S. Senator Jim Webb visited Burma, visiting with junta leader Gen. Than Shwe and later with Suu Kyi. During the visit, Webb negotiated Yettaw's release and deportation from Burma. Following the verdict of the trial, lawyers of Suu Kyi said they would appeal against the 18-month sentence. On 18 August, United States President Barack Obama asked the country's military leadership to set free all political prisoners, including Aung San Suu Kyi. In her appeal, Aung San Suu Kyi had argued that the conviction was unwarranted. However, her appeal against the August sentence was rejected by a Burmese court on 2 October 2009. Although the court accepted the argument that the 1974 constitution, under which she had been charged, was null and void, it also said the provisions of the 1975 security law, under which she has been kept under house arrest, remained in force. The verdict effectively meant that she would be unable to participate in the elections scheduled to take place in 2010 – the first in Burma in two decades. Her lawyer stated that her legal team would pursue a new appeal within 60 days.
2009: International pressure for release and 2010 Burmese general election.
It was announced prior to the Burmese general election that Aung San Suu Kyi may be released "so she can organize her party," However, Suu Kyi was not allowed to run. On 1 October 2010 the government announced that she would be released on 13 November 2010.
U.S. President Barack Obama personally advocated the release of all political prisoners, especially Aung San Suu Kyi, during the US-ASEAN Summit of 2009.
The U. S. Government hoped that successful general elections would be an optimistic indicator of the Burmese government's sincerity towards eventual democracy. The Hatoyama government which spent 2.82 billion yen in 2008, has promised more Japanese foreign aid to encourage Burma to release Aung San Suu Kyi in time for the elections; and to continue moving towards democracy and the rule of law.
In a personal letter to Suu Kyi, UK Prime Minister Gordon Brown cautioned the Burmese government of the potential consequences of rigging elections as "condemning Burma to more years of diplomatic isolation and economic stagnation".
Suu Kyi has met with many heads of state, and opened a dialog with the Minister of Labor Aung Kyi (not to be confused with Aung San Suu Kyi). She was allowed to meet with senior members of her NLD party at the State House, however these meetings took place under close supervision.
2010 release.
On the evening of 13 November 2010, Suu Kyi was released from house arrest. This was the date her detention had been set to expire according to a court ruling in August 2009 and came six days after a widely criticised general election. She appeared in front of a crowd of her supporters, who rushed to her house in Rangoon when nearby barricades were removed by the security forces. The Nobel Peace Prize laureate had been detained for 15 of the past 21 years. The government newspaper "New Light of Myanmar" reported the release positively, saying she had been granted a pardon after serving her sentence "in good conduct". The New York Times suggested that the military government may have released Suu Kyi because it felt it was in a confident position to control her supporters after the election. The role that Suu Kyi will play in the future of democracy in Burma remains a subject of much debate.
Her son Kim Aris was granted a visa in November 2010 to see his mother shortly after her release, for the first time in 10 years. He visited again in 5 July 2011, to accompany her on a trip to Bagan, her first trip outside Yangon since 2003. Her son visited again in 8 August 2011, to accompany her on a trip to Pegu, her second trip.
Discussions were held between Suu Kyi and the Burmese government during 2011, which led to a number of official gestures to meet her demands. In October, around a tenth of Burma's political prisoners were freed in an amnesty and trade unions were legalised.
In November 2011, following a meeting of its leaders, the NLD announced its intention to re-register as a political party in order to contend 48 by-elections necessitated by the promotion of parliamentarians to ministerial rank. Following the decision, Suu Kyi held a telephone conference with U.S. President Barack Obama, in which it was agreed that Secretary of State Hillary Clinton would make a visit to Burma, a move received with caution by Burma's ally China. On 1 December 2011, Suu Kyi met with Hillary Clinton at the residence of the top-ranking US diplomat in Yangon.
On 21 December 2011, Thai Prime Minister Yingluck Shinawatra met Suu Kyi in Yangoon, becoming Suu Kyi's "first-ever meeting with the leader of a foreign country".
On 5 January 2012, British Foreign Minister William Hague met Aung San Suu Kyi and his Burmese counterpart. This represented a significant visit for Suu Kyi and Burma. Suu Kyi studied in the UK and maintains many ties there, whilst Britain is Burma's largest bilateral donor.
Aung San Suu Kyi is on her visit to Europe and is due to visit the Swiss parliament and collect her 1991 Nobel Prize in Oslo.
2012 by-elections.
In December 2011, there was speculation that Suu Kyi would run in the 2012 national by-elections to fill vacant seats. On 18 January 2012, Suu Kyi formally registered to contest a Pyithu Hluttaw (lower house) seat in the Kawhmu Township constituency in special parliamentary elections to be held on 1 April 2012. The seat was previously held by Soe Tint, who vacated it after being appointed Construction Deputy Minister, in the 2010 election. She ran against Union Solidarity and Development Party candidate Soe Min, a retired army physician and native of Twante Township.
On 3 March 2012, at a large campaign rally in Mandalay, Suu Kyi unexpectedly left after 15 minutes, because of exhaustion and airsickness.
In an official campaign speech broadcast on Burmese state television's MRTV on 14 March 2012, Suu Kyi publicly campaigned for reform of the 2008 Constitution, removal of restrictive laws, more adequate protections for people's democratic rights, and establishment of an independent judiciary. The speech was leaked online a day before it was broadcast. A paragraph in the speech, focusing on the Tatmadaw's repression by means of law, was censored by authorities.
Suu Kyi has also called for international media to monitor the upcoming by-elections, while publicly pointing out irregularities in official voter lists, which include deceased individuals and exclude other eligible voters in the contested constituencies. On 21 March 2012, Aung San Suu Kyi was quoted as saying "Fraud and rule violations are continuing and we can even say they are increasing."
When asked whether she would assume a ministerial post if given the opportunity, she said the following:
On 26 March 2012, Suu Kyi suspended her nationwide campaign tour early, after a campaign rally in Myeik (Mergui), a coastal town in the south, citing health problems due to exhaustion and hot weather.
On 1 April 2012, the NLD announced that Suu Kyi had won the vote for a seat in Parliament. A news broadcast on state-run MRTV, reading the announcements of the Union Election Commission, confirmed her victory, as well as her party's victory in 43 of the 45 contested seats, officially making Suu Kyi the Leader of the Opposition in the lower house.
Although she and other MP-elects were expected to take office on 23 April when the Hluttaws resume session, National League for Democracy MP-elects, including Suu Kyi, said they might not take their oaths because of its wording; in its present form, parliamentarians must vow to "safeguard" the constitution. In an address on Radio Free Asia, she said "We don't mean we will not attend the parliament, we mean we will attend only after taking the oath... Changing that wording in the oath is also in conformity with the Constitution. I don't expect there will be any difficulty in doing it."
On 2 May 2012, National League for Democracy MP-elects, including Aung San Suu Kyi, took their oaths and took office, though the wording of the oath was not changed. According to the Los Angeles Times, "Suu Kyi and her colleagues decided they could do more by joining as lawmakers than maintaining their boycott on principle."
On 9 July 2012, she attended the Parliament for the first time as a lawmaker.
On 6 July 2012, Suu Kyi announced on the World Economic Forum’s website that she wants to run for the presidency in Myanmar's 2015 elections. The current Constitution, which came into effect in 2008, bars her from the presidency because she is the widow and mother of foreigners. These measures seem to have been written in part to prevent her ever getting there.
Rohingya controversy.
Some activists criticised Aung San Suu Kyi for her silence on the 2012 Rakhine State riots. After receiving a peace prize, she told reporters she did not know if the Rohingya could be regarded as Burmese citizens. Under the 1982 Citizenship Law, most Rohingya are unable to qualify for Burmese citizenship. As such, they are treated as illegal immigrants, with restrictions on their movement and withholding of land rights, education and public service. Some describe her stance as politically motivated. However she said that she wanted to work towards reconciliation and that she cannot take sides as "violence has been committed by both sides." According to The Economist, her "halo has even slipped among foreign human-rights lobbyists, disappointed at her failure to make a clear stand on behalf of the Rohingya minority."
However, she has spoken out "against a ban on Rohingya families near the Bangladeshi border having more than two children."
Political belief.
Asked what democratic models Myanmar could look to, she said: "We have many, many lessons to learn from various places, not just the Asian countries like South Korea, Taiwan, Mongolia and Indonesia." She also cited "the eastern European countries, which made the transition from communist autocracy to democracy in the 1980s and 1990s, and the Latin American countries, which made the transition from military governments. "And we cannot of course forget South Africa, because although it wasn't a military regime, it was certainly an authoritarian regime." She added: "We wish to learn from everybody who has achieved a transition to democracy, and also ... our great strong point is that, because we are so far behind everybody else, we can also learn which mistakes we should avoid."
In a nod to the current deep US political divide between Republicans led by Mitt Romney and the Democrats of Obama—battling to win the Presidential election on 6 November—she stressed with a smile "Those of you who are familiar with American politics I'm sure understand the need for negotiated compromise."
International support.
Aung San Suu Kyi has received vocal support from Western nations in Europe, Australia and North and South America, as well as India, Israel, Japan the Philippines and South Korea. In December 2007, the US House of Representatives voted unanimously 400–0 to award Aung San Suu Kyi the Congressional Gold Medal; the Senate concurred on 25 April 2008. On 6 May 2008, President George Bush signed legislation awarding Suu Kyi the Congressional Gold Medal. She is the first recipient in American history to receive the prize while imprisoned. More recently, there has been growing criticism of her detention by Burma's neighbours in the Association of Southeast Asian Nations, particularly from Indonesia, Thailand, the Philippines and Singapore. At one point Malaysia warned Burma that it faced expulsion from ASEAN as a result of the detention of Suu Kyi. Other nations including South Africa, Bangladesh and the Maldives also called for her release. The United Nations has urged the country to move towards inclusive national reconciliation, the restoration of democracy, and full respect for human rights. In December 2008, the United Nations General Assembly passed a resolution condemning the human rights situation in Burma and calling for Suu Kyi's release—80 countries voting for the resolution, 25 against and 45 abstentions. Other nations, such as China and Russia, are less critical of the regime and prefer to cooperate only on economic matters. Indonesia has urged China to push Burma for reforms. However, Samak Sundaravej, former Prime Minister of Thailand, criticised the amount of support for Suu Kyi, saying that "Europe uses Aung San Suu Kyi as a tool. If it's not related to Aung San Suu Kyi, you can have deeper discussions with Myanmar."
Vietnam, however, did not support calls by other ASEAN member states for Myanmar to free Aung San Suu Kyi, state media reported Friday, 14 August 2009. The state-run Việt Nam News said Vietnam had no criticism of Myanmar's decision 11 August 2009 to place Suu Kyi under house arrest for the next 18 months, effectively barring her from elections scheduled for 2010. "It is our view that the Aung San Suu Kyi trial is an internal affair of Myanmar", Vietnamese government spokesman Le Dung stated on the website of the Ministry of Foreign Affairs. In contrast with other ASEAN member states, Dung said Vietnam has always supported Myanmar and hopes it will continue to implement the "roadmap to democracy" outlined by its government.
Aung San Suu Kyi was awarded the Nobel Peace Prize in 1991. The decision of the Nobel Committee mentions:
In 1995 Aung San Suu Kyi delivered the keynote address at the Fourth World Conference on Women in Beijing.
Nobel Peace Prize winners (Archbishop Desmond Tutu, the Dalai Lama, Shirin Ebadi, Adolfo Pérez Esquivel, Mairead Corrigan, Rigoberta Menchú, Prof. Elie Wiesel, U.S. President Barack Obama, Betty Williams, Jody Williams and former U.S. President Jimmy Carter) called for the rulers of Burma to release Suu Kyi in order to "create the necessary conditions for a genuine dialogue with Daw Aung San Suu Kyi and all concerned parties and ethnic groups in order to achieve an inclusive national reconciliation with the direct support of the United Nations." Some of the money she received as part of the award helps fund London-based charity Prospect Burma, which provides higher education grants to Burmese students.
On 16 June 2012, Aung San Suu Kyi was finally able to deliver her Nobel acceptance speech (Nobel lecture) at Oslo's City Hall, two decades after being awarded the peace prize.
In September 2012, Aung San Suu Kyi received in person the United States Congressional Gold Medal, which is the highest Congressional award. Although she was awarded this medal in 2008, at the time she was under house arrest, and was unable to receive the medal. Aung San Suu Kyi was greeted with bipartisan support at Congress, as part of a coast-to-coast tour in the United States. In addition, Aung San Suu Kyi met President Barack Obama at the White House. The experience was described by Aung San Suu Kyi as "one of the most moving days of my life."
As of 2014, she is listed as the 61st most powerful woman in the world by "Forbes".
Health problems.
She had surgery for a gynecological condition in September 2003 at Asia Royal Hospital during her house arrest. She had undergone a minor foot surgery in December 2013. Her doctor said that she has no serious health problems but weighs only 48 kg, has low blood pressure and can become weak easily.

</doc>
<doc id="2851" url="http://en.wikipedia.org/wiki?curid=2851" title="Abraham Joshua Heschel">
Abraham Joshua Heschel

Abraham Joshua Heschel (January 11, 1907 – December 23, 1972) was a Polish-born American rabbi and one of the leading Jewish theologians and Jewish philosophers of the 20th century. Heschel, a professor of Jewish mysticism at the Jewish Theological Seminary of America, authored a number of widely read books on Jewish philosophy and was active in the American Civil Rights movement.
Biography.
Abraham Joshua Heschel was born in 1907 as the youngest of six children of Moshe Mordechai and Reizel Perlow. He was descended from preeminent European rabbis on both sides of his family. His paternal great-great-grandfather and namesake was Rebbe Avraham Yehoshua Heshel of Apt in present-day Poland. His mother was also a descendant of Avraham Yehoshua Heshel and other Hasidic dynasties. His siblings were Sarah, Dvora Miriam, Esther Sima, Gittel, and Jacob. Their father Moshe died of influenza in 1916 when Abraham was nine. 
After a traditional yeshiva education and studying for Orthodox rabbinical ordination semicha, Heschel pursued his doctorate at the University of Berlin and a liberal rabbinic ordination at the Hochschule für die Wissenschaft des Judentums. There he studied under some of the finest Jewish educators of the time: Chanoch Albeck, Ismar Elbogen, Julius Guttmann, and Leo Baeck. Heschel later taught Talmud there. He joined a Yiddish poetry group, Jung Vilna, and in 1933, published a volume of Yiddish poems, "Der Shem Hamefoyrosh: Mentsch," dedicated to his father.
In late October 1938, when Heschel was living in a rented room in the home of a Jewish family in Frankfurt, he was arrested by the Gestapo and deported to Poland. He spent ten months lecturing on Jewish philosophy and Torah at Warsaw's Institute for Jewish Studies. Six weeks before the German invasion of Poland, Heschel left Warsaw for London with the help of Julian Morgenstern, president of Hebrew Union College, who had been working to obtain visas for Jewish scholars in Europe.
Heschel's sister Esther was killed in a German bombing. His mother was murdered by the Nazis, and two other sisters, Gittel and Devorah, died in Nazi concentration camps. He never returned to Germany, Austria or Poland. He once wrote, "If I should go to Poland or Germany, every stone, every tree would remind me of contempt, hatred, murder, of children killed, of mothers burned alive, of human beings asphyxiated."
Heschel arrived in New York City in March 1940. He served on the faculty of Hebrew Union College (HUC), the main seminary of Reform Judaism, in Cincinnati for five years. In 1946, he took a position at the Jewish Theological Seminary of America (JTS) in New York City, the main seminary of Conservative Judaism. He served as professor of Jewish ethics and Mysticism until his death in 1972.
Marriage and family.
Heschel married Sylvia Straus, a concert pianist, on December 10, 1946, in Los Angeles. Their daughter, Susannah Heschel, became a Jewish scholar in her own right. Heschel's papers are held in the Rubenstein Rare Book & Manuscript Library at Duke University.
Ideology.
Heschel explicated many facets of Jewish thought, including studies on medieval Jewish philosophy, Kabbalah, and Hasidism. According to some scholars, he was more interested in spirituality than in critical text study; the latter was a specialty of many scholars at JTS. He was not given a graduate assistant for many years and was relegated to teach mainly in the education school or Rabbinical school, not in the academic graduate program. Heschel became quite friendly with his colleague Mordecai Kaplan. Though they differed in their approach to Judaism, they had a very cordial relationship and visited each other's homes from time to time.
Heschel believed the teachings of the Hebrew prophets were a clarion call for social action in the United States and worked for African Americans' civil rights and against the Vietnam War 
He also specifically criticized what he called "pan-halakhism", or an exclusive focus upon religiously compatible behavior to the neglect of the non-legalistic dimension of rabbinic tradition.
Influence outside Judaism.
Heschel is a widely read Jewish theologian whose most influential works include "Man Is Not Alone", "God in Search of Man", "The Sabbath," and "The Prophets". At the Vatican Council II, as representative of American Jews, Heschel persuaded the Roman Catholic Church to eliminate or modify passages in its liturgy that demeaned the Jews, or referred to an expected conversion to Christianity. His theological works argued that religious experience is a fundamentally human impulse, not just a Jewish one. He believed that no religious community could claim a monopoly on religious truth.
Published work.
"The Prophets" (1962).
In his book "The Prophets", Abraham Joshua Heschel describes the unique aspect of the Jewish prophets as compared to other similar figures. Whereas other nations have soothsayers and diviners who attempt to discover the will of their gods, according to Heschel the Hebrew prophets are characterized by their experience of what he calls theotropism—God turning towards humanity. Heschel argues for the view of Hebrew prophets as receivers of the "Divine Pathos", of the wrath and sorrow of God over his nation that has forsaken him. In this view, prophets do not speak for God so much as they remind their audience of God's voice for the voiceless, the poor and oppressed.
He writes:
Commemoration.
Four schools have been named for Heschel, in the Upper West Side of New York City, Northridge, California, Agoura Hills, California, and Toronto, Canada. In 2009, a highway in Missouri was named "Dr. Abraham Joshua Heschel Highway" after a Springfield, Missouri area Neo-Nazi group cleaned the stretch of highway as part of an "Adopt-A-Highway" plan. Heschel's daughter, Susannah, has objected to the adoption of her father's name in this context.

</doc>
<doc id="2853" url="http://en.wikipedia.org/wiki?curid=2853" title="Aberdeen Bestiary">
Aberdeen Bestiary

The Aberdeen Bestiary (Aberdeen University Library, Univ Lib. MS 24) is a 12th-century English illuminated manuscript bestiary that was first listed in 1542 in the inventory of the Old Royal Library at the Palace of Westminster. 
Information about its origins and patron are circumstantial. It probably comes from the 12th century and was owned by an ecclesiastical patron of the north or south province. The "Aberdeen Bestiary" is related to other bestiaries of the Middle Ages and especially the "Ashmole Bestiary". Some argue that the "Aberdeen Bestiary" might be the older of the two.
Beasts ("Bestiae").
After folio 9 verso some leaves are missing which should have contained Antelope ("Antalops"), Unicorn ("Unicornis"), Lynx ("Lynx"), Griffin ("Gryps") and part of Elephant ("Elephans").
After folio 15 verso some leaves are missing which should have contained Crocodile ("Crocodilus"), Manticore ("Mantichora") and part of Parandrus ("Parandrus").
Livestocks ("Pecora").
After folio 21 verso two leaves are missing which should have contained Ox ("Bos"), Camel ("Camelus"), Dromedary ("Dromedarius"), Ass ("Asinus"), Onager ("Onager") and part of Horse ("Equus").

</doc>
<doc id="2856" url="http://en.wikipedia.org/wiki?curid=2856" title="Latin American Integration Association">
Latin American Integration Association

The Latin American Integration Association / Asociación Latinoamericana de Integración / Associação Latino-Americana de Integração (LAIA / ALADI) is an international and regional scope organization. It was created on 12 August 1980 by the 1980 Montevideo Treaty, replacing the Latin American Free Trade Association (LAFTA / ALALC). Currently, it has 13 member countries, and any of the Latin American States may apply for accession.
Objectives.
The development of the integration process developed within the framework of the ALADI aims at promoting the harmonious and balanced socio-economic development of the region, and its long-term objective is the gradual and progressive establishment of a Latin-American Common Market.
Integration Mechanisms.
The ALADI promotes the establishment of an area of economic preferences within the region, in order to create a Latin-American common market, through three mechanisms:
The Relatively Less Economically Developed Countries of the region (Bolivia, Ecuador and Paraguay) benefit from a preferential system, through the lists of markets opening offered by the countries in favor of the Relatively Less Economically Developed Countries; special programs of cooperation (business rounds, pre-investment, financing, technological support); and countervailing measures in favor of the land-locked countries, the full participation of such countries in the integration process is sought.
The ALADI includes in its legal structure the strongest sub-regional, plurilateral and bilateral integration agreements arising in growing numbers in the continent. As a result, the ALADI – as an institutional and legal framework or “umbrella” of the regional integration- develops actions in order to support and foster these efforts for the progressive establishment of a common economic space.
Accession of other Latin-American countries.
The 1980 Montevideo Treaty is open to the accession of any Latin-American country. 
On 26 August 1999, the first accession to the 1980 Montevideo Treaty was executed, with the incorporation of the Republic of Cuba as a member country of the ALADI. 
On 10 May 2012, the Republic of Panama became the thirteenth member country of the ALADI. 
Likewise, the accession of the Republic of Nicaragua was accepted in the Sixteenth Meeting of the Council of Ministers (Resolution 75 (XVI)), held on 11 August 2011. Currently, Nicaragua moves towards the fulfillment of conditions for becoming a member country of the ALADI.
The ALADI opens its field of actions for the rest of Latin America through multilateral links or partial agreements with other countries and integration areas of the continent (Article 25).
The Latin-American Integration Association also contemplates the horizontal cooperation with other integration movements in the world and partial actions with third developing countries or their respective integration areas (Article 27).
Institutional Structure.
The Council of Ministers is the supreme body of the ALADI, and adopts the decisions for the superior political management of the integration process. 
It is constituted by the Ministers of Foreign Affairs of the member countries. Notwithstanding, when one of such member countries assigns the competence of the integration affairs to a different Minister or Secretary of State, the member countries may be represented, with full powers, by the respective Minister or Secretary. It is convened by the Committee of Representatives, meets and makes decisions with the presence of all the member countries.
It is in charge, among others, of analyzing the functioning of the integration process in all its aspects, promoting the convergence of the partial scope agreements seeking their progressive multilateralization, and promoting greater scope actions as regards economic integration. It is made up of Plenipotentiaries of the member countries.
It is the permanent political body and negotiating forum of the ALADI, where all the initiatives for the fulfillment of the objectives established by the 1980 Montevideo Treaty are analyzed and agreed on. It is composed of a Permanent Representative of each member country with right to one vote and an Alternate Representative. It meets regularly every 15 days and its Resolutions are adopted by the affirmative vote of two thirds of the member countries.
It is the technical body of the ALADI, and it may propose, evaluate, study and manage for the fulfillment of the objectives of the ALADI. It is composed of technical and administrative personnel, and directed by a Secretary-General, who has the support of two Undersecretaries, elected for a three-year period, renewable for the same term.

</doc>
<doc id="2858" url="http://en.wikipedia.org/wiki?curid=2858" title="Aircraft spotting">
Aircraft spotting

Aircraft spotting or plane spotting is the observation, photographing aircraft, and logging of the registration numbers of aircraft: gliders, powered aircraft, balloons, airships, helicopters, and microlights.
When spotting aircraft, observers notice the key attributes of an aircraft. They may notice a distinctive noise from its engine or the number of vapour trails it is leaving. They will assess the size of the aircraft and the number, type and position of its engines. Another clue is the position of wings relative to the fuselage and the degree to which they are swept rearwards. Are the wings above the fuselage, below it, or fixed at midpoint? Perhaps it is a monoplane, biplane, or triplane. The position of the tailplane relative to the fin(s) and the shape of the fin are also clues to its type. If it is an antique or light aircraft it might have a tail wheel. Some aircraft types have a fixed undercarriage while others have retractable wheels.
Other features include the speed, cockpit placement, colour scheme or special equipment that changes the silhouette of the aircraft. Taken together these clues will enable the identification of an aircraft. If the observer is familiar with the airfield being used by the aircraft and its normal traffic patterns, he or she is more likely to leap quickly to a decision about the aircraft's identity - they may have seen the same type of aircraft from the same angle many times.
Spotters use equipment such as ADS-B decoders to track the movements of aircraft. The two most famous devices used are the AirNav Systems RadarBox and Kinetic Avionics SBS series. Both of them read and process the radar data and show the movements on a computer screen. Most of the decoders also allow the exporting of logs from a certain route or airport.
Spotting styles.
Some spotters will note and compile the markings, a national insignia or airline livery or logo, a squadron badge or code letters in the case of a military aircraft. Published manuals allow more information to be deduced, such as the delivery date or the manufacturer's construction number. Camouflage markings differ, depending on the surroundings in which that aircraft is expected to operate.
In general, most spotters attempt to see as many aircraft of a given type (i.e.: Boeing 747), a particular airline (American Airlines for example) or a particular subset of aircraft such as business jets (a.k.a. Biz Jets), Commercial Airliners, Military and/or general aviation. Some spotters attempt to see every airframe and are known as "frame spotters." Others are keen to see every registration worn by each aircraft. 
Ancillary activities might include listening-in to air traffic transmissions (using radio scanners, where that is legal), liaising with other "spotters" to clear up uncertainties as to what aircraft have been seen at specific times or in particular places. Several internet mailing list groups have been formed to help communicate aircraft seen at airports, queries and anomalies. These groups can cater to certain regions (North American Spotters), certain aircraft types (The Biz List) or may appeal to a wider audience such as the Civil Spotters group. Many of these groups originated from the original Oxford.vax group which pioneered this type of communication. The result is that information on aircraft movements can be delivered worldwide in a real-time fashion to spotters.
The hobbyist might travel long distances to visit a different airport from their usual one, to see an unusual aircraft or to view the remains of aircraft withdrawn from use. Some aircraft may be placed in the care of museums (see Aviation archaeology) - or perhaps be cannibalized in order to repair a similar aircraft already preserved.
Aircraft registrations can be found in books, with online resources or in monthly magazines from enthusiast groups. Most spotters maintained books of different aircraft fleets and would underline or check each aircraft seen. Each year, a revised version of the books would be published and the spotter would need to re-underline every aircraft seen. With the development of commercial aircraft databases the spotter was finally able to record their sightings in an electronic database and produce reports that emulated the underlined books.
During hostilities.
During World War II and the subsequent Cold War some countries encouraged their citizens to become "plane spotters" in an "observation corps" or similar public body for reasons of public security. Britain had the Royal Observer Corps which operated between 1925 and 1995. A journal called The Aeroplane Spotter was published in January 1940. The publication included a glossary that was refined in 2010 and published online (see http://www.aeroplanemonthly.co.uk/glossary/).
Air shows.
Air shows usually draw large numbers of spotters as it is a chance to enter airfields and air bases worldwide that are usually closed to the public and to see displayed aircraft at close range. The world's largest military airshow is the Royal International Air Tattoo in the United Kingdom.
Legal ramifications.
The legal repercussions of the hobby were dramatically shown in November 2001 when fourteen aircraft spotters (twelve British, two Dutch) were arrested by Greek police after being observed at an open day at the Greek Air Force base at Kalamata. They were charged with espionage, and faced a possible 20-year prison sentence if found guilty. After being held for six weeks, they were eventually released on £9,000 bail, and the charges reduced to the misdemeanour charge of illegal information collection. Confident of their innocence they returned for their trial in April 2002 and were stunned to be found guilty, with eight of the group sentenced to three years, the rest for one year. At their appeal a year later all were acquitted.
Fight against terrorism.
In the wake of the targeting of airports by terrorists, enthusiasts' organizations and police in the UK have cooperated in creating a code of conduct for plane spotters. By asking enthusiasts to contact police if spotters believe they see or hear something suspicious, this is an attempt to allow enthusiasts to continue their hobby while increasing security around airports. Birmingham and Stansted pioneered this approach in England and prior to the 2012 London Olympics, RAF Northolt introduced a "Flightwatch" scheme based on the same cooperative principles.
The organization of such groups has now been echoed in parts of North America. For example, the Bensenville Illinois Police Department have sponsored an "Airport Watch" group at the Chicago O'Hare Airport. Members are issued identification cards and given training to accurately record and report unusual activities around the airport perimeter (members are not permitted airside). Meetings are attended and supported by the F.B.I., Chicago Department of Aviation and the T.S.A who also provide regular training to group members. The Bensenville program was modeled on similar programs in Toronto, Ottawa and Minneapolis.
Extraordinary rendition.
Following the events of 9/11 information collected by planespotters helped uncover what is known as "extraordinary rendition" by the CIA. Information on unusual movements of rendition aircraft provided data which led first to news reports and then to a number of governmental and inter-governmental investigations.

</doc>
<doc id="2861" url="http://en.wikipedia.org/wiki?curid=2861" title="Advertising">
Advertising

Advertising or advertizing in business is a form of marketing communication used to encourage, persuade, or manipulate an audience (viewers, readers or listeners; sometimes a specific group) to take or continue to take some action. Most commonly, the desired result is to drive consumer behavior with respect to a commercial offering, although political and ideological advertising is also common. This type of work belongs to a category called affective labor.
In Latin, ad vertere means "to turn toward". The purpose of advertising may also be to reassure employees or shareholders that a company is viable or successful. Advertising messages are usually paid for by sponsors and viewed via various old media; including mass media such as newspaper, magazines, television advertisement, radio advertisement, outdoor advertising or direct mail; or new media such as blogs, websites or text messages.
Commercial advertisers often seek to generate increased consumption of their products or services through "branding", which involves associating a product name or image with certain qualities in the minds of consumers. Non-commercial advertisers who spend money to advertise items other than a consumer product or service include political parties, interest groups, religious organizations and governmental agencies. Nonprofit organizations may rely on free modes of persuasion, such as a public service announcement (PSA).
Modern advertising was created with the innovative techniques introduced with tobacco advertising in the 1920s, most significantly with the campaigns of Edward Bernays, which is often considered the founder of modern, Madison Avenue advertising.
In 2010, spending on advertising was estimated at $143 billion in the United States and $467 billion worldwide
Internationally, the largest ("big four") advertising conglomerates are Interpublic, Omnicom, Publicis, and WPP.
History.
Egyptians used papyrus to make sales messages and wall posters. Commercial messages and political campaign displays have been found in the ruins of Pompeii and ancient Arabia. Lost and found advertising on papyrus was common in Ancient Greece and Ancient Rome. Wall or rock painting for commercial advertising is another manifestation of an ancient advertising form, which is present to this day in many parts of Asia, Africa, and South America. The tradition of wall painting can be traced back to Indian rock art paintings that date back to 4000 BCE.
In ancient China, the earliest advertising known was oral, as recorded in the Classic of Poetry (11th to 7th centuries BC) of bamboo flutes played to sell candy. Advertisement usually takes in the form of calligraphic signboards and inked papers. A copper printing plate dated back to the Song dynasty used to print posters in the form of a square sheet of paper with a rabbit logo with "Jinan Liu’s Fine Needle Shop" and "We buy high quality steel rods and make fine quality needles, to be ready for use at home in no time" written above and below [http://depts.washington.edu/chinaciv/graph/tcommain.htm] is considered the world's earliest identified printed advertising medium.
In Europe, as the towns and cities of the Middle Ages began to grow, and the general populace was unable to read, instead of signs that read "cobbler", "miller", "tailor", or "blacksmith" would use an image associated with their trade such as a boot, a suit, a hat, a clock, a diamond, a horse shoe, a candle or even a bag of flour. Fruits and vegetables were sold in the city square from the backs of carts and wagons and their proprietors used street callers (town criers) to announce their whereabouts for the convenience of the customers.
In the 18th century advertisements started to appear in weekly newspapers in England. These early print advertisements were used mainly to promote books and newspapers, which became increasingly affordable with advances in the printing press; and medicines, which were increasingly sought after as disease ravaged Europe. However, false advertising and so-called "quack" advertisements became a problem, which ushered in the regulation of advertising content.
19th century.
Thomas J. Barratt from London has been called "the father of modern advertising". Working for the Pears Soap company, Barratt created an effective advertising campaign for the company products, which involved the use of targeted slogans, images and phrases. One of his slogans, ""Good morning. Have you used Pears' soap?" was famous in its day and into the 20th century. Under Barratt's guidance, Pears Soap became the world's first legally registered brand.
An advertising tactic that he used was to associate the Pears brand with high culture and quality. Most famously, he used the painting "Bubbles" by John Everett Millais as an advertisement by adding a bar of Pears soap into the foreground. (Millais protested at this alteration of his work, but in vain as Barrat had bought the copyright.) Barratt continued this theme with a series of adverts of well groomed middle-class children, associating Pears with domestic comfort and aspirations of high society.
Barrat established "Pears Annual" in 1891 as a spin-off magazine which promoted contemporary illustration and colour printing and in 1897 added the "Pears Cyclopedia" a one-volume encyclopedia. From the early 20th century Pears was famous for the annual "Miss Pears" competition in which parents entered their children into the high-profile hunt for a young brand ambassador to be used on packaging and in consumer promotions. He recruited scientists and the celebrities of the day to publicly endorse the product. Lillie Langtry, a British music hall singer and stage actress with a famous ivory complexion, received income as the first woman to endorse a commercial product, advertising Pears Soap.
Barratt introduced many of the crucial ideas that lie behind successful advertising and these were widely circulated in his day. He constantly stressed the importance of a strong and exclusive brand image for Pears and of emphasizing the product's availability through saturation campaigns. He also understood the importance of constantly reevaluating the market for changing tastes and mores, stating in 1907 that "tastes change, fashions change, and the advertiser has to change with them. An idea that was effective a generation ago would fall flat, stale, and unprofitable if presented to the public today. Not that the idea of today is always better than the older idea, but it is different – it hits the present taste."
As the economy expanded across the world during the 19th century, advertising grew alongside. In the United States, the success of this advertising format eventually led to the growth of mail-order advertising.
In June 1836, French newspaper "La Presse" was the first to include paid advertising in its pages, allowing it to lower its price, extend its readership and increase its profitability and the formula was soon copied by all titles. Around 1840, Volney B. Palmer established the roots of the modern day advertising agency in Philadelphia. In 1842 Palmer bought large amounts of space in various newspapers at a discounted rate then resold the space at higher rates to advertisers. The actual ad – the copy, layout, and artwork – was still prepared by the company wishing to advertise; in effect, Palmer was a space broker. The situation changed in the late 19th century when the advertising agency of N.W. Ayer & Son was founded. Ayer and Son offered to plan, create, and execute complete advertising campaigns for its customers. By 1900 the advertising agency had become the focal point of creative planning, and advertising was firmly established as a profession.
20th century.
Advertising increased dramatically in the United States as industrialization expanded the supply of manufactured products. In order to profit from this higher rate of production, industry needed to recruit workers as consumers of factory products. It did so through the invention of mass marketing designed to influence the population's economic behavior on a larger scale. In the 1910s and 1920s, advertisers in the U.S. adopted the doctrine that human instincts could be targeted and harnessed – "sublimated" into the desire to purchase commodities. Edward Bernays, a nephew of Sigmund Freud, became associated with the method and is now often considered the founder of modern advertising.
The tobacco industry was one of the firsts to make use of mass production, with the introduction of the Bonsack machine to roll cigarettes. The Bonsack machine allowed the production of cigarettes for a mass markets, and the tobacco industry needed to match such an increase in supply with the creation of a demand from the masses through advertising. The tobacco companies pioneered the new advertising techniques when they hired Bernays to create positive associations with tobacco smoking.
Advertising was also used as a vehicle for cultural assimilation, encouraging workers to exchange their traditional habits and community structure in favor of a shared "modern" lifestyle. An important tool for influencing immigrant workers was the American Association of Foreign Language Newspapers (AAFLN). The AAFLN was primarily an advertising agency but also gained heavily centralized control over much of the immigrant press.
At the turn of the 20th century, there were few career choices for women in business; however, advertising was one of the few. Since women were responsible for most of the purchasing done in their household, advertisers and agencies recognized the value of women's insight during the creative process. In fact, the first American advertising to use a sexual sell was created by a woman – for a soap product. Although tame by today's standards, the advertisement featured a couple with the message "The skin you love to touch".
Influence of early psychology.
In the early 20th century, psychologists Walter D. Scott and John B. Watson contributed applied psychological theory to the field of advertising. Scott said, “Man has been called the reasoning animal but he could with greater truthfulness be called the creature of suggestion. He is reasonable, but he is to a greater extent suggestible”. He demonstrated this through his advertising technique of a direct command to the consumer. The former chair at Johns Hopkins University, John B. Watson was a highly recognized psychologist in the 1920s. After leaving the field of academia he turned his attention towards advertising where he implemented the concepts of Behaviorism into advertising. This focused on appealing to the basic emotions of the consumer: love, hate, and fear. This type of advertising proved to be extremely effective as it suited the changing social context which lead to heavy influence of future advertising strategy and cemented the place of psychology in advertising.
On the radio from the 1920s.
In the early 1920s, the first radio stations were established by radio equipment manufacturers and retailers who offered programs in order to sell more radios to consumers. As time passed, many non-profit organizations followed suit in setting up their own radio stations, and included: schools, clubs and civic groups.
When the practice of sponsoring programs was popularized, each individual radio program was usually sponsored by a single business in exchange for a brief mention of the business' name at the beginning and end of the sponsored shows. However, radio station owners soon realized they could earn more money by selling sponsorship rights in small time allocations to multiple businesses throughout their radio station's broadcasts, rather than selling the sponsorship rights to single businesses per show.
Public service advertising in WW2.
The advertising techniques used to promote commercial goods and services can be used to inform, educate and motivate the public about non-commercial issues, such as HIV/AIDS, political ideology, energy conservation and deforestation.
Advertising, in its non-commercial guise, is a powerful educational tool capable of reaching and motivating large audiences. "Advertising justifies its existence when used in the public interest – it is much too powerful a tool to use solely for commercial purposes." Attributed to Howard Gossage by David Ogilvy.
Public service advertising, non-commercial advertising, public interest advertising, cause marketing, and social marketing are different terms for (or aspects of) the use of sophisticated advertising and marketing communications techniques (generally associated with commercial enterprise) on behalf of non-commercial, public interest issues and initiatives.
In the United States, the granting of television and radio licenses by the FCC is contingent upon the station broadcasting a certain amount of public service advertising. To meet these requirements, many broadcast stations in America air the bulk of their required public service announcements during the late night or early morning when the smallest percentage of viewers are watching, leaving more day and prime time commercial slots available for high-paying advertisers.
Public service advertising reached its height during World War I and World War II under the direction of more than one government. During WWII President Roosevelt commissioned the creation of The War Advertising Council (now known as the Ad Council) which is the nation's largest developer of PSA campaigns on behalf of government agencies and non-profit organizations, including the longest-running PSA campaign, Smokey Bear.
Commercial television in the 1950s.
This practice was carried over to commercial television in the late 1940s and early 1950s.
A fierce battle was fought between those seeking to commercialise the radio and people who argued that the radio spectrum should be considered a part of the commons – to be used only non-commercially and for the public good. The United Kingdom pursued a public funding model for the BBC, originally a private company, the British Broadcasting Company, but incorporated as a public body by Royal Charter in 1927. In Canada, advocates like Graham Spry were likewise able to persuade the federal government to adopt a public funding model, creating the Canadian Broadcasting Corporation. However, in the United States, the capitalist model prevailed with the passage of the Communications Act of 1934 which created the Federal Communications Commission (FCC). However, the U.S. Congress did require commercial broadcasting companies to operate in the "public interest, convenience, and necessity". Public broadcasting now exists in the United States due to the 1967 Public Broadcasting Act which led to the Public Broadcasting Service (PBS) and National Public Radio (NPR).
In the early 1950s, the DuMont Television Network began the modern practice of selling advertisement time to multiple sponsors. Previously, DuMont had trouble finding sponsors for many of their programs and compensated by selling smaller blocks of advertising time to several businesses. This eventually became the standard for the commercial television industry in the United States. However, it was still a common practice to have single sponsor shows, such as The United States Steel Hour. In some instances the sponsors exercised great control over the content of the show – up to and including having one's advertising agency actually writing the show. The single sponsor model is much less prevalent now, a notable exception being the Hallmark Hall of Fame.
Media diversification in the 1960s.
In the 1960s, campaigns featuring heavy spending in different mass media channels became more prominent. For example, the Esso gasoline company spent hundreds of millions of dollars on a brand awareness campaign built around the simple and alliterative theme "Put a Tiger in Your Tank". Psychologist Ernest Dichter and DDB Worldwide copywriter Sandy Sulcer learned that motorists desired both power and play while driving, and chose the tiger as an easy-to-remember symbol to communicate those feelings. The North American and later European campaign featured extensive television and radio and magazine ads, including photos with tiger tails supposedly emerging from car gas tanks, promotional events featuring real tigers, billboards, and in Europe station pump hoses "wrapped in tiger stripes" as well as pop music songs. Tiger imagery can still be seen on the pumps of successor firm ExxonMobil.
Cable television from the 1980s.
The late 1980s and early 1990s saw the introduction of cable television and particularly MTV. Pioneering the concept of the music video, MTV ushered in a new type of advertising: the consumer tunes in "for" the advertising message, rather than it being a by-product or afterthought. As cable and satellite television became increasingly prevalent, specialty channels emerged, including channels entirely devoted to advertising, such as QVC, Home Shopping Network, and ShopTV Canada.
On the Internet from the 1990s.
With the advent of the ad server, marketing through the Internet opened new frontiers for advertisers and contributed to the "dot-com" boom of the 1990s. Entire corporations operated solely on advertising revenue, offering everything from coupons to free Internet access. At the turn of the 20th to 21st century, a number of websites, including the search engine Google, started a change in online advertising by emphasizing contextually relevant ads based on an individual's browsing interests. This has led to a plethora of similar efforts and an increasing trend of interactive advertising.
The share of advertising spending relative to GDP has changed little across large changes in media. For example, in the US in 1925, the main advertising media were newspapers, magazines, signs on streetcars, and outdoor posters. Advertising spending as a share of GDP was about 2.9 percent. By 1998, television and radio had become major advertising media. Nonetheless, advertising spending as a share of GDP was slightly lower – about 2.4 percent.
A recent advertising innovation is "guerrilla marketing", which involves unusual approaches such as staged encounters in public places, giveaways of products such as cars that are covered with brand messages, and interactive advertising where the viewer can respond to become part of the advertising message. Guerrilla advertising is becoming increasingly more popular with a lot of companies. This type of advertising is unpredictable and innovative, which causes consumers to buy the product or idea. This reflects an increasing trend of interactive and "embedded" ads, such as via product placement, having consumers vote through text messages, and various innovations utilizing social network services such as Facebook or Twitter.
The advertising business model has also been adapted in recent years. A new development is media for equity. Here, advertising is not sold, but provided to start-up companies in return for equity. If the company grows and is sold, media companies receive cash for their shares.
Domain owners (usually those who buy domains as an investment) sometimes "park" their domains and allow advertising companies to place ads on their sites in return for a per-click payment.
Advertising theory.
Hierarchy-of-effects models.
Various competing models of hierarchies of effects attempt to provide a theoretical underpinning to advertising practice.
Marketing mix.
The marketing mix has been a key concept to advertising, it was proposed by professor E. Jerome McCarthy in the 1960s. The marketing mix consists of four basic elements called the four P's.
Product is the first P representing the actual product. Price represents the process of determining the value of a product. Place represents the variables of getting the product to the consumer such as distribution channels, market coverage and movement organization. The last P stands for Promotion which is the process of reaching the target market and convincing them to buy the product.
Types of advertising.
Virtually any medium can be used for advertising. Commercial advertising media can include wall paintings, billboards, street furniture components, printed flyers and rack cards, radio, cinema and television adverts, web banners, mobile telephone screens, shopping carts, web popups, skywriting, bus stop benches, human billboards and forehead advertising, magazines, newspapers, town criers, sides of buses, banners attached to or sides of airplanes ("logojets"), in-flight advertisements on seatback tray tables or overhead storage bins, taxicab doors, roof mounts and passenger screens, musical stage shows, subway platforms and trains, elastic bands on disposable diapers, doors of bathroom stalls, stickers on apples in supermarkets, shopping cart handles (grabertising), the opening section of streaming audio and video, posters, and the backs of event tickets and supermarket receipts. Any place an "identified" sponsor pays to deliver their message through a medium is advertising.
Purpose of advertising.
Advertising is at the front of delivering the proper message to customers and prospective customers. The purpose of advertising is to convince customers that a company's services or products are the best, enhance the image of the company, point out and create a need for products or services, demonstrate new uses for established products, announce new products and programs, reinforce the salespeople's individual messages, draw customers to the business, and to hold existing customers.
Sales promotions.
Sales promotions are another way to advertise. Sales promotions are double purposed because they are used to gather information about what type of customers one draws in and where they are, and to jumpstart sales. Sales promotions include things like contests and games, sweepstakes, product giveaways, samples coupons, loyalty programs, and discounts. The ultimate goal of sales promotions is to stimulate potential customers to action.
Media and advertising approaches.
Increasingly, other media are overtaking many of the "traditional" media such as television, radio and newspaper because of a shift toward consumer's usage of the Internet for news and music as well as devices like digital video recorders (DVRs) such as TiVo.
Digital signage is poised to become a major mass media because of its ability to reach larger audiences for less money. Digital signage also offers the unique ability to see the target audience where they are reached by the medium. Technological advances have also made it possible to control the message on digital signage with much precision, enabling the messages to be relevant to the target audience at any given time and location which in turn, gets more response from the advertising. Digital signage is being successfully employed in supermarkets. Another successful use of digital signage is in hospitality locations such as restaurants and malls.
Advertising on the World Wide Web is a recent phenomenon. Prices of Web-based advertising space are dependent on the "relevance" of the surrounding web content and the traffic that the website receives.
Reasons for online display advertising: Display ads generate awareness quickly. Unlike search, which requires someone to be aware of a need, display advertising can drive awareness of something new and without previous knowledge. Display works well for direct response. Display is not only used for generating awareness, it’s used for direct response campaigns that link to a landing page with a clear ‘call to action’.
E-mail advertising is another recent phenomenon. Unsolicited bulk E-mail advertising is known as "e-mail spam". Spam has been a problem for e-mail users for many years.
A new form of advertising that is growing rapidly is social network advertising. It is online advertising with a focus on social networking sites. This is a relatively immature market, but it has shown a lot of promise as advertisers are able to take advantage of the demographic information the user has provided to the social networking site. Friendertising is a more precise advertising term in which people are able to direct advertisements toward others directly using social network services.
As the mobile phone became a new mass media in 1998 when the first paid downloadable content appeared on mobile phones in Finland, it was only a matter of time until mobile advertising followed, also first launched in Finland in 2000. By 2007 the value of mobile advertising had reached $2 billion and providers such as Admob delivered billions of mobile ads.
More advanced mobile ads include banner ads, coupons, Multimedia Messaging Service picture and video messages, advergames and various engagement marketing campaigns. A particular feature driving mobile ads is the 2D barcode, which replaces the need to do any typing of web addresses, and uses the camera feature of modern phones to gain immediate access to web content. 83 percent of Japanese mobile phone users already are active users of 2D barcodes.
Some companies have proposed placing messages or corporate logos on the side of booster rockets and the International Space Station.
Unpaid advertising (also called "publicity advertising"), can provide good exposure at minimal cost. Personal recommendations ("bring a friend", "sell it"), spreading buzz, or achieving the feat of equating a brand with a common noun (in the United States, "Xerox" = "photocopier", "Kleenex" = tissue, "Vaseline" = petroleum jelly, "Hoover" = vacuum cleaner, and "Band-Aid" = adhesive bandage) – these can be seen as the pinnacle of any advertising campaign. However, some companies oppose the use of their brand name to label an object. Equating a brand with a common noun also risks turning that brand into a genericized trademark – turning it into a generic term which means that its legal protection as a trademark is lost.
From time to time, The CW Television Network airs short programming breaks called "Content Wraps", to advertise one company's product during an entire commercial break. The CW pioneered "content wraps" and some products featured were Herbal Essences, Crest, Guitar Hero II, CoverGirl, and recently Toyota.
A new promotion concept has appeared, "ARvertising", advertising on Augmented Reality technology.
Controversy exists on the effectiveness of subliminal advertising (see mind control), and the pervasiveness of mass messages (see propaganda).
Rise in new media.
With the Internet came many new advertising opportunities. Popup, Flash, banner, Popunder, advergaming, and email advertisements (all of which are often unwanted or spam in the case of email) are now commonplace. Particularly since the rise of "entertaining" advertising, some people may like an advertisement enough to wish to watch it later or show a friend. In general, the advertising community has not yet made this easy, although some have used the Internet to widely distribute their ads to anyone willing to see or hear them. In the last three-quarters of 2009 mobile and internet advertising grew by 18% and 9% respectively. Older media advertising saw declines: −10.1% (TV), −11.7% (radio), −14.8% (magazines) and −18.7% (newspapers).
Niche marketing.
Another significant trend regarding future of advertising is the growing importance of the niche market using niche or targeted ads. Also brought about by the Internet and the theory of The Long Tail, advertisers will have an increasing ability to reach specific audiences. In the past, the most efficient way to deliver a message was to blanket the largest mass market audience possible. However, usage tracking, customer profiles and the growing popularity of niche content brought about by everything from blogs to social networking sites, provide advertisers with audiences that are smaller but much better defined, leading to ads that are more relevant to viewers and more effective for companies' marketing products. Among others, Comcast Spotlight is one such advertiser employing this method in their video on demand menus. These advertisements are targeted to a specific group and can be viewed by anyone wishing to find out more about a particular business or practice at any time, right from their home. This causes the viewer to become proactive and actually choose what advertisements they want to view.
Google AdSense is an example of niche marketing. Google calculates the primary purpose of a website and adjusts ads accordingly; it uses key words on the page (or even in emails) to find the general ideas of topics disused and places ads that will most likely be clicked on by viewers of the email account or website visitors.
Crowdsourcing.
The concept of crowdsourcing has given way to the trend of user-generated advertisements. User-generated ads are created by consumers as opposed to an advertising agency or the company themselves, most often they are a result of brand sponsored advertising competitions. For the 2007 Super Bowl, the Frito-Lays division of PepsiCo held the "Crash the Super Bowl" contest, allowing consumers to create their own Doritos commercial. Chevrolet held a similar competition for their Tahoe line of SUVs. Due to the success of the Doritos user-generated ads in the 2007 Super Bowl, Frito-Lays relaunched the competition for the 2009 and 2010 Super Bowl. The resulting ads were among the most-watched and most-liked Super Bowl ads. In fact, the winning ad that aired in the 2009 Super Bowl was ranked by the USA Today Super Bowl Ad Meter as the top ad for the year while the winning ads that aired in the 2010 Super Bowl were found by Nielsen's BuzzMetrics to be the "most buzzed-about".
This trend has given rise to several online platforms that host user-generated advertising competitions on behalf of a company. Founded in 2007, Zooppa has launched ad competitions for brands such as Google, Nike, Hershey's, General Mills, Microsoft, NBC Universal, Zinio, and Mini Cooper. Crowdsourced advertisements have gained popularity in part to its cost effective nature, high consumer engagement, and ability to generate word-of-mouth. However, it remains controversial, as the long-term impact on the advertising industry is still unclear.
Global advertising.
Advertising has gone through five major stages of development: domestic, export, international, multi-national, and global. For global advertisers, there are four, potentially competing, business objectives that must be balanced when developing worldwide advertising: building a brand while speaking with one voice, developing economies of scale in the creative process, maximising local effectiveness of ads, and increasing the company's speed of implementation. Born from the evolutionary stages of global marketing are the three primary and fundamentally different approaches to the development of global advertising executions: exporting executions, producing local executions, and importing ideas that travel.
Advertising research is key to determining the success of an ad in any country or region. The ability to identify which elements and/or moments of an ad contribute to its success is how economies of scale are maximised. Once one knows what works in an ad, that idea or ideas can be imported by any other market. Market research measures, such as , and provide insight into what is working in an ad in any country or region because the measures are based on the visual, not verbal, elements of the ad.
Foreign public messaging.
Foreign governments, particularly those that own marketable commercial products or services, often promote their interests and positions through the advertising of those goods because the target audience is not only largely unaware of the forum as a vehicle for foreign messaging but also willing to receive the message while in a mental state of absorbing information from advertisements during television commercial breaks, while reading a periodical, or while passing by billboards in public spaces. A prime example of this messaging technique is advertising campaigns to promote international travel. While advertising foreign destinations and services may stem from the typical goal of increasing revenue by drawing more tourism, some travel campaigns carry the additional or alternative intended purpose of promoting good sentiments or improving existing ones among the target audience towards a given nation or region. It is common for advertising promoting foreign countries to be produced and distributed by the tourism ministries of those countries, so these ads often carry political statements and/or depictions of the foreign government's desired international public perception. Additionally, a wide range of foreign airlines and travel-related services which advertise separately from the destinations, themselves, are owned by their respective governments; examples include, though are not limited to, the Emirates airline (Dubai), Singapore Airlines (Singapore), Qatar Airways (Qatar), China Airlines (Taiwan/Republic of China), and Air China (People's Republic of China). By depicting their destinations, airlines, and other services in a favorable and pleasant light, countries market themselves to populations abroad in a manner that could mitigate prior public impressions.
Diversification.
In the realm of advertising agencies, continued industry diversification has seen observers note that "big global clients don't need big global agencies any more". This is reflected by the growth of non-traditional agencies in various global markets, such as Canadian business TAXI and SMART in Australia and has been referred to as "a revolution in the ad world".
New technology.
The ability to record shows on digital video recorders (such as TiVo) allow users to record the programs for later viewing, enabling them to fast forward through commercials. Additionally, as more seasons of pre-recorded box sets are offered for sale of television programs; fewer people watch the shows on TV. However, the fact that these sets are sold, means the company will receive additional profits from the sales of these sets.
To counter this effect, a variety of strategies have been employed. Many advertisers have opted for product placement on TV shows like Survivor. Other strategies include integrating advertising with internet-connected EPGs, advertising on companion devices (like smartphones and tablets) during the show, and creating TV apps. Additionally, some like brands have opted for social television sponsorship.
Advertising education.
Advertising education has become popular with bachelor, master and doctorate degrees becoming available in the emphasis. A surge in advertising interest is typically attributed to the strong relationship advertising plays in cultural and technological changes, such as the advance of online social networking. A unique model for teaching advertising is the student-run advertising agency, where advertising students create campaigns for real companies. Organizations such as American Advertising Federation and AdU Network partner established companies with students to create these campaigns.
Criticisms.
While advertising can be seen as necessary for economic growth, it is not without social costs. Unsolicited commercial e-mail and other forms of spam have become so prevalent as to have become a major nuisance to users of these services, as well as being a financial burden on internet service providers. Advertising is increasingly invading public spaces, such as schools, which some critics argue is a form of child exploitation. In addition, advertising frequently uses psychological pressure (for example, appealing to feelings of inadequacy) on the intended consumer, which may be harmful. Many even feel that often, advertisements exploit the desires of a consumer, by making a particular product more appealing, by manipulating the consumer's needs and wants.
Regulation.
There have been increasing efforts to protect the public interest by regulating the content and the influence of advertising. Some examples are: the ban on television tobacco advertising imposed in many countries, and the total ban of advertising to children under 12 imposed by the Swedish government in 1991. Though that regulation continues in effect for broadcasts originating within the country, it has been weakened by the European Court of Justice, which had found that Sweden was obliged to accept foreign programming, including those from neighboring countries or via satellite. Greece's regulations are of a similar nature, "banning advertisements for children's toys between 7 am and 10 pm and a total ban on advertisement for war toys".
In Europe and elsewhere, there is a vigorous debate on whether (or how much) advertising to children should be regulated. This debate was exacerbated by a report released by the Kaiser Family Foundation in February 2004 which suggested fast food advertising that targets children was an important factor in the epidemic of childhood obesity in the United States.
In New Zealand, South Africa, Pakistan, Afghanistan, Canada, and many European countries, the advertising industry operates a system of self-regulation. Advertisers, advertising agencies and the media agree on a code of advertising standards that they attempt to uphold. The general aim of such codes is to ensure that any advertising is 'legal, decent, honest and truthful'. Some self-regulatory organizations are funded by the industry, but remain independent, with the intent of upholding the standards or codes like the Advertising Standards Authority in the UK.
In the UK, most forms of outdoor advertising such as the display of billboards is regulated by the UK Town and County Planning system. Currently, the display of an advertisement without consent from the Planning Authority is a criminal offense liable to a fine of £2,500 per offence. All of the major outdoor billboard companies in the UK have convictions of this nature.
In the US, many communities believe that many forms of outdoor advertising blight the public realm. As long ago as the 1960s in the US there were attempts to ban billboard advertising in the open countryside. Cities such as São Paulo have introduced an outright ban with London also having specific legislation to control unlawful displays.
Many advertisers employ a wide-variety of linguistic devices to bypass regulatory laws (e.g. In France, printing English words in bold and French translations in fine print to deal with the Article 120 of the 1994 Toubon Law limiting the use of English). The advertisement of controversial products such as cigarettes and condoms are subject to government regulation in many countries. For instance, the tobacco industry is required by law in most countries to display warnings cautioning consumers about the health hazards of their products. Linguistic variation is often used by advertisers as a creative device to reduce the impact of such requirements.
Advertising research.
Advertising research is a specialized form of research that works to improve the effectiveness and efficiency of advertising. It entails numerous forms of research which employ different methodologies. Advertising research includes pre-testing (also known as copy testing) and post-testing of ads and/or campaigns – pre-testing is done before an ad airs to gauge how well it will perform and post-testing is done after an ad airs to determine the in-market impact of the ad or campaign on the consumer. Continuous ad tracking and the Communicus System are competing examples of post-testing advertising research types.
Semiotics.
Today's culture is made up of meanings between consumers and marketers. These meanings depict signs and symbols that are encoded in everyday objects. Semiotics is the study of signs and how they are interpreted. Advertising has many hidden signs and meanings within brand names, logos, package designs, print advertisements, and television advertisements. The purpose of semiotics is to study and interpret the message being conveyed in advertisements. Logos and advertisements can be interpreted at two levels known as the surface level and the underlying level. The surface level uses signs creatively to create an image or personality for their product. These signs can be images, words, fonts, colors, or slogan. The underlying level is made up of hidden meanings. The combination of images, words, colors, and slogan must be interpreted by the audience or consumer. The "key to advertising analysis" is the signifier and the signified. The signifier is the object and the signified is the mental concept. A product has a signifier and a signified. The signifier is the color, brand name, logo design, and technology. The signified has two meanings known as denotative and connotative. The denotative meaning is the meaning of the product. A television’s denotative meaning would be that it is high definition. The connotative meaning is the product’s deep and hidden meaning. A connotative meaning of a television would be that it is top of the line.
Apple is an excellent example of using semiotics in their advertising campaign. Apple's commercials used a black silhouette of a person that was the age of Apple's target market. They placed the silhouette in front of a blue screen so that the picture behind the silhouette could be constantly changing. However, the one thing that stays the same in these ads is that there is music in the background and the silhouette is listening to that music on a white iPod through white headphones. Through advertising, the white color on a set of earphones now signifies that the music device is an iPod. The white color signifies almost all of Apple's products.
The semiotics of gender plays a key influence on the way in which signs are interpreted. When considering gender roles in advertising, individuals are influenced by three categories. Certain characteristics of stumuli may enhance or decrease the elaboration of the message (if the product is perceived as feminine or masculine). Second, the characteristics of individuals can affect attention and elaboration of the message (traditional or non-traditional gender role orientation). Lastly, situational factors may be important to influence the elaboration of the message.
There are two types of marketing communication claims-objective and subjective. Objective claims stem from the extent to which the claim associates the brand with a tangible product or service feature. For instance, the camera has auto focus features. Subjective claims convey emotional, subjective, impressions of intangible aspects of a product or service. They are non-physical features of a product or service that cannot be directly perceived, as they have no physical reality. For instance the brochure has a beautiful design. Males tend to respond better to objective marketing communications claims while females tend to respond better to subjective marketing communications claims.
In advertisements, men are represented as independent. They are shown in more occupations than women. Women are represented mainly as housewives and mothers. Men are more likely to be shown advertising cars or business products, while women advertise domestic products. Men are more likely to be shown outdoors or in business settings. Women are depicted in domestic settings. Men are more often portrayed as authorities. As far as ads go, with age men seem to gain wisdom and authority. On the other hand women seem to disappear with age. Voiceovers are commonly used in advertising. Most voiceovers are men (figures of up to 94% have been reported). There have been more female voiceovers in recent years but mainly for food, household products, and feminine care products.
Gender effects in the processing of advertising.
According to a 1977 study by David Statt, females process information comprehensively, while males process information through heuristic devices such as procedures, methods or strategies for solving problems, which could have an effect on how they interpret advertising. According to this study, men prefer to have available and apparent cues to interpret the message where females engage in more creative, associative, imagery-laced interpretation.
More recently, research by Martin (2003) reveals that males and females differ in how they react to advertising depending on their mood at the time of exposure to the ads, and the affective tone of the advertising. When feeling sad, males prefer happy ads to boost their mood. In contrast, females prefer happy ads when they are feeling happy. The television programs in which the ads are embedded are shown to influence a consumer's mood state.
References.
Notes
Bibliography

</doc>
<doc id="2862" url="http://en.wikipedia.org/wiki?curid=2862" title="AI-complete">
AI-complete

In the field of artificial intelligence, the most difficult problems are informally known as AI-complete or AI-hard, implying that the difficulty of these computational problems is equivalent to that of solving the central artificial intelligence problem—making computers as intelligent as people, or strong AI. To call a problem AI-complete reflects an attitude that it would not be solved by a simple specific algorithm. 
AI-complete problems are hypothesised to include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real world problem.
With current technology, AI-complete problems cannot be solved by computer alone, but also require human computation. This property can be useful, for instance to test for the presence of humans as with CAPTCHAs, and for computer security to circumvent brute-force attacks.
History.
The term was coined by Fanya Montalvo by analogy with NP-complete and NP-hard in complexity theory, which formally describes the most famous class of difficult problems. Early uses of the term are in Erik Mueller's 1987 Ph.D. dissertation and in Eric Raymond's 1991 Jargon File.
AI-complete problems.
AI-complete problems are hypothesised to include:
Machine translation.
To translate accurately, a machine must be able to understand the text. It must be able to follow the author's argument, so it must have some ability to reason. It must have extensive world knowledge so that it knows what is being discussed — it must at least be familiar with all the same commonsense facts that the average human translator knows. Some of this knowledge is in the form of facts that can be explicitly represented, but some knowledge is unconscious and closely tied to the human body: for example, the machine may need to understand how an ocean makes one "feel" to accurately translate a specific metaphor in the text. It must also model the authors' goals, intentions, and emotional states to accurately reproduce them in a new language. In short, the machine is required to have wide variety of human intellectual skills, including reason, commonsense knowledge and the intuitions that underlie motion and manipulation, perception, and social intelligence. Machine translation, therefore, is believed to be AI-complete: it may require strong AI to be done as well as humans can do it.
Software brittleness.
Current AI systems can solve very simple restricted versions of AI-complete problems, but never in their full generality. When AI researchers attempt to "scale up" their systems to handle more complicated, real world situations, the programs tend to become excessively brittle without commonsense knowledge or a rudimentary understanding of the situation: they fail as unexpected circumstances outside of its original problem context begin to appear. When human beings are dealing with new situations in the world, they are helped immensely by the fact that they know what to expect: they know what all things around them are, why they are there, what they are likely to do and so on. They can recognize unusual situations and adjust accordingly. A machine without strong AI has no other skills to fall back on.
Formalization.
Computational complexity theory deals with the relative computational difficulty of computable functions. By definition it does not cover problems whose solution is unknown or has not been characterised formally. Since many AI problems have no formalisation yet, conventional complexity theory does not allow the definition of AI-completeness.
To address this problem, a complexity theory for AI has been proposed. It is based on a model of computation that splits the computational burden between a computer and a human: one part is solved by computer and the other part solved by human. This is formalised by a human-assisted Turing machine. The formalisation defines algorithm complexity, problem complexity and reducibility which in turn allows equivalence classes to be defined.
The complexity of executing an algorithm with a human-assisted Turing machine is given by a pair formula_1, where the first element represents the complexity of the human's part and the second element is the complexity of the machine's part.
Results.
The complexity of solving the following problems with a human-assisted Turing machine is:

</doc>
