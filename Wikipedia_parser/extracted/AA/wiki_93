<doc id="6654" url="http://en.wikipedia.org/wiki?curid=6654" title="Chicago Cubs">
Chicago Cubs

The Chicago Cubs are a professional baseball team located in Chicago, Illinois, USA. They are members of the Central Division of Major League Baseball's National League.
The club played its first games in 1870 as the Chicago White Stockings, before officially becoming the Chicago Cubs for the 1907 season. The Cubs are the oldest currently active American professional sports club, continuously existing in the same city for their entire history. They are one of the two remaining charter members of the National League (the other being the Atlanta Braves). Since Chicago did not have a fully operating White Stockings team for due to the Great Chicago Fire, differences continue to be voiced when considering the elder status of this ball club: Although the Braves have played for more consecutive seasons, the Cubs hold the distinction of having been founded a full season earlier (Cubs in 1870 and Braves in 1871).
The Cubs are also one of two active major league clubs based in Chicago, the other being the Chicago White Sox of the American League. The team is currently owned by Thomas S. Ricketts, son of TD Ameritrade founder Joe Ricketts.
In 1906, the franchise recorded a Major League Baseball record 116 wins (tied by the 2001 Seattle Mariners) and posted a modern-era record winning percentage of .763, still held today. They appeared in their first World Series the same year, falling to their crosstown rivals, the White Sox, four games to two. The Cubs won back-to-back World Series championships in 1907 and 1908, becoming the first Major League club to play three consecutive times in the Fall Classic and the first to win it twice. The club has appeared in seven World Series following their 1908 title, most recently in 1945. The Cubs have not won the World Series in years, the longest championship drought of any major North American professional sports team,
and are often referred to as the "Lovable Losers" because of this distinction. They are also known as "The North Siders" because Wrigley Field, their home park since 1916, is located in Chicago's north side Lake View community at 1060 West Addison Street.
Early Club History.
1870-1875:The Beginning.
Although 1876 is generally recognized as the birth-year of the Cubs franchise, the Chicago club was initially founded in 1870 as the Chicago White Stockings and played a single season in a pro-am league called the National Association of Base Ball Players. The White Stockings won that league's championship and followed that effort playing the next five seasons (with the exception of 1872 and 1873, when the club temporarily ceased operations following the Great Chicago Fire) in the National Association of Professional Base Ball Players along with the Boston Red Stockings (now the Atlanta Braves). Both of these early leagues were hampered by a variety of ethical issues, such as the "throwing" of games and the league's inability (or unwillingness) to enforce rules and player contracts. Because of these challenges, William Hulbert, the president of the Chicago club, spearheaded the development of the National League, whose inaugural season was 1876.
1876–1902: A National League.
After back-to-back pennants in 1880 and 1881, Hulbert died, and Spalding, who had retired to start Spalding sporting goods, assumed ownership of the club. The White Stockings, with Anson acting as player/manager, captured their third consecutive pennant in 1882, and Anson established himself as the game's first true superstar. In 1885 and '86, after winning N.L. pennants, the White Stockings met the short-lived American Association champion in that era's version of a World Series. Both seasons resulted in match ups with the St. Louis Brown Stockings, with the clubs tying in 1885 and with St. Louis winning in 1886. This was the genesis of what would eventually become one of the greatest rivalries in sports. In all, the Anson-led Chicago Base Ball Club won six National League pennants between 1876 and 1886. As a result, Chicago's club nickname transitioned, and by 1890 they had become known as the Chicago Colts, or sometimes "Anson's Colts," referring to Cap's influence within the club. Anson was the first player in history credited with collecting 3,000 career hits. After a disappointing record of 59-73 and a 9th place finish in 1897, Anson was released by the Cubs as both a player and manager. Due to Anson's absence from the club after twenty two years, local newspaper reporters started to refer to the Cubs as the "Orphans".
After the 1900 season, the American Base-Ball League formed as a rival professional league, and incidentally the club's old White Stockings nickname would be adopted by a new American League neighbor to the south.
1902–1920: A Cub Dynasty.
The next season, veteran catcher Johnny Kling left the team to become a professional pocket billiards player. Some historians think Kling's absence was significant enough to prevent the Cubs from also winning a third straight title in 1909, as they finished 6 games out of first place. When Kling returned the next year, the Cubs won the pennant again, but lost to the Philadelphia Athletics in the 1910 World Series.
In 1914, advertising executive Albert Lasker obtained a large block of the club's shares and before the 1916 season assumed majority ownership of the franchise. Lasker brought in a wealthy partner, Charles Weeghman, the proprietor of a popular chain of lunch counters who had previously owned the Chicago Whales of the short-lived Federal League. As principal owners, the pair moved the club from the West Side Grounds to the much newer Weeghman Park, which had been constructed for the Whales only two years earlier. The Cubs responded by winning a pennant in the war-shortened season of 1918, where they played a part in another team's curse: the Boston Red Sox defeated Grover Cleveland Alexander's Cubs four games to two in the 1918 World Series, Boston's last Series championship until 2004.
Beginning in 1916, Bill Wrigley of chewing-gum fame acquired an increasing quantity of stock in the Cubs. By 1921 he was the majority owner, maintaining that status into the 1930s.
Meanwhile, the year 1919 saw the start of the tenure of Bill Veeck, Sr. as team president. Veeck would hold that post throughout the 1920s and into the 30s. The management team of Wrigley and Veeck came to be known as the "double-Bills."
The Wrigley years (1921–1981).
1929–1938: Every three years.
After the "double-Bills" (Wrigley and Veeck) died in 1932 and 1933 respectively, P.K. Wrigley, son of Bill Wrigley, took over as majority owner. He was unable to extend his father's baseball success beyond 1938, and the Cubs slipped into years of mediocrity, although the Wrigley family would retain control of the team until 1981.
1945: The Curse.
The Cubs enjoyed one more pennant at the close of World War II, finishing 98–56. Due to the wartime travel restrictions, the first three games of the 1945 World Series were played in Detroit, where the Cubs won two games, including a one-hitter by Claude Passeau, and the final four were played at Wrigley. In Game 4 of the Series, the Curse of the Billy Goat was allegedly laid upon the Cubs when P.K. Wrigley ejected Billy Sianis, who had come to Game 4 with two box seat tickets, one for him and one for his goat. They paraded around for a few innings, but Wrigley demanded the goat leave the park due to its unpleasant odor. Upon his ejection, Mr. Sianis uttered, ""The Cubs, they ain't gonna win no more."" The Cubs lost Game 4, lost the Series, and have not been back since. It has also been said by many that Sianis put a "curse" on the Cubs, apparently preventing the team from playing in the World Series. After losing the 1945 World Series to the Detroit Tigers, the Cubs finished with winning seasons the next two years, but those teams did not enter post-season play.
In the following two decades after Sianis' ill will, the Cubs played mostly forgettable baseball, finishing among the worst teams in the National League on an almost annual basis. Longtime infielder/manager Phil Cavarretta, who had been a key player during the '45 season, was fired during spring training in 1954 after admitting the team was unlikely to finish above fifth place. Although shortstop Ernie Banks would become one of the star players in the league during the next decade, finding help for him proved a difficult task, as quality players such as Hank Sauer were few and far between. This, combined with poor ownership decisions such as the College of Coaches, and the ill-fated trade of future Hall of Famer Lou Brock to the Cardinals for pitcher Ernie Broglio (who won only 7 games over the next three seasons), hampered on-field performance.
1969: The Fall of '69.
The late-1960s brought hope of a renaissance, with third baseman Ron Santo, pitcher Ferguson Jenkins, and outfielder Billy Williams joining Banks. After losing a dismal 103 games in 1966, the Cubs brought home consecutive winning records in '67 and '68, marking the first time a Cub team had accomplished that feat in over two decades.
In the Cubs, managed by Leo Durocher, built a substantial lead in the newly created National League Eastern Division by mid-August. Ken Holtzman pitched a no-hitter on August 19, and the division lead grew to 8 games over the St. Louis Cardinals and by 9 games over the New York Mets. After the game of September 2, the Cubs record was 84-52 with the Mets in second place at 77-55. But then a losing streak began just as a Mets winning streak was beginning. The Cubs lost the final game of a series at Cincinnati, then came home to play the resurgent Pittsburgh Pirates (who would finish in third place). After losing the first two games by scores of 9-2 and 13-4, the Cubs led going into the ninth inning. A win would be a positive springboard since the Cubs were to play a crucial series with the Mets the very next day. But Willie Stargell drilled a 2-out, 2-strike pitch from the Cubs' ace reliever, Phil Regan, onto Sheffield Avenue to tie the score in the top of the ninth. The Cubs would lose 7-5 in extra innings.
Burdened by a four-game losing streak, the Cubs traveled to Shea Stadium for a short two-game set. The Mets won both games, and the Cubs left New York with a record of 84-58 just 1⁄2 game in front. Disaster followed in Philadelphia, as a 99 loss Phillies team nonetheless defeated the Cubs twice, to extend Chicago's losing streak to eight games. In a key play in the second game, on September 11, Cubs starter Dick Selma threw a surprise pickoff attempt to third baseman Ron Santo, who was nowhere near the bag or the ball. Selma's throwing error opened the gates to a Phillies rally.
After that second Philly loss, the Cubs were 84-60 and the Mets had pulled ahead at 85-57. The Mets would not look back. The Cubs' eight-game losing streak finally ended the next day in St. Louis, but the Mets were in the midst of a ten-game winning streak, and the Cubs, wilting from team fatigue, generally deteriorated in all phases of the game. The Mets (who had lost a record 120 games 7 years earlier), would go on to win the World Series. The Cubs, despite a respectable 92-70 record, would be remembered for having lost a remarkable 17 1⁄2 games in the standings to the Mets in the last quarter of the season.
1977–1979: The June Swoon.
Following the '69 season, the club posted winning records for the next few seasons, but no playoff action. After the core players of those teams started to move on, the 70s got worse for the team, and they became known as ""The Loveable Losers."" In , the team found some life, but ultimately experienced one of its biggest collapses. The Cubs hit a high-water mark on June 28 at 47–22, boasting an game NL East lead, as they were led by Bobby Murcer (27 Hr/89 RBI), and Rick Reuschel (20–10). However, the Philadelphia Phillies cut the lead to two by the All-star break, as the Cubs sat 19 games over .500, but they swooned late in the season, going 20–40 after July 31. The Cubs finished in 4th place at 81–81, while Philadelphia surged, finishing with 101 wins. The following two seasons also saw the Cubs get off to a fast start, as the team rallied to over 10 games above .500 well into both seasons, only to again wear down and play poorly later on, and ultimately settling back to mediocrity. This trait became known as the ""June Swoon"." Again, the Cubs' unusually high number of day games is often pointed to as one reason for the team's inconsistent late season play.
P.K. Wrigley died in 1977. The Wrigley family sold the team to the Chicago Tribune in 1981, ending a 65-year family relationship with the Cubs.
Tribune Company years (1981–2008).
1984: Heartbreak.
Green shored up the 1984 roster with a series of transactions. In December, 1983 Scott Sanderson was acquired from Montreal in a three-team deal with San Diego for Carmelo Martinez. Pinch hitter Richie Hebner (.333 BA in 1984) was signed as a free-agent. In spring training, moves continued: LF Gary Matthews and CF Bobby Dernier came from Philadelphia on March 26, for Bill Campbell and a minor leaguer. Reliever Tim Stoddard (10–6 3.82, 7 saves) was acquired the same day for a minor leaguer; veteran pitcher Ferguson Jenkins was released.
The team's commitment to contend was complete when Green made a midseason deal on June 15 to shore up the starting rotation due to injuries to Rick Reuschel (5–5) and Sanderson. The deal brought 1979 NL Rookie of the Year pitcher Rick Sutcliffe from the Cleveland Indians. Iowa Cub Joe Carter and CF Mel Hall were sent to Cleveland for Sutcliffe and back-up C Ron Hassey (.333 with Cubs in 1984). Sutcliffe (5–5 with the Indians) immediately joined Sanderson (8–5 3.14), Eckersley (10–8 3.03), Steve Trout (13–7 3.41) and Dick Ruthven (6–10 5.04) in the starting rotation. Sutcliffe proceeded to go 16–1 for Cubs and capture the Cy Young Award.
The Cubs 1984 starting lineup was very strong. It consisted of LF Matthews (.291 14–82 101 runs 17 SB), C Jody Davis (.256 19–94), RF Keith Moreland (.279 16–80), SS Larry Bowa (.223 10 SB), 1B Leon "Bull" Durham (.279 23–96 16SB), CF Dernier (.278 45 SB), 3B Ron Cey (.240 25–97), Closer Lee Smith(9–7 3.65 33 saves) and 1984 NL MVP Ryne Sandberg (.314 19–84 114 runs, 19 triples,32 SB).
Reserve players Hebner, Thad Bosley, Henry Cotto, Hassey and Dave Owen produced exciting moments. The bullpen depth of Rich Bordi, George Frazier, Warren Brusstar and Dickie Noles did their job in getting the game to Smith or Stoddard.
At the top of the order, Dernier and Sandberg were exciting, aptly coined "the Daily Double" by Harry Caray. With strong defense – Dernier CF and Sandberg 2B, won the NL Gold Glove- solid pitching and clutch hitting, the Cubs were a well balanced team. Following the "Daily Double," Matthews, Durham, Cey, Moreland and Davis gave the Cubs an order with no gaps to pitch around. Sutcliffe anchored a strong top to bottom rotation and Smith was one of the top closers in the game.
The shift in the Cubs' fortunes was characterized June 23 on the "NBC Saturday Game of the Week" contest against the St. Louis Cardinals. it has since been dubbed simply "The Sandberg Game." With the nation watching and Wrigley Field packed, Sandberg emerged as a superstar with not one, but two game-tying home runs against Cardinals closer Bruce Sutter. With his shots in the 9th and 10th innings Wrigley Field erupted and Sandberg set the stage for a comeback win that cemented the Cubs as the team to beat in the East. No one would catch them, except the Padres in the playoffs.
In early August the Cubs swept the Mets in a 4-game home series that further distanced them from the pack. An infamous Keith Moreland-Ed Lynch fight erupted after Lynch hit Moreland with a pitch, perhaps forgetting Moreland was once a linebacker at the University of Texas. It was the second game of a double header and the Cubs had won the first game in part due to a three run home run by Moreland. After the bench-clearing fight the Cubs won the second game, and the sweep put the Cubs at 68–45.
When the Cubs clinched the Eastern Division with a Sutcliffe complete-game 2-hitter at Pittsburgh on September 24, the franchise had its first title of any kind since 1945. The celebrating crowd outside at Wrigley Field was enormous despite the fact the Cubs were on the road. The Cubs concluded the regular season with a 96–65 record, 6.5 games ahead of the 2nd place Mets.
In 1984, the two leagues, American and National, each had two divisions, East and West. The divisional winners met in a best-of-5 series to advance to the World Series, in a "2–3" format, first two games were played at the home of the team who did not have home field advantage. Then the last three games were played at the home of the team, with home field advantage. Thus the first two games were played at Wrigley Field and the next three at the home of their opponents, San Diego. A common and unfounded myth is that since Wrigley Field did not have lights at that time the National League decided to give the home field advantage to the winner of the NL West. In fact, home field advantage had rotated between the winners of the East and West since 1969 when the league expanded. In even numbered years, the NL West had home field advantage. In odd numbered years, the NL East had home field advantage. Since the NL East winners had had home field advantage in 1983, the NL West winners were entitled to it.
The confusion may stem from the fact that Major League Baseball did decide that, should the Cubs make it to the World Series, the American League winner would have home field advantage unless the Cubs hosted home games at an alternate site since the Cubs home field of Wrigley Field did not yet have lights. Rumor was the Cubs could hold home games across town at Comiskey Park, home of the American League's Chicago White Sox. Rather than hold any games in the cross town rival Sox Park, the Cubs made arrangements with the August A. Busch, owner of the St. Louis Cardinals, to use Busch Stadium in St. Louis as the Cubs "home field" for the World Series. This was approved by Major League Baseball and would have enabled the Cubs to host games 1 and 2, along with games 6 and 7 if necessary. At the time home field advantage was rotated between each league. Odd numbered years the AL had home field advantage. Even numbered years the NL had home field advantage. In the 1982 World Series the St. Louis Cardinals of the NL had home field advantage. In the 1983 World Series the Baltimore Orioles of the AL had home field advantage.
In the NLCS, the Cubs easily won the first two games at Wrigley Field against the San Diego Padres. The Padres were the winners of the Western Division with Steve Garvey, Tony Gwynn, Eric Show, Goose Gossage and Alan Wiggins. With wins of 13–0 and 4–2, the Cubs needed to win only one game of the next three in San Diego to make it to the World Series. After being beaten in Game 3 7–1, the Cubs lost Game 4 when Smith, with the game tied 5–5, allowed a game-winning home run to Garvey in the bottom of the ninth inning. In Game 5 the Cubs took a 3–0 lead into the 6th inning, and a 3–2 lead into the seventh with Sutcliffe (who won the Cy Young Award that year) still on the mound. Then, Leon Durham had a sharp grounder go under his glove. This critical error helped the Padres win the game 6–3, with a 4-run 7th inning and keep Chicago out of the 1984 World Series against the Detroit Tigers. The loss ended a spectacular season for the Cubs, one that brought alive a slumbering franchise and made the Cubs relevant for a whole new generation of Cubs fans.
The Padres would be defeated in 5 games by Sparky Anderson's Tigers in the World Series. Baseball experts felt the Cubs would have better represented the National League and would have won at least two World Series games.
1989: NL East champions.
In 1989, the first full season with night baseball at Wrigley Field, Don Zimmer's Cubs were led by a core group of veterans in Ryne Sandberg, Rick Sutcliffe and Andre Dawson, who were boosted by a crop of youngsters such as Mark Grace, Shawon Dunston, Greg Maddux, Rookie of the Year Jerome Walton, and Rookie of the Year Runner-Up Dwight Smith. The Cubs won the NL East once again that season winning 93 games. This time the Cubs met the San Francisco Giants in the NLCS. After splitting the first two games at home, the Cubs headed to the Bay Area, where despite holding a lead at some point in each of the next three games, bullpen meltdowns and managerial blunders ultimately led to three straight losses. The Cubs couldn't overcome the efforts of Will Clark, whose home run off Maddux, just after a managerial visit to the mound, led Maddux to think Clark knew what pitch was coming. Afterward, Maddux would speak into his glove during any mound conversation, beginning what is a norm today. Mark Grace was 11–17 in the series with 8 RBI. Eventually, the Giants lost to ""The Bash Brothers"" and the Oakland A's in the famous ""Earthquake Series"."
1998: Wild card race and home run chase.
The '98 season would begin on a somber note with the death of legendary broadcaster Harry Caray. After the retirement of Sandberg and the trade of Dunston, the Cubs had holes to fill and the signing of Henry Rodriguez, known affectionately as ""H-Rod"" to bat cleanup provided protection for Sammy Sosa in the lineup, as Rodriguez slugged 31 round-trippers in his first season in Chicago. Kevin Tapani led the club with a career high 19 wins, Rod Beck anchored a strong bullpen and Mark Grace turned in one of his best seasons. The Cubs were swamped by media attention in 1998, and the team's two biggest headliners were Sosa and rookie flamethrower Kerry Wood. Wood's signature performance was one-hitting the Houston Astros, a game in which he tied the major league record of 20 strikeouts in nine innings. His torrid strikeout numbers earned Wood the nickname ""Kid K,"" and ultimately earned him the 1998 NL Rookie of the Year award. Sosa caught fire in June, hitting a major league record 20 home runs in the month, and his home run race with Cardinals slugger Mark McGwire transformed the pair into international superstars in a matter of weeks. McGwire finished the season with a new major league record of 70 home runs, but Sosa's .308 average and 66 homers earned him the National League MVP Award. After a down-to-the-wire Wild Card chase with the San Francisco Giants, Chicago and San Francisco ended the regular season tied, and thus squared off in a one game playoff at Wrigley Field in which third baseman Gary Gaetti hit the eventual game winning homer. The win propelled the Cubs into the postseason once again with a 90–73 regular season tally. Unfortunately, the bats went cold in October, as manager Jim Riggleman's club batted .183 and scored only four runs en route to being swept by Atlanta. On a positive note, the home run chase between Sosa, McGwire and Ken Griffey, Jr. helped professional baseball to bring in a new crop of fans as well as bringing back some fans who had been disillusioned by the 1994 strike. The Cubs retained many players who experienced career years in '98, and after a fast start in 1999, they collapsed again (starting with being swept at the hands of the cross-town White Sox in mid-June) and finished in the bottom of the division for the next two seasons.
2001: Playoff push.
Despite losing fan favorite Grace to free agency, and the lack of production from newcomer Todd Hundley, skipper Don Baylor's Cubs put together a good season in 2001. The season started with Mack Newton being brought in to preach "positive thinking." One of the biggest stories of the season transpired as the club made a midseason deal for Fred McGriff, which was drawn out for nearly a month as McGriff debated waiving his no-trade clause, as the Cubs led the wild card race by 2.5 games in early September. That run died when Preston Wilson hit a three run walk off homer off of closer Tom "Flash" Gordon, which halted the team's momentum. The team was unable to make another serious charge, and finished at 88–74, five games behind both Houston and St. Louis, who tied for first. Sosa had perhaps his finest season and Jon Lieber led the staff with a 20 win season.
2003: Five more outs.
The Cubs had high expectations in 2002, but the squad played poorly. On July 5, 2002 the Cubs promoted assistant general manager and player personnel director Jim Hendry to the General Manager position. The club responded by hiring Dusty Baker and by making some major moves in '03. Most notably, they traded with the Pittsburgh Pirates for outfielder Kenny Lofton and third baseman Aramis Ramirez, and rode dominant pitching, led by Kerry Wood and Mark Prior, as the Cubs led the division down the stretch.
Chicago halted St. Louis' run to the playoffs by taking 4 of 5 games from the Cardinals at Wrigley Field in early September, after which the hapless Cubs finally won their first division title in 14 years. They then went on to defeat the Atlanta Braves in a dramatic five-game Division Series, the franchise's first postseason series win since beating the Detroit Tigers in the 1908 World Series.
After losing an extra-inning game in Game 1, the Cubs rallied and took a 3 games to 1 lead over the Wild Card Florida Marlins in the NLCS. Florida shut the Cubs out in Game 5, but young pitcher Mark Prior led the Cubs in Game 6 as they took a 3–0 lead into the 8th inning and it was at this point when a now-infamous incident took place. Several spectators attempted to catch a foul ball off the bat of Luis Castillo. A Chicago Cubs fan by the name of Steve Bartman, of Northbrook, IL, reached for the ball and deflected it away from the glove of Moisés Alou for the second out of the 8th inning. Alou reacted angrily toward the stands, and after the game stated that he would have caught the ball. Alou at one point recanted, saying he would not have been able to make the play, but later said this was just an attempt to make Bartman feel better and believing the whole incident should be forgotten. Interference was not called on the play, as the ball was ruled to be on the spectator side of the wall. Castillo was eventually walked by Prior. Two batters later, and to the chagrin of the packed stadium, Cubs shortstop Alex Gonzalez misplayed an inning ending double play, loading the bases and leading to eight Florida runs and a Marlin victory. Despite sending Kerry Wood to the mound and holding a lead twice, the Cubs ultimately dropped Game 7, and failed to reach the World Series.
2004–2006.
In 2004, the Cubs were a consensus pick by most media outlets to win the World Series. The offseason acquisition of Derek Lee (who was acquired in a trade with Florida for Hee-seop Choi) and the return of Greg Maddux only bolstered these expectation. Despite a mid-season deal for Nomar Garciaparra, misfortune struck the Cubs again. They led the Wild Card by 1.5 games over San Francisco and Houston on September 25, and both of those teams lost that day, giving the Cubs a chance at increasing the lead to a commanding 2.5 games with only eight games remaining in the season, but reliever LaTroy Hawkins blew a save to the Mets, and the Cubs lost the game in extra innings, a defeat that seemingly deflated the team, as they proceeded to drop 6 of their last 8 games as the Astros won the Wild Card. Despite the fact that the Cubs had won 89 games, this fallout was decidedly unlovable, as the Cubs traded superstar Sammy Sosa after he had left the season's final game early and then lied about it publicly. Already a controversial figure in the clubhouse after his corked-bat incident, Sammy's actions alienated much of his once strong fan base as well as the few teammates still on good terms with him, (many teammates had tired of Sosa's playing of loud salsa music in the locker room)and possibly tarnished his place in Cubs' lore for years to come. The disappointing season also saw fans start to become frustrated with the constant injuries to ace pitchers Mark Prior and Kerry Wood. Additionally, the '04 season led to the departure of popular commentator Steve Stone, who had become increasingly critical of management during broadcasts and was verbally attacked by reliever Kent Mercker. Things were no better in 2005, despite a career year from first baseman Derrek Lee and the emergence of closer Ryan Dempster. The club struggled and suffered more key injuries, only managing to win 79 games after being picked by many to be a serious contender for the N.L. pennant. In 2006, bottom fell out as the Cubs finished 66–96, last in the NL Central.
2007–2008: Back to back division titles.
After finishing last in the NL Central with 66 wins in 2006, the Cubs re-tooled and went from "worst to first" in 2007. In the offseason they signed Alfonso Soriano to the richest contract in Cubs history at 8 years for $136 million, and replaced manager Dusty Baker with fiery veteran manager Lou Piniella. After a rough start, which included a brawl between Michael Barrett and Carlos Zambrano, the Cubs overcame the Milwaukee Brewers, who had led the division for most of the season, with winning streaks in June and July, coupled with a pair of dramatic, late-inning wins against the Reds, and ultimately clinched the NL Central with a record of 85–77. The Cubs traded Barrett to the Padres, and later acquired Jason Kendall from Oakland. Kendall was highly successful with his management of the pitching rotation and helped at the plate as well. By September, Geovany Soto became the full-time starter behind the plate, replacing the veteran Kendall. They met Arizona in the NLDS, but controversy followed as Piniella, in a move that has since come under scrutiny, pulled Carlos Zambrano after the sixth inning of a pitcher's duel with D-Backs ace Brandon Webb, to "...save Zambrano for (a potential) Game 4." The Cubs, however, were unable to come through, losing the first game and eventually stranding over 30 baserunners in a 3-game Arizona sweep.
The Cubs successfully defended their National League Central title in 2008, going to the postseason in consecutive years for the first time since 1906–08. The offseason was dominated by three months of unsuccessful trade talks with the Orioles involving 2B Brian Roberts, as well as the signing of Chunichi Dragons star Kosuke Fukudome. The team recorded their 10,000th win in April, while establishing an early division lead. Reed Johnson and Jim Edmonds were added early on and Rich Harden was acquired from the Oakland Athletics in early July. The Cubs headed into the All-Star break with the N.L.'s best record, and tied the league record with eight representatives to the All-Star game, including catcher Geovany Soto, who was named Rookie of the Year. The Cubs took control of the division by sweeping a four game series in Milwaukee. On September 14, in a game moved to Miller Park due to Hurricane Ike, Zambrano pitched a no-hitter against the Astros, and six days later the team clinched by beating St. Louis at Wrigley. The club ended the season with a 97–64 record and met Los Angeles in the NLDS. The heavily favored Cubs took an early lead in Game 1, but James Loney's grand slam off Ryan Dempster changed the series' momentum. Chicago committed numerous critical errors and were outscored 20–6 in a Dodger sweep, which provided yet another sudden ending.
2009–present: the Ricketts era.
The Ricketts family acquired a majority interest in the Cubs in 2009, ending the Tribune years. Apparently handcuffed by the Tribune's bankruptcy and the sale of the club to the Ricketts family, the Cubs' quest for a NL Central 3-peat started with notice that there would be less invested into contracts than in previous years. Chicago engaged St. Louis in a see-saw battle for first place into August 2009, but the Cardinals played to a torrid 20–6 pace that month, designating their rivals to battle in the Wild Card race, from which they were eliminated in the season's final week. The Cubs were plagued by injuries in 2009, and were only able to field their Opening Day starting lineup three times the entire season. Third baseman Aramis Ramírez injured his throwing shoulder in an early May game against the Milwaukee Brewers, sidelining him until early July and forcing journeyman players like Mike Fontenot and Aaron Miles into more prominent roles. Additionally, key players like Derrek Lee (who still managed to hit .306 with 35 HR and 111 RBI that season), Alfonso Soriano and Geovany Soto also nursed nagging injuries. The Cubs posted a winning record (83–78) for the third consecutive season, the first time the club had done so since 1972, and a new era of ownership under the Ricketts' family was approved by MLB owners in early October.
On December 3, Cubs broadcaster and former third baseman, Ron Santo, died due to complications from bladder cancer and diabetes. He spent 13 seasons as a player with the Cubs, and at the time of his death was regarded as one of the greatest players not in the Hall of Fame. He has since been elected to the Major League Baseball Hall of Fame.
Despite trading for pitcher Matt Garza and signing free-agent slugger Carlos Pena, the Cubs finished the 2011 season 20 games under .500 with a record of 71-91. Weeks after the season came to an end, the club was rejuvenated in the form of a new philosophy, as owner Tom Ricketts signed Theo Epstein away from the Boston Red Sox, naming him club President and giving him a five-year contract worth over $18M, and subsequently discharged the GM Jim Hendry manager Mike Quade. Epstein, a proponent of sabremetrics and one of the architects of two world series titles in Boston brought along Jed Hoyer to fill the role of GM and hired Dale Sveum as manager. Although the team had a dismal 2012 season, losing 101 games (the worst record since 1966) it was largely expected. The youth movement ushered in by Epstein and Hoyer began as longtime fan favorite Kerry Wood retired in May, followed by Ryan Dempster and Geovany Soto being traded to Texas at the All-Star break for a group of minor league prospects headlined by Christian Villanueva. The development of Castro, Anthony Rizzo, Darwin Barney, Brett Jackson and pitcher Jeff Samardzija as well as the replenishing of the minor-league system with prospects such as Javier Baez, Albert Almora, and Jorge Soler became the primary focus of the season, a philosophy which the new management said would carry over at least through the 2013 season. The club also made a market savvy move in transitioning its long running single-A affiliation with the Peoria Chiefs to one with the nearby Kane County Cougars.
The 2013 season resulted in much as the same the year before. Shortly before the trade deadline, the Cubs traded Matt Garza to the Texas Rangers for Mike Olt, C. J. Edwards, and Justin Grimm. Three days later, the Cubs sent Alfonso Soriano to the New York Yankees for minor leaguer Corey Black. The mid season fire sale led to another last place finish in the NL Central, finishing with a record of 66-96. Although there was a five game improvement in the record from the year before, Anthony Rizzo and Starlin Castro seemed to take steps backward in their development. On September 30, 2013, Theo Epstein made the decision to fire manager Dale Sveum after just two seasons at the helm of the Cubs. The regression of several young players was thought to be the main focus point, as the front office said Dale would not be judged based on wins and losses. In two seasons as skipper, Sveum finished with a record of 127-197.
On November 7, 2013, the Cubs hired San Diego Padres bench coach Rick Renteria to be the 53rd manager in team history. The deal is a three year contract, with club options that could keep him in Chicago through the 2018 baseball season.
Memorable events and records.
Merkle's Boner.
On September 23, 1908, the Cubs and New York Giants were involved in a tight pennant race. The two clubs were tied in the bottom of the ninth inning at the Polo Grounds, and N.Y. had runners on first and third and two outs when Al Bridwell singled, scoring Moose McCormick from third with the Giants' apparent winning run, but the runner on first base, rookie Fred Merkle, left the field without touching second base. As fans swarmed the field, Cub infielder Johnny Evers retrieved the ball and touched second. Since there were two outs, a forceout was called at second base, ending the inning and the game. Because of the tie the Giants and Cubs ended up tied for first place. The Giants lost the ensuing one-game playoff and the Cubs went on to the World Series.
Riot at Wrigley.
Slugger Hack Wilson had a combative streak and frequently initiated fights with opposing players and fans. On June 22, 1928, a riot broke out in the ninth inning at Wrigley Field against the St. Louis Cardinals when Wilson jumped into the box seats to attack a heckling fan. An estimated 5,000 spectators swarmed the field before police could separate the combatants and restore order. The fan sued Wilson for $20,000, but a jury ruled in Wilson's favor.
RBI record.
Hack Wilson set a record of 56 home-runs and 190 runs-batted-in in 1930, breaking Lou Gehrig's MLB record of 176 RBI. (In 1999, a long-lost extra RBI mistakenly credited to Charlie Grimm had been found by Cooperstown researcher Cliff Kachline and verified by historian Jerome Holtzman, increasing the record number to 191.) As of 2013 the record still stands, with no serious threats coming since Gehrig (184) and Hank Greenberg (183) in the same era. The closest anyone has come to the mark in the last 75 years was Manny Ramirez's 165 RBI in 1999. In addition to the RBI record, Wilson 56 home-runs stood as the National League record until 1998, when Sammy Sosa and Mark McGwire hit 66 and 70, respectively. Wilson was named "Most Useful" player that year by the Baseball Writers Association of America, as the official N.L. Most Valuable Player Award was not awarded until the next season.
The Homer in the Gloamin'.
On September 28, 1938, with the Cubs and Pirates tied at 5, Gabby Hartnett stepped to the plate in a lightless Wrigley Field that was gradually being overcome by darkness and visibility was becoming difficult. With two outs in the bottom of the ninth and the umpires ready to end the game, Hartnett launched Pirate hurler Mace Brown's offering into the gloom and haze. This would be remembered as his ""Homer in the Gloamin."" It was ranked by ESPN as the 47th greatest home run of all time.
Rick Monday and the U.S. flag.
On April 25, 1976, at Dodger Stadium, father-and-son protestors ran into the outfield and tried to set fire to a U.S. flag. When Cubs outfielder Rick Monday noticed the flag on the ground and the man and boy fumbling with matches and lighter fluid, he dashed over and snatched the flag to thunderous applause. When he came up to bat in the next half-inning, he got a standing ovation from the crowd and the stadium titantron flashed the message, "RICK MONDAY... YOU MADE A GREAT PLAY..." Monday later said, "If you're going to burn the flag, don't do it around me. I've been to too many veterans' hospitals and seen too many broken bodies of guys who tried to protect it."
The Sandberg game.
On June 23, 1984, Chicago trailed St. Louis 9–8 in the bottom of the ninth on NBC's Game of the Week when Ryne Sandberg, known mostly for his glove, slugged a game-tying home run off ace closer Bruce Sutter. Despite this, the Cardinals scored two runs in the top of the tenth. Sandberg came up again facing Sutter with one man on base, and hit yet another game tying home run, and "Ryno" became a household name. The Cubs won what has become known as ""The Sandberg Game"" in the 11th inning.
Most home-runs in a month.
In June, 1998 Sammy Sosa exploded into the pursuit of Roger Maris' home run record. Sosa had 13 home runs entering the month, representing less than half of Mark McGwire's total. Sosa had his first of four multi-home run games that month on June 1, and went on to break Rudy York's record with 20 home runs in the month, a record that still stands. By the end of his historic month, the outfielder's 33 home runs tied him with Ken Griffey, Jr. and left him only four behind McGwire's 37. Sosa finished with 66 and won the NL MVP Award.
Wood's 20K game.
In only his third career start, Kerry Wood struck out 20 batters against Houston on May 6, 1998. This is the franchise record and tied for the Major League record. The game is often considered the most dominant pitching performance of all time. Wood hit one batter, Craig Biggio, and allowed one hit, a scratch single by Ricky Gutiérrez off third baseman Kevin Orie's glove. The play was nearly scored an error, which would have given Wood a no-hitter.
Championship drought.
The Chicago Cubs have not won a World Series championship since 1908, and have not appeared in the Fall Classic since 1945, although between their postseason appearance in 1984 and their most recent in 2008, they have made the postseason six times. It is the longest title drought in all four of the major American professional sports leagues, which includes the NFL, NBA, NHL, as well as Major League Baseball. In fact, the Cubs' last World Series title occurred before those other three leagues even existed, and even the Cubs' last World Series appearance predates the founding of the NBA. The much publicized drought was concurrent to championship droughts by the Boston Red Sox and the Chicago White Sox, who both had over 80 years between championships. It is this unfortunate distinction that has led to the club often being known as "The Lovable Losers." The team was one win away from breaking what is often called "The Curse of the Billy Goat" in 1984 and 2003, but was unable get the victory that would send it to the World Series.
Individual Awards.
Retired numbers.
The Chicago Cubs retired numbers are commemorated on pinstriped flags flying from the foul poles at Wrigley Field, with the exception of Jackie Robinson, the Brooklyn Dodgers player whose number 42 was retired for all clubs. The first retired number flag, Ernie Banks' number 14, was raised on the left field pole, and they have alternated since then. 14, 10 and 31 (Jenkins) fly on the left field pole; and 26, 23 and 31 (Maddux) fly on the right field pole.
Minor league affiliations.
Before signing a developmental agreement with the Kane County Cougars in 2012, the Cubs had a Class A minor league affiliation on two occasions with the Peoria Chiefs (1985-1995 and 2004-2012). Ryne Sandberg managed the Chiefs from 2006-2010. In the period between those associations with the Chiefs the club had affiliations with the Dayton Dragons and Lansing Lugnuts. The Lugnuts were often affectionately referred to by Chip Caray as "Steve Stone's favorite team." The 2007 developmental contract with the Tennessee Smokies was preceded by Double A affiliations with the Orlando Cubs and West Tenn Diamond Jaxx.
Spring training history.
The Cubs spring training facility is located in Mesa, Arizona, where they play in the Cactus League. The club plays its games at Cubs Park The park seats 15,000, making it Major League Baseball's largest spring training ballpark by capacity, and the Cubs annually sell out most of their games both at home and on the road. Before Cubs Park opened in 2014, the team played games at HoHoKam Park (1979–2013), Dwight Patterson Field. "HoHoKam" is literally translated from Native American as "those who vanished." The Northsiders have called Mesa their spring home for most seasons since 1952.
In addition to Mesa, the club has held spring training in Champaign, Illinois (1901–02, 1906); Los Angeles (1903–04, 1948–1949), Santa Monica, California (1905); New Orleans (1907, 1911–1912); French Lick, Indiana. (1908, 1943-1945); Hot Springs, Arkansas (1909–1910); Tampa (1913–1916); Pasadena, Cal. (1917–1921); Santa Catalina Island, California (1922–1942, 1946–1947, 1950–1951); Rendezvous Park in Mesa (1952–1965); Long Beach, California (1966); and Scottsdale, Arizona (1967–1978).
The curious location on Catalina Island stemmed from Cubs owner William Wrigley Jr.'s then-majority interest in the island in 1919. Wrigley constructed a ballpark on the island to house the Cubs in spring training: it was built to the same dimensions as Wrigley Field. (The ballpark is long gone, but a clubhouse built by Wrigley to house the Cubs exists as the Catalina County Club.) However by 1951 the team chose to leave Catalina Island and spring training was shifted to Mesa, Arizona. The Cubs' 30-year association with Catalina is chronicled in the book, "The Cubs on Catalina," by Jim Vitti . . . which was named International 'Book of the Year' by "The Sporting News".
The former location in Mesa is actually the second HoHoKam Park; the first was built in 1976 as the spring-training home of the Oakland Athletics who left the park in 1979. Apart from HoHoKam Park and Cubs Park the Cubs also have another Mesa training facility called Fitch Park, this complex provides of team facilities, including major league clubhouse, four practice fields, one practice infield, enclosed batting tunnels, batting cages, a maintenance facility, and administrative offices for the Cubs.
Media.
Radio.
The Cubs' flagship radio station is WGN/720 AM. The Chicago Cubs Radio Network consists of 45 stations and covers at least eleven states. WGN Radio is owned and operated by Tribune Company. However, the club has signed a deal with CBS Radio to move games to the latter's all-news station WBBM (AM) starting in the 2015 season.
Pat Hughes has been the Cubs' radio play-by-play voice since 1996. Until 2010, his color analyst was former Cubs third basemanRon Santo. Former Cub Ron Coomer became analyst in 2014.
Print.
The club also produces its own print media; the Cubs' official magazine "Vineline", which has 12 annual issues, is in its third decade, and spotlights players and events involving the club. The club also publishes a traditional media guide.
Television.
Cubs telecasts are locally aired on three different outlets: 
Len Kasper has been the Cubs' television play-by-play announcer since 2005 and was joined by Jim Deshaies in 2013. Bob Brenly (analyst, 2005-12), Chip Caray (play-by-play, 1998-2004), Steve Stone (analyst, 1983-2000, 2003-04), Joe Carter (analyst for WGN-TV games, 2001-02) and Dave Otto (analyst for FSN Chicago games, 2001-02) also have spent time broadcasting from the Cubs booth since the death of Harry Caray in 1998.
Jack Brickhouse and Harry Caray.
Jack Brickhouse manned the Cubs radio and especially the TV booth for parts of five decades, the 34-season span from 1948 to 1981. He covered the games with a level of enthusiasm that often seemed unjustified by the team's poor performance on the field for many of those years. His trademark call ""Hey Hey!"" always followed a home run. That expression is spelled out in large letters vertically on both foul pole screens at Wrigley Field. "Whoo-boy!" and "Wheeee!" and "Oh, brother!" were among his other pet expressions. When he approached retirement age, he personally recommended his successor.
Harry Caray's stamp on the team is perhaps even deeper than that of Brickhouse, although his 17-year tenure, from 1982 to 1997, was half as long. First, Caray had already become a well-known Chicago figure by broadcasting White Sox games for a decade, after having been a St Louis Cardinals icon for 25 years. Caray also had the benefit of being in the booth during the NL East title run in 1984, which was widely seen due to WGN's status as a cable-TV superstation. His trademark call of ""Holy Cow!"" and his enthusiastic singing of ""Take me out to the ballgame"" during the 7th inning stretch (as he had done with the White Sox) made Caray a fan favorite both locally and nationally.
Caray had lively discussions with commentator Steve Stone, who was hand-picked by Harry himself, and producer Arne Harris. Caray often playfully quarreled with Stone over Stone's cigar and why Stone was single, while Stone would counter with poking fun at Harry being "under the influence." Stone disclosed in his book ""Where's Harry"" that most of this "arguing" was staged, and usually a ploy developed by Harry himself to add flavor to the broadcast. The Cubs still have a "guest conductor," usually a celebrity, lead the crowd in singing "Take me out to the ballgame" during the 7th inning stretch to honor Caray's memory.
Ownership.
Ownership History.
Al Spalding, who also owned Spalding sporting goods, played for the team for two seasons under club founder William Hulbert. After Hulbert's death Spalding owned the club for twenty one years, after which the Cubs were purchased by Albert Lasker and Charles Weeghman. That pair were followed by the Wrigley family, owners of Wrigley's chewing gum. In 1981, after 6 decades under the Wrigley family, the Cubs were purchased by Tribune Company for $20,500,000. Tribune, which also owned the "Chicago Tribune", "Los Angeles Times", "WGN Television", "WGN Radio" and many other media outlets, controlled the club until December 2007, when Sam Zell completed his purchase of the entire Tribune organization and announced his intention to sell the baseball team. After a nearly two-year process which involved potential buyers such as Mark Cuban and a group led by Hank Aaron, a family trust of TD Ameritrade Joe Ricketts won the bidding process as the 2009 season came to a close. Ultimately, the sale was unanimously approved by MLB owners and the Ricketts family took control on October 27, 2009.
Other information.
Tinker to Evers to Chance.
""Baseball's Sad Lexicon,"" also known as ""Tinker to Evers to Chance"" after its refrain, is a 1910 baseball poem by Franklin Pierce Adams. The poem is presented as a single, rueful stanza from the point of view of a New York Giants fan seeing the talented Chicago Cubs infield of shortstop Joe Tinker, second baseman Johnny Evers, and first baseman Frank Chance complete a double play. The trio began playing together with the Cubs in 1902, and formed a double play combination that lasted through April 1912. The Cubs won the pennant four times between 1906 and 1910, often defeating the Giants en route to the World Series.
The poem was first published in the "New York Evening Mail" on July 12, 1912. Popular among sportswriters, numerous additional verses were written. The poem gave Tinker, Evers, and Chance increased popularity and has been credited with their elections to the National Baseball Hall of Fame in 1946.
""White flag time at Wrigley!"".
The term ""White flag time at Wrigley!"", coined by former play-by-play broadcaster Chip Caray, means the Cubs have won.
Beginning in the days of P.K. Wrigley and the 1937 bleacher/scoreboard reconstruction, and prior to modern media saturation, a flag with either a "W" or an "L" has flown from atop the scoreboard masthead, indicating the day's result(s) when baseball was played at Wrigley. In case of a doubleheader that results in a split, both the "win" and "loss" flags are flown.
Past Cubs media guides show that originally the flags were blue with a white "W" and white with a blue "L". In 1978, consistent with the dominant colors of the flags, blue and white lights were mounted atop the scoreboard, denoting "win" and "loss" respectively for the benefit of nighttime passers-by.
The flags were replaced by 1990, the first year in which the Cubs media guide reports the switch to the now familiar colors of the flags: White with blue "W" and blue with white "L". In addition to needing to replace the worn-out flags, by then the retired numbers of Banks and Williams were flying on the foul poles, as white with blue numbers; so the "good" flag was switched to match that scheme.
This long-established tradition has evolved to fans carrying the white-with-blue-W flags to both home and away games, and displaying them after a Cub win. The flags have become more and more popular each season since 1998, and are now even sold as T-shirts with the same layout. In 2009, the tradition spilled over to the NHL as Chicago Blackhawks fans adopted a red and black "W" flag of their own.
Mascots.
The official Cub mascot is a young bear cub, named Clark, described by the team's press release as a young and friendly Cub. Clark made his debut at Advocate Health Care on January 13, 2014, the same day as the press release announcing his installation as the clubs first ever official physical mascot. The bear cub itself was used in the clubs since the early 1900s and was the inspiration of the Chicago Staleys changing their team's name to the Chicago Bears, due to the Cubs allowing the football team to play at Wrigley Field in the 1930s.
The Cubs had no official physical mascot prior to Clark, though a man in a 'polar bear' looking outfit, called "The Bear-man" (or Beeman), which was mildly popular with the fans, paraded the stands briefly in the early 1990s. There is no record of whether or not he was just a fan in a costume or employed by the club. Through the 2013 season, there were "Cubbie-bear" mascots outside of Wrigley on game day, but none are employed by the team. They pose for pictures with fans for tips. The most notable of these was "Billy Cub" who worked outside of the stadium until for over 6 years until July 2013, when the club asked him to stop. Billy Cub, who is played by fan John Paul Weier, had unsuccessfully petitioned the team to become the official mascot.
Another unofficial but much more well-known mascot is Ronnie "Woo Woo" Wickers who is a longtime fan and local celebrity in the Chicago area. He is known to Wrigley Field visitors for his idiosyncratic cheers at baseball games, generally punctuated with an exclamatory "Woo!" (e.g., "Cubs, woo! Cubs, woo! Big-Z, woo! Zambrano, woo! Cubs, woo!") Longtime Cubs announcer Harry Caray dubbed Wickers "Leather Lungs" for his ability to shout for hours at a time. He is not employed by the team, although the club has on two separate occasions allowed him into the broadcast booth and allow him some degree of freedom once he purchases or is given a ticket by fans to get into the games. He is largely allowed to roam the park and interact with fans by Wrigley Field security, although there have been numerous minor occurrences where Wickers has had confrontations with fans who do not approve of his antics, one in which Wickers was assaulted with a hula hoop by a fan named Greg Hoden, known as "The Stinger." Hoden was the belligerent sidekick popular heckler "Derek the Five Dollar Kid" in the late 1990s and early 2000s.
Wrigley Field and Wrigleyville.
The Cubs have played their home games at Wrigley Field, also known as ""The Friendly Confines"" since 1916. It was built in 1914 as Weeghman Park for the Chicago Whales, a Federal League baseball team. The Cubs also shared the park with the Chicago Bears of the NFL for 50 years. The ballpark includes a manual scoreboard, ivy-covered brick walls, and relatively small dimensions.
Located in Chicago's Lake View neighborhood, Wrigley Field sits on an irregular block bounded by Clark and Addison Streets and Waveland and Sheffield Avenues. The area surrounding the ballpark is typically referred to as Wrigleyville. There is a dense collection of sports bars and restaurants in the area, most with baseball inspired themes, including Sluggers, Murphy's Bleachers and The Cubby Bear. Many of the apartment buildings surrounding Wrigley Field on Waveland and Sheffield Avenues have built bleachers on their rooftops for fans to view games and other sell space for advertisement. One building on Sheffield Avenue has a sign atop its roof which says "Eamus Catuli!" which is Latin for "Let's Go Cubs!" and another chronicles the time since the last Division title, pennant, and World Series championship, as shown in the picture to the left. The 02 denotes two years since the 2008 NL Central title, 65 years since the 1945 pennant and 102 years since the 1908 World Series championship. On game days, many residents rent out their yards and driveways to people looking for parking spots. The uniqueness of the neighborhood itself has ingrained itself into the culture of the Chicago Cubs as well as the Wrigleyville neighborhood, and has led to being used for concerts and other sporting events, such as the 2010 NHL Winter Classic between the Chicago Blackhawks and Detroit Red Wings, as well as a 2010 NCAA men's football game between the Northwestern Wildcats and Illinois Fighting Illini.
In 2013, Tom Ricketts and team president Crane Kenney unveiled plans for a $300M renovation to Wrigley Field to Chicago mayor Rahm Emanuel. The proposed plans include a 6000 square foot jumbotron in left center field, and a return of some of the parks features from the 1930s, including recreating the green terra-cotta canopies and wrought-iron fencing that were part of Wrigley in that era. Previously, mostly all efforts to conduct large-scale renovations the field to former mayor Richard M. Daley (a staunch White Sox fan) had been opposed by the city and rooftop owners.
Bleacher Bums.
The "Bleacher Bums" is a name given to fans, many of whom spend much of the day heckling, who sit in the bleacher section at Wrigley Field. Initially, the group was called "bums" because it referred to a group of fans who were at most games, and since those games were all day games, it was assumed they did not work. Many of those fans were, and are still, students at Chicago area colleges, such as DePaul University, Loyola, Northwestern University, and Illinois-Chicago. A Broadway play, starring Joe Mantegna, Dennis Farina, Dennis Franz, and James Belushi ran for years and was based on a group of Cub fans who frequented the club's games. The group was started in 1967 by dedicated fans Ron Grousl, Tom Nall and "mad bugler" Mike Murphy, who was a sports radio host during mid days on Chicago-based WSCR AM 670 "The Score". Murphy alleges that Grousl started the Wrigley tradition of throwing back opposing teams' home run balls. The current group is headed by Derek Schaul (Derek the Five Dollar Kid). Prior to the 2006 season, they were updated, with new shops and private bar (The Batter's Eye) being added, and Bud Light bought naming rights to the bleacher section, dubbing them the Bud Light Bleachers. Bleachers at Wrigley are general admission, except during the playoffs. The bleachers have been referred to as the ""World's Largest Beer Garden."" A popular t-shirt (sold inside the park and licensed by the club) which says "Wrigley Bleachers" on the front and the phrase "Shut Up and Drink Your Beer" on the reverse fuels this stereotype.
Emil Verban Society.
In 1975, a group of Chicago Cubs fans based in Washington, D.C. formed the Emil Verban Society. The society is a select club of high profile Cub fans, currently headed by Illinois Senator Dick Durbin which is named for Emil Verban, who in three seasons with the Cubs in the 1940s batted .280 with 39 runs batted in and one home run. Verban was picked as the epitome of a Cub player, explains columnist George Will, because "He exemplified mediocrity under pressure, he was competent but obscure and typifying of the work ethics." Verban initially believed he was being ridiculed, but his ill feeling disappeared several years later when he was flown to Washington to meet President Ronald Reagan, also a society member, at the White House. Hillary Clinton, Jim Belushi, Joe Mantegna, Rahm Emanuel, Dick Cheney and many others have been included among its membership.
Music.
During the summer of 1969, a Chicago studio group produced a single record called "Hey Hey! Holy Mackerel! (The Cubs Song)" whose title and lyrics incorporated the catch-phrases of the respective TV and radio announcers for the Cubs, Jack Brickhouse and Vince Lloyd. Several members of the Cubs recorded an album called "Cub Power" which contained a cover of the song. The song received a good deal of local airplay that summer, associating it very strongly with that bittersweet season. It was played much less frequently thereafter, although it remained an unofficial Cubs theme song for some years after.
For many years, Cubs radio broadcasts started with "It's a Beautiful Day for a Ball Game" by the Harry Simeone Chorale. In 1979, Roger Bain released a 45 rpm record of his song "Thanks Mr. Banks," to honor “Mr. Cub” Ernie Banks.
The song "Go, Cubs, Go!" by Steve Goodman was recorded early in the 1984 season, and was heard frequently during that season. Goodman died in September of that year, four days before the Cubs clinched the National League Eastern Division title, their first title in 39 years. Since 1984, the song started being played from time to time at Wrigley Field; since 2007, the song has been played over the loudspeakers following each Cubs home victory.
The Mountain Goats recorded a song entitled "Cubs in Five" on its 1995 EP Nine Black Poppies which refers to the seeming impossibility of the Cubs winning a World Series in both its title and Chorus.
In 2007, Pearl Jam frontman Eddie Vedder composed a song dedicated to the team called "All the Way". Vedder, a Chicago native, and lifelong Cubs fan, composed the song at the request of Ernie Banks.
Pearl Jam has only played this song live one time, on August 2, 2007 at the Vic Theater in Chicago, IL.
Eddie Vedder has played this song live twice, at his solo shows at the Chicago Auditorium on August 21 and 22, 2008.
An album entitled "Take Me Out to a Cubs Game" was released in 2008. It is a collection of 17 songs and other recordings related to the team, including Harry Caray's final performance of "Take Me Out to the Ball Game" on September 21, 1997, the Steve Goodman song mentioned above, and a newly recorded rendition of "Talkin' Baseball" (subtitled "Baseball and the Cubs") by Terry Cashman. The album was produced in celebration of the 100th anniversary of the Cubs' 1908 World Series victory and contains sounds and songs of the Cubs and Wrigley Field.

</doc>
<doc id="6655" url="http://en.wikipedia.org/wiki?curid=6655" title="Coldcut">
Coldcut

Coldcut are an English electronic music duo composed of Matt Black and Jonathan More. Credited as pioneers for pop sampling in the ‘80s, Coldcut are also considered the first stars of UK electronic dance music due to their innovative style, which featured cut-up samples of hip-hop, breaks, jazz, spoken word and various other types of music, as well as video and multimedia. According to "Spin", “in ’87 Coldcut pioneered the British fad for ‘DJ records’”.
Coldcut’s records first introduced the public to pop artists Yazz and Lisa Stansfield, through which these artists achieved pop chart success. In addition, Coldcut has remixed and created productions on tracks by the likes of Eric B & Rakim, Yazz, James Brown, Queen Latifah, Eurythmics, INXS, Steve Reich, Blondie, The Fall, Pierre Henri, Nina Simone, Fog, Red Snapper, and BBC Radiophonic Workshop.
Beyond their work as a production duo, Coldcut are the founders of Ninja Tune, an independent record label in London, England (with satellite offices in Montreal and Los Angeles) with an overall emphasis on encouraging interactive technology and finding innovative uses of software. The label’s first releases (the first four volumes of DJ Food - 'Jazz Brakes') were produced by Coldcut in the early 90s, and composed of instrumental hip-hop cuts that led the duo to help pioneer the trip-hop genre, with artists such as Funky Porcini, The Herbaliser and DJ Vadim.
History.
1980s.
In 1986, computer programmer Matt Black and ex-art teacher Jonathan More were part-time DJs on the rare groove scene. More also DJed on pirate radio, hosting the "Meltdown Show" on Kiss FM and worked at the Reckless Records store on Berwick Street, London where Black visited as a customer. The first collaboration between the two artists was 'Say Kids What Time Is It?' on a white label in January 1987, which mixed Jungle Book's "King of the Swingers" with the break from James Brown's "Funky Drummer." The innovation of “Say Kids...” caused More and Black to be heralded by SPIN as “the first Brit artists to really get hip-hop’s class-cutup aesthetic”. It’s regarded as the UK’s first breaks record, the first UK record to be built entirely of samples and “the final link in the chain connecting European collage-experiment with the dance-remix-scratch edit”. This was later sampled in "Pump Up the Volume" by MARRS, a single that reached #1 in the UK in October 1987.
Though Black had joined Kiss FM with his own mix-based show, the pair eventually joined forces on its own show later in 1987 called Solid Steel. The eclectic show became a unifying force in underground experimental electronic music and is still running to date, celebrating 25 years in 2013.
The duo adopted the name Coldcut and set up a record label called Ahead Of Our Time to release the single Beats + Pieces (one of the formats also included "That Greedy Beat") in 1987. All of these tracks were assembled using cassette pause button edits and later spliced tape edits that would sometimes run "all over the room". The duo used sampling from Led Zeppelin to James Brown. Electronic act The Chemical Brothers have described ‘Beats + Pieces’ as the ‘first bigbeat record’, a style which appeared in the mid 90s.
Coldcut's first mainstream success came when Julian Palmer from Island Records asked them to remix Eric B. & Rakim's "Paid in Full". Released in October 1987, the landmark remix is said to have “laid the groundwork for hip hop’s entry into the UK mainstream”, becoming a breakthrough hit for Eric B & Rakim outside the U.S., reaching #15 in the UK and the top 20 in a number of European countries. It featured a prominent Ofra Haza sample and many other vocal cut ups as well as a looped rhythm which later, when speeded up, proved popular in the Breakbeat genre. Off the back of its success in clubs, the Coldcut "Seven Minutes of Madness" remix ended up being promoted as the single in the UK.
In 1988, More and Black formed Hex, a self-titled “multimedia pop group,” with Mile Visman and Rob Pepperell. While working on videos for artists such as Kevin Saunderson, Queen Latifah and Spiritualized, Hex’s collaborative work went on to incorporate 3D modelling, punk video art, and algorithmic visuals on desktop machines. The video for Coldcut’s ‘Christmas Break’ in 1989 is arguably one of the first pop promos produced entirely on microcomputers.
In 1988, Coldcut released ‘Out To Lunch With Ahead Of Our Time,’ a double LP of Coldcut productions and re-cuts, and the various aliases under which the duo had recorded. This continued the duo’s tradition of releasing limited available vinyl.
The next Coldcut single, released in February 1988, moved towards a more house-influenced style. "Doctorin' the House", which debuted singer Yazz, became a top ten hit, and peaked at #6. In the same year, under the guise Yazz and the Plastic Population, they produced "The Only Way Is Up", a cover of a Northern Soul song. The record reached #1 in the UK in August, and remained there for five weeks, becoming 1988’s second biggest selling single. Producer Youth of Killing Joke also helped Coldcut with this record. The duo had another top hit in September with "Stop This Crazy Thing", which featured reggae vocalist Junior Reid and reached number 21 in the UK.
The single "People Hold On" became another UK Top 20 hit. Released in March 1989, it helped launch the career of the then relatively unknown singer Lisa Stansfield. Coldcut and Mark Saunders produced her debut solo single "This Is the Right Time", which became another UK Top 20 hit in August as well as reaching #21 on the U.S. "Billboard" Hot 100 the following year.
As the duo started to enjoy critical and commercial success, their debut album What's That Noise? was released in April 1989 on Ahead of Our Time and distributed by Big Life Records. The album gave “breaks the full length treatment”, and showcased “their heady blend of hip-hop production aesthetics and proto-acid house grooves”. It also rounded up a heap of unconventional guest features, quoted by SPIN as having “somehow found room at the same table for Queen Latifah and Mark E. Smith”. The album’s track ‘I’m in Deep’ (featuring Smith) prefigured the Indie-dance guitar-breaks crossover of such bands as the Stone Roses and Happy Mondays, utilizing Smith’s freestyle raucous vocals over an acid house backing, and also including psych guitar samples from British rock band Deep Purple. What’s That Noise? reached the Top 20 in the UK and was certified Silver.
1990s.
Coldcut's second album, Some Like It Cold released in 1990 on Ahead Of Our Time, featured a collaboration with Queen Latifah on the single "Find a Way". Though "Find a Way" was a minor hit in the UK, no more singles were released from the album. The duo was given the BPI "Producer of the Year Award" in 1990. Hex - alongside some other London visual experimenters such as iE - produced a series of videos for a longform VHS version of the album. This continued Coldcut and Hex’s pioneering of the use of microcomputers to synthesize electronic music visuals.
After their success with Lisa Stansfield, Coldcut signed with her label, Arista Conflicts arose with the major label, as Coldcut’s “vision extended beyond the formulae of house and techno” and mainstream pop culture (CITATION: The Virgin Encyclopedia Of Nineties Music, 2000). Eventually, the duo’s album Philosophy emerged in 1993. Singles "Dreamer" and "Autumn Leaves" (1994) were both minor hits but the album did not chart.
“Autumn Leaves” had strings recorded at Abbey Road, with a 30 piece string section and an arrangement by film composer Ed Shearmur. The leader of the string section was Simon Jeffes of Penguin Cafe Orchestra. Coldcut’s insistence on their friend Mixmaster Morris to remix “Autumn Leaves” led to one of Morris’ most celebrated remixes, which became a minor legend in ambient music. It has appeared on numerous compilations.
In 1990, whilst on their first tour in Japan (which also featured Norman Cook, who later became Fatboy Slim), Matt and Jon formed their second record label, Ninja Tune, as a self-titled ’technocoloured escape pod,’ and a way to escape the creative control of major labels. The label enabled them to release music under different aliases (e.g.. Bogus Order, DJ Food), which also helped them to avoid pigeonholing as producers. Ninja Tune’s first release was Bogus Order’s ‘Zen Brakes.’ The name Coldcut stayed with Arista so there were no official Coldcut releases for the next three years.
During this time, Coldcut still produced for artists on their new label, releasing a flood of material under different names and continuing to work with young groups. They additionally kept on with Solid Steel on Kiss FM and running the night club Stealth (Club of the Year in the NME, The Face, and Mixmag in 1996).
In 1991, Hex released their first video game, ‘Top Banana’, which was included on a Hex release for the Commodore CDTV machine in 1992, arguably the first complete purpose-designed multimedia system. ‘Top Banana’ was innovative in that it used sampled graphics, contained an ecological theme and a female lead character (dubbed ‘KT’), and its music changed through random processes. Coldcut and Hex presented this multimedia project as an example of the forthcoming convergence of pop music and computer game characters.
In 1992, Hex’s first single - ‘Global Chaos’ / ‘Digital Love Opus 1’ - combined rave visuals with techno and ambient interactive visuals.
In November of that year, Hex released Global Chaos CDTV, which took advantage of the possibilities of the new CD-ROM medium. The Global Chaos CDTV disk (which contained the ‘Top Banana’ game, interactive visuals and audio), was a forerunner of the “CD+” concept, uniting music, graphics, and video games into one. This multi dimensional entertainment product received wide coverage in the national media, including features on Dance Energy, Kaleidoscope on BBC Radio 4, What's Up Doc? on ITV and Reportage on BBC 2. i-D Magazine was quoted as saying, "It's like your TV tripping".
Coldcut videos were made for most songs, often by Hexstatic, and used a lot of stock and sampled footage. Their ‘Timber’ video, which created an AV collage piece using analogous techniques to audio sample collage, was put on heavy rotation on MTV. Stuart Warren Hill of Hexstatic referred to this technique as: “What you see is what you hear.” ‘Timber’ (which appears on both ‘Let Us Play’, Coldcut’s fourth album, and ‘Let Us Replay,’ their fifth) won awards for its innovative use of repetitive video clips synced to the music, including being shortlisted at the Edinburgh Television and Film Festival in their top five music videos of the year in 1998.
Coldcut began integrating video sampling into their live DJ gigs at the time, and incorporated multimedia content that caused press to credit the act as segueing “into the computer age”. Throughout the 90s, Hex created visuals for Coldcut’s live performances, and developed the CD-ROM portion of Coldcut’s ‘Let Us Play’ and ‘Let Us Replay,’ in addition to software developed specifically for the album’s world tour. Hex’s inclusion of music videos and ‘playtools’ (playful art/music software programs) on Coldcut’s CD-Roms was completely ahead of the curve at that time, offering viewers/listeners a high level of interactivity. Playtools such as My Little Funkit and Playtime were the prototypes for Ninja Jamm, the app Coldcut designed and launched 16 years later. Playtime followed on from Coldcut and Hex’s Synopticon installation, developing the auto-cutup algorhythm, and using other random processes to generate surprising combinations. Coldcut and Hex performed live using Playtime at the 1st Sonar Festival in 1994. Playtime was also used to generate the backing track for Coldcut’s collaboration with Jello Biafra, ‘Every Home a Prison’.
In 1994 Coldcut and Hex contributed an installation to the Glasgow Gallery of Modern Art. The piece, called 'Generator' was installed in the Fire Gallery. Generator was an interactive installation which allowed users to mix sound, video, text and graphics and make their own audio-visual mix, modelled on the techniques and technology used by Coldcut in clubs and live performance events. It consisted of two consoles: the left controlling how the sounds are played, the right controlling how the images are played.
As part of the JAM exhibition of “Style, Music and Media” at the Barbican Art Gallery in 1996, Coldcut and Hex were commissioned to produce an interactive audiovisual piece called Synopticon. Conceived and designed by Robert Pepperell and Matt Black, the digital culture synthesiser allows users to “remix” sounds, images, text and music in a partially random, partially controlled way.
The year 1996 also brought the Coldcut name back to More and Black, and the pair celebrated with ‘70 Minutes of Madness,’ a mix CD that became part of the Journeys by DJ series. The release was credited with “bringing to wider attention the sort of freestyle mixing the pair were always known for through their radio show on KISS FM, Solid Steel, and their steady club dates”. It was voted "Best Compilation of All Time" by Jockey Slut in 1998.
In February 1997, they released a double pack single "Atomic Moog 2000" / "Boot the System", the first Coldcut release on Ninja Tune. This was not eligible for the UK chart because time and format restrictions prevented the inclusion of the ‘Natural Rhythm’ video on the CD. In August 1997, a reworking of the early track "More Beats + Pieces" gave them their first UK Top 40 hit since 1989.
The album Let Us Play! followed in September and also made the Top 40. The fourth album by Coldcut, Let Us Play! paid homage to the greats that inspired them. Their first album to be released on Ninja Tune, it featured guest appearances by Grandmaster Flash, Steinski, Jello Biafra, Jimpster, The Herbaliser, Talvin Singh, Daniel Pemberton and Selena Saliva. Coldcut’s cut 'n' paste method on the album was compared to that of Dadaism and William Burroughs. Hex collaborated with Coldcut to produce the multimedia CD-Rom for the album. Hex later evolved the software into the engine that was used on the Let Us Play! world tour.
In 1997, Matt Black - alongside Cambridge based developers Camart - created real-time video manipulation software VJAMM. It allowed users to be a “digital video jockey,”, remixing and collaging sound and images and trigger audio and visual samples simultaneously, subsequently bringing futuristic technology to the audio-visual field. VJAMM rivalled some of the features of high-end and high cost tech at the time. The VJAMM technology, praised as being proof of how far computers changed the face of live music, became seminal in both Coldcut's live sets (which were called a “revelaton” by Melody Maker and DJ sets. Their CCTV live show was featured at major festivals including Glastonbury, Roskilde, Sónar, the Montreux Jazz Festival, and John Peel's Meltdown. The “beautifully simple and devastatingly effective” software was deemed revolutionary, and became recognized as a major factor in the evolution of clubs. It eventually earned a place in the American Museum of the Moving Image's permanent collection. As quoted by The Independent: "Coldcut's motto? 'Don't hate the media, be the media." NME was quoted as saying: “Veteran duo Coldcut are so cool they invented the remix - now they are doing the same for television.”
Also working with Camart, Black designed DJamm software in 1998, which Coldcut used on laptops for their live shows, providing the audio bed alongside VJAMM’s audiovisual samples. Matt Black explained they designed DJamm so they “could perform electronic music in a different way – i.e., not just taking a session band out to reproduce what you put together in the studio using samples. It had a relationship to DJing, but was more interactive and more effective.” Excitingly at that time, DJamm was pioneering in its ability to shuffle sliced loops into intricate sequences, enabling users to split loops into any number of parts.
In 1999, Let Us Replay! was released, a double-disc remix album where Coldcut’s classic tunes were remixed by the likes of Cornelius (which was heralded as a highlight of the album, Irresistible Force, Shut Up And Dance, Carl Craig and J Swinscoe. Let Us Replay! pieces together “short sharp shocks that put the mental in ‘experimental’ and still bring the breaks till the breakadawn”. It also includes a few live tracks from the duo’s innovative world tour. The CD-Rom of the album, which also contained a free demo disc of the VJamm software, was one of the earliest audiovisual CD- ROMs on the market, and Muzik claimed deserved to “have them canonized...it’s like buying an entire mini studio for under $15.”.
2000s.
In 2000, the Solid Steel show moved to BBC London.
Coldcut continued to forge interesting collaborations, including 2001's "Revolution," an EP in which Coldcut created their own political party (The Guilty Party). Featuring scratches and samples of Tony Blair and William Hague speeches, the 3-track EP included Nautilus' "Space Journey," which won an Intermusic contest in 2000. The video was widely played on MTV. With ‘Space Journey,’ Coldcut were arguably the first group to give fans access to the multitrack parts, or “stems,” of their songs, building on the idea of interactivity and sharing from Let Us Play.
In 2001, Coldcut produced tracks for the Sega music video game REZ. REZ replaced typical video game sound effect with electronic music; the player created sounds and melodies, intended to simulate a form of synesthesia. The soundtrack also featured Adam Freeland and Oval.
In 2002, while utilizing VJamm and Detraktor, Coldcut and Juxta remixed Herbie Hancock’s classic ‘Rockit,’ creating both an audio and video remix.
Working with Marcus Clements in 2002, Coldcut released the sample manipulation algorhythm from their DJamm software as a standalone VST plugin that could be used in other software, naming it the Coldcutter.
Also in 2002, Coldcut with UK VJs Headspace (now mainly performing as the VJamm Allstars developed Gridio, an interactive, immersive audio-visual installation for the Pompidou Centre as part of the ‘Sonic Process’ exhibition. The ‘Sonic Process’ exhibition was launched at the MACBA in Barcelona in conjunction with Sónar, featuring Gridio as its centerpiece. In 2003, a commission for Graz led to a specially built version of Gridio, in a cave inside the castle mountain in Austria. Gridio was later commissioned by O2 for two simultaneous customised installations at the O2 Wireless Festivals in Leeds and London in 2007. That same year, Gridio was featured as part of Optronica at the opening week of the new BFI Southbank development in London.
In 2003, Black worked with Penny Rimbaud (ex Crass) on Crass Agenda's Savage Utopia project. Black performed the piece with Rimbaud, Eve Libertine and other players at London’s Vortex Jazz Club.
In 2004, Coldcut collaborated with American video mashup artist TV Sheriff to produce their cut-up entitled ‘Revolution USA.’ The tactical-media project (coordinated with Canadian art duo NomIg) followed on from the UK version and extended the premise “into an open access participatory project”. Through the multimedia political art project, over 12 gigabytes of footage from the last 40 years of US politics were made accessible to download, allowing participants to create a cut-up over a Coldcut beat. Coldcut also collaborated with TV Sheriff and NomIg to produce two audiovisual pieces "World of Evil" (2004) and "Revolution '08" (2008), both composed of footage from the United States presidential elections of respective years. The music used was composed by Coldcut, with "Revolution '08" featuring a remix by the Qemists.
Later that year, a collaboration with the British Antarctic Survey (BAS) led to the psychedelic art documentary 'Wavejammer.’ Coldcut was given access to the BAS archive in order to create sounds and visuals for the short film.
2004 also saw Coldcut produce a radio play in conjunction with renowned young author Hari Kunzru for BBC Radio 3 (incidentally called 'Sound Mirrors').
Coldcut returned with the single "Everything Is Under Control” at the end of 2005, featuring Jon Spencer (of Jon Spencer Blues Explosion) and Mike Ladd. It was followed in 2006 by their fifth studio album Sound Mirrors, which was quoted as being “one of the most vital and imaginative records Jon Moore and Matt Black have ever made”, and saw the duo “continue, impressively, to find new ways to present political statements through a gamut of pristine electronics and breakbeats” (CITATION: Future Music, 2007). The fascinating array of guest vocalists included Soweto Kinch, Annette Peacock, Ameri Baraka, and Saul Williams. The latter followed on from Coldcut’s remix of Williams’ ‘The Pledge’ for a project with DJ Spooky.
A 100-date audiovisual world tour commenced for ‘Sound Mirrors,’ which was considered “no small feat in terms of technology or human effort”. Coldcut was accompanied by scratch DJ Raj and AV artist Juxta, in addition to guest vocalists from the album, including UK rapper Juice Aleem, Roots Manuva, Mpho Skeef, Jon Spencer and house legend Robert Owens.
Three further singles were released from the album including the Top 75 hit "True Skool" with Roots Manuva. The same track appeared on the soundtrack of the video game FIFA Street 2.
Sponsored by the British Council, in 2005 Coldcut introduced AV mixing to India with the Union project, alongside collaborators Howie B and Aki Nawaz of Fun-Da-Mental. Coldcut created an A/V remix of the Bollywood hit movie ‘Kal Ho Naa Ho’.
In 2006, Coldcut performed an A/V set based on “Music for 18 Musicians” as part of Steve Reich’s 70th birthday gig at the Barbican Centre in London.
Coldcut remixed another classic song in 2007: Nina Simone’s ‘Save Me.’ This was part of a remix album called ‘Nina Simone: Remixed & Re-imagined,’ featuring remixes from Tony Humphries, Francois K and Chris Coco.
In February 2007, Coldcut and Mixmaster Morris created a psychedelic AV obituary/tribute Coldcut, Mixmaster Morris, Ken Campbell, Bill Drummond and Alan Moore (March 18, 2007). Robert Anton Wilson tribute show. Queen Elizabeth Hall, London: Mixmaster Morris. (August 28, 2009) to Robert Anton Wilson, the 60s author of Illuminatus! Trilogy. The tribute featured graphic novel writer Alan Moore and artist Bill Drummond and a performance by experimental theatre legend Ken Campbell. Coldcut and Morris’ hour and a half performance resembled a documentary being remixed on the fly, cutting up nearly 15 hours’ worth of Wilson’s lectures.
In 2008, an international group of party organisers, activists and artists including Coldcut received a grant from the Intelligent Energy Department of the European Union, to create a project that promoted intelligent energy and environmental awareness to the youth of Europe. The result was Energy Union, a piece of VJ cinema, political campaign, music tour, party, art exhibition and social media hub. Energy Union toured 12 EU countries throughout 2009 and 2010, completing 24 events in total. Coldcut created the Energy Union show for the tour, a one hour Audio/Visual montage on the theme of Intelligent Energy. In presenting new ideas for climate, environmental and energy communication strategies, the Energy Union tour was well received, and reached a widespread audience in cities across the UK, Germany, Belgium, The Netherlands, Croatia, Slovenia, Austria, Hungary, Bulgaria, Spain and the Czech Republic.
Also in 2008, Coldcut was asked to remix the theme song for British cult TV show Dr Who for the program’s 40th anniversary. In October 2008, Coldcut celebrated the legacy of the BBC Radiophonic Workshop (the place where the Doctor Who theme was created) with a live DJ mix at London’s legendary Roundhouse. The live mix incorporated classic Radiophonic Workshop compositions with extended sampling of the original gear.
Additionally in 2008, Coldcut remixed "Ourselves", a Japanese #1 hit from the single "&" by Ayumi Hamasaki. This mix was included on the album .
Starting in 2009, Matt Black, with musician/artist/coder Paul Miller (creator of the TX Modular Open Source synth), developed Granul8, a new type of visual fx/source Black termed a ‘granular video synthesiser’. Granul8 allows the use of realtime VJ techniques including video feedback combined with VDMX VJ software.
From 2009 onwards, Black has been collaborating with coder and psychedelic mathematician William Rood to create a forthcoming project called Liveloom, a social media AV mixer.
Recent work.
In 2010, Coldcut celebrated 20 years of releasing music with its label, Ninja Tune. A book entitled Ninja Tune: 20 Years of Beats and Pieces was released on 12 August 2010, and an exhibition was held at Black Dog Publishing's Black Dog Space in London, showcasing artwork, design and photography from the label's 20 year history. A compilation album was released on 20 September in two formats: a regular version consisting of two 2-disc volumes, and a limited edition which contained six CDs, six 7" vinyl singles, a hardback copy of the book, a poster and additional items. Ninja Tune also incorporated a series of international parties. This repositioned Ninja as a continually compelling and influential label, being one of the “longest-running (and successful) UK indie labels to come out of the late-1980s/early-90s explosion in dance music and hip-hop” (Pitchfork, September 28, 2010). Pitchfork claimed it had a “right to show off a little”.
In July 2013, Coldcut produced a piece entitled ‘D’autre’ based on the writings of French poet Arthur Rimbaud, for Forum Des Images in Paris.The following month, in August, Coldcut produced a new soundtrack for a section of André Sauvage’s classic film Études sur Paris, which was shown as part of Noise of Art at the BFI in London, which celebrated 100 years of Electronic Music and Silent Cinema. Coldcut put new music to films from the Russolo era, incorporating original recordings of Russolo's proto-synths.
In April 2013, Coldcut released Ninja Jamm, an iOS music remix app, in collaboration with London-based arts and technology firm Seeper. Geared toward both casual listeners and more experienced DJs and music producers, the freemium app allows users to download and remix "Tunepacks" that feature original tracks and mixes by Coldcut, as well as other Ninja artists, creating something new altogether. With the “intuitive yet deep” app, users can turn instruments on and off, swap between clips, add glitches and effects, trigger and pitch-bend stabs and one-off samples, and change the tempo of the track instantly. Users can additionally record as they mix and instantly upload to SoundCloud or save the mixes locally. Tunepack releases for Ninja Jamm are increasingly synchronised with Ninja Tune releases on conventional formats. To date over 20 tunepacks have been released, including Amon Tobin, Bonobo, Coldcut, DJ Food, Martyn, Emika, Machinedrum, Raffertie, Irresistible Force, FaltyDL, Shuttle, Starkey. Ninja Jamm was featured by Apple in the New and Noteworthy section of the App Store in the week of release and it received over 100,000 downloads in the first week. Coldcut intend to develop Ninja Jamm further.
In 2013, Coldcut are working on a new album, collaborating with producer Dave Taylor (aka Solid Groove aka Switch). This is planned for 2014 release.

</doc>
<doc id="6656" url="http://en.wikipedia.org/wiki?curid=6656" title="Cuisine">
Cuisine

Cuisine ( , from French "cuisine", "cooking; culinary art; kitchen"; ultimately from Latin "coquere", "to cook") is a characteristic style of cooking practices and traditions, often associated with a specific culture. Cuisines are often named after the geographic areas or regions from which they originate. A cuisine is primarily influenced by the ingredients that are available locally or through trade. Religious food laws, such as Islamic dietary laws and Jewish dietary laws, can also exercise a strong influence on cuisine. Regional food preparation traditions, customs and ingredients often combine to create dishes unique to a particular region.
History.
Cuisine can be stated as the foods and methods of food preparation traditional to a region or population. The major factors shaping a cuisine are climate, which in large measure determines the native raw materials that are available, economic conditions, which affect trade and can affect food distribution, imports and exports, and religiousness or sumptuary laws, under which certain foods are required or proscribed.
Climate also affects the supply of fuel for cooking; a common Chinese food preparation method was cutting food into small pieces to cook foods quickly and conserve scarce firewood and charcoal. Foods preserved for winter consumption by smoking, curing, and pickling have remained significant in world cuisines for their altered gustatory properties even when these preserving techniques are no longer strictly necessary to the maintenance of an adequate food supply.
New cuisines continue to evolve in contemporary times. An example is fusion cuisine, which combines elements of various culinary traditions while not being categorized per any one cuisine style, and generally refers to the innovations in many contemporary restaurant cuisines since the 1970s.
Regional cuisines.
Global cuisines is a cuisine that is practiced around the world, and can be categorized by various regions according to the common use of major foodstuffs, including grains, produce and cooking fats. Regional cuisines may vary based upon food availability and trade, cooking traditions and practices, and cultural differences. For example, in Central and South America, corn (maize), both fresh and dried, is a staple food. In northern Europe, wheat, rye, and fats of animal origin predominate, while in southern Europe olive oil is ubiquitous and rice is more prevalent. In Italy the cuisine of the north, featuring butter and rice, stands in contrast to that of the south, with its wheat pasta and olive oil. China likewise can be divided into rice regions and noodle & bread regions. Throughout the Middle East and Mediterranean there is a common thread marking the use of lamb, olive oil, lemons, peppers, and rice. The vegetarianism practiced in much of India has made pulses (crops harvested solely for the dry seed) such as chickpeas and lentils as significant as wheat or rice. From India to Indonesia the use of spices is characteristic; coconuts and seafood are used throughout the region both as foodstuffs and as seasonings.

</doc>
<doc id="6658" url="http://en.wikipedia.org/wiki?curid=6658" title="October 2003">
October 2003

__NOTOC__
October 2003: January – February – March – April – May – June – July – August – September – October – November – December
Events.
October 21, 2003.
[http://www.bayarea.com/mld/mercurynews/news/local/7097388.htm]

</doc>
<doc id="6660" url="http://en.wikipedia.org/wiki?curid=6660" title="Codec">
Codec

A codec is a device or computer program capable of encoding or decoding a digital data stream or signal. The word codec is a portmanteau of "coder-decoder" or, less commonly, "compressor-decompressor". A "codec" (the "program") should not be confused with a coding or compression "format" or "standard" – a format is a document (the standard), a way of storing data, while a codec is a program (an "implementation") which can read or write such files. In practice, however, "codec" is sometimes used loosely to refer to formats.
A codec encodes a data stream or signal for transmission, storage or encryption, or decodes it for playback or editing. Codecs are used in videoconferencing, streaming media and video editing applications. A video camera's analog-to-digital converter (ADC) converts its analog signals into digital signals, which are then passed through a video compressor for digital transmission or storage. A receiving device then runs the signal through a video decompressor, then a digital-to-analog converter (DAC) for analog display. The term "codec" is also used as a generic name for a videoconferencing unit.
Related concepts.
An endec (encoder/decoder) is a similar yet different concept mainly used for hardware. In the mid 20th century, a "codec" was hardware that coded analog signals into pulse-code modulation (PCM) and decoded them back. Late in the century the name came to be applied to a class of software for converting among digital signal formats, and including compander functions.
A modem is a contraction of modulator/demodulator (although they were referred to as "datasets" by telcos) and converts digital data from computers to analog for phone line transmission. On the receiving end the analog is converted back to digital. Codecs do the opposite (convert audio analog to digital and then computer digital sound back to audio). 
An audio codec converts analog audio signals into digital signals for transmission or storage. A receiving device then converts the digital signals back to analog using an audio decompressor, for playback. An example of this is the codecs used in the sound cards of personal computers. A video codec accomplishes the same task for video signals.
Compression quality.
Media codecs.
Two principal techniques are used in codecs, pulse-code modulation and delta modulation. Codecs are often designed to emphasize certain aspects of the media to be encoded. For example, a digital video (using a DV codec) of a sports event needs to encode motion well but not necessarily exact colors, while a video of an art exhibit needs to encode color and surface texture well.
Audio codecs for cell phones need to have very low latency between source encoding and playback. In contrast, audio codecs for recording or broadcast can use high-latency audio compression techniques to achieve higher fidelity at a lower bit-rate.
There are thousands of audio and video codecs, ranging in cost from free to hundreds of dollars or more. This variety of codecs can create compatibility and obsolescence issues. The impact is lessened for older formats, for which free or nearly-free codecs have existed for a long time. The older formats are often ill-suited to modern applications, however, such as playback in small portable devices. For example, raw uncompressed PCM audio (44.1 kHz, 16 bit stereo, as represented on an audio CD or in a .wav or .aiff file) has long been a standard across multiple platforms, but its transmission over networks is slow and expensive compared with more modern compressed formats, such as MP3.
Many multimedia data streams contain both audio and video, and often some metadata that permit synchronization of audio and video. Each of these three streams may be handled by different programs, processes, or hardware; but for the multimedia data streams to be useful in stored or transmitted form, they must be encapsulated together in a container format. 
Lower bitrate codecs allow more users, but they also have more distortion. Beyond the initial increase in distortion, lower bit rate codecs also achieve their lower bit rates by using more complex algorithms that make certain assumptions, such as those about the media and the packet loss rate. Other codecs may not make those same assumptions. When a user with a low bitrate codec talks to a user with another codec, additional distortion is introduced by each transcoding.
AVI is sometimes erroneously described as a codec, but AVI is actually a container format, while a codec is a software or hardware tool that encodes or decodes audio or video into or from some audio or video format. Audio and video encoded with many codecs might be put into an AVI container, although AVI is not an ISO standard. There are also other well-known container formats, such as Ogg, ASF, QuickTime, RealMedia, Matroska, and DivX Media Format. Some container formats which are ISO standards are MPEG transport stream, MPEG program stream, MP4 and ISO base media file format.

</doc>
<doc id="6663" url="http://en.wikipedia.org/wiki?curid=6663" title="Clyde Tombaugh">
Clyde Tombaugh

Clyde William Tombaugh (February 4, 1906January 17, 1997) was an American astronomer. Although he is best known for discovering the dwarf-planet Pluto in 1930, the first object to be discovered in what would later be identified as the Kuiper belt, Tombaugh also discovered many asteroids; he also called for the serious scientific research of unidentified flying objects, or "U.F.O.s".
Biography.
Tombaugh was born in Streator, Illinois. After his family moved to Burdett, Kansas in 1922, Tombaugh's plans for attending college were frustrated when a hailstorm ruined his family's farm crops. Starting in 1926, he built several telescopes with lenses and mirrors he ground himself. He sent drawings of Jupiter and Mars to the Lowell Observatory, which offered him a job. Tombaugh worked there from 1929 to 1945.
Following his discovery of Pluto, Tombaugh earned bachelor's and master's degrees in astronomy from the University of Kansas in 1936 and 1938. During World War II he taught naval personnel navigation at Northern Arizona University. He worked at White Sands Missile Range in the early 1950s, and taught astronomy at New Mexico State University from 1955 until his retirement in 1973.
The asteroid 1604 Tombaugh, discovered in 1931, is named after him. He discovered hundreds of asteroids, beginning with 2839 Annette in 1929, mostly as a by-product of his search for Pluto and his searches for other celestial objects. Tombaugh named some of them after his wife, children and grandchildren. The Royal Astronomical Society awarded him the Jackson-Gwilt Medal in 1931. 
In August 1992, JPL scientist Robert Staehle called Tombaugh, requesting permission to visit his planet. "I told him he was welcome to it," Tombaugh later remembered, "though he's got to go one long, cold trip." The call eventually led to the launch of the New Horizons space probe to Pluto in 2006.
Death.
Tombaugh died on January 17, 1997 in Las Cruces, New Mexico, at the age of 90. A small portion of his ashes were placed aboard the New Horizons spacecraft. The container includes the inscription: "Interned (sic) herein are remains of American Clyde W. Tombaugh, discoverer of Pluto and the solar system's 'third zone'. Adelle and Muron's boy, Patricia's husband, Annette and Alden's father, astronomer, teacher, punster, and friend: Clyde W. Tombaugh (1906–1997)".
Tombaugh was survived by his widow, Patricia (1912–2012), and their children, daughter Annette and son Alden. Tombaugh was an active Unitarian-Universalist.
Family.
Through the daughter of his youngest brother, Robert M., Tombaugh is the great uncle of Los Angeles Dodgers pitcher Clayton Kershaw.
Discovery of Pluto.
While a young researcher working for the Lowell Observatory in Flagstaff, Arizona, Tombaugh was given the job to perform a systematic search for a trans-Neptunian planet (also called Planet X), which had been predicted by Percival Lowell and William Pickering.
Tombaugh used the observatory's 13-inch astrograph to take photographs of the same section of sky several nights apart. He then used a blink comparator to compare the different images. When he shifted between the two images, a moving object, such as a planet, would appear to jump from one position to another, while the more distant objects such as stars would appear stationary. Tombaugh noticed such a moving object in his search, near the place predicted by Lowell, and subsequent observations showed it to have an orbit beyond that of Neptune. This ruled out classification as an asteroid, and they decided this was the ninth planet that Lowell had predicted. The discovery was made on Tuesday, February 18, 1930, using images taken the previous month. The name "Pluto" was reportedly suggested by Venetia Burney, then an 11-year-old English school girl, who died in April 2009, having lived to see the reclassification of Pluto as a dwarf planet. It won out over numerous other suggestions because it was the name of the Roman god of the underworld, who was able to render himself invisible, and because Percival Lowell's initials PL formed the first 2 letters. The name Pluto was officially adopted on May 1, 1930.
Following the discovery, starting in the 1990s, of other Kuiper belt objects, Pluto began to be seen not as a planet orbiting alone at 40 AU, but as the largest of a group of icy bodies in that region of space. After it was shown that at least one such body was larger than Pluto, on August 24, 2006 the International Astronomical Union (IAU) reclassified Pluto, grouping it with two similarly sized "dwarf planets" rather than with the eight "classical planets".
Tombaugh's widow Patricia stated after the IAU's decision that while Clyde may have been disappointed with the change since he had resisted attempts to remove Pluto's planetary status in his lifetime, he would have accepted the decision now if he were alive. She noted that he "was a scientist. He would understand they had a real problem when they start finding several of these things flying around the place." Hal Levison offered this perspective on Tombaugh's place in history: "Clyde Tombaugh discovered the Kuiper Belt. That's a helluva lot more interesting than the ninth planet."
Further search.
Tombaugh continued searching for some years after the discovery of Pluto, and the lack of further discoveries left him satisfied that no other object of a comparable apparent magnitude existed near the ecliptic. No more trans-Neptunian objects were discovered until , in 1992.
However, more recently the relatively bright object has been discovered. It has a relatively high orbital inclination, but at the time of Tombaugh's discovery of Pluto, Makemake was only a few degrees from the ecliptic near the border of Taurus and Auriga at an apparent magnitude of 16. This position was also very near the galactic equator, making it almost impossible to find such an object within the dense concentration of background stars of the Milky Way.
Asteroids discovered.
Tombaugh discovered nearly 800 asteroids during his search for Pluto and years of follow-up searches looking for another candidate for the postulated Planet X. Tombaugh is also credited with the discovery of periodic comet 274P/Tombaugh–Tenagra. He also discovered hundreds of variable stars, as well as star clusters, galaxy clusters, and a galaxy supercluster.
Interest in UFOs.
Tombaugh was probably the most eminent astronomer to have reported seeing unidentified flying objects and to support the extraterrestrial hypothesis. On August 20, 1949, Tombaugh saw several unidentified objects near Las Cruces, New Mexico. He described them as six to eight rectangular lights, stating: "I doubt that the phenomenon was any terrestrial reflection, because... nothing of the kind has ever appeared before or since... I was so unprepared for such a strange sight that I was really petrified with astonishment."
Tombaugh observed these rectangles of light for about 3 seconds and his wife saw them for about 1½ seconds. He never supported the interpretation as a spaceship that has often been attributed to him. He considered other possibilities, with a temperature inversion as the most likely cause.From my own studies of the solar system I cannot entertain any serious possibility for intelligent life on other planets, not even for Mars... The logistics of visitations from planets revolving around the nearer stars is staggering. In consideration of the hundreds of millions of years in the geologic time scale when such visits may have possibly occurred, the odds of a single visit in a given century or millennium are overwhelmingly against such an event.
A much more likely source of explanation is some natural optical phenomenon in our own atmosphere. In my 1949 sightings the faintness of the object, together with the manner of fading in intensity as it traveled away from the zenith towards the southeastern horizon, is quite suggestive of a reflection from an optical boundary or surface of slight contrast in refractive index, as in an inversion layer.
I have never seen anything like it before or since, and I have spent a lot of time where the night sky could be seen well. This suggests that the phenomenon involves a comparatively rare set of conditions or circumstances to produce it, but nothing like the odds of an interstellar visitation.
Another sighting by Tombaugh a year or two later while at a White Sands observatory was of an object of −6 magnitude, four times brighter than Venus at its brightest, going from the zenith to the southern horizon in about 3 seconds. The object executed the same maneuvers as in Tombaugh's first sighting.
Tombaugh later reported having seen three of the mysterious green fireballs, which suddenly appeared over New Mexico in late 1948 and continued at least through the early 1950s. Despite this, the final report of Project Twinkle claimed that he "... never observed an unexplainable aerial object despite his continuous and extensive observations of the sky."
In 1956 Tombaugh had the following to say about his various sightings: "I have seen three objects in the last seven years which defied any explanation of known phenomenon, such as Venus, atmospheric optic, meteors or planes. I am a professional, highly skilled, professional astronomer. In addition I have seen three green fireballs which were unusual in behavior from normal green fireballs... I think that several reputable scientists are being unscientific in refusing to entertain the possibility of extraterrestrial origin and nature."
Shortly after this in January 1957, in an Associated Press article in the "Alamogordo Daily News" titled "Celestial Visitors May Be Invading Earth's Atmosphere," Tombaugh was again quoted on his sightings and opinion about them. "Although our own solar system is believed to support no other life than on Earth, other stars in the galaxy may have hundreds of thousands of habitable worlds. Races on these worlds may have been able to utilize the tremendous amounts of power required to bridge the space between the stars..." Tombaugh stated that he had observed celestial phenomena which he could not explain, but had seen none personally since 1951 or 1952. "These things, which do appear to be directed, are unlike any other phenomena I ever observed. Their apparent lack of obedience to the ordinary laws of celestial motion gives credence."
In 1949, Tombaugh had also told the Naval missile director at White Sands Missile Range, Commander Robert McLaughlin, that he had seen a bright flash on Mars on August 27, 1941, which he now attributed to an atomic blast. Tombaugh also noted that the first atomic bomb tested in New Mexico would have lit up the dark side of the Earth like a neon sign and that Mars was coincidentally quite close at the time, the implication apparently being that the atomic test would have been visible from Mars.
In June 1952, Dr. J. Allen Hynek, an astronomer acting as a scientific consultant to the Air Force's Project Blue Book UFO study, secretly conducted a survey of fellow astronomers on UFO sightings and attitudes while attending an astronomy convention. Tombaugh and four other astronomers, including Dr. Lincoln LaPaz of the University of New Mexico, told Hynek about their sightings. Tombaugh also told Hynek that his telescopes were at the Air Force's disposal for taking photos of UFOs, if he was properly alerted.
Near-Earth satellite search.
Tombaugh's offer may have led to his involvement in a search for near-Earth satellites, first announced in late 1953 and sponsored by the Army Office of Ordnance Research. Another public statement was made on the search in March 1954 (photo at right), emphasizing the rationale that such an orbiting object would serve as a natural space station. However, according to Donald Keyhoe, later director of the National Investigations Committee on Aerial Phenomena (NICAP), the real reason for the sudden search was because two near-Earth orbiting objects had been picked up on new long-range radar in the summer of 1953, according to his Pentagon source.
By May 1954, Keyhoe was making public statements that his sources told him the search had indeed been successful, and either one or two objects had been found. However, the story did not break until August 23, 1954, when "Aviation Week" magazine stated that two satellites had been found only 400 and 600 miles out. They were termed "natural satellites" and implied that they had been recently captured, despite this being a virtual impossibility. The next day, the story was in many major newspapers. Dr. LaPaz was implicated in the discovery in addition to Tombaugh. LaPaz had earlier conducted secret investigations on behalf of the Air Force on the green fireballs and other unidentified aerial phenomena over New Mexico. The "New York Times" reported on August 29 that "a source close to the O. O. R. unit here described as 'quite accurate' the report in the magazine Aviation Week that two previously unobserved satellites had been spotted and identified by Dr. Lincoln Lepaz of the University of New Mexico as natural and not artificial objects. This source also said there was absolutely no connection between the reported satellites and flying saucer reports." However, in the October 10th issue, LaPaz said the magazine article was "false in every particular, in so far as reference to me is concerned."
Both LaPaz and Tombaugh were to issue public denials that anything had been found. The October 1955 issue of "Popular Mechanics" magazine reported: "Professor Tombaugh is closemouthed about his results. He won't say whether or not any small natural satellites have been discovered. He does say, however, that newspaper reports of 18 months ago announcing the discovery of natural satellites at 400 and 600 miles out are not correct. He adds that there is no connection between the search program and the reports of so-called flying saucers."
At a meteor conference in Los Angeles in 1957, Tombaugh reiterated that his four-year search for "natural satellites" had been unsuccessful. In 1959, Tombaugh was to issue a final report stating that nothing had been found in his search. His personal 16-inch telescope was reassembled and dedicated on September 17, 2009 at Rancho Hidalgo, New Mexico (near Animas, New Mexico), adjacent to "Astronomy" 's new observatory.

</doc>
<doc id="6666" url="http://en.wikipedia.org/wiki?curid=6666" title="Christopher Báthory">
Christopher Báthory

Christopher Báthory (Hungarian: "Báthory Kristóf") (1530, Szilágysomlyó – May 27, 1581, Gyulafehérvár) was a voivode of Transylvania. He succeeded his brother Stephen Báthory. He was the father of Sigismund Báthory and Gryzelda Bathory.

</doc>
<doc id="6667" url="http://en.wikipedia.org/wiki?curid=6667" title="CPAN">
CPAN

CPAN, the Comprehensive Perl Archive Network, is an archive of over 129,703 modules of software in 29,092 distributions, written by 11,274 authors, written in the Perl programming language, as well as documentation for them. It has a presence on the World Wide Web at metacpan.org or via the old interface www.cpan.org and is mirrored worldwide at more than 260 locations. "CPAN" can denote either the archive network itself, or the Perl program that acts as an interface to the network and as an automated software installer (somewhat like a package manager). Most software on CPAN is free and open source software. CPAN was conceived in 1993, and the first web-accessible mirror was launched in January 1997.
Modules.
Like many programming languages, Perl has mechanisms to use external libraries of code, making one file contain common routines used by several programs. Perl calls these "modules". Perl modules are typically installed in one of several directories whose paths are placed in the Perl interpreter when it is first compiled; on Unix-like operating systems, common paths include "/usr/lib/perl5", "/usr/local/lib/perl5", and several of their subdirectories.
Perl comes with a small set of "core modules". Some of these perform bootstrapping tasks, such as , which is used for building and installing other extension modules; others, like CGI.pm, are merely commonly used. The authors of Perl do not expect this limited group to meet every need, however.
Role.
The CPAN's main purpose is to help programmers locate modules and programs not included in the Perl standard distribution. Its structure is decentralized. Authors maintain and improve their own modules. Forking, and creating competing modules for the same task or purpose is common. There is no formal bug tracking system, but there is a third-party bug tracking system that CPAN designated as the suggested official method of reporting issues with modules. Continuous development on modules is rare; many are abandoned by their authors, or go years between new versions being released. Sometimes a maintainer will be appointed to an abandoned module. They can release new versions of the module, and accept patches from the community to the module as their time permits. CPAN has no revision control system, although the source for the modules is often stored on GitHub. Also, the complete history of the CPAN and all its modules is available as the GitPAN project, allowing to easily see the complete history for all the modules and for easy maintenance of forks. CPAN is also used to distribute new versions of Perl, as well as related projects, such as Parrot.
The CPAN is an important resource for the professional Perl programmer. With over 23,000 modules (containing 20,000,000 lines of code) as of July 2011, the CPAN can save programmers weeks of time, and large Perl programs often make use of dozens of modules. Some of them, such as the DBI family of modules used for interfacing with SQL databases, are nearly irreplaceable in their area of function; others, such as the module, are simply handy resources containing a few common functions.
Structure.
Files on the CPAN are referred to as "distributions". A distribution may consist of one or more modules, documentation files, or programs packaged in a common archiving format, such as a gzipped tar archive or a ZIP file. Distributions will often contain installation scripts (usually called "Makefile.PL" or "Build.PL") and test scripts which can be run to verify the contents of the distribution are functioning properly. New distributions are uploaded to the Perl Authors Upload Server, or PAUSE (see the section Uploading distributions with PAUSE).
In 2003, distributions started to include metadata files, called "META.yml", indicating the distribution's name, version, dependencies, and other useful information; however, not all distributions contain metadata. When metadata is not present in a distribution, the PAUSE's software will usually try to analyze the code in the distribution to look for the same information; this is not necessarily very reliable.
With thousands of distributions, CPAN needs to be structured to be useful. Distributions on the CPAN are divided into 24 broad "chapters" based on their purpose, such as "Internationalization and Locale"; "Archiving, Compression, And Conversion"; and "Mail and Usenet News". Distributions can also be browsed by author. Finally, the natural hierarchy of Perl module names (such as "Apache::DBI" or "Lingua::EN::Inflect") can sometimes be used to browse modules in the CPAN.
CPAN module distributions usually have names in the form of "CGI-Application-3.1" (where the :: used in the module's name has been replaced with a dash, and the version number has been appended to the name), but this is only a convention; many prominent distributions break the convention, especially those that contain multiple modules. Security restrictions prevent a distribution from ever being replaced, so virtually all distribution names do include a version number.
Components.
Mirrors.
The heart of CPAN is its worldwide network of more than 260 mirrors in more than 60 countries. CPAN's master site has over 149 direct public mirrors. Each site contains up to the full 3.9 gigabytes of data, or a subset of it if the mirror's maintainer wishes to selectively choose.
Most mirrors update themselves hourly, daily or bidaily from the CPAN master site. Some sites are major FTP servers which mirror lots of other software, but others are simply servers owned by companies that use Perl heavily. There are at least two mirrors on every continent except Antarctica.
For more information on CPAN mirrors, see mirrors.cpan.org.
Search engines.
Several search engines have been written to help Perl programmers sort through the CPAN. The most popular and official is search.cpan.org, which includes textual search, a browsable index of modules, and extracted copies of all distributions currently on the CPAN. Other CPAN search engines that have been set up are:
Testers.
CPAN Testers are a group of volunteers, who will download and test distributions as they are uploaded to CPAN. This enables the authors to have their modules tested on many platforms and environments that they would otherwise not have access to, thus helping to promote portability, as well as a degree of quality. Smoke testers send reports, which are then collated and used for a variety of presentation websites, including the main reports site, statistics and dependencies.
Other supporting websites.
A family of other loosely integrated support websites have been created as the CPAN has grown in size and scale. These are created and managed by individual Perl developers, and provide data feeds to each other in various ad-hoc ways.
CPAN.pm and CPANPLUS.
There is also a Perl core module named CPAN; it is usually differentiated from the repository itself by using the name CPAN.pm. CPAN.pm is mainly an interactive shell which can be used to search for, download, and install distributions. An interactive shell called cpan is also provided in the Perl core, and is the usual way of running CPAN.pm. After a short configuration process and mirror selection, it uses tools available on the user's computer to automatically download, unpack, compile, test, and install modules. It is also capable of updating itself.
More recently, an effort to replace CPAN.pm with something cleaner and more modern has resulted in the CPANPLUS (or CPAN++) set of modules. CPANPLUS separates the back-end work of downloading, compiling, and installing modules from the interactive shell used to issue commands. It also supports several advanced features, such as cryptographic signature checking and test result reporting. Finally, CPANPLUS can uninstall a distribution. CPANPLUS was added to the Perl core in version 5.10.0.
Both modules can check a distribution's dependencies and can be set to recursively install any prerequisites, either automatically or with individual user approval. Both support FTP and HTTP and can work through firewalls and proxies.
Uploading distributions with PAUSE.
Authors can upload new distributions to the CPAN through the "Perl Authors Upload Server" (PAUSE). To do so, they must request a PAUSE account. Registration information can be found at the PAUSE faq
Registrations are manually reviewed, so the process may take a week or longer.
Once registered, the new PAUSE account has a directory in the CPAN under "authors/id/(first letter)/(first two letters)/(author ID)". They may use a web interface at pause.perl.org, or the PAUSE ftp server to upload files to their directory and delete them. PAUSE will warn an administrator if a user uploads a module that already exists, unless they are listed as a "co-maintainer". This can be specified through PAUSE's web interface.
Influence.
Experienced Perl programmers often comment that half of Perl's power is in the CPAN. It has been called Perl's killer app. Though the TeX typesetting language has an equivalent, the CTAN (and in fact the CPAN's name is based on the CTAN), few languages have an exhaustive central repository for libraries. The PHP language has PECL and PEAR, Python has a PyPI (Python Package Index) repository, Ruby has RubyGems, R has CRAN, Node.js has npm, Lua has LuaRocks, Haskell has Hackage and an associated installer/make clone cabal; but none of these are as large as the CPAN. Recently, Common Lisp has a de facto CPAN-like system—the Quicklisp repositories. Other major languages, such as Java and C++, have nothing similar to the CPAN (though for Java there is central Maven).
The CPAN has grown so large and comprehensive over the years that Perl users are known to express surprise when they start to encounter topics for which a CPAN module "doesn't" exist already.
The CPAN's influence on Perl's eclectic culture should not be underestimated either. As a hive of activity in the Perl world, the CPAN both shapes and is shaped by Perl culture. Its "self-appointed master librarian", Jarkko Hietaniemi, often takes part in the April Fools Day jokes so popular on the Internet; on 1 April 2002 the site was temporarily named to "CJAN", where the "J" stood for "Java". In 2003, the www.cpan.org domain name was redirected to Matt's Script Archive, a site infamous in the Perl community for having badly written code.
Beyond April Fools', however, some of the distributions on the CPAN are jokes in themselves. The Acme:: hierarchy is reserved for joke modules; for instance, Acme::Don't adds a codice_1 function that doesn't run the code given to it (to complement the codice_2 built-in, which does). Even outside the Acme:: hierarchy, some modules are still written largely for amusement; one example is Lingua::Romana::Perligata, which can be used to write Perl programs in a subset of Latin.
Derivative works.
In 2005, a group of Perl developers who also had an interest in JavaScript got together to create JSAN, the JavaScript Archive Network. The JSAN is a near-direct port of the CPAN infrastructure for use with the JavaScript language, which for most of its lifespan did not have a cohesive "community".
In 2008, after a chance meeting with CPAN admin Adam Kennedy at the Open Source Developers Conference, Linux kernel developer Rusty Russell created the CCAN, the Comprehensive C Archive Network. The CCAN is a direct port of the CPAN architecture for use with the C language.

</doc>
<doc id="6669" url="http://en.wikipedia.org/wiki?curid=6669" title="Colorado Rockies">
Colorado Rockies

The Colorado Rockies are a Major League Baseball (MLB) franchise based in Denver, Colorado. They are currently members of the National League West Division. Their home venue is Coors Field. Their manager is Walt Weiss.
The Colorado Rockies have won one National League championship (2007). They mounted a spirited rally in the last month of the 2007 regular season, winning 21 of their final 22 games, and reached the 2007 World Series. However, they were swept by the American League champion Boston Red Sox in four games.
History.
Denver had long been a hotbed of Denver Bears minor league baseball and many in the area desired a major league team. Following the Pittsburgh drug trials, an unsuccessful attempt was made to purchase the Pittsburgh Pirates and relocate them. However, in 1991, as part of Major League Baseball's two-team expansion (they also added the Florida (now Miami) Marlins), an ownership group representing Denver led by John Antonucci and Michael I. Monus were granted a franchise; they took the name "Rockies" due to Denver's proximity to the Rocky Mountains, which is reflected in their logo. They began play in 1993, sharing Mile High Stadium with the National Football League's Denver Broncos their first two seasons while Coors Field was constructed. It was completed for the 1995 Major League Baseball season.
In 1993 they started play in the western division of the National League. Since that date, the Rockies have reached the MLB postseason three times, each time as the National League wild card team. Twice (1995 and 2009) they were eliminated in the first round of the playoffs. In 2007 the Rockies advanced all the way to the World Series, only to be swept by the Boston Red Sox.
The Rockies have played their home games at Coors Field since the beginning of the franchise. Their newest Spring Training home, Salt River Fields at Talking Stickin Scottsdale, Arizona, opened in March 2011 and is shared with the Arizona Diamondbacks.
Uniform.
At the start of the 2012 season, the Rockies introduced "Purple Mondays" in which the team wears its purple uniform every Monday gameday.
Baseball Hall of Famers.
No inducted members of the Baseball Hall of Fame have played for or managed the Rockies.
Retired numbers.
Apart from number 42 (Jackie Robinson's number, retired throughout all of baseball in 1997), the Rockies originally had no retired numbers of their own. On February 6, 2014, the Rockies announced that Todd Helton will be the first Rockies player to have his number retired. This will occur on Sunday, August 17, 2014. 
Radio and television.
As of 2010, Rockies' flagship radio station is KOA 850AM, with some late-season games broadcast on KHOW 630 AM due to conflicts with Denver Broncos games. Jerry Schemmel and Jack Corrigan are the radio announcers which both serve as backup TV announcers whenever Drew Goodman is not on the broadcast. The Rockies Radio Network is composed of 38 affiliate stations in eight states.
As of 2013, Spanish broadcasts of the Rockies are heard on KNRV 1150 AM.
As of 2013, all games will be produced and televised by Root Sports Rocky Mountain. All 150 games produced by Root Sports Rocky Mountain will be broadcast in HD. Jeff Huson, Drew Goodman and George Frazier form the TV broadcast team with Marc Stout, Jenny Cavnar and Tracy Ringolsby handling the pre-game and post-game shows.

</doc>
<doc id="6670" url="http://en.wikipedia.org/wiki?curid=6670" title="Cement">
Cement

A cement is a binder, a substance that sets and hardens as the cement dries and also reacts with carbon dioxide in the air dependently, and can bind other materials together. The word "cement" traces to the Romans, who used the term "opus caementicium" to describe masonry resembling modern concrete that was made from crushed rock with burnt lime as binder. The volcanic ash and pulverized brick additives that were added to the burnt lime to obtain a hydraulic binder were later referred to as "cementum", "cimentum", "cäment", and "cement".
Cements used in construction can be characterized as being either hydraulic or non-hydraulic, depending upon the ability of the cement to be used in the presence of water (see hydraulic and non-hydraulic lime plaster).
Non-hydraulic cement will not set in wet conditions or underwater, and is attacked by some aggressive chemicals after setting.
Hydraulic cement is made by replacing some of the cement in a concrete mix with activated aluminium silcates, pozzolanas, such as fly ash, to activate cement setting in wet condition or underwater and further protects hardened concrete from chemical attack ("e.g.," Portland cement).
The chemical process for hydraulic cement found by ancient Romans used volcanic ash (activated aluminium silicates), to activate cement hardening between the anhydrous cement powder and water or plaster and water instead of relying on water drying out and simultaneously reacting with airborne carbon dioxode.
Presently cheaper, pollution free fly ash from power stations or other waste or by products are used as pozzolanas with plain cement to produce hydraulic cement.
Pozzolanas can replace up to 40% of Portland cement.
Thus, cement can harden underwater or when constantly exposed to wet weather. The chemical reaction results in hydrates that are not very water-soluble and so are quite durable in water and from chemical attack. Non-hydraulic cements and plasters do not harden in wet conditions.
The most important uses of cement are as an ingredient in the production of mortar in masonry, and of concrete, a combination of cement and an aggregate to form a strong building material.
Chemistry.
Non-hydraulic cement, such as slaked lime (calcium hydroxide mixed with water), harden by carbonation in presence of the carbon dioxide naturally present in the air. First calcium oxide is produced by lime calcination at temperatures above 825 °C (1,517 °F) for about 10 hours at atmospheric pressure: 
The calcium oxide is then "spent" (slaked) mixing it to water to make slaked lime:
Once the water in excess from the slaked lime is completely evaporated (this process is technically called "setting"), the carbonation starts:
This reaction takes a significant amount of time because the partial pressure of carbon dioxide in the air is small. The reaction of carbonation requires the air be in contact with the dry cement, hence, for this reason the slaked lime is a non-hydraulic cement and cannot be used under water. This whole process is called the "lime cycle".
Conversely, the chemistry ruling the action of the hydraulic cement is hydration. Hydraulic cements (such as Portland cement) are made of a mixture of silicates and oxides, the four main components being: 
The silicates are responsible of the mechanical properties of the cement, the celite and the browmillerite are essential to allow the formation of the liquid phase during the cooking.
The chemistry of the above listed reactions is not completely clear and is still the object of research.
History of the origin of cement.
Cements before the 18th century.
An early version of cement made with lime, sand, and gravel was used in Mesopotamia in the third millennium B.C. and later in Egypt. It is uncertain where it was first discovered that a combination of hydrated non-hydraulic lime and a pozzolan produces a hydraulic mixture (see also: Pozzolanic reaction), but concrete made from such mixtures was first used by the Ancient Macedonians and three centuries later on a large scale by Roman engineers. They used both natural pozzolans (trass or pumice) and artificial pozzolans (ground brick or pottery) in these concretes. Many excellent examples of structures made from these concretes are still standing, notably the huge dome of the Pantheon in Rome and the massive Baths of Caracalla. The vast system of Roman aqueducts also made extensive use of hydraulic cement. Although any preservation of this knowledge in literary sources from the Middle Ages is unknown, medieval masons and some military engineers maintained an active tradition of using hydraulic cement in structures such as canals, fortresses, harbors, and shipbuilding facilities. This technical knowledge of making hydraulic cement was later formalized by French and British engineers in the 18th century.
Cements in the 18th, 19th, and 20th centuries.
John Smeaton made an important contribution to the development of cements when he was planning the construction of the third Eddystone Lighthouse (1755–9) in the English Channel now known as Smeaton's Tower. He needed a hydraulic mortar that would set and develop some strength in the twelve hour period between successive high tides. He performed experiments with combinations of different limestones and additives including trass and pozzolanas and did exhaustive market research on the available hydraulic limes, visiting their production sites, and noted that the "hydraulicity" of the lime was directly related to the clay content of the limestone from which it was made. Smeaton was a civil engineer by profession, and took the idea no further.
In Britain particularly, good quality building stone became ever more expensive during a period of rapid growth, and it became a common practice to construct prestige buildings from the new industrial bricks, and to finish them with a stucco to imitate stone. Hydraulic limes were favored for this, but the need for a fast set time encouraged the development of new cements. Most famous was Parker's "Roman cement". This was developed by James Parker in the 1780s, and finally patented in 1796. It was, in fact, nothing like material used by the Romans, but was a "natural cement" made by burning septaria – nodules that are found in certain clay deposits, and that contain both clay minerals and calcium carbonate. The burnt nodules were ground to a fine powder. This product, made into a mortar with sand, set in 5–15 minutes. The success of "Roman cement" led other manufacturers to develop rival products by burning artificial hydraulic lime cements of clay and chalk.
Roman cement quickly became popular but was largely replaced by Portland cement in the 1850s.
In Russia, Egor Cheliev created a new binder by mixing lime and clay. His results were published in 1822 in his book "A Treatise on the Art to Prepare a Good Mortar" published in St. Petersburg. A few years later in 1825, he published another book, which described the various methods of making cement and concrete, as well as the benefits of cement in the construction of buildings and embankments.
Apparently unaware of Smeaton's work, the same principle was identified by Frenchman Louis Vicat in the first decade of the nineteenth century. Vicat went on to devise a method of combining chalk and clay into an intimate mixture, and, burning this, produced an "artificial cement" in 1817 considered the "principle forerunner" of Portland cement and "...Edgar Dobbs of Southwark patented a cement of this kind in 1811."
James Frost, working in Britain, produced what he called "British cement" in a similar manner around the same time, but did not obtain a patent until 1822. In 1824, Joseph Aspdin patented a similar material, which he called "Portland cement", because the render made from it was in color similar to the prestigious Portland stone. However, Aspdins' cement was nothing like modern Portland cement but was a first step in its development, called a "proto-Portland cement". Joseph Aspdins' son William Aspdin had left his fathers company and in his cement manufacturing apparently accidentally produced calcium silicates in the 1840s, a middle step in the development of Portland cement. William Aspdin's innovation was counterintuitive for manufacturers of "artificial cements", because they required more lime in the mix (a problem for his father), a much higher kiln temperature (and therefore more fuel), and the resulting clinker was very hard and rapidly wore down the millstones, which were the only available grinding technology of the time. Manufacturing costs were therefore considerably higher, but the product set reasonably slowly and developed strength quickly, thus opening up a market for use in concrete. The use of concrete in construction grew rapidly from 1850 onward, and was soon the dominant use for cements. Thus Portland cement began its predominant role.
Isaac Charles Johnson further refined the production of "meso-Portland cement" (middle stage of development) and claimed to be the real father of Portland cement.
Setting time and "early strength" are important characteristics of cements. Hydraulic limes, "natural" cements, and "artificial" cements all rely upon their belite content for strength development. Belite develops strength slowly. Because they were burned at temperatures below , they contained no alite, which is responsible for early strength in modern cements. The first cement to consistently contain alite was made by William Aspdin in the early 1840s: This was what we call today "modern" Portland cement. Because of the air of mystery with which William Aspdin surrounded his product, others ("e.g.," Vicat and Johnson) have claimed precedence in this invention, but recent analysis of both his concrete and raw cement have shown that William Aspdin's product made at Northfleet, Kent was a true alite-based cement. However, Aspdin's methods were "rule-of-thumb": Vicat is responsible for establishing the chemical basis of these cements, and Johnson established the importance of sintering the mix in the kiln.
Sorel cement was patented in 1867 by Frenchman Stanislas Sorel and was stronger than Portland cement but its poor water restive and corrosive qualities limited its use in building construction. The next development with the manufacture of Portland cement was the introduction of the rotary kiln which allowed a stronger, more homogeneous mixture and a continuous manufacturing process.
Also, tabby, a wall building method using lime, sand and oyster shells to form a concrete, was introduced to the Americas by the Spanish in the sixteenth century. The lime may have been made from burned oyster shells which were available in some coastal areas in the form of shell middens. Calcium aluminate cements were patented in 1908 in France by Jules Bied for better resistance to sulfates.
In the US the first large scale use of cement was Rosendale cement, a natural cement mined from a massive deposit of a large dolostone rock deposit discovered in the early 19th century near Rosendale, New York. Rosendale cement was extremely popular for the foundation of buildings ("e.g.", Statue of Liberty, Capitol Building, Brooklyn Bridge) and lining water pipes. But its long curing time of at least a month made it unpopular after World War One in the construction of highways and bridges and many states and construction firms turned to the use of Portland cement. Because of the switch to Portland cement, by the end of the 1920s of the 15 Rosendale cement companies, only one had survived. But in the early 1930s it was discovered that, while Portland cement had a faster setting time it was not as durable, especially for highways, to the point that some states stopped building highways and roads with cement. Bertrain H. Wait, an engineer whose company had worked on the construction of the New York City's Catskill Aqueduct, was impressed with the durability of Rosendale cement, and came up with a blend of both Rosendale and synthetic cements which had the good attributes of both: it was highly durable and had a much faster setting time. Mr. Wait convinced the New York Commissioner of Highways to construct an experimental section of highway near New Paltz, New York, using one sack of Rosendale to six sacks of synthetic cement. It was proved a success and for decades the Rosendale-synthetic cement blend became common use in highway and bridge construction.
Modern cements.
Modern hydraulic cements began to be developed from the start of the Industrial Revolution (around 1800), driven by three main needs:
Types of modern cement.
Portland cement.
Portland cement is by far the most common type of cement in general use around the world. This cement is made by heating limestone (calcium carbonate) with small quantities of other materials (such as clay) to 1450 °C in a kiln, in a process known as calcination, whereby a molecule of carbon dioxide is liberated from the calcium carbonate to form calcium oxide, or quicklime, which is then blended with the other materials that have been included in the mix. The resulting hard substance, called 'clinker', is then ground with a small amount of gypsum into a powder to make 'Ordinary Portland Cement', the most commonly used type of cement (often referred to as OPC).
Portland cement is a basic ingredient of concrete, mortar and most non-specialty grout. The most common use for Portland cement is in the production of concrete. Concrete is a composite material consisting of aggregate (gravel and sand), cement, and water. As a construction material, concrete can be cast in almost any shape desired, and once hardened, can become a structural (load bearing) element. Portland cement may be grey or white.
Energetically modified cement.
The grinding process to produce energetically modified cement (EMC) yields materials made from pozzolanic minerals that have been treated using a patented milling process ("EMC Activation"). This yields a high-level replacement of Portland cement in concrete with lower costs, performance and durability improvements, with significant energy and carbon dioxide savings. 
The resultant concretes can have the same, if not improved, physical characteristics as "normal" concretes, at a fraction of the cost of using Portland cement.
Portland cement blends.
Portland cement blends are often available as inter-ground mixtures from cement producers, but similar formulations are often also mixed from the ground components at the concrete mixing plant.
Portland blastfurnace cement contains up to 70% ground granulated blast furnace slag, with the rest Portland clinker and a little gypsum. All compositions produce high ultimate strength, but as slag content is increased, early strength is reduced, while sulfate resistance increases and heat evolution diminishes. Used as an economic alternative to Portland sulfate-resisting and low-heat cements.
Portland flyash cement contains up to 35% fly ash. The fly ash is pozzolanic, so that ultimate strength is maintained. Because fly ash addition allows a lower concrete water content, early strength can also be maintained. Where good quality cheap fly ash is available, this can be an economic alternative to ordinary Portland cement.
Portland pozzolan cement includes fly ash cement, since fly ash is a pozzolan, but also includes cements made from other natural or artificial pozzolans. In countries where volcanic ashes are available (e.g. Italy, Chile, Mexico, the Philippines) these cements are often the most common form in use.
Portland silica fume cement. Addition of silica fume can yield exceptionally high strengths, and cements containing 5–20% silica fume are occasionally produced. However, silica fume is more usually added to Portland cement at the concrete mixer.
Masonry cements are used for preparing bricklaying mortars and stuccos, and must not be used in concrete. They are usually complex proprietary formulations containing Portland clinker and a number of other ingredients that may include limestone, hydrated lime, air entrainers, retarders, waterproofers and coloring agents. They are formulated to yield workable mortars that allow rapid and consistent masonry work. Subtle variations of Masonry cement in the US are Plastic Cements and Stucco Cements. These are designed to produce controlled bond with masonry blocks.
Expansive cements contain, in addition to Portland clinker, expansive clinkers (usually sulfoaluminate clinkers), and are designed to offset the effects of drying shrinkage that is normally encountered with hydraulic cements. This allows large floor slabs (up to 60 m square) to be prepared without contraction joints.
White blended cements may be made using white clinker and white supplementary materials such as high-purity metakaolin.
Colored cements are used for decorative purposes. In some standards, the addition of pigments to produce "colored Portland cement" is allowed. In other standards (e.g. ASTM), pigments are not allowed constituents of Portland cement, and colored cements are sold as "blended hydraulic cements".
Very finely ground cements are made from mixtures of cement with sand or with slag or other pozzolan type minerals that are extremely finely ground together. Such cements can have the same physical characteristics as normal cement but with 50% less cement particularly due to their increased surface area for the chemical reaction. Even with intensive grinding they can use up to 50% less energy to fabricate than ordinary Portland cements.
Pozzolan-lime cements. Mixtures of ground pozzolan and lime are the cements used by the Romans, and can be found in Roman structures still standing (e.g. the Pantheon in Rome). They develop strength slowly, but their ultimate strength can be very high. The hydration products that produce strength are essentially the same as those produced by Portland cement.
Slag-lime cements. Ground granulated blast furnace slag is not hydraulic on its own, but is "activated" by addition of alkalis, most economically using lime. They are similar to pozzolan lime cements in their properties. Only granulated slag (i.e. water-quenched, glassy slag) is effective as a cement component.
Supersulfated cements. These contain about 80% ground granulated blast furnace slag, 15% gypsum or anhydrite and a little Portland clinker or lime as an activator. They produce strength by formation of ettringite, with strength growth similar to a slow Portland cement. They exhibit good resistance to aggressive agents, including sulfate.
Calcium aluminate cements are hydraulic cements made primarily from limestone and bauxite. The active ingredients are monocalcium aluminate CaAl2O4 (CaO · Al2O3 or CA in Cement chemist notation, CCN) and mayenite Ca12Al14O33 (12 CaO · 7 Al2O3, or C12A7 in CCN). Strength forms by hydration to calcium aluminate hydrates. They are well-adapted for use in refractory (high-temperature resistant) concretes, e.g. for furnace linings.
Calcium sulfoaluminate cements are made from clinkers that include ye'elimite (Ca4(AlO2)6SO4 or C4A3 in Cement chemist's notation) as a primary phase. They are used in expansive cements, in ultra-high early strength cements, and in "low-energy" cements. Hydration produces ettringite, and specialized physical properties (such as expansion or rapid reaction) are obtained by adjustment of the availability of calcium and sulfate ions. Their use as a low-energy alternative to Portland cement has been pioneered in China, where several million tonnes per year are produced. Energy requirements are lower because of the lower kiln temperatures required for reaction, and the lower amount of limestone (which must be endothermically decarbonated) in the mix. In addition, the lower limestone content and lower fuel consumption leads to a CO2 emission around half that associated with Portland clinker. However, SO2 emissions are usually significantly higher.
"Natural" cements correspond to certain cements of the pre-Portland era, produced by burning argillaceous limestones at moderate temperatures. The level of clay components in the limestone (around 30–35%) is such that large amounts of belite (the low-early strength, high-late strength mineral in Portland cement) are formed without the formation of excessive amounts of free lime. As with any natural material, such cements have highly variable properties.
Geopolymer cements are made from mixtures of water-soluble alkali metal silicates and aluminosilicate mineral powders such as fly ash and metakaolin.
Curing (Setting).
Cement sets or cures when mixed with water which causes a series of hydration chemical reactions. The constituents slowly hydrate and crystallize; the interlocking of the crystals gives cement its strength. Maintaining a high moisture content in cement during curing increases both the speed of curing, and its final strength. Gypsum is often added to Portland cement to prevent early hardening or "flash setting", allowing a longer working time. The time it takes for cement to cure varies depending on the mixture and environmental conditions; initial hardening can occur in as little as twenty minutes, while full cure can take over a month. Cement typically cures to the extent that it can be put into service within 24 hours to a week.
Safety issues.
Bags of cement routinely have health and safety warnings printed on them because not only is cement highly alkaline, but the setting process is exothermic. As a result, wet cement is strongly caustic and can easily cause severe skin burns if not promptly washed off with water. Similarly, dry cement powder in contact with mucous membranes can cause severe eye or respiratory irritation. Some ingredients can be specifically allergenic and may cause allergic dermatitis. Reducing agents are sometimes added to cement to prevent the formation of carcinogenic chromate in cement. Cement users should wear protective clothing.
Cement industry in the world.
In 2010, the world production of hydraulic cement was 3,300 million tonnes. The top three producers were China with 1,800, India with 220, and USA with 63.5 million tonnes for a combined total of over half the world total by the world's three most populated states.
For the world capacity to produce cement in 2010, the situation was similar with the top three states (China, India, and USA) accounting for just under half the world total capacity.
Over 2011 and 2012, global consumption continued to climb, rising to 3585 Mt in 2011 and 3736 Mt in 2012, while annual growth rates eased to 8.3% and 4.2%, respectively.
China, representing an increasing share of world cement consumption, continued to be the main engine of global growth. By 2012, Chinese demand was recorded at 2160 Mt, representing 58% of world consumption. Annual growth rates, which reached 16% in 2010, appear to have softened, slowing to 5–6% over 2011 and 2012, as China’s economy targets a more sustainable growth rate.
Outside of China, worldwide consumption climbed by 4.4% to 1462 Mt in 2010, 5% to 1535 Mt in 2011, and finally 2.7% to 1576 Mt in 2012.
Iran is now the 3rd largest cement producer in the world and has increased its output by over 10% from 2008 to 2011. Due to climbing energy costs in Pakistan and other major cement-producing countries, Iran is a unique position as a trading partner, utilizing its own surplus petroleum to power clinker plants. Now a top producer in the Middle-East, Iran is further increasing its dominant position in local markets and abroad.
The performance in North America and Europe over the 2010–12 period contrasted strikingly with that of China, as the global financial crisis evolved into a sovereign debt crisis for many economies in this region and recession. Cement consumption levels for this region fell by 1.9% in 2010 to 445 Mt, recovered by 4.9% in 2011, then dipped again by 1.1% in 2012.
The performance in the rest of the world, which includes many emerging economies in Asia, Africa and Latin America and representing some 1020 Mt cement demand in 2010, was positive and more than offset the declines in North America and Europe. Annual consumption growth was recorded at 7.4% in 2010, moderating to 5.1% and 4.3% in 2011 and 2012, respectively.
As at year-end 2012, the global cement industry consisted of 5673 cement production facilities, including both integrated and grinding, of which 3900 were located in China and 1773 in the rest of the world.
Total cement capacity worldwide was recorded at 5245 Mt in 2012, with 2950 Mt located in China and 2295 Mt in the rest of the world.
China.
"For the past 18 years, China consistently has produced more cement than any other country in the world. [...] (However,) China's cement export peaked in 1994 with 11 million tonnes shipped out and has been in steady decline ever since. Only 5.18 million tonnes were exported out of China in 2002. Offered at $34 a ton, Chinese cement is pricing itself out of the market as Thailand is asking as little as $20 for the same quality."
In 2006, it was estimated that China manufactured 1.235 billion tonnes of cement, which was 44% of the world total cement production. "Demand for cement in China is expected to advance 5.4% annually and exceed 1 billion tonnes in 2008, driven by slowing but healthy growth in construction expenditures. Cement consumed in China will amount to 44% of global demand, and China will remain the world's largest national consumer of cement by a large margin."
In 2010, 3.3 billion tonnes of cement was consumed globally. Of this, China accounted for 1.8 billion tonnes.
Environmental impacts.
Cement manufacture causes environmental impacts at all stages of the process. These include emissions of airborne pollution in the form of dust, gases, noise and vibration when operating machinery and during blasting in quarries, and damage to countryside from quarrying. Equipment to reduce dust emissions during quarrying and manufacture of cement is widely used, and equipment to trap and separate exhaust gases are coming into increased use. Environmental protection also includes the re-integration of quarries into the countryside after they have been closed down by returning them to nature or re-cultivating them.
CO2 emissions.
Carbon concentration in cement spans from ≈5% in cement structures to ≈8% in the case of roads in cement. Cement manufacturing releases CO2 in the atmosphere both directly when calcium carbonate is heated, producing lime and carbon dioxide, and also indirectly through the use of energy if its production involves the emission of CO2. The cement industry produces about 5% of global man-made CO2 emissions, of which 50% is from the chemical process, and 40% from burning fuel.
The amount of CO2 emitted by the cement industry is nearly 900 kg of CO2 for every 1000 kg of cement produced.
In the European union the specific energy consumption for the production of cement clinker has been reduced by approximately 30% since the 1970s. This reduction in primary energy requirements is equivalent to approximately 11 million tonnes of coal per year with corresponding benefits in reduction of CO2 emissions. This accounts for approximately 5% of anthropogenic CO2.
The high proportion of carbon dioxide produced in the chemical reaction leads to large decrease in mass in the conversion from limestone to cement. So, to reduce the transport of heavier raw materials and to mimimize the associated costs, it is more economical for cement plants to be closer to the limestone quarries rather than to the consumer centers.
In certain applications, lime mortar reabsorbs the same amount of CO2 as was released in its manufacture, and has a lower energy requirement in production than mainstream cement. Newly developed cement types from Novacem and Eco-cement can absorb carbon dioxide from ambient air during hardening. Use of the Kalina cycle during production can also increase energy efficiency.
Heavy metal emissions in the air.
In some circumstances, mainly depending on the origin and the composition of the raw materials used, the high-temperature calcination process of limestone and clay minerals can release in the atmosphere gases and dust rich in volatile heavy metals, a.o, thallium, cadmium and mercury are the most toxic. Heavy metals (Tl, Cd, Hg, ...) are often found as trace elements in common metal sulfides (pyrite (FeS2), zinc blende (ZnS), galena (PbS), ...) present as secondary minerals in most of the raw materials. Environmental regulations exist in many countries to limit these emissions. As of 2011 in the United States, cement kilns are "legally allowed to pump more toxins into the air than are hazardous-waste incinerators."
Heavy metals present in the clinker.
The presence of heavy metals in the clinker arises both from the natural raw materials and from the use of recycled by-products or alternative fuels. The high pH prevailing in the cement porewater (12.5 < pH < 13.5) limits the mobility of many heavy metals by decreasing their solubility and increasing their sorption onto the cement mineral phases. Nickel, zinc and lead are commonly found in cement in non-negligible concentrations.
Use of alternative fuels and by-products materials.
A cement plant consumes 3 to 6 GJ of fuel per tonne of clinker produced, depending on
the raw materials and the process used. Most cement kilns today use coal and petroleum coke as primary fuels, and to a lesser extent natural gas and fuel oil. Selected waste and by-products with recoverable calorific value can be used as fuels in a cement kiln (referred to as co-processing), replacing a portion of conventional fossil fuels, like coal, if they meet strict specifications. Selected waste and by-products containing useful minerals such as calcium, silica, alumina, and iron can be used as raw materials in the kiln, replacing raw materials such as clay, shale, and limestone. Because some materials have both useful mineral content and recoverable calorific value, the distinction between alternative fuels and raw materials is not always clear. For example, sewage sludge has a low but significant calorific value, and burns to give ash containing minerals useful in the clinker matrix.
Normal operation of cement kilns provides combustion conditions which are more than adequate for the destruction of even the most difficult to destroy organic substances. This is primarily due to the very high temperatures of the kiln gases (2000 °C in the combustion gas from the main burners and 1100 °C in the gas from the burners in the precalciner). The gas residence time at high temperature in the rotary kiln is of the order of 5–10 seconds and in the precalciner more than 3 seconds.
Due to bovine spongiform encephalopathy (BSE) crisis in the European beef industry, the use of animal-derived products to feed cattle is now severely restricted. Large quantities of waste animal meat and bone meal (MBM), also known as animal flour, have to be safely disposed of or transformed. The production of cement kilns, together with the incineration, is to date one of the two main ways to treat this solid effluent of the food industry.
Green cement.
Green cement is a cementitious material that meets or exceeds the functional performance capabilities of ordinary Portland cement by incorporating and optimizing recycled materials, thereby reducing consumption of natural raw materials, water, and energy, resulting in a more sustainable construction material.
The manufacturing process for green cement succeeds in reducing, and even eliminating, the production and release of damaging pollutants and greenhouse gasses, particularly CO2.
Growing environmental concerns and increasing cost of fuels of fossil origin have resulted in many countries in sharp reduction of the resources needed to produce cement and effluents (dust and exhaust gases).

</doc>
<doc id="6671" url="http://en.wikipedia.org/wiki?curid=6671" title="Cincinnati Reds">
Cincinnati Reds

The Cincinnati Reds are a Major League Baseball team based in Cincinnati, Ohio. They are members of the National League Central Division. Established in 1881 as an independent club, the team became a charter member of the American Association in 1882, and joined the National League in 1890. The club traditionally traces its origin to baseball's first openly professional team in 1869.
The Reds have won five World Series titles, one American Association pennant, nine National League pennants and ten division titles. The Reds played in the National League West between 1969 and 1993 and have been in the National League Central since 1994.
Since 2003, the Reds have played at Great American Ball Park, built next to their home from 1970 to 2002, Riverfront Stadium. Bob Castellini has owned the Cincinnati Reds since 2006.
Franchise history.
The Birth of the Reds and the American Association (1881–1889).
The origins of the modern Cincinnati Reds can be traced to the expulsion of an earlier team bearing that name. In 1876, Cincinnati became one of the charter members of the new National League, but the club ran afoul of league organizer and long-time president William Hulbert for selling beer at the ballpark and playing games on Sunday, both important activities to entice the city's large German population. While Hulbert made clear his distaste for both beer and Sunday baseball at the founding of the league, neither practice was actually against league rules in those early years. On October 6, 1880, however, seven of the eight team owners pledged at a special league meeting to formally ban both beer and Sunday baseball at the regular league meeting that December. Only Cincinnati president W. H. Kennett refused to sign the pledge, so the other owners formally expelled Cincinnati for violating a rule that would not actually go into effect for two more months.
Cincinnati's expulsion from the National League incensed "Cincinnati Enquirer" sports editor O. P. Caylor, who made two attempts to form a new league on behalf of the receivers for the now bankrupt Reds franchise. When these attempts failed, he formed a new independent ballclub known as the Red Stockings in the Spring of 1881, and brought the team to St. Louis for a weekend exhibition. The Reds' first game was a 12–3 victory over the St. Louis club. The Reds have played each year since then, without going on hiatus (as the Cubs did, out of necessity) or relocating (like the Braves), making Cincinnati the oldest club to have played every year of its existence in one city. After the 1881 series proved a success, Caylor and a former president of the old Reds named Justus Thorner received an invitation from Philadelphia businessman Horace Phillips to attend a meeting of several clubs in Pittsburgh with the intent of establishing a rival to the National League. Upon arriving in the city, however, Caylor and Thorner discovered that no other owners had decided to accept the invitation, with even Phillips not bothering to attend his own meeting. By chance, the duo met a former pitcher named Al Pratt, who hooked them up with former Pittsburgh Alleghenys president H. Denny McKnight. Together, the three men hatched a scheme to form a new league by sending a telegram to each of the other owners who were supposed to attend the meeting stating that he was the only person who did not attend and that everyone else was enthusiastic about the new venture and eager to attend a second meeting in Cincinnati. The ploy worked, and the American Association was officially formed at the Hotel Gibson in Cincinnati with the new Reds a charter member with Thorner as president.
Led by the hitting of third baseman Hick Carpenter, the defense of future Hall of Fame second baseman Bid McPhee, and the pitching of 40-game-winner Will White, the Reds won the inaugural AA pennant in 1882. With the establishment of the Union Association Justus Thorner left the club to finance the Cincinnati Outlaw Reds and managed to acquire the lease on the Reds Bank Street Grounds playing field, forcing new president Aaron Stern to relocate three blocks away at the hastily built League Park. The club never placed higher than second or lower than fifth for the rest of its tenure in the American Association.
The National League returns to Cincinnati (1890–1911).
The Cincinnati Red Stockings left the American Association on November 14, 1889 and joined the National League along with the Brooklyn Bridegrooms after a dispute with St. Louis Browns owner Chris Von Der Ahe over the selection of a new league president. The National League was happy to accept the teams in part due to the emergence of the new Player's League. This new league, an early failed attempt to break the reserve clause in baseball, threatened both existing leagues. Because the National League decided to expand while the American Association was weakening, the team accepted an invitation to join the National League. It was also at this time that the team first shortened their name from "Red Stockings" to "Reds". The Reds wandered through the 1890s signing local stars and aging veterans. During this time, the team never finished above third place (1897) and never closer than 10½ games (1890).
At the start of the 20th century, the Reds had hitting stars Sam Crawford and Cy Seymour. Seymour's .377 average in 1905 was the first individual batting crown won by a Red. In 1911, Bob Bescher stole 81 bases, which is still a team record. Like the previous decade, the 1900s (decade) were not kind to the Reds, as much of the decade was spent in the league's second division.
Redland Field to the Great Depression (1912–1932).
In 1912, the club opened a new steel-and-concrete ballpark, Redland Field (later to be known as Crosley Field). The Reds had been playing baseball on that same site, the corner of Findlay and Western Avenues on the city's west side, for 28 years, in wooden structures that had been occasionally damaged by fires. By the late 1910s the Reds began to come out of the second division. The 1918 team finished fourth, and new manager Pat Moran led the Reds to an NL pennant in 1919, in what the club advertised as its "Golden Anniversary". The 1919 team had hitting stars Edd Roush and Heinie Groh while the pitching staff was led by Hod Eller and left-hander Harry "Slim" Sallee. The Reds finished ahead of John McGraw's New York Giants, and then won the world championship in eight games over the Chicago White Sox.
By 1920, the "Black Sox" scandal had brought a taint to the Reds' first championship. After 1926, and well into the 1930s, the Reds were second division dwellers. Eppa Rixey, Dolf Luque and Pete Donohue were pitching stars, but the offense never lived up to the pitching. By 1931, the team was bankrupt, the Great Depression was in full swing and Redland Field was in a state of disrepair.
Championship Baseball and revival (1933–1940).
Powel Crosley, Jr., an electronics magnate who, with his brother Lewis M. Crosley, produced radios, refrigerators, and other household items, bought the Reds out of bankruptcy in 1933, and hired Larry MacPhail to be the General Manager. Crosley had started WLW radio, the Reds flagship radio broadcaster, and the Crosley Broadcasting Corporation in Cincinnati, where he was also a prominent civic leader. MacPhail began to develop the Reds' minor league system and expanded the Reds' fan base. The Reds, throughout the 1930s, became a team of "firsts". The now-renamed Crosley Field became the host of the first night game in 1935, which was also the first baseball fireworks night, the fireworks at the game were shot by Joe Rozzi of Rozzi's Famous Fireworks. Johnny Vander Meer became the only pitcher in major league history to throw back-to-back no-hitters in 1938. Thanks to Vander Meer, Paul Derringer and second baseman/third baseman-turned-pitcher Bucky Walters, the Reds had a solid pitching staff. The offense came around in the late 1930s. By 1938 the Reds, now led by manager Bill McKechnie, were out of the second division finishing fourth. Ernie Lombardi was named the National League's Most Valuable Player in 1938. By 1939, they were National League champions, but in the World Series, they were swept by the New York Yankees. In 1940, they repeated as NL Champions, and for the first time in 21 years, the Reds captured a World championship, beating the Detroit Tigers 4 games to 3. Frank McCormick was the 1940 NL MVP. Other position players included Harry Craft, Lonny Frey, Ival Goodman, Lew Riggs and Bill Werber.
1941–1969.
World War II and age finally caught up with the Reds. Throughout the 1940s and early 1950s, Cincinnati finished mostly in the second division. In 1944, Joe Nuxhall (who was later to become part of the radio broadcasting team), at age 15, pitched for the Reds on loan from Wilson Junior High school in Hamilton, Ohio. He became the youngest person ever to play in a major league game—a record that still stands today. Ewell "The Whip" Blackwell was the main pitching stalwart before arm problems cut short his career. Ted Kluszewski was the NL home run leader in 1954. The rest of the offense was a collection of over-the-hill players and not-ready-for-prime-time youngsters.
In 1956, led by National League Rookie of the Year Frank Robinson, the Redlegs hit 221 HR to tie the NL record. By 1961, Robinson was joined by Vada Pinson, Wally Post, Gordy Coleman, and Gene Freese. Pitchers Joey Jay, Jim O'Toole, and Bob Purkey led the staff.
The Reds captured the 1961 National League pennant, holding off the Los Angeles Dodgers and the San Francisco Giants, only to be defeated by the perennially powerful New York Yankees in the World Series.
The Reds had winning teams during the rest of the 1960s, but did not produce any championships. They won 98 games in 1962, paced by Purkey's 23, but finished third. In 1964, they lost the pennant by one game to the Cardinals after having taken first place when the Phillies collapsed in September. Their beloved manager Fred Hutchinson died of cancer just weeks after the end of the 1964 season. The failure of the Reds to win the 1964 pennant led to owner Bill DeWitt's selling off key components of the team, in anticipation of relocating the franchise. In response to DeWitt's threatened move, the women of Cincinnati banded together to form the Rosie Reds to urge DeWitt to keep the franchise in Cincinnati. The Rosie Reds are still in existence, and are currently the oldest fan club in Major League Baseball. After the 1965 season he executed what may be the most lopsided trade in baseball history, sending former Most Valuable Player Frank Robinson to the Baltimore Orioles for pitchers Milt Pappas and Jack Baldschun, and outfielder Dick Simpson. Robinson went on to win the MVP and triple crown in the American league for 1966, and lead Baltimore to its first ever World Series title in a sweep of the Los Angeles Dodgers. The Reds did not recover from this trade until the rise of the "Big Red Machine" of the 1970s.
Starting in the early 1960s, the Reds' farm system began producing a series of stars, including Jim Maloney (the Reds' pitching ace of the 1960s), Pete Rose, Tony Pérez, Johnny Bench, Lee May, Tommy Helms, Bernie Carbo, Hal McRae, Dave Concepción, and Gary Nolan. The tipping point came in 1967 with the appointment of Bob Howsam as general manager. That same year the Reds avoided a move to San Diego when the city of Cincinnati and Hamilton County agreed to build a state of the art, downtown stadium on the edge of the Ohio River. The Reds entered into a 30-year lease in exchange for the stadium commitment keeping the franchise in its original home city. In a series of strategic moves, Howsam brought in key personnel to complement the homegrown talent. The Reds' final game at Crosley Field, home to more than 4,500 baseball games, was played on June 24, 1970, a 5–4 victory over the San Francisco Giants.
Twice in the 1950s (the McCarthy era), the Reds, fearing that their traditional club nickname would associate them with the threat of Communism, officially changed the name of the team to the "Cincinnati Redlegs". From 1956 to 1960, the club's logo was altered to remove the term "REDS" from the inside of the "wishbone "C" symbol. The "REDS" reappeared on the 1961 uniforms, but the point of the "C" was removed, leaving a smooth, non-wishbone curve. The traditional home-uniform logo was restored in 1967.
Under Howsam's administration starting in the late 1960s, the Reds instituted a strict rule barring the team's players from wearing facial hair and long hair. The clean cut look was meant to present the team as wholesome in an era of turmoil. All players coming to the Reds were required to shave and cut their hair for the next three decades. Over the years, the rule was controversial, but persisted well into the ownership of Marge Schott. On at least one occasion, in the early 1980s, enforcement of this rule lost them the services of star reliever and Ohio native Rollie Fingers, who would not shave his trademark handlebar mustache in order to join the team. The rule was not officially rescinded until 1999 when the Reds traded for slugger Greg Vaughn, who had a goatee. The New York Yankees continue to have a similar rule today, though unlike the Reds during this period, Yankees players are permitted to have mustaches. Much like when players leave the Yankees today, players who left the Reds took advantage with their new teams; Pete Rose, for instance, grew his hair out much longer than would be allowed by the Reds once he signed with the Philadelphia Phillies in 1979.
The Reds' rules also included conservative uniforms. In Major League Baseball, a club generally provides most of the equipment and clothing needed for play. However, players are required to supply their gloves and shoes themselves. Many players enter into sponsorship arrangements with shoe manufacturers, but through the mid-1980s, the Reds had a strict rule that players were to wear only plain black shoes with no prominent logo. Reds players decried what they considered to be the boring color choice as well as the denial of the opportunity to earn more money through shoe contracts. A compromise was struck in which players were allowed to wear red shoes.
The Big Red Machine (1970–1976).
In , little known George "Sparky" Anderson was hired as manager, and the Reds embarked upon a decade of excellence, with a team that came to be known as "The Big Red Machine". Playing at Crosley Field until June 30, 1970, when the Reds moved into brand-new Riverfront Stadium, a 52,000 seat multi-purpose venue on the shores of the Ohio River, the Reds began the 1970s with a bang by winning 70 of their first 100 games. Johnny Bench, Tony Pérez, Pete Rose, Lee May and Bobby Tolan were the early Red Machine offensive leaders; Gary Nolan, Jim Merritt, Wayne Simpson and Jim McGlothlin led a pitching staff which also contained veterans Tony Cloninger and Clay Carroll and youngsters Pedro Borbón and Don Gullett. The Reds breezed through the 1970 season, winning the NL West and captured the NL pennant by sweeping the Pittsburgh Pirates in three games. By time the club got to the World Series, however, the Reds pitching staff had run out of gas and the veteran Baltimore Orioles beat the Reds in five games.
After the disastrous season (the only season of the 1970s during which the Reds finished with a losing record) the Reds reloaded by trading veterans Jimmy Stewart, May, and Tommy Helms for Joe Morgan, César Gerónimo, Jack Billingham, Ed Armbrister, and Denis Menke. Meanwhile, Dave Concepción blossomed at shortstop. 1971 was also the year a key component of the future world championships was acquired in George Foster from the San Francisco Giants in a trade for shortstop Frank Duffy.
The Reds won the NL West in baseball's first ever strike-shortened season and defeated the Pittsburgh Pirates in an exciting five-game playoff series. They then faced the Oakland Athletics in the World Series. Six of the seven games were won by one run. With powerful slugger Reggie Jackson sidelined due to an injury incurred during Oakland's playoff series, Ohio native Gene Tenace got a chance to play in the series, delivering four home runs that tied the World Series record for homers, propelling Oakland to a dramatic seven-game series win. This was one of the few World Series in which no starting pitcher for either side pitched a complete game.
The Reds won a third NL West crown in after a dramatic second half comeback, that saw them make up games on the Los Angeles Dodgers after the All-Star break. However they lost the NL pennant to the New York Mets in five games in the NLCS. In game one, Tom Seaver faced Jack Billingham in a classic pitching duel, with all three runs of the 2–1 margin being scored on home runs. John Milner provided New York's run off Billingham, while Pete Rose tied the game in the seventh inning off Seaver, setting the stage for a dramatic game ending home run by Johnny Bench in the bottom of the ninth. The New York series provided plenty of controversy with the riotous behavior of Shea Stadium fans towards Pete Rose when he and Bud Harrelson scuffled after a hard slide by Rose into Harrelson at second base during the fifth inning of Game 3. A full bench-clearing fight resulted after Harrelson responded to Rose's aggressive move to prevent him from completing a double play by calling him a name. This also led to two more incidents in which play was stopped. The Reds trailed 9–3 and New York's manager, Yogi Berra, and legendary outfielder Willie Mays, at the request of National League president Warren Giles, appealed to fans in left field to restrain themselves. The next day the series was extended to a fifth game when Rose homered in the 12th inning to tie the series at two games each.
The Reds won 98 games in but they finished second to the 102-win Los Angeles Dodgers. The 1974 season started off with much excitement, as the Atlanta Braves were in town to open the season with the Reds. Hank Aaron entered opening day with 713 home runs, one shy of tying Babe Ruth's record of 714. The first pitch Aaron swung at in the '74 season was the record tying home run off Jack Billingham. The next day the Braves benched Aaron, hoping to save him for his record breaking home run on their season opening homestand. The commissioner of baseball, Bowie Kuhn, ordered Braves management to play Aaron the next day, where he narrowly missed the historic home run in the fifth inning. Aaron went on to set the record in Atlanta two nights later. 1974 also was the debut of Hall of Fame radio announcer Marty Brennaman, who replaced Al Michaels, after Michaels left the Reds to broadcast for the San Francisco Giants.
With 1975, the Big Red Machine lineup solidified with the "Great Eight" starting team of Johnny Bench (catcher), Tony Pérez (first base), Joe Morgan (second base), Dave Concepción (shortstop), Pete Rose (third base), Ken Griffey (right field), César Gerónimo (center field), and George Foster (left field). The starting pitchers included Don Gullett, Fred Norman, Gary Nolan, Jack Billingham, Pat Darcy, and Clay Kirby. The bullpen featured Rawly Eastwick and Will McEnaney combining for 37 saves, and veterans Pedro Borbón and Clay Carroll. On Opening Day, Rose still played in left field, Foster was not a starter, while John Vukovich, an off-season acquisition, was the starting third baseman. While Vuckovich was a superb fielder, he was a weak hitter. In May, with the team off to a slow start and trailing the Dodgers, Sparky Anderson made a bold move by moving Rose to third base, a position where he had very little experience, and inserting Foster in left field. This was the jolt that the Reds needed to propel them into first place, with Rose proving to be reliable on defense, while adding Foster to the outfield gave the offense some added punch. During the season, the Reds compiled two notable streaks: (1) by winning 41 out of 50 games in one stretch, and (2) by going a month without committing any errors on defense.
In the 1975 season, Cincinnati clinched the NL West with 108 victories, then swept the Pittsburgh Pirates in three games to win the NL pennant. In the World Series, the Boston Red Sox were the opponents. After splitting the first four games, the Reds took Game 5. After a three-day rain delay, the two teams met in Game 6, one of the most memorable baseball games ever played and considered by many to be the best World Series game ever. The Reds were ahead 6–3 with 5 outs left, when the Red Sox tied the game on former Red Bernie Carbo's three-run home run. It was Carbo's second pinch-hit three-run homer in the series. After a few close-calls either way, Carlton Fisk hit a dramatic 12th inning home run off the foul pole in left field to give the Red Sox a 7–6 win and force a deciding Game 7. Cincinnati prevailed the next day when Morgan's RBI single won Game 7 and gave the Reds their first championship in 35 years. The Reds have not lost a World Series game since Carlton Fisk's home run, a span of 9 straight wins.
1976 saw a return of the same starting eight in the field. The starting rotation was again led by Nolan, Gullett, Billingham, and Norman, while the addition of rookies Pat Zachry and Santo Alcalá comprised an underrated staff in which four of the six had ERAs below 3.10. Eastwick, Borbon, and McEnaney shared closer duties, recording 26, 8, and 7 saves respectively. The Reds won the NL West by ten games. They went undefeated in the postseason, sweeping the Philadelphia Phillies (winning Game 3 in their final at-bat) to return to the World Series. They continued to dominate by sweeping the Yankees in the newly renovated Yankee Stadium, the first World Series games played in Yankee Stadium since 1964. This was only the second ever sweep of the Yankees in the World Series. In winning the Series, the Reds became the first NL team since the 1921–22 New York Giants to win consecutive World Series championships, and the Big Red Machine of 1975–76 is considered one of the best teams ever. So far in MLB history, the 1975 and '76 Reds were the last NL team to repeat as champions.
Beginning with the 1970 National League pennant, the Reds beat either the Philadelphia Phillies or the Pittsburgh Pirates to win their pennants (Pirates in 1970, 1972, 1975, and 1990, Phillies in 1976), making The Big Red Machine part of the rivalry between the two Pennsylvania teams. In 1979, Pete Rose added further fuel in The Big Red Machine being part of the rivalry when he signed with the Phillies and helped them win their first World Series championship in .
The Machine dismantled (1977–1989).
The later years of the 1970s brought turmoil and change. Popular Tony Pérez was sent to Montreal after the 1976 season, breaking up the Big Red Machine's starting lineup. Manager Sparky Anderson and General Manager Bob Howsam later considered this trade the biggest mistake of their careers. Starting pitcher Don Gullett left via free agency and signed with the New York Yankees. In an effort to fill that gap, a trade with the Oakland A's for starting ace Vida Blue was arranged during the '76–'77 off-season. However, Bowie Kuhn, the Commissioner of Baseball, vetoed the trade for the stated reason of maintaining competitive balance in baseball. Some have suggested that the actual reason had more to due with Kuhn's continued feud with Oakland A's owner Charlie Finley. On June 15, 1977, the Reds acquired Mets' franchise pitcher Tom Seaver for Pat Zachry, Doug Flynn, Steve Henderson, and Dan Norman. In other deals that proved to be less successful, the Reds traded Gary Nolan to the Angels for Craig Hendrickson, Rawly Eastwick to St. Louis for Doug Capilla and Mike Caldwell to Milwaukee for Rick O'Keeffe and Garry Pyka, and got Rick Auerbach from Texas. The end of the Big Red Machine era was heralded by the replacement of General Manager Bob Howsam with Dick Wagner.
In Rose's last season as a Red, he gave baseball a thrill as he challenged Joe DiMaggio's 56-game hitting streak, tying for the second-longest streak ever at 44 games. The streak came to an end in Atlanta after striking out in his fifth at bat in the game against Gene Garber. Rose also earned his 3,000th hit that season, on his way to becoming baseball's all-time hits leader when he rejoined the Reds in the mid-1980s. The year also witnessed the only no-hitter of Hall of Fame pitcher Tom Seaver's career, coming against the St. Louis Cardinals on June 16, 1978.
After the 1978 season and two straight second place finishes, Wagner fired manager Anderson—an unpopular move. Pete Rose, who since 1963 had played almost every position for the team except pitcher and catcher, signed with Philadelphia as a free agent. By 1979, the starters were Bench (c), Dan Driessen (1b), Morgan (2b), Concepción (ss), Ray Knight (3b), with Griffey, Foster, and Geronimo again in the outfield. The pitching staff had experienced a complete turnover since 1976 except for Fred Norman. In addition to ace starter Tom Seaver; the remaining starters were Mike LaCoss, Bill Bonham, and Paul Moskau. In the bullpen, only Borbon had remained. Dave Tomlin and Mario Soto worked middle relief with Tom Hume and Doug Bair closing. The Reds won the 1979 NL West behind the pitching of Tom Seaver but were dispatched in the NL playoffs by Pittsburgh. Game 2 featured a controversial play in which a ball hit by Pittsburgh's Phil Garner was caught by Cincinnati outfielder Dave Collins but was ruled a trap, setting the Pirates up to take a 2–1 lead. The Pirates swept the series 3 games to 0 and went on to win the World Series against the Baltimore Orioles.
The 1981 team fielded a strong lineup, but with only Concepción, Foster, and Griffey retaining their spots from the 1975–76 heyday. After Johnny Bench was able to play only a few games at catcher each year after 1980 due to ongoing injuries, Joe Nolan took over as starting catcher. Driessen and Bench shared 1st base, and Knight starred at third. Morgan and Geronimo had been replaced at second base and center field by Ron Oester and Dave Collins. Mario Soto posted a banner year starting on the mound, only surpassed by the outstanding performance of Seaver's Cy Young runner-up season. La Coss, Bruce Berenyi, and Frank Pastore rounded out the starting rotation. Hume again led the bullpen as closer, joined by Bair and Joe Price. In , Cincinnati had the best overall record in baseball, but they finished second in the division in both of the half-seasons that were created after a mid-season players' strike, and missed the playoffs. To commemorate this, a team photo was taken, accompanied by a banner that read "Baseball's Best Record 1981".
By , the Reds were a shell of the original Red Machine; they lost 101 games that year. Johnny Bench, after an unsuccessful transition to 3rd base, retired a year later.
After the heartbreak of 1981, General Manager Dick Wagner pursued the strategy of ridding the team of veterans including third-baseman Knight and the entire starting outfield of Griffey, Foster, and Collins. Bench, after being able to catch only seven games in 1981, was moved from platooning at first base to be the starting third baseman; Alex Treviño became the regular starting catcher. The outfield was staffed with Paul Householder, César Cedeño, and future Colorado Rockies & Pittsburgh Pirates manager Clint Hurdle on opening day. Hurdle was an immediate bust, and rookie Eddie Milner took his place in the starting outfield early in the year. The highly touted Householder struggled throughout the year despite extensive playing time. Cedeno, while providing steady veteran play, was a disappointment, and was unable to recapture his glory days with the Houston Astros. The starting rotation featured the emergence of a dominant Mario Soto, and featured strong years by Pastore and Bruce Berenyi, but Seaver was injured all year, and their efforts were wasted without a strong offensive lineup. Tom Hume still led the bullpen, along with Joe Price. But the colorful Brad "The Animal" Lesley was unable to consistently excel, and former all-star Jim Kern was a big disappointment. Kern was also publicly upset over having to shave off his prominent beard to join the Reds, and helped force the issue of getting traded during mid-season by growing it back.
The Reds fell to the bottom of the Western Division for the next few years. After his injury-riddled 1982 season, Seaver was traded back to the Mets. The year 1983 found Dann Bilardello behind the plate, Bench returning to part-time duty at first base, rookies Nick Esasky taking over at third base and Gary Redus taking over from Cedeno. Tom Hume's effectiveness as a closer had diminished, and no other consistent relievers emerged. Dave Concepción was the sole remaining starter from the Big Red Machine era.
Wagner's "reign of terror" ended in 1983, when Howsam, the architect of the Big Red Machine, was brought back. The popular Howsam began his second term as Reds' General Manager by signing Cincinnati native Dave Parker as a free agent from Pittsburgh. In the Reds began to move up, depending on trades and some minor leaguers. In that season Dave Parker, Dave Concepción and Tony Pérez were in Cincinnati uniforms. In August 1984, Pete Rose was reacquired and hired to be the Reds player-manager. After raising the franchise from the grave, Howsam gave way to the administration of Bill Bergesch, who attempted to build the team around a core of highly regarded young players in addition to veterans like Parker. However, he was unable to capitalize on an excess of young and highly touted position players including Kurt Stillwell, Tracy Jones, and Kal Daniels by trading them for pitching. Despite the emergence of Tom Browning as rookie of the year in 1985 when he won 20 games, the rotation was devastated by the early demise of Mario Soto's career to arm injury.
Under Bergesch, from –89 the Reds finished second four times. Among the highlights, Rose became the all-time hits leader, Tom Browning threw a perfect game, Eric Davis became the first player in baseball history to hit at least 35 home runs and steal 50 bases, and Chris Sabo was the 1988 National League Rookie of the Year. The Reds also had a bullpen star in John Franco, who was with the team from 1984 to 1989. Rose once had Concepcion pitch late in a game at Dodger Stadium. Following the release of the Dowd Report which accused Rose for betting on baseball games, in Rose was banned from baseball by Commissioner Bart Giamatti, who declared Rose guilty of "conduct detrimental to baseball". Controversy also swirled around Reds owner Marge Schott, who was accused several times of ethnic and racial slurs.
World Championship and the End of an Era (1990–2002).
In , General Manager Bergesch was replaced by Murray Cook, who initiated a series of deals that would finally bring the Reds back to the championship, starting with acquisitions of Danny Jackson and José Rijo. An aging Dave Parker was let go after a revival of his career in Cincinnati following the Pittsburgh drug trials. Barry Larkin emerged as the starting shortstop over Kurt Stillwell, who along with reliever Ted Power, was traded for Jackson. In , Cook was succeeded by Bob Quinn, who put the final pieces of the championship puzzle together, with the acquisitions of Hal Morris, Billy Hatcher, and Randy Myers.
In , the Reds under new manager Lou Piniella shocked baseball by leading the NL West from wire-to-wire. They started off 33–12, winning their first nine games, and maintained their lead throughout the year. Led by Chris Sabo, Barry Larkin, Eric Davis, Paul O'Neill and Billy Hatcher in the field, and by José Rijo, Tom Browning and the "Nasty Boys" of Rob Dibble, Norm Charlton and Randy Myers on the mound, the Reds took out the Pirates in the NLCS. The Reds swept the heavily favored Oakland Athletics in four straight, and extended a Reds winning streak in the World Series to 9 consecutive games. The World Series, however, saw Eric Davis severely bruise a kidney diving for a fly ball in Game 4, and his play was greatly limited the next year. In winning the World Series the Reds became the only National League team to go wire to wire.
In , Quinn was replaced in the front office by Jim Bowden. On the field, manager Lou Piniella wanted outfielder Paul O'Neill to be a power-hitter to fill the void Eric Davis left when he was traded to the Los Angeles Dodgers in exchange for Tim Belcher. However, O'Neill only hit .246 and 14 homers. The Reds returned to winning after a losing season in 1991, but 90 wins was only enough for 2nd place behind the division-winning Atlanta Braves. Before the season ended, Piniella got into an altercation with reliever Rob Dibble. In the off season, Paul O'Neill was traded to the New York Yankees for outfielder Roberto Kelly. Kelly was a disappointment for the Reds over the next couple of years, while O'Neill blossomed, leading a down-trodden Yankees franchise to a return to glory. Also, the Reds would replace their outdated "Big Red Machine" era uniforms in favor of a pinstriped uniform with no sleeves.
For the 1993 season Piniella was replaced by fan favorite Tony Pérez, but he lasted only 44 games at the helm, replaced by Davey Johnson. With Johnson steering the team, the Reds made steady progress upward. In 1994, the Reds were in the newly created National League Central Division with the Chicago Cubs, St. Louis Cardinals, as well as fellow rivals Pittsburgh Pirates and Houston Astros. By the time the strike hit, the Reds finished a half-game ahead of the Astros for first-place in the NL Central. By , the Reds won the division thanks to Most Valuable Player Barry Larkin. After defeating the NL West champion Dodgers in the first NLDS since 1981, they lost to the Atlanta Braves.
Team owner Marge Schott announced mid-season that Johnson would be gone by the end of the year, regardless of outcome, to be replaced by former Reds third baseman Ray Knight. Johnson and Schott had never gotten along and she did not approve of Johnson living with his fiancée before they were married, In contrast, Knight, along with his wife, professional golfer Nancy Lopez, were friends of Schott. The team took a dive under Knight and he was unable to complete two full seasons as manager, subject to complaints in the press about his strict managerial style.
In the Reds won 96 games, led by manager Jack McKeon, but lost to the New York Mets in a one game playoff. Earlier that year, Schott sold controlling interest in the Reds to Cincinnati businessman Carl Lindner. Despite an 85–77 finish in 2000, and being named 1999 NL manager of the year, McKeon was fired after the 2000 season. The Reds did not have another winning season until 2010.
Contemporary era (2003–).
Riverfront Stadium, then known as Cinergy Field, was demolished in . Great American Ball Park opened in with high expectations for a team led by local favorites, including outfielder Ken Griffey, Jr., shortstop Barry Larkin, and first baseman Sean Casey. Although attendance improved considerably with the new ballpark, the team continued to lose. Schott had not invested much in the farm system since the early 1990s, leaving the team relatively thin on talent. After years of promises that the club was rebuilding toward the opening of the new ballpark, General Manager Jim Bowden and manager Bob Boone were fired on July 28. This broke up the father-son combo of manager Bob Boone and third baseman Aaron Boone, and Aaron was soon traded to the New York Yankees. Following the season Dan O'Brien was hired as the Reds' 16th General Manager.
The and seasons continued the trend of big hitting, poor pitching, and poor records. Griffey, Jr. joined the 500 home run club in 2004, but was again hampered by injuries. Adam Dunn emerged as consistent home run hitter, including a home run against José Lima. He also broke the major league record for strikeouts in 2004. Although a number of free agents were signed before 2005, the Reds were quickly in last place and manager Dave Miley was forced out in the 2005 mid season and replaced by Jerry Narron. Like many other small market clubs, the Reds dispatched some of their veteran players and began entrusting their future to a young nucleus that included Adam Dunn and Austin Kearns.
Late summer 2004 saw the opening of the Cincinnati Reds Hall of Fame (HOF). The Reds HOF had been in existence in name only since the 1950s, with player plaques, photos and other memorabilia scattered throughout their front offices. Ownership and management desired a stand-alone facility, where the public could walk through inter-active displays, see locker room recreations, watch videos of classic Reds moments and peruse historical items. The first floor houses a movie theater which resembles an older, ivy-covered brick wall ball yard. The hallways contain many vintage photographs. The rear of the building features a three-story wall containing a baseball for every hit Pete Rose had during his career. The third floor contains interactive exhibits including a pitcher's mound, radio booth, and children's area where the fundamentals of baseball are taught through videos featuring former Reds players.
Robert Castellini took over as controlling owner from Lindner in 2006. Castellini promptly fired general manager Dan O'Brien and hired Wayne Krivsky. The Reds made a run at the playoffs but ultimately fell short. The 2007 season was again mired in mediocrity. Midway through the season Jerry Narron was fired as manager and replaced by Pete Mackanin. The Reds ended up posting a winning record under Mackanin, but finished the season in 5th place in the Central Division. Mackanin was manager in an interim capacity only, and the Reds, seeking a big name to fill the spot, ultimately brought in Dusty Baker. Early in the 2008 season, Krivsky was fired and replaced by Walt Jocketty. Though the Reds did not win under Krivsky, he is credited with revamping the farm system and signing young talent that could potentially lead the Reds to success in the future.
The Reds failed to post winning records in both 2008 and 2009. In 2010, with NL MVP Joey Votto and Gold Glovers Brandon Phillips and Scott Rolen the Reds posted a 91-71 record and were NL Central champions. The following week, the Reds became only the second team in MLB history to be no-hit in a postseason game when Philadelphia's Roy Halladay shut down the National League's number one offense in game one of the NLDS. The Reds lost in a 3 game sweep of the NLDS for Philadelphia.
After coming off their surprising 2010 NL Central Division Title, the Reds fell short of many expectations for the 2011 season. Multiple injuries and inconsistent starting pitching played a big role in their mid-season collapse, along with a less productive offense as compared to the previous year. The Reds ended the season at 79-83. The Reds won the 2012 NL Central Division Title. On September 28, Homer Bailey threw a 1-0 no-hitter against the Pittsburgh Pirates at PNC Park, this was the first Reds no-hitter since Tom Browning's perfect game in September of the 1988 season. Finishing with a 97–65 record, they earned the second seed in the Division Series and a match-up with the eventual World Series champion San Francisco Giants. After taking a 2–0 lead with road victories at AT&T Park, they headed home looking to win the series. However, they lost three straight at their home ballpark to become the first National League team to lose a division series after leading 2–0.
In the off-season, the team traded outfielder Drew Stubbs, as part of a three team deal with the Arizona Diamondbacks and Cleveland Indians, to the Indians, and in turn received right fielder Shin-Soo Choo. On July 2, 2013, Homer Bailey pitched a no-hitter against the San Francisco Giants for a 3-0 Reds victory, making Bailey the third pitcher in Reds history with two complete game no-hitters in their career.
Following six consecutive losses to close out the 2013 season, including a loss in the National League wild-card playoff game, the Reds decided to fire Dusty Baker. During his six years as manager, Baker led the Reds to the playoff three times; however, they never advanced beyond the first round.
On October 22, 2013, the Reds hired pitching coach Bryan Price to replace Baker as manager.
Ballpark.
The Cincinnati Reds play their home games at Great American Ball Park, located at 100 Joe Nuxhall Way, in downtown Cincinnati. Great American Ball Park opened in 2003 at the cost of $290 million and has a capacity of 42,271. Along with serving as the home field for the Reds, the stadium also holds the Cincinnati Reds Hall of Fame. The Hall of Fame was added as a part of Reds tradition allowing fans to walk through the history of the franchise as well as participating in many interactive baseball features.
Great American Ball Park is the seventh home of the Cincinnati Reds, built immediately to the north of the site on which Riverfront Stadium, later named Cinergy Field, once stood. The first ballpark the Reds occupied was Bank Street Grounds from 1882–1883 until they moved to League Park I in 1884, where they would remain until 1893. Through the late 1890s and early 1900s (decade), the Reds moved to two different parks where they stayed for less than ten years. League Park II was the third home field for the Reds from 1894–1901, and then moved to the Palace of the Fans which served as the home of the Reds in the 1910s. It was in 1912 that the Reds moved to Crosley Field which they called home for fifty-eight years. Crosley served as the home field for the Reds for two World Series titles and five National League pennants. Beginning June 30, 1970, and during the dynasty of the Big Red Machine, the Reds played in Riverfront Stadium, appropriately named due to its location right by the Ohio River. Riverfront saw three World Series titles and five National League pennants. It was in the late 1990s that the city agreed to build two separate stadiums on the riverfront for the Reds and the Cincinnati Bengals. Thus, in 2003, the Reds began a new era with the opening of the current stadium.
The Reds hold their spring training in Goodyear, Arizona at Goodyear Ballpark. The Reds moved into this stadium and the Cactus League in 2010 after staying in the Grapefruit League for most of their history. The Reds share Goodyear Park with their rivals in Ohio, the Cleveland Indians.
Logos and uniforms.
Logo.
Throughout the history of the Cincinnati Reds, many different variations of the classic wishbone "C" logo have been introduced. For most of the history of the Reds, especially during the early history, the Reds logo has been simply the wishbone "C" with the word "REDS" inside, the only colors used being red and white. However, during the 1950s, during the renaming and re-branding of the team as the Cincinnati Redlegs because of the connections to communism of the word 'Reds', the color blue was introduced as part of the Reds color combination. During the 1960s and 1970s the Reds saw a move towards the more traditional colors, abandoning the navy blue. A new logo also appeared with the new era of baseball in 1972, when the team went away from the script "REDS" inside of the "C", instead, putting their mascot Mr. Redlegs in its place as well as putting the name of the team inside of the wishbone "C". In the 1990s the more traditional, early logos of Reds came back with the current logo reflecting more of what the team's logo was when they were first founded.
Uniform.
Along with the logo, the Reds' uniforms have been changed many different times throughout their history. Following their departure from being called the "Redlegs" in 1956 the Reds made a groundbreaking change to their uniforms with the use of sleeveless jerseys, seen only once before in the Major Leagues by the Chicago Cubs. At home and away, the cap was all-red with a white wishbone C insignia. The long-sleeved undershirts were red. The uniform was plain white with a red wishbone C logo on the left and the uniform number on the right. On the road the wishbone C was replaced by the mustachioed "Mr. Red" logo, the pillbox-hat-wearing man with a baseball for a head. The home stockings were red with six white stripes. The away stockings had only three white stripes.
The Reds changed uniforms again in 1961, when they replaced the traditional wishbone C insignia with an oval C logo, but continued to use the sleeveless jerseys. At home, the Reds wore white caps with the red bill with the oval C in red, white sleeveless jerseys with red pinstripes, with the oval C-REDS logo in black with red lettering on the left breast and the number in red on the right. The gray away uniform included a gray cap with the red oval C and a red bill. Their gray away uniforms, which also included a sleeveless jersey, bore CINCINNATI in an arched block style across with the number below on the left. In 1964, players' last names were placed on the back of each set of uniforms, below the numbers. Those uniforms were scrapped after the 1966 season.
However, the Cincinnati uniform design most familiar to baseball enthusiasts is the one whose basic form, with minor variations, held sway for the 26 seasons from 1967 to 1992. Most significantly, the point was restored to the C insignia, making it a wishbone again. During this era, the Reds wore all-red caps both at home and on the road. The caps bore the simple wishbone C insignia in white. The uniforms were standard short-sleeved jerseys and standard trousers—white at home and grey on the road. The home uniform featured the Wishbone C-REDS logo in red with white type on the left breast and the uniform number in red on the right. The away uniform bore CINCINNATI in an arched block style across the front with the uniform number below on the left. Red, long-sleeved undershirts and plain red stirrups over white sanitary stockings completed the basic design.
The 1993 uniforms (which did away with the pullovers and brought back button-down jerseys) kept white and gray as the base colors for the home and away uniforms, but added red pinstripes. The home jerseys were sleeveless, showing more of the red undershirts. The color scheme of the C-REDS logo on the home uniform was reversed, now red lettering on a white background. A new home cap was created that had a red bill and a white crown with red pinstripes and a red wishbone C insignia. The away uniform kept the all-red cap, but moved the uniform number to the left, to more closely match the home uniform. The only additional change to these uniforms was the introduction of black as a primary color of the Reds in 1999, especially on their road uniforms.
The Reds latest uniform change came in December 2006 which featured a very drastic difference from the uniforms worn by the team the previous eight seasons. The home caps returned to all-red with a white wishbone C, lightly outlined in black. Caps with red crowns and black bill became the new road caps. The batting helmets, however, remain all-red regardless of what cap they are wearing. Additionally, the sleeveless jerseys were abandoned for more traditional shirts. The numbers and the lettering for the names on the backs of the jerseys were changed to an early-1900s (decade) style typeface. It had been rumored that navy blue was to make a return as a trim color, but the unveiled designs did not end up featuring any navy blue. The alternate club logo and jersey Mr. Red emblem was replaced by the moustachioed Mr. Redleg from 1956.
Awards.
Retired numbers.
The Cincinnati Reds have retired nine numbers in franchise history, as well as honoring Jackie Robinson, retired in all major league baseball. Since Pete Rose was banned from baseball, the Reds have not retired his #14. However, they have not reissued it except for Pete Rose, Jr. in his 11 game tenure in 1997.
On April 15, 1997, #42 was retired throughout Major League Baseball in honor of Jackie Robinson.
All of the retired numbers are located at Great American Ball Park behind home-plate on the outside of the press box. Along with the retired player and manager number, the following broadcasters are honored with microphones by the broadcast booth: Marty Brennaman, Waite Hoyt, and Joe Nuxhall.
Ohio Cup.
The Ohio Cup was an annual pre-season baseball game, which pitted the Ohio rivals Cleveland Indians and Cincinnati Reds. In its first series it was a single-game cup, played each year at minor-league Cooper Stadium in Columbus, Ohio, was staged just days before the start of each new Major League Baseball season. A total of eight Ohio Cup games were played, in 1989 to 1996, with the Indians winning six of them. It stopped because interleague play started in 1997. The winner of the game each year was awarded the Ohio Cup in postgame ceremonies. The Ohio Cup was a favorite among baseball fans in Columbus, with attendances regularly topping 15,000. In 1997 and after, the two teams competed annually in the regular-season Battle of Ohio or Buckeye Series. In 2008 the Ohio Cup restarted. The Indians currently lead the interleague series 36–35. 
Media.
Radio.
The Reds' flagship radio station has been WLW, 700AM since 1969. Prior to that, the Reds were heard over: WKRC, WCPO, WSAI and WCKY. WLW, a 50,000-watt station, is "clear channel" in more than one way, as Clear Channel Communications owns the "blowtorch" outlet which is also known as "The Nation's Station".
Marty Brennaman has been the Reds' play-by-play voice since 1974 and has won the Ford C. Frick Award for his work, which includes his famous call of "... and this one belongs to the Reds!" after a win. Joining him for years on color was former Reds pitcher Joe Nuxhall, who worked in the radio booth from 1967 (the year after his retirement as an active player) until 2004, plus three more seasons doing select home games until his death, in 2007.
In 2007, Thom Brennaman, a veteran announcer seen nationwide on Fox Sports, joined his father Marty in the radio booth. Retired relief pitcher Jeff Brantley, formerly of ESPN, also joined the network in 2007. As of 2010, Brantley and Thom Brennaman's increased TV schedule (see below) has led to more appearances for Jim Kelch, who had filled in on the network since 2008.
Television.
Televised games are seen exclusively on Fox Sports Ohio (in Cincinnati, Dayton, Columbus and Kentucky) and Fox Sports Indiana. Fox Sports South also pipes in Fox Sports Ohio broadcasts of Reds games to Tennessee and western North Carolina. George Grande, who hosted the first "SportsCenter" on ESPN in 1979, was the play-by-play announcer from 1993 until his retirement during the final game of the '09 season, usually alongside Chris Welsh. Since 2009 Grande has worked part-time for the Reds as play-by-play announcer in September when Thom is working for Fox Sports covering the NFL. He also has made guest appearances throughout the season. Thom Brennaman has been the head play-by-play commentator since 2010, and Welsh and Brantley, share time as the color commentator. Paul Keels, the current radio play-by-play announcer for The Ohio State University Buckeyes Radio Network, was the Reds backup play-by-play television announcer for 2010. Jim Kelch will replaced Keels for the 2011 season. The Reds also added former Cincinnati First Baseman Sean Casey, "The Mayor", as Cincinnatians call him, to do color commentary for 15 games in 2011.
NBC affiliate WLWT carried Reds games from 1948–1995. Among those that have called games for WLWT include Waite Hoyt, Ray Lane, Steve Physioc, Johnny Bench, Joe Morgan, and Ken Wilson. Al Michaels, who later went on to a long career with ABC and NBC, spent three years in Cincinnati before being drafted by NBC. WSTR-TV aired games from 1996–1998, and the Reds have not broadcast over-the-air locally on a regular basis since then. Since 2010, WKRC-TV has simulcast Fox Sports Ohio's feed of the Opening Day game; they were the first games broadcast locally over-the-air since Opening Day 2002.

</doc>
<doc id="6672" url="http://en.wikipedia.org/wiki?curid=6672" title="Caribbean cuisine">
Caribbean cuisine

Caribbean cuisine is a fusion of African, Amerindian, European, East Indian, Arab and Chinese cuisine. These traditions were brought from many different countries when they came to the Caribbean. In addition, the population has created styles that are unique to the region.
Ingredients which are common in most islands' dishes are rice, plantains, beans, cassava, cilantro, bell peppers, chickpeas, tomatoes, sweet potatoes, coconut, and any of various meats that are locally available like beef, poultry, pork or fish. A characteristic seasoning for the region is a green herb and oil based marinade which imparts a flavor profile which is quintessentially Caribbean in character. Ingredients may include garlic, onions, scotch bonnet peppers, celery, green onions, and herbs like cilantro, marjoram, rosemary, tarragon and thyme. This green seasoning is used for a variety of dishes like curries, stews and roasted meats.
Traditional dishes are so important to regional culture that, for example, the local version of Caribbean goat stew has been chosen as the official national dish of Montserrat and is also one of the signature dishes of St. Kitts and Nevis. Another popular dish in the Anglophone Caribbean is called "Cook-up", or Pelau. Ackee and Salt Fish is another popular dish that is unique to Jamaica. Callaloo is a dish containing leafy vegetables and sometimes okra amongst others, widely distributed in the Caribbean, with a distinctively mixed African and indigenous character.
The variety of dessert dishes in the area also reflects the mixed origins of the recipes. In some areas, Black Cake, a derivative of English Christmas pudding may be served, especially on special occasions.

</doc>
<doc id="6673" url="http://en.wikipedia.org/wiki?curid=6673" title="Central Powers">
Central Powers

The Central Powers (; ; ; ) were one of the two warring factions in World War I (1914–18), composed of Germany, Austria-Hungary, the Ottoman Empire and Bulgaria (hence also known as the Quadruple Alliance (). This alignment originated in the alliance of Germany and Austria-Hungary, and fought against the Allied Powers that had formed around the Triple Entente. The Central Powers regarded the assassination of Austro-Hungarian Archduke Francis Ferdinand by several militants as being an act supported by the Kingdom of Serbia, and given an unwillingness of Serbia to fully comply with Austro-Hungarian demands for a full investigation of Serbian complicity in the assassination, war between Austria-Hungary and Serbia was justified. This resulted in war with Russia, which opposed Austro-Hungarian intervention and supported Serbia, and ignited several alliance systems to bring the major European powers into a major war.
Member states.
The Central Powers consisted of the German Empire and the Austro-Hungarian Empire at the beginning of the war. The Ottoman Empire joined the Central Powers later in 1914. In 1915, the Kingdom of Bulgaria joined the alliance. The name "Central Powers" is derived from the location of these countries; all four (including the other groups that supported them except for Finland and Lithuania) were located between the Russian Empire in the east and France and the United Kingdom in the west. Finland, Azerbaijan, and Lithuania joined them in 1918 before the war ended and after the Russian Empire collapsed.
The Central Powers were composed of the following nations:
Combatants.
Germany.
War Justifications.
In early July 1914, in the aftermath of the assassination of Austro-Hungarian Archduke Francis Ferdinand and the immediate likelihood of war between Austria-Hungary and Serbia, Kaiser Wilhelm II and the German government informed the Austro-Hungarian government that Germany would uphold its alliance with Austria-Hungary and defend it from possible Russia intervention if a war between Austria-Hungary and Serbia took place. When Russia enacted a general mobilization, Germany viewed the act as provocative. The Russian government promised Germany that its general mobilization did not mean preparation for war with Germany but was a reaction to the events between Austria-Hungary and Serbia. The German government regarded the Russian promise of no war with Germany to be nonsense in light of its general mobilization, and Germany in turn mobilized for war. On August 1, Germany sent an ultimatum to Russia stating that since both Germany and Russia were in a state of military mobilization, an effective state of war existed between the two countries. Later that day, France, an ally of Russia, declared a state of general mobilization,
In August 1914, Germany waged war on Russia, the German government justified military action against Russia as necessary because of Russian aggression as demonstrated by the mobilization of the Russian army that had resulted in Germany mobilizing in response.
After Germany declared war on Russia, France with its alliance with Russia prepared a general mobilization in expectation of war. On 3 August 1914, Germany responded to this action by declaring war on France. Germany facing a two-front war enacted what was known as the Schlieffen Plan, that involved German armed forces needing to move through Belgium and swing south into France and towards the French capital of Paris. This plan was hoped to quickly gain victory against the French and allow German forces to concentrate on the Eastern Front. Belgium was a neutral country and would not accept German forces crossing its territory. Germany disregarded Belgian neutrality and invaded the country to launch an offensive towards Paris. This act of violation of Belgian neutrality, escalating the conflict resulted in the United Kingdom declaring war on Germany.
Subsequently several states declared war on Germany, including: Japan declaring war on Germany in late August 1914; Italy declaring war on Austria-Hungary in 1915 and Germany on August 27, 1916; the United States declaring war on Germany on April 6, 1917 and Greece declaring war on Germany in July 1917.
Colonies and Dependencies.
In Europe.
Upon its founding in 1871, the German Empire controlled Alsace-Lorraine as an "imperial territory", that was incorporated from France after the Franco-Prussian War, and was held within Germany's sovereign territory.
In Africa.
Germany held multiple African colonies at the time of World War I. All of Germany's African colonies were invaded and occupied by Allied forces during the war.
Cameroon, German East Africa, and German Southwest Africa were German colonies in Africa. Togoland was a German protectorate in Africa.
In Asia.
German New Guinea was a German protectorate in the Pacific. It was occupied by Australian forces in 1914.
The Kiautschou Bay concession was a German dependency in East Asia leased from China in 1898. It was occupied by Japanese forces following the Siege of Tsingtao.
Austria-Hungary.
War Justifications.
Austria-Hungary regarded the assassination of Arch Duke Francis Ferdinand as being orchestrated with the assistance of Serbia. The country viewed the assassination as setting a dangerous precedent of encouraging the country's South Slav population to rebel and threaten to tear apart the multinational country. Austria-Hungary formally sent an ultimatum to Serbia demanding a full-scale investigation of Serbian government complicity in the assassination, and complete compliance by Serbia in agreeing to the terms demanded by Austria-Hungary. Serbia submitted to accept most of the demands, however Austria-Hungary viewed this as insufficient and used this lack of full compliance to justify military intervention. These demands have been viewed as a diplomatic cover for what was going to be an inevitable Austro-Hungarian declaration of war on Serbia.
Austria-Hungary had been warned by Russia that the Russian government would not tolerate Austria-Hungary crushing Serbia. However with Germany supporting Austria-Hungary's actions, the Austro-Hungarian government hoped that Russia would not intervene and that the conflict with Serbia would be a regional conflict.
Austria-Hungary's invasion of Serbia resulted in Russia declaring war on the country and Germany in turn declared war on Russia, setting off the beginning of the clash of alliances that resulted in the World War.
Territory.
Austria-Hungary was internally divided into two states with their own governments, joined in communion through the Habsburg throne. Austrian Cisleithania contained various duchies and principalities but also the Kingdom of Bohemia, the Kingdom of Dalmatia, the Kingdom of Galicia and Lodomeria. Hungarian Transleithania comprised the Kingdom of Hungary and the Kingdom of Croatia-Slavonia. In Bosnia and Herzegovina sovereign authority was shared by both Austria and Hungary.
Ottoman Empire.
War justifications.
The Ottoman Empire joined the war on the side of the Central Powers in November 1914. The Ottoman Empire had gained strong economic connections with Germany through the Berlin-to-Baghdad railway project that was still incomplete at the time. The Ottoman Empire made a formal alliance with Germany signed on 2 August 1914. The alliance treaty expected that the Ottoman Empire would become involved in the conflict in a short amount of time. However, for the first several months of the war the Ottoman Empire maintained neutrality though it allowed a German naval squadron to enter and stay near the strait of Bosphorus. Ottoman officials informed the German government that the country needed time to prepare for conflict. Germany provided financial aid and weapons shipments to the Ottoman Empire
After pressure escalated from the German government demanding that the Ottoman Empire fulfill its treaty obligations, or else Germany would expel the country from the alliance and terminate economic and military assistance, the Ottoman government entered the war with the recently acquired cruisers from Germany, the "Yavuz Sultan Selim" (formerly "SMS Goeben") and the "Midilli" (formerly "SMS Breslau") launching a naval raid on the Russian port of Odessa, thus engaging in a military action in accordance with its alliance obligations with Germany. Russia and the Triple Entente declared war on the Ottoman Empire.
Bulgaria.
War Justifications.
Bulgaria was still resentful after its defeat in July 1913 at the hands of Serbia, Greece, Romania and the Ottoman Empire. It signed a treaty of defensive alliance with the Ottoman Empire on 19 August 1914. It was the last country to join the Central Powers, which Bulgaria did in October 1915 by declaring war on Serbia. It invaded Serbia in conjunction with German and Austro-Hungarian forces. Bulgaria held irredentist aims on the region of Vardar Macedonia held by Serbia.
Minor co-belligerent state combatants.
Dervish State.
The Dervish State was a rebel Somali state seeking independence of Somali territories. Dervish forces fought against Italian and British forces in Italian Somaliland and British Somaliland during World War I in the Somaliland Campaign. The Dervish State received support from Germany and the Ottoman Empire.
Sultanate of Darfur.
The Sultanate of Darfur forces fought against British forces in Anglo-Egyptian Sudan during World War I in the Anglo-Egyptian Darfur Expedition.
Client states.
During 1917 and 1918, the Finns under Carl Gustaf Emil Mannerheim and Lithuanian nationalists fought Russia for a common cause. With the Union of Soviet Socialist Republics's aggression of late 1917, the government of Ukraine sought military protection first from the Central Powers and later from the armed forces of the Entente.
The Ottoman Empire also had its own allies in Azerbaijan and the Northern Caucasus. The three nations fought alongside each other under the Army of Islam in the Battle of Baku.
German client states.
Belarus (Belarusian People's Republic).
The Belarusian People's Republic was a client state of Germany created in 1918.
Courland and Semigallia.
The Duchy of Courland and Semigallia was a client state of Germany created in 1918.
Don (Don Republic).
The Don Republic was closely associated with the German Empire and fought against the Bolsheviks.
Finland (Kingdom of Finland).
The Kingdom of Finland was a client state of Germany created in 1918
Georgia (Democratic Republic of Georgia).
In 1918, the Democratic Republic of Georgia, facing Bolshevik revolution and opposition from the Georgian Mensheviks and nationalists, was occupied by the German Empire, which expelled the Bolsheviks and supported the Mensheviks.
Lithuania (Kingdom of Lithuania).
The Kingdom of Lithuania was a client state of Germany created in 1918.
Northern Caucasus (Mountainous Republic of the Northern Caucasus).
The Mountainous Republic of the Northern Caucasus was associated with the Central Powers.
Poland (Kingdom of Poland).
The Kingdom of Poland was a client state of Germany created in 1916. This government was recognized by the emperors of Germany and Austria-Hungary in November 1916, and it adopted a constitution in 1917. The decision to create a state of Poland was taken by Germany in order to attempt to legitimize its military occupation amongst the Polish inhabitants, following upon German propaganda sent to Polish inhabitants in 1915 that German soldiers were arriving as liberators to free Poland from subjugation by Russia.
The state was utilized by the German government alongside punitive threats to induce Polish landowners living in the German-occupied Baltic territories to move to the state and sell their Baltic property to Germans in exchange for moving to Poland, and efforts were made to induce similar emigration of Poles from Prussia to the state.
South Africa (South African Republic).
In opposition to the Union of South Africa, which had joined the war, Boer rebels founded the South African Republic in 1914 and engaged in the Maritz Rebellion. Germany assisted the rebels, and the rebels operated in and out of the German colony of German South-West Africa. The rebels were defeated by British imperial forces.
Ukraine (Ukrainian State).
The Ukrainian State was a client state of Germany led by Pavlo Skoropadskyi, who overthrew the government of the Ukrainian People's Republic.
United Baltic Duchy.
The United Baltic Duchy was a proposed client state of Germany created in 1918
Ottoman client states.
Azerbaijan (Azerbaijan Democratic Republic).
In 1918, the Azerbaijan Democratic Republic, facing Bolshevik revolution and opposition from the Muslim Musavat Party, was then occupied by the Ottoman Empire, which expelled the Bolsheviks while supporting the Musavat Party. The Ottoman Empire maintained a presence in Azerbaijan until the end of the war in November 1918.
Jabal Shammar.
Jabal Shammar was an Arab state in the Middle East that was closely associated with the Ottoman Empire.
Non-state combatants.
Other movements supported the efforts of the Central Powers for their own reasons, such as the Irish Nationalists who launched the Easter Rising in Dublin in April 1916; they referred to their "gallant allies in Europe". In 1914, Józef Piłsudski was permitted by Germany and Austria-Hungary to form independent Polish legions. Piłsudski wanted his legions to help the Central Powers defeat Russia and then side with France and the UK and win the war with them.
Armistice and treaties.
Bulgaria signed an armistice with the Allies on 29 September 1918, following a successful Allied advance in Macedonia. The Ottoman Empire followed suit on 30 October 1918 in the face of British and Arab gains in Palestine and Syria. Austria and Hungary concluded ceasefires separately during the first week of November following the disintegration of the Habsburg Empire and the Italian offensive at Vittorio Veneto; Germany signed the armistice ending the war on the morning of 11 November 1918 after the Hundred Days Offensive, and a succession of advances by New Zealand, Australian, Canadian, Belgian, British, French and US forces in north-eastern France and Belgium. There was no unified treaty ending the war; the Central Powers were dealt with in separate treaties.

</doc>
<doc id="6675" url="http://en.wikipedia.org/wiki?curid=6675" title="Conservatism">
Conservatism

Conservatism as a political and social philosophy promotes retaining traditional social institutions. A person who follows the philosophies of conservatism is referred to as a traditionalist or conservative. Some conservatives seek to preserve things as they are, emphasizing stability and continuity, while others, called reactionaries, oppose modernism and seek a return to "the way things were". The first established use of the term in a political context originated with François-René de Chateaubriand in 1818,
during the period of Bourbon restoration that sought to roll back the policies of the French Revolution. The term, historically associated with right-wing politics, has since been used to describe a wide range of views. There is no single set of policies that are universally regarded as conservative, because the meaning of conservatism depends on what is considered traditional in a given place and time. Thus conservatives from different parts of the world—each upholding their respective traditions—may disagree on a wide range of issues.
Edmund Burke, an 18th-century politician who opposed the French Revolution but supported the American Revolution, is credited as one of the main theorists of conservatism in Great Britain in the 1790s. According to Quintin Hogg, the chairman of the British Conservative Party in 1959, "Conservatism is not so much a philosophy as an attitude, a constant force, performing a timeless function in the development of a free society, and corresponding to a deep and permanent requirement of human nature itself."
Development of Western conservatism.
Great Britain.
In Britain conservative ideas (though not yet called that) emerged in the Tory movement, during the English Restoration period (1660–1688). Toryism supported a hierarchical society with a monarch who ruled by divine right. Tories opposed the idea that sovereignty was derived from the people, and rejected the authority of parliament and freedom of religion. Robert Filmer's "Patriarcha: or the Natural Power of Kings", which had been written before the English Civil War, became accepted as the statement of their doctrine. However, the Glorious Revolution of 1688 destroyed this principle to some degree by establishing a constitutional government in England, leading to the hegemony of the opposed Whig ideology. Faced with defeat, the Tories reformed their movement, now holding that sovereignty was vested in the three estates of Crown, Lords, and Commons rather than solely in the Crown. Toryism became marginalized during the long period of Whig ascendancy in the 18th century.
Conservatives typically see Richard Hooker as the founding father of conservatism, along with the Marquess of Halifax, David Hume and Edmund Burke. Halifax promoted pragmatism in government, whilst Hume argued against political rationalism. Burke was the private secretary to the Marquis of Rockingham and official pamphleteer to the Rockingham branch of the Whig Party. Together with the Tories, they were the conservatives in the late 18th century United Kingdom. Burke's views were a mixture of liberal and conservative, with reference to the meaning of those terms in that time period (which was markedly different from their implications today). He supported the American Revolution but abhorred the violence of the French Revolution. He accepted the liberal ideals of private property and the economics of Adam Smith, but thought that economics should be kept subordinate to the conservative social ethic, that capitalism should be subordinate to the medieval social tradition and that the business class should be subordinate to aristocracy. He insisted on standards of honor derived from the medieval aristocratic tradition, and saw the aristocracy as the nation's natural leaders. That meant limits on the powers of the Crown, since he found the institutions of Parliament to be better informed than commissions appointed by the executive. He favored an established church, but allowed for a degree of religious toleration. Burke justified the social order on the basis of tradition: tradition represented the wisdom of the species and he valued community and social harmony over social reforms. Burke was a leading theorist in this early period, finding extreme idealism (either Tory or Whig) an endangerment to broader liberties, and (like Hume) rejecting abstract reason as an unsound guide for political theory. Despite their influence on future conservative thought, none of these early contributors were explicitly involved in Tory politics. Hooker lived in the 16th century, long before the advent of toryism, whilst Hume was an apolitical philosopher and Halifax similarly politically independent. Burke described himself as a Whig.
Shortly after Burke's death in 1797, conservatism was revived as a mainstream political force as the Whigs suffered a series of internal divisions. This new generation of conservatives derived their politics not from Burke but from his predecessor, the Viscount Bolingbroke, who was a Jacobite and traditional Tory, lacking Burke's sympathies for Whiggish policies such as Catholic Emancipation and American independence (famously attacked by Samuel Johnson in "Taxation No Tyranny"). In the first half of the 19th century there were many newspapers, magazines, and journals promoting loyalist or right-wing attitudes in religion, politics, and international affairs. Burke was seldom mentioned but William Pitt the Younger was a conspicuous hero. The most prominent journals were "The Quarterly Review," founded in 1809 as a counterweight to the Whigs' "Edinburgh Review," and the even more conservative "Blackwood's Edinburgh Magazine." Sack finds that the "Quarterly Review" promoted a balanced Canningite toryism; was neutral on Catholic emancipation and only mildly critical of Nonconformist Dissent; it opposed slavery and supported the current poor laws. It was "aggressively imperialist." The high church clergy of the Church of England read the "Orthodox Churchman's Magazine" which was equally hostile to Jewish, Catholic, Jacobin, Methodist, and Unitarian spokesmen. Anchoring the ultra tories was "Blackwood's Edinburgh Magazine," which stood firmly against Catholic emancipation, and favoured slavery, cheap money, mercantilism, the Navigation acts, and the Holy Alliance.
In the 19th century, conflict between wealthy businessmen and the aristocracy split the British conservative movement, with the aristocracy calling for a return to medieval ideas while the business classes called for laissez-faire capitalism.
Although conservatives opposed attempts to allow greater representation of the middle class in parliament, in 1834 they conceded that electoral reform could not be reversed and promised to support further reforms so long as they did not erode the institutions of church and state. These new principles were presented in the Tamworth Manifesto which is considered by historians to be the basic statement of the beliefs of the new Conservative Party.
Some conservatives lamented the passing of a pastoral world where the ethos of "noblesse oblige" had promoted respect from the lower classes. They saw the Anglican Church and the aristocracy as balances against commercial wealth. They worked toward legislation for improved working conditions and urban housing. This viewpoint would later be called Tory Democracy. However since Burke there has always been tension between traditional aristocratic conservatism and the wealthy business class.
In 1835, Tory Prime Minister Robert Peel issued the Tamworth Manifesto in which he pledged to endorse moderate political reform. This marked the beginning of the transformation of British conservatism from High Tory reactionism towards a more modern form based on 'conservation.' The party became known as the Conservative Party as a result, a name it has retained to this day. Peel, however, would also be the root of a split in the party between the traditional Tories (led by the Earl of Derby and Benjamin Disraeli) and the 'Peelites' (led first by Peel himself, then by the Earl of Aberdeen). The split occurred in 1846 over the issue of free-trade, which Peel supported, versus protectionism, supported by Derby. The majority of the party sided with Derby, whilst about a third split away, eventually merging with the Whigs and the radicals to form the Liberal Party. Despite the split, the mainstream Conservative Party accepted the doctrine of free-trade in 1852.
In the second half of the century it was the Liberals who were faced with political schisms, the most major being over Irish Home Rule. Leader William Gladstone (himself a former Peelite) sought to give Ireland a degree of autonomy, a move that was opposed by elements in both the left and right wings of his party. These split off to become the Liberal Unionists (led by Joseph Chamberlain), forming a coalition with the Conservatives before merging with them in 1912. The Liberal Unionist influence dragged the party towards the left; Conservative governments passing a number of progressive reforms at the turn of the 20th century. By the late 19th century, the traditional business supporters of the UK Liberal Party had joined the Conservatives, making them the party of business and commerce.
After a period of Liberal dominance before the First World War, the Conservatives gradually became more influential in government, regaining full control of the cabinet in 1922. In the interwar period conservatism was the major ideology in Britain, as the Liberal Party vied with the Labour Party for control of the left. After the Second World War, the first Labour government under Clement Attlee embarked on a program of nationalization of industry and social welfare state. The policies were generally accepted by the Conservatives until the 1980s. Many of Labour's programs were reversed in the 1980s by the Conservative government of Margaret Thatcher, which was guided by neoliberal economics.
Small conservative political parties (such as the United Kingdom Independence Party and the Democratic Unionist Party) began to appear, although they have yet to make any significant impact at Westminster (the DUP is currently part of a ruling coalition in the Northern Ireland Assembly).
Germany.
Conservative thought developed alongside nationalism in Germany, culminating in Germany's victory over France in the Franco-Prussian War, the creation of the unified German Empire in 1871, and the simultaneous rise of Otto von Bismarck on the European political stage. Bismarck's "balance of power" model maintained peace in Europe for decades at the end of the 19th century, and his "revolutionary conservatism" led to significant popular reforms in insurance law, labor, and wages. These and other policies made socialism seem less desirable to the average German family, and propelled Bismarck to high renown during his lifetime.
With the rise of Nazism in 1933, agrarian conservatism faded and was supplanted by a more command-based economy and forced social integration. Though Adolf Hitler succeeded in garnering the support of many German industrialists, prominent traditionalists openly and secretly opposed his policies of euthanasia, genocide, and attacks on organized religion, including Claus von Stauffenberg, Dietrich Bonhoeffer, Henning von Tresckow, Bishop Clemens August Graf von Galen, and the monarchist Carl Friedrich Goerdeler.
More recently, the work of conservative CDU leader Helmut Kohl helped bring about German Reunification, along with the closer integration of Europe in the form of the Maastricht Treaty. Today, German conservatism is often associated with Chancellor Angela Merkel, whose tenure has been marked by attempts to save the common European currency (EURO) from demise.
United States.
In the United States, conservatism is rooted in the American Revolution and its commitment to republicanism, sovereignty of the people, and the rights and liberties of Englishmen while expelling the king and his supporters. Most European conservative writers do not accept American conservatism as genuine; they consider it to be a variety of liberalism. Modern American liberals in the New Deal do not disagree with that consensus view, but conservatives spend much more emphasis on the Revolutionary origins, with the Tea Party advocates using an episode from the 1770s for their name and some even dress in costumes from that era at their rallies.
Historian Gregory Schneider identifies several constants in American conservatism: respect for tradition, support of republicanism, "the rule of law and the Christian religion," and a defense of "Western civilization from the challenges of modernist culture and totalitarian governments."
Latin Europe.
Another form of conservatism developed in France in parallel to conservatism in Britain. It was influenced by Counter-Enlightenment works by men such as Joseph de Maistre and Louis de Bonald. Latin conservatism was less pragmatic and more reactionary than the conservatism of Burke. Many Continental or Traditionalist conservatives do not support separation of Church and state, with most supporting state recognition of and cooperation with the Catholic Church, such as had existed in France before the Revolution.
Eventually conservatives added patriotism and nationalism to the list of traditional values they support. German conservatives were the first to embrace nationalism, which was previously associated with liberalism and the Revolution in France.
Forms of conservatism.
Liberal conservatism.
Liberal conservatism is a variant of conservatism that combines conservative values and policies with classical liberal stances. As these latter two terms have had different meanings over time and across countries, liberal conservatism also has a wide variety of meanings. Historically, the term often referred to the combination of economic liberalism, which champions laissez-faire markets, with the classical conservatism concern for established tradition, respect for authority and religious values. It contrasted itself with classical liberalism, which supported freedom for the individual in both the economic and social spheres.
Over time, the general conservative ideology in many countries adopted economic liberal arguments, and the term "liberal conservatism" was replaced with "conservatism". This is also the case in countries where liberal economic ideas have been the tradition, such as the United States, and are thus considered conservative. In other countries where liberal conservative movements have entered the political mainstream, such as Italy and Spain, the terms "liberal" and "conservative" may be synonymous. The liberal conservative tradition in the United States combines the economic individualism of the classical liberals with a Burkean form of conservatism (which has also become part of the American conservative tradition, such as in the writings of Russell Kirk).
A secondary meaning for the term "liberal conservatism" that has developed in Europe is a combination of more modern conservative (less traditionalist) views with those of social liberalism. This has developed as an opposition to the more collectivist views of socialism. Often this involves stressing what are now conservative views of free-market economics and belief in individual responsibility, with social liberal views on defence of civil rights, environmentalism and support for a limited welfare state. In continental Europe, this is sometimes also translated into English as social conservatism.
Conservative liberalism.
Conservative liberalism is a variant of liberalism that combines liberal values and policies with conservative stances, or, more simply, the right wing of the liberal movement. The roots of conservative liberalism are found at the beginning of the history of liberalism. Until the two World Wars, in most European countries the political class was formed by conservative liberals, from Germany to Italy. Events after World War I brought the more radical version of classical liberalism to a more conservative (i.e. more moderate) type of liberalism.
Libertarian conservatism.
Libertarian conservatism describes certain political ideologies within the United States and Canada which combine libertarian economic issues with aspects of conservatism. Its five main branches are Constitutionalism, paleolibertarianism, neolibertarianism, small government conservatism and Christian libertarianism. They generally differ from paleoconservatives, in that they are in favor of more personal and economic freedom.
Agorists such as Samuel Edward Konkin III labeled libertarian conservatism right-libertarianism.
In contrast to paleoconservatives, libertarian conservatives support strict laissez-faire policies such as free trade, opposition to any national bank and opposition to business regulations. They are vehemently opposed to environmental regulations, corporate welfare, subsidies, and other areas of economic intervention.
Many conservatives, especially in the United States, believe that the government should not play a major role in regulating business and managing the economy. They typically oppose efforts to charge high tax rates and to redistribute income to assist the poor. Such efforts, they argue, do not properly reward people who have earned their money through hard work.
Fiscal conservatism.
Fiscal conservatism is the economic philosophy of prudence in government spending and debt. Edmund Burke, in his "Reflections on the Revolution in France", argued that a government does not have the right to run up large debts and then throw the burden on the taxpayer:
...is to the property of the citizen, and not to the demands of the creditor of the state, that the first and original faith of civil society is pledged. The claim of the citizen is prior in time, paramount in title, superior in equity. The fortunes of individuals, whether possessed by acquisition or by descent or in virtue of a participation in the goods of some community, were no part of the creditor's security, expressed or implied...[The public, whether represented by a monarch or by a senate, can pledge nothing but the public estate; and it can have no public estate except in what it derives from a just and proportioned imposition upon the citizens at large.
Most conservatives believe that government action cannot solve society's problems, such as poverty and inequality. Many believe that government programs that seek to provide services and opportunities for the poor actually encourage dependence and reduce self-reliance. Most conservatives oppose "affirmative action" policies-that is, policies in employment, education, and other areas that aim to counteract past discrimination by giving special help to members of disadvantaged groups. Conservatives believe that the government should not give special treatment to individuals on the basis of group identity.
Many conservatives, especially in the United States, believe that the government should not play a major role in regulating business and managing the economy. They typically oppose efforts to charge high tax rates and to redistribute income to assist the poor. Such efforts, they argue, do not properly reward people who have earned their money through hard work.
Green conservatism.
Green conservatism is a term used to refer to conservatives who have incorporated green concerns into their ideology. One of the first uses of the term "green conservatism" was by former United States Republican House Speaker Newt Gingrich, in a debate on environmental issues with John Kerry. Around this time, the green conservative movement was sometimes referred to as the "crunchy con" movement, a term popularized by "National Review" magazine and the writings of Rod Dreher. The group Republicans for Environmental Protection seeks to strengthen the Republican Party's stance on environmental issues, and supports efforts to conserve natural resources and protect human and environmental health.
National and traditional conservatism.
National conservatism is a political term used primarily in Europe to describe a variant of conservatism which concentrates more on national interests than standard conservatism as well as upholding cultural and ethnic identity, while not being outspokenly nationalist or supporting a far-right approach. In Europe, national conservatives are usually eurosceptics.
National conservatism is heavily oriented towards the traditional family and social stability as well as in favour of limiting immigration. As such, national conservatives can be distinguished from economic conservatives, for whom free market economic policies, deregulation and fiscal conservatism are the main priorities. Some commentators have identified a growing gap between national and economic conservatism: "most parties of the Right are run by economic conservatives who, in varying degrees, have marginalized social, cultural, and national conservatives." National conservatism is also related to traditionalist conservatism.
Traditionalist conservatism is a political philosophy emphasizing the need for the principles of natural law and transcendent moral order, tradition, hierarchy and organic unity, agrarianism, classicism and high culture, and the intersecting spheres of loyalty. Some traditionalists have embraced the labels "reactionary" and "counterrevolutionary", defying the stigma that has attached to these terms since the Enlightenment. Having a hierarchical view of society, many traditionalist conservatives, including a few Americans, defend the monarchical political structure as the most natural and beneficial social arrangement.
Cultural and social conservatism.
Cultural conservatives support the preservation of the heritage of one nation, or of a shared culture that is not defined by national boundaries. The shared culture may be as divergent as Western culture or Chinese culture. In the United States, the term "cultural conservative" may imply a conservative position in the culture war. Cultural conservatives hold fast to traditional ways of thinking even in the face of monumental change. They believe strongly in traditional values and traditional politics, and often have an urgent sense of nationalism.
Social conservatism is distinct from cultural conservatism, although there are some overlaps. Social conservatives believe that the government has a role in encouraging or enforcing what they consider traditional values or behaviors. A social conservative wants to preserve traditional morality and social mores, often through civil law or regulation. Social change is generally regarded as suspect.
A second meaning of the term "social conservatism" developed in the Nordic countries and continental Europe. There it refers to liberal conservatives supporting modern European welfare states.
Social conservatives (in the first meaning of the word) in many countries generally favor the pro-life position in the abortion controversy and oppose human embryonic stem cell research (particularly if publicly funded); oppose both eugenics and human enhancement (transhumanism) while supporting bioconservatism; support a traditional definition of marriage as being one man and one woman; view the nuclear family model as society's foundational unit; oppose expansion of civil marriage and child adoption rights to couples in same-sex relationships; promote public morality and traditional family values; oppose atheism, especially militant atheism, secularism and the separation of church and state; support the prohibition of drugs, prostitution, and euthanasia; and support the censorship of pornography and what they consider to be obscenity or indecency. Most conservatives in the U.S. support the death penalty.
Religious conservatism.
Religious conservatives principally seek to apply the teachings of particular religions to politics, sometimes by merely proclaiming the value of those teachings, at other times by having those teachings influence laws.
In most modern democracies, political conservatism seeks to uphold traditional family structures and social values. Conservatives typically oppose abortion, homosexual behavior, drug use, and sexual activity outside of marriage. In some cases, conservatives values are grounded in religious beliefs, and some conservatives seek to increase the role of religion in public life.
Progressive conservatism.
Progressive conservatism incorporates progressive policies alongside conservative policies. It stresses the importance of a social safety net to deal with poverty, support of limited redistribution of wealth along with government regulation to regulate markets in the interests of both consumers and producers. Progressive conservatism first arose as a distinct ideology in the United Kingdom under Prime Minister Benjamin Disraeli's "One Nation" Toryism.
There have been a variety of progressive conservative governments. In the UK, the Prime Ministers Disraeli, Stanley Baldwin, Neville Chamberlain, Winston Churchill, Harold Macmillan, and present Prime Minister David Cameron are progressive conservatives. The Catholic Church's "Rerum Novarum" (1891) advocates a progressive conservative doctrine known as social Catholicism. In the United States, the administration of President William Howard Taft was progressive conservative and he described himself as "a believer in progressive conservatism" and President Dwight D. Eisenhower declared himself an advocate of "progressive conservatism". In Germany, Chancellor Leo von Caprivi promoted a progressive conservative agenda called the "New Course". In Canada, a variety of conservative governments have been progressive conservative, with Canada's major conservative movement being officially named the Progressive Conservative Party of Canada from 1942 to 2003. In Canada, the Prime Ministers Arthur Meighen, R.B. Bennett, John Diefenbaker, Joe Clark, Brian Mulroney, and Kim Campbell led progressive conservative federal governments.
Historic Conservatism in different countries.
Conservative political parties vary widely from country to country in the goals they wish to achieve. Both conservative and liberal parties tend to favor private ownership of property, in opposition to communist, socialist and green parties, which favor communal ownership or laws requiring social responsibility on the part of property owners. Where conservatives and liberals differ is primarily on social issues. Conservatives tend to reject behavior that does not conform to some social norm. For many years, conservative parties fought to stop extension of voting rights to groups such as to non-Christians, non-whites and women. Modern conservative parties often define themselves by their opposition to liberal or labor parties. The United States usage of the term "conservative" is unique to that country.
According to Alan Ware, Belgium, Denmark, Iceland, Finland, France, Greece, Iceland, Luxembourg, Netherlands, Norway, Sweden, Switzerland, and the UK retained viable conservative parties into the 1980s. Ware said that Australia, Germany, Israel, Italy, Japan, Malta, New Zealand, Spain and the US had no conservative parties, although they had either Christian Democrats or liberals as major right-wing parties. Canada, Ireland, and Portugal had right-wing political parties that defied categorization: the Progressive Conservative Party of Canada; Fianna Fáil, Fine Gael, and Progressive Democrats in Ireland; and the Social Democratic Party of Portugal. Since then, the Swiss People's Party has moved to the extreme right and is no longer considered to be conservative.
Klaus von Beyme, who developed the method of party categorization, found that no modern Eastern European parties could be considered conservative, although the communist and communist-successor parties had strong similarities.
In Italy, which was united by liberals and radicals ("risorgimento"), liberals not conservatives emerged as the party of the Right. In the Netherlands, conservatives merged into a new Christian democratic party in 1980. In Austria, Germany, Portugal and Spain, conservatism was transformed into and incorporated into fascism or the far right. In 1940, all Japanese parties were merged into a single fascist party. Following the war, Japanese conservatives briefly returned to politics but were largely purged from public office.
Louis Hartz explained the absence of conservatism in Australia or the United States as a result of their settlement as radical or liberal fragments of Great Britain. Although he said English Canada had a negligible conservative influence, subsequent writers claimed that loyalists opposed to the American Revolution brought a Tory ideology into Canada. Hartz explained conservatism in Quebec and Latin America as a result of their settlement as feudal societies. The American conservative writer Russell Kirk provided the opinion that conservatism had been brought to the US and interpreted the American revolution as a "conservative revolution".
Conservative elites have long dominated Latin American nations. Mostly this has been achieved through control of and support for civil institutions, the church and the armed forces, rather than through party politics. Typically the church was exempt from taxes and its employees immune from civil prosecution. Where national conservative parties were weak or non-existent, conservatives were more likely to rely on military dictatorship as a preferred form of government. However in some nations where the elites were able to mobilize popular support for conservative parties, longer periods of political stability were achieved. Chile, Colombia and Venezuela are examples of nations that developed strong conservative parties. Argentina, Brazil, El Salvador and Peru are examples of nations where this did not occur. The Conservative Party of Venezuela disappeared following the Federal Wars of 1858-1863. Chile's conservative party, the National Party disbanded in 1973 following a military coup and did not re-emerge as a political force following the subsequent return to democracy.
The conservative Union Nationale governed the province of Quebec in periods from 1936 to 1960, in a close alliance with English Canadian business elites and the Catholic Church. This period, known as the Great Darkness ended with the Quiet Revolution and the party went into terminal decline.
Belgium.
Founded in 1945 as the Christian People's Party, the Flemish Christian Democrats (CD&V) dominated politics in post-war Belgium. In 1999, the party's support collapsed and it became the country's fifth largest party.
Canada.
Canada's "Conservatives" had their roots in the Loyalists – Tories – who left America after the American Revolution. They developed in the socio-economic and political cleavages that existed during the first three decades of the 19th century, and had the support of the business, professional and established Church (Anglican) elites in Ontario and to a lesser extent in Quebec. Holding a monopoly over administrative and judicial offices, they were called the "Family Compact" in Ontario and the "Chateau Clique" in Quebec. John A. Macdonald's successful leadership of the movement to confederate the provinces and his subsequent tenure as prime minister for most of the late 19th century rested on his ability to bring together the English-speaking Protestant oligarchy and the ultramontane Catholic hierarchy of Quebec and to keep them united in a conservative coalition.
The Conservatives combined pro-market liberalism and Toryism. They generally supported an activist government and state intervention in the marketplace, and their policies were marked by "noblesse oblige", a paternalistic responsibility of the elites for the less well-off. From 1942, the party was known as the Progressive Conservatives, until 2003, when the national party merged with the Canadian Alliance to form the Conservative Party of Canada.
Colombia.
The Colombian Conservative Party, founded in 1849, traces its origins to opponents of General Francisco de Paula Santander's 1833–37 administration. While the term "liberal" had been used to describe all political forces in Colombia, the conservatives began describing themselves as "conservative liberals" and their opponents as "red liberals". From the 1860s until the present, the party has supported strong central government, and supported the Catholic Church, especially its role as protector of the sanctity of the family, and opposed separation of church and state. Its policies include the legal equality of all men, the citizen's right to own property and opposition to dictatorship. It has usually been Colombia's second largest party, with the Colombian Liberal Party being the largest.
Denmark.
Founded in 1915, the Conservative People's Party of Denmark. was the successor of "Højre" (literally "right"). In the 2005 election it won 18 out of 179 seats in the "Folketing" and became a junior partner in coalition with the Liberals. The party is preceded by 11 years by the Young Conservatives (KU), today the youth movement of the party. The Party suffered a major defeat in the parliamentary elections of September 2011 in which the party lost more than half of its seat and also lost governmental power.
Finland.
The conservative party in Finland is the National Coalition Party (in Finnish "Kansallinen Kokoomus", "Kok"). The party was founded in 1918 when several monarchist parties united. Although in the past the party was right-wing, today it is a moderate party. While the party advocates economic liberalism, it is committed to the social market economy.
France.
Following the Second World War, conservatives in France supported Gaullist groups and have been nationalistic, and emphasized tradition, order, and the regeneration of France. Gaullists held divergent views on social issues. The number of Conservative groups, their lack of stability, and their tendency to be identified with local issues defy simple categorization. Conservatism has been the major political force in France since the second world war. Unusually, post-war French conservatism was formed around the personality of a leader, Charles de Gaulle, and did not draw on traditional French conservatism, but on the Bonapartism tradition. Gaullism in France continues under the Union for a Popular Movement. The word "conservative" itself is a term of abuse in France.
Greece.
The main interwar conservative party was called the People's Party (PP), which supported constitutional monarchy and opposed the republican Liberal Party. It was able to re-group after the Second World War as part of a "United Nationalist Front" which achieved power campaigning on a simple anticommunist, ultranationalist platform. However, the vote received by the PP declined, leading them to create an expanded party, the Greek Rally, under the leadership of the charismatic General Alexandros Papagos. The conservatives opposed the far right dictatorship of the colonels (1967–1974) and established the New Democratic Party following the fall of the dictatorship. The new party had four objectives: to confront Turkish expansionism in Cyprus, to reestablish and solidify democratic rule, to give the country a strong government, and to make a powerful moderate party a force in Greek politics.
The Independent Greeks, a newly formed political party in Greece has also supported conservatism, particularly national and religious conservatism. The Founding Declaration of the Independent Greeks strongly emphasises in the preservation of the Greek state and its sovereignty, the Greek people and the Greek Orthodox Church.
Iceland.
Founded in 1926 as the Conservative Party, Iceland's Independence Party adopted its current name in 1929. From the beginning they have been the largest vote-winning party, averaging around 40%. They combine liberalism and conservatism, supporting nationalization and opposed to class conflict. While mostly in opposition during the 1930s, they embraced economic liberalism, but accepted the welfare state after the war and participated in governments supportive of state intervention and protectionism. Unlike other Scandanivian conservative (and liberal) parties, it has always had a large working-class following.
Italy.
After WW2 in Italy the conservative theories were mainly represented by the Christian Democracy, which government form the foundation of the Republic until party's dissolution in 1994. Officially DC refused the ideology of Conservatism, but in many aspects, for example family values, it was a typical social conservative party.
In 1994 the media tycoon and entrepreneur Silvio Berlusconi founded the liberal conservative Forza Italia movement. Berlusconi won three elections in 1994, 2001 and 2008 governing the country for almost ten years as Prime Minister.
Besides FI, now the conservative ideas are mainly expressed by the New Centre-Right party led by Angelino Alfano, former Berlusconi's protégé who split from the reborn Forza Italia founding a new conservative movement. Alfano is the current Minister of the Interior in the government of Matteo Renzi.
Luxembourg.
Luxembourg's major conservative party, the Christian Social People's Party (CSV or PCS) was formed as the Party of the Right in 1914, and adopted its present name in 1945. It was consistently the largest political party in Luxembourg and dominated politics throughout the 20th century.
Norway.
The Conservative Party of Norway (Norwegian: Høyre, literally "right") was formed by the old upper class of state officials and wealthy merchants to fight the populist democracy of the Liberal Party, but lost power in 1884 when parliamentarian government was first practised. It formed its first government under parliamentarism in 1889, and continued to alternate in power with the Liberals until the 1930s, when Labour became the dominant political party. It has elements both of paternalism, stressing the responsibilities of the state, and of economic liberalism. It first returned to power in the 1960s. During Kåre Willoch's premiership in the 1980s, much emphasis was laid on liberalizing the credit- and housing market and abolishing the NRK TV and radio monopoly, while supporting law and order in criminal justice and traditional norms in education
Sweden.
Sweden's conservative party, the Moderate Party, was formed in 1904, two years after the founding of the liberal party. The party emphasizes tax reductions, deregulation of private enterprise, and privatization of schools, hospitals and kindergartens.
Switzerland.
There are a number of conservative parties in Switzerland's parliament, the Federal Assembly. These include the largest, the Swiss People's Party (SVP), the Christian Democratic People's Party (CVP), represented in the Federal Council or cabinet by Doris Leuthard (in 2011), and the Conservative Democratic Party of Switzerland (BDP), which is a splinter of the SVP created after a failed attempt to expel Eveline Widmer-Schlumpf from the SVP.
The Swiss People's Party (SVP or UDC) was formed from the 1971 merger of the Party of Farmers, Traders, and Citizens, formed in 1917 and the smaller Swiss Democratic Party, formed in 1942. The SVP emphasized agricultural policy, and was strong among farmers in German-speaking Protestant areas. As Switzerland considered closer relations with the European Union in the 1990s, the SVP adopted a more militant protectionist and isolationist stance. This stance has allowed it to expand into German-speaking Catholic mountainous areas. The Anti-Defamation League, a non-Swiss lobby group based in the USA has accused them of manipulating issues such as immigration, Swiss neutrality and welfare benefits, awakening anti-Semitism and racism. The Council of Europe has called the SVP "extreme right", although some scholars dispute this classification. Hans-Georg Betz for example describes it as "populist radical right".
United Kingdom.
Conservatism in the United Kingdom is related to its counterparts in other Western nations, but has a distinct tradition. Edmund Burke is often considered the "father of conservatism" in the English-speaking world. Burke was a Whig, while the term "Tory" is given to the later Conservative Party. One Australian scholar argues, "For Edmund Burke and Australians of a like mind, the essence of conservatism lies not in a body of theory, but in the disposition to maintain those institutions seen as central to the beliefs and practices of society."
The old established form of English and, after the Act of Union, British conservatism, was the Tory Party. It reflected the attitudes of a rural land owning class, and championed the institutions of the monarchy, the Anglican Church, the family, and property as the best defence of the social order. In the early stages of the industrial revolution, it seemed to be totally opposed to a process that seemed to undermine some of these bulwarks. The new industrial elite were seen by many as enemies to the social order. Robert Peel was able to reconcile the new industrial class to the Tory landed class by persuading the latter to accept the repeal of the Corn Laws in 1846. He created a new political group that sought to preserve the old status quo while accepting the basics of laissez-faire and free trade. The new coalition of traditional landowners and sympathetic industrialists constituted the new Conservative Party.
Benjamin Disraeli gave the new party a political ideology. As a young man, he was influenced by the romantic movement and medievalism, and developed a devastating critique of industrialism. In his novels, he outlined an England divided into two nations, each living in perfect ignorance of each other. He foresaw, like Karl Marx, the phenomenon of an alienated industrial proletariat. His solution involved a return to an idealised view of a corporate or organic society, in which everyone had duties and responsibilities towards other people or groups. This "one nation" conservatism is still a significant tradition in British politics. It has animated a great deal of social reform undertaken by successive Conservative governments.
Although nominally a Conservative, Disraeli was sympathetic to some of the demands of the Chartists and argued for an alliance between the landed aristocracy and the working class against the increasing power of the middle class, helping to found the Young England group in 1842 to promote the view that the rich should use their power to protect the poor from exploitation by the middle class. The conversion of the Conservative Party into a modern mass organization was accelerated by the concept of Tory Democracy attributed to Lord Randolph Churchill, father of Britain's wartime Prime Minister Winston Churchill.
A Liberal-Conservative coalition during World War I, coupled with the ascent of the Labour Party, hastened the collapse of the Liberals in the 1920s. After World War II, the Conservative Party made concessions to the socialist policies of the Left. This compromise was a pragmatic measure to regain power, but also the result of the early successes of central planning and state ownership forming a cross-party consensus. This was known as Butskellism, after the almost identical Keynesian policies of Rab Butler on behalf of the Conservatives, and Hugh Gaitskell for Labour.
However, in the 1980s, under the leadership of Margaret Thatcher, and the influence of Keith Joseph, there was a dramatic shift in the ideological direction of British conservatism, with a movement towards free-market economic policies. As one commentator explains, "The privatization of state owned industries, unthinkable before, became commonplace Thatcher's government and has now been imitated all over the world." Some commentators have questioned whether Thatcherism was consistent with the traditional concept of conservatism in the United Kingdom, and saw her views as more consistent with radical classical liberalism. Thatcher was described as "a radical in a conservative party", and her ideology has been seen as confronting "established institutions" and the "accepted beliefs of the elite", both concepts incompatible with the traditional conception of conservatism as signifying support for the established order and existing social convention.
Modern Conservatism in different countries.
While conservatism has been seen as an appeal to traditional, hierarchical society, some writers, such as Samuel P. Huntington, see it as situational. Under this definition, conservatives are seen as defending the established institutions of their time.
Australia.
The Liberal Party of Australia adheres to the principles of social conservatism and liberal conservatism. It is Liberal in the sense of economics. The party is considered to be more right-wing than the Conservative Party (UK). Other conservative parties are the National Party of Australia, a sister party of the Liberals, Family First Party, Democratic Labor Party, Shooters Party and the Katter's Australian Party.
The second largest party in the country, the Australian Labor Party's dominant faction is Labor Right, a socially conservative element. Australia is generally considered one of the most conservative western nations. Australia undertook significant economic reform under the Australian Labor Party in the mid-1980s. Consequently issues like protectionism, welfare reform, privatization and deregulation are no longer debated in the political space as they are in Europe or North America. Moser and Catley explain, "In America, 'liberal' means left-of-center, and it is a pejorative term when used by conservatives in adversarial political debate. In Australia, of course, the conservatives are in the Liberal Party." Jupp points out that, " decline in English influences on Australian reformism and radicalism, and appropriation of the symbols of Empire by conservatives continued under the Liberal Party leadership of Sir Robert Menzies, which lasted until 1966."
Bosnia and Herzegovina.
Founded 1990 as Party of Democratic Action. It's one of the first Conservative parties in Bosnia after Yugoslavia collapse. It is also known as Bosniak Conservative Party. There is also Croatian Democratic Community of Bosnia and Herzegovina (HDZBIH) which is considered Croatian Conservative Party and Serbian Democratic Party as Serbian Conservative Party. Conservativism is very popular in Bosnia.
South Korea.
South Korea's major conservative party, the Saenuri Party or 새누리당, changed its form throughout its history. First it was the Democratic-Republican Party(1963~1980); its head was Park Chung-hee who seized power in a 1961 military coup d'état and ruled as an unelected military strongman until his formal election as president in 1963. He was president for 16 years, until his assassination on October 26, 1979. The Democratic Justice Party inherited the same ideology as the Democrati-Republican Party. Its head, Chun Doo-hwan, also gained power through a coup. His followers called themselves the Hanahoe. The Democratic Justice Party changed its form and acted to suppress the opposition party and to follow the people's demand for direct elections. The party's Roh Tae-woo became the first president who was elected through direct election. The next form of the major conservative party was the Democratic-Liberal Party. Again, through election, its second leader, Kim Young-sam, became the fourteenth president of Korea. When the conservative party was beaten by the opposition party in the general election, it changed its form again to follow the party members' demand for reforms. It became the New Korean Party. It changed again one year later since the President Kim Young-sam was blamed by the citizen for the IMF. It changed its name to Grand National Party (Hannara-dang) (1998~2011). Since the late Kim Dae-jung assumed the presidency in 1998, GNP had not been the ruling party until Lee Myung-bak won the presidential election of 2007. It renamed to Saenoori Party (새누리당) in 2011.
United States.
The meaning of "conservatism" in America has little in common with the way the word is used elsewhere. As Ribuffo (2011) notes, "what Americans now call conservatism much of the world calls liberalism or neoliberalism." Since the 1950s conservatism in the United States has been chiefly associated with the Republican Party. However, during the era of segregation many Southern Democrats were conservatives, and they played a key role in the Conservative Coalition that controlled Congress from 1937 to 1963.
Major movements within American conservatism include support for tradition, law-and-order, Christianity, anti-communism, and a defense of "Western civilization from the challenges of modernist culture and totalitarian governments." Economic conservatives and libertarians favor small government, low taxes, limited regulation, and free enterprise. Some social conservatives see traditional social values threatened by secularism, so they support school prayer and oppose abortion and homosexuality. Neoconservatives want to expand American ideals throughout the world and show a strong support for Israel. Paleoconservatives, in opposition to multiculturalism, press for restrictions on immigration. Most U.S. conservatives prefer Republicans over Democrats, and most factions favor a strong foreign policy and a strong military. The conservative movement of the 1950s attempted to bring together these divergent strands, stressing the need for unity to prevent the spread of "Godless Communism", which Reagan later labeled an "evil empire". During the Reagan administration, conservatives also supported the so-called "Reagan Doctrine" under which the U.S., as part of a Cold War strategy, provided military and other support to guerrilla insurgencies that were fighting governments aligned with the Soviet Union.
Other modern conservative positions include opposition to world government and environmentalism.
Most recently, the Tea Party movement, founded in 2009, has proven a large outlet for populist American conservative ideas. Their stated goals include rigorous adherence to the U.S. Constitution, lower taxes, and opposition to a growing role for the federal government in health care. Electorally, it was considered a key force in Republicans reclaiming control of the U.S. House of Representatives in 2010.
Psychology.
Following the Second World War, psychologists conducted research into the different motives and tendencies that account for ideological differences between left and right. The early studies focused on conservatives, beginning with Theodor W. Adorno's "The Authoritarian Personality" (1950). This book has been heavily criticized on theoretical and methodological grounds, but some of its findings have been confirmed by further empirical research.
In 1973, British psychologist Glenn Wilson published an influential book providing evidence that a general factor underlying conservative beliefs is "fear of uncertainty". A meta-analysis of research literature by Jost, Glaser, Kruglanski, and Sulloway in 2003 found that many factors, such as intolerance of ambiguity and need for cognitive closure, contribute to the degree of one's political conservatism. A study by Kathleen Maclay stated these traits "might be associated with such generally valued characteristics as personal commitment and unwavering loyalty." The research also suggested that while most people are resistant to change, liberals are more tolerant of it.
According to psychologist Bob Altemeyer, individuals who are politically conservative tend to rank high in Right-Wing Authoritarianism on his RWA scale. This finding was echoed by Theodor Adorno. A study done on Israeli and Palestinian students in Israel found that RWA scores of right-wing party supporters were significantly higher than those of left-wing party supporters. However, a 2005 study by H. Michael Crowson and colleagues suggested a moderate gap between RWA and other conservative positions. "The results indicated that conservatism is not synonymous with RWA."
Psychologist Felicia Pratto and her colleagues have found evidence to support the idea that a high Social Dominance Orientation (SDO) is strongly correlated with conservative political views, and opposition to social engineering to promote equality, though Pratto's findings have been highly controversial. Pratto and her colleagues found that high SDO scores were highly correlated with measures of prejudice. They were refuted in this claim by David J. Schneider, who wrote that "correlations between prejudice and political conservative are reduced virtually to zero when controls for SDO are instituted". Kenneth Minogue criticized Pratto's work, saying "It is characteristic of the conservative temperament to value established identities, to praise habit and to respect prejudice, not because it is irrational, but because such things anchor the darting impulses of human beings in solidities of custom which we do not often begin to value until we are already losing them. Radicalism often generates youth movements, while conservatism is a condition found among the mature, who have discovered what it is in life they most value."
A 1996 study on the relationship between racism and conservatism found that the correlation was stronger among more educated individuals, though specifically anti-Black racism did not increase. They also found that the correlation between racism and conservatism could be entirely accounted for by their mutual relationship with social dominance orientation. The authors concluded that opposition to affirmative action, especially among more highly educated conservatives, was better explained by social dominance orientation than by principled conservatism.
A 2008 research report found that conservatives are happier than liberals, and that as income inequality increases, this difference in relative happiness increases, because conservatives (more than liberals) possess an ideological buffer against the negative hedonic effects of economic inequality.

</doc>
<doc id="6677" url="http://en.wikipedia.org/wiki?curid=6677" title="Classical liberalism">
Classical liberalism

Classical liberalism is a political philosophy and ideology belonging to liberalism in which primary emphasis is placed on securing the freedom of the individual by limiting the power of the government. The philosophy emerged as a response to the Industrial Revolution and urbanization in the 19th century in Europe and the United States. It advocates civil liberties with a limited government under the rule of law, private property rights, and belief in "laissez-faire" economic liberalism. Classical liberalism is built on ideas that had already arisen by the end of the 18th century, including ideas of Adam Smith, John Locke, Jean-Baptiste Say, Thomas Malthus, and David Ricardo. It drew on a psychological understanding of individual liberty, natural law, utilitarianism, and a belief in progress.
In the early 20th century, liberals split on several issues, and in the United States in particular, a distinction grew up between classical liberals and social liberals.
Meaning of the term.
In the late 19th century, classical liberalism developed into neo-classical liberalism, which argued for government to be as small as possible in order to allow the exercise of individual freedom. In its most extreme form, it advocated Social Darwinism. Libertarianism is a modern form of neo-classical liberalism.
The term "classical liberalism" was applied in retrospect to distinguish earlier 19th-century liberalism from the newer social liberalism. The phrase "classical liberalism" is also sometimes used to refer to all forms of liberalism before the 20th century, and some conservatives and libertarians use the term classical liberalism to describe their belief in the primacy of individual freedom and minimal government. It is not always clear which meaning is intended.
Evolution of core beliefs.
Core beliefs of classical liberals included new ideas—which departed from both the older conservative idea of society as a family and from later sociological concept of society as complex set of social networks—that individuals were "egoistic, coldly calculating, essentially inert and atomistic" and that society was no more than the sum of its individual members.
These beliefs were complemented by a belief that "labour", i.e. individuals without capital, can only be motivated by fear of hunger and by a reward, while "men of higher rank" can be motivated by ambition, as well. This led politicians at the time to pass the Poor Law Amendment Act 1834, which limited the provision of social assistance, because classical liberals believed in "an unfettered market" as the mechanism that will most efficiently lead to a nation's wealth. Adopting Thomas Malthus's population theory, they saw poor urban conditions as inevitable, as they believed population growth would outstrip food production; and they considered that to be desirable, as starvation would help limit population growth. They opposed any income or wealth redistribution, which they believed would be dissipated by the lowest orders.
Classical liberals agreed with Thomas Hobbes that government had been created by individuals to protect themselves from one another. They thought that individuals should be free to pursue their self-interest without control or restraint by society. Individuals should be free to obtain work from the highest-paying employers, while the profit motive would ensure that products that people desired were produced at prices they would pay. In a free market, both labour and capital would receive the greatest possible reward, while production would be organised efficiently to meet consumer demand.
Drawing on selected ideas of Adam Smith, classical liberals believed that all individuals are able to equally freely pursue their own economic self-interest, without government direction, serving the common good. They were critical of welfare state as interfering in a free market. They criticized labour's group rights being pursued at the expense of individual rights, while they accepted big corporations' rights being pursued at the expense of inequality of bargaining power noted by Adam Smith:
It was not until emergence of social liberalism that child labour was forbidden, minimum standards of worker safety were introduced, a minimum wage and old age pensions were established, and financial institutions regulations with the goal of fighting cyclic depressions, monopolies, and cartels, were introduced. They were met by classical liberalism as an unjust interference of the state. So called slim state was argued for, instead, serving only the following functions:
They believed that rights are of a "negative" nature which require other individuals (and governments) to refrain from interfering with free market, whereas social liberalism believes labour has a right to be provided with certain benefits or services via taxes paid by corporations.
Core beliefs of classical liberals did not necessarily include democracy where law is made by majority vote by citizens, because "there is nothing in the bare idea of majority rule to show that majorities will always respect the rights of property or maintain rule of law." For example, James Madison argued for a constitutional republic with protections for individual liberty over a pure democracy, reasoning that, in a pure democracy, a "common passion or interest will, in almost every case, be felt by a majority of the whole...and there is nothing to check the inducements to sacrifice the weaker party..."
Hayek's typology of beliefs.
Friedrich Hayek identified two different traditions within classical liberalism: the "British tradition" and the "French tradition". Hayek saw the British philosophers Bernard Mandeville, David Hume, Adam Smith, Adam Ferguson, Josiah Tucker, Edmund Burke and William Paley as representative of a tradition that articulated beliefs in empiricism, the common law, and in traditions and institutions which had spontaneously evolved but were imperfectly understood. The French tradition included Rousseau, Condorcet, the Encyclopedists and the Physiocrats. This tradition believed in rationalism and sometimes showed hostility to tradition and religion. Hayek conceded that the national labels did not exactly correspond to those belonging to each tradition: Hayek saw the Frenchmen Montesquieu, Constant and Tocqueville as belonging to the "British tradition" and the British Thomas Hobbes, Priestley, Richard Price and Thomas Paine as belonging to the "French tradition". Hayek also rejected the label "laissez faire" as originating from the French tradition and alien to the beliefs of Hume, Smith and Burke.
History.
Classical liberalism in Britain developed from Whiggery and radicalism, and represented a new political ideology. Whiggery had become a dominant ideology following the Glorious Revolution of 1688, and was associated with the defence of Parliament, upholding the rule of law and defending landed property. The origins of rights were seen as being in an ancient constitution, which had existed from time immemorial. These rights, which some Whigs considered to include freedom of the press and freedom of speech, were justified by custom rather than by natural rights. They believed that the power of the executive had to be constrained. While they supported limited suffrage, they saw voting as a privilege, rather than as a right. However there was no consistency in Whig ideology, and diverse writers including John Locke, David Hume, Adam Smith and Edmund Burke were all influential among Whigs, although none of them was universally accepted.
British radicals, from the 1790s to the 1820s, concentrated on parliamentary and electoral reform, emphasizing natural rights and popular sovereignty. Richard Price and Joseph Priestley adapted the language of Locke to the ideology of radicalism. The radicals saw parliamentary reform as a first step toward dealing with their many grievances, including the treatment of Protestant Dissenters, the slave trade, high prices and high taxes.
There was greater unity to classical liberalism ideology than there had been with Whiggery. Classical liberals were committed to individualism, liberty and equal rights. They believed that required a free economy with minimal government interference. Writers such as John Bright and Richard Cobden opposed both aristocratic privilege and property, which they saw as an impediment to the development of a class of yeoman farmers. Some elements of Whiggery opposed this new thinking, and were uncomfortable with the commercial nature of classical liberalism. These elements became associated with conservatism.
Classical liberalism was the dominant political theory in Britain from the early 19th century until the First World War. Its notable victories were the Catholic Emancipation Act of 1829, the Reform Act of 1832, and the repeal of the Corn Laws in 1846. The Anti-Corn Law League brought together a coalition of liberal and radical groups in support of free trade under the leadership of Richard Cobden and John Bright, who opposed militarism and public expenditure. Their policies of low public expenditure and low taxation were adopted by William Ewart Gladstone when he became chancellor of the exchequer and later prime minister. Classical liberalism was often associated with religious dissent and nonconformism.
Although classical liberals aspired to a minimum of state activity, they accepted the principle of government intervention in the economy from the early 19th century with passage of the Factory Acts. From around 1840 to 1860, "laissez-faire" advocates of the Manchester School and writers in "The Economist" were confident that their early victories would lead to a period of expanding economic and personal liberty and world peace but would face reversals as government intervention and activity continued to expand from the 1850s. Jeremy Bentham and James Mill, although advocates of "laissez faire", non-intervention in foreign affairs, and individual liberty, believed that social institutions could be rationally redesigned through the principles of Utilitarianism. The Conservative prime minister, Benjamin Disraeli, rejected classical liberalism altogether and advocated Tory Democracy. By the 1870s, Herbert Spencer and other classical liberals concluded that historical development was turning against them. By the First World War, the Liberal Party had largely abandoned classical liberal principles.
The changing economic and social conditions of the 19th century led to a division between neo-classical and social liberals who, while agreeing on the importance of individual liberty, differed on the role of the state. Neo-classical liberals, who called themselves "true liberals", saw Locke's "Second Treatise" as the best guide, and emphasised "limited government", while social liberals supported government regulation and the welfare state. Herbert Spencer in Britain and William Graham Sumner were the leading neo-classical liberal theorists of the 19th century. Neo-classical liberalism has continued into the contemporary era, with writers such as Robert Nozick.
In the United States, liberalism took a strong root because it had little opposition to its ideals, whereas in Europe liberalism was opposed by many reactionary interests. In a nation of farmers, especially farmers whose workers were slaves, little attention was paid to the economic aspects of liberalism. Thomas Jefferson adopted many of the ideals of liberalism but, in the Declaration of Independence, changed Locke's "life, liberty, and property" to the more socially liberal "life, liberty, and the pursuit of happiness". As America grew, industry became a larger and larger part of American life; and, during the term of America's first populist president, Andrew Jackson, economic questions came to the forefront. The economic ideas of the Jacksonian era were almost universally the ideas of classical liberalism. Freedom was maximised when the government took a "hands off" attitude toward industrial development and supported the value of the currency by freely exchanging paper money for gold. The ideas of classical liberalism remained essentially unchallenged until a series of depressions, thought to be impossible according to the tenets of classical economics, led to economic hardship from which the voters demanded relief. In the words of William Jennings Bryan, "You shall not crucify the American farmer on a cross of gold." Classical liberalism remained the orthodox belief among American businessmen until the Great Depression. The Great Depression saw a sea change in liberalism, leading to the development of modern liberalism. In the words of Arthur Schlesinger Jr.:
Intellectual sources.
John Locke.
Central to classical liberal ideology was their interpretation of John Locke's "Second Treatise of Government" and "A Letter Concerning Toleration", which had been written as a defence of the Glorious Revolution of 1688. Although these writings were considered too radical at the time for Britain's new rulers, they later came to be cited by Whigs, radicals and supporters of the American Revolution. However, much of later liberal thought was absent in Locke's writings or scarcely mentioned, and his writings have been subject to various interpretations. There is little mention, for example, of constitutionalism, the separation of powers, and limited government.
James L. Richardson identified five central themes in Locke's writing: individualism, consent, the concepts of the rule of law and government as trustee, the significance of property, and religious toleration. Although Locke did not develop a theory of natural rights, he envisioned individuals in the state of nature as being free and equal. The individual, rather than the community or institutions, was the point of reference. Locke believed that individuals had given consent to government and therefore authority derived from the people rather than from above. This belief would influence later revolutionary movements.
As a trustee, Government was expected to serve the interests of the people, not the rulers, and rulers were expected to follow the laws enacted by legislatures. Locke also held that the main purpose of men uniting into commonwealths and governments was for the preservation of their property. Despite the ambiguity of Locke's definition of property, which limited property to "as much land as a man tills, plants, improves, cultivates, and can use the product of", this principle held great appeal to individuals possessed of great wealth.
Locke held that the individual had the right to follow his own religious beliefs and that the state should not impose a religion against Dissenters. But there were limitations. No tolerance should be shown for atheists, who were seen as amoral, or to Catholics, who were seen as owing allegiance to the Pope over their own national government.
Adam Smith.
Adam Smith's "The Wealth of Nations", published in 1776, was to provide most of the ideas of economics, at least until the publication of J. S. Mill's "Principles" in 1848. Smith addressed the motivation for economic activity, the causes of prices and the distribution of wealth, and the policies the state should follow in order to maximise wealth.
Smith wrote that as long as supply, demand, prices, and competition were left free of government regulation, the pursuit of material self-interest, rather than altruism, would maximize the wealth of a society through profit-driven production of goods and services. An "invisible hand" directed individuals and firms to work toward the nation's good as an unintended consequence of efforts to maximize their own gain. This provided a moral justification for the accumulation of wealth, which had previously been viewed by some as sinful.
He assumed that workers could be paid as low as was necessary for their survival, which was later transformed by Ricardo and Malthus into the "Iron Law of Wages". His main emphasis was on the benefit of free internal and international trade, which he thought could increase wealth through specialization in production. He also opposed restrictive trade preferences, state grants of monopolies, and employers' organisations and trade unions. Government should be limited to defence, public works and the administration of justice, financed by taxes based on income.
Smith's economics was carried into practice in the nineteenth century with the lowering of tariffs in the 1820s, the repeal of the Poor Relief Act, that had restricted the mobility of labour, in 1834, and the end of the rule of the East India Company over India in 1858.
Say, Malthus, and Ricardo.
In addition to Adam Smith's legacy, Say's law, Malthus theories of population and Ricardo's iron law of wages became central doctrines of classical economics. The pessimistic nature of these theories led to Carlyle calling economics the dismal science and it provided a basis of criticism of capitalism by its opponents.
Jean-Baptiste Say was a French economist who introduced Adam Smith's economic theories into France and whose commentaries on Smith were read in both France and Britain. Say challenged Smith's labour theory of value, believing that prices were determined by utility and also emphasised the critical role of the entrepreneur in the economy. However neither of those observations became accepted by British economists at the time. His most important contribution to economic thinking was Say's law, which was interpreted by classical economists that there could be no overproduction in a market, and that there would always be a balance between supply and demand. This general belief influenced government policies until the 1930s. Following this law, since the economic cycle was seen as self-correcting, government did not intervene during periods of economic hardship because it was seen as futile.
Thomas Malthus wrote two books, "An essay on the principle of population", published in 1798, and "Principles of political economy", published in 1820. The second book which was a rebuttal of Say's law had little influence on contemporary economists. His first book however became a major influence on classical liberalism. In that book, Malthus claimed that population growth would outstrip food production, because population grew geometrically, while food production grew arithmetically. As people were provided with food, they would reproduce until their growth outstripped the food supply. Nature would then provide a check to growth in the forms of vice and misery. No gains in income could prevent this, and any welfare for the poor would be self-defeating. The poor were in fact responsible for their own problems which could have been avoided through self-restraint.
David Ricardo, who was an admirer of Adam Smith, covered many of the same topics but while Smith drew conclusions from broadly empirical observations, Ricardo used induction, drawing conclusions by reasoning from basic assumptions. While Ricardo accepted Smith's labour theory of value, he acknowledged that utility could influence the price of some rare items. Rents on agricultural land were seen as the production that was surplus to the subsistence required by the tenants. Wages were seen as the amount required for workers' subsistence and to maintain current population levels. According to his Iron Law of Wages, wages could never rise beyond subsistence levels. Ricardo explained profits as a return on capital, which itself was the product of labour. But a conclusion many drew from his theory was that profit was a surplus appropriated by capitalists to which they were not entitled.
Utilitarianism.
Utilitarianism provided the political justification for implementation of economic liberalism by British governments, which was to dominate economic policy from the 1830s. Although utilitarianism prompted legislative and administrative reform and John Stuart Mill's later writings on the subject foreshadowed the welfare state, it was mainly used as a justification for "laissez faire".
The central concept of utilitarianism, which was developed by Jeremy Bentham, was that public policy should seek to provide "the greatest happiness of the greatest number". While this could be interpreted as a justification for state action to reduce poverty, it was used by classical liberals to justify inaction with the argument that the net benefit to all individuals would be higher.
Political economy.
Classical liberals saw utility as the foundation for public policies. This broke both with conservative "tradition" and Lockean "natural rights", which were seen as irrational. Utility, which emphasises the happiness of individuals, became the central ethical value of all liberalism. Although utilitarianism inspired wide-ranging reforms, it became primarily a justification for "laissez-faire" economics. However, classical liberals rejected Adam Smith's belief that the "invisible hand" would lead to general benefits and embraced Thomas Robert Malthus' view that population expansion would prevent any general benefit and David Ricardo's view of the inevitability of class conflict. "Laissez faire" was seen as the only possible economic approach, and any government intervention was seen as useless and harmful. The Poor Law Amendment Act 1834 was defended on "scientific or economic principles" while the authors of the Elizabethan Poor Law of 1601 were seen as not having had the benefit of reading Malthus.
Commitment to "laissez faire", however, was not uniform. Some economists advocated state support of public works and education. Classical liberals were also divided on free trade. Ricardo, for example, expressed doubt that the removal of grain tariffs advocated by Richard Cobden and the Anti-Corn Law League would have any general benefits. Most classical liberals also supported legislation to regulate the number of hours that children were allowed to work and usually did not oppose factory reform legislation.
Despite the pragmatism of classical economists, their views were expressed in dogmatic terms by such popular writers as Jane Marcet and Harriet Martineau. The strongest defender of "laissez faire" was "The Economist" founded by James Wilson in 1843. "The Economist" criticised Ricardo for his lack of support for free trade and expressed hostility to welfare, believing that the lower orders were responsible for their economic circumstances. "The Economist" took the position that regulation of factory hours was harmful to workers and also strongly opposed state support for education, health, the provision of water, and granting of patents and copyrights.
"The Economist" also campaigned against the Corn Laws that protected landlords in the United Kingdom of Great Britain and Ireland against competition from less expensive foreign imports of cereal products. A rigid belief in "laissez faire" guided the government response in 1846–1849 to the Great Famine in Ireland, during which an estimated 1.5 million people died. The minister responsible for economic and financial affairs, Charles Wood, expected that private enterprise and free trade, rather than government intervention, would alleviate the famine. The Corn Laws were finally repealed in 1846 by removal tariffs on grain which kept the price of bread artificially high. However, repeal of the Corn Laws came too late to stop Irish famine, partly because it was done in stages over three years.
Free trade and world peace.
Several liberals, including Adam Smith and Richard Cobden, argued that the free exchange of goods between nations could lead to world peace, a view recognised by such modern American political scientists as Robert Alan Dahl, Michael W. Doyle, Bruce Martin Rassett and John Robert Oneal. Dr. Erik Gartzke of Columbia University states, "Scholars like Montesquieu, Adam Smith, Richard Cobden, Norman Angell, and Richard Rosecrance have long speculated that free markets have the potential to free states from the looming prospect of recurrent warfare." American political scientists John R. Oneal and Bruce M. Russett, well known for their work on the democratic peace theory, state:
Adam Smith argued in the "Wealth of Nations" that, as societies progressed from hunter gatherers to industrial societies, the spoils of war would rise but that the costs of war would rise further, making war difficult and costly for industrialised nations.
Cobden believed that military expenditures worsened the welfare of the state and benefited a small but concentrated elite minority, summing up British imperialism, which he believed was the result of the economic restrictions of mercantilist policies. To Cobden, and many classical liberals, those who advocated peace must also advocate free markets.
Relationship to modern liberalism.
Many modern scholars of liberalism argue that no particularly meaningful distinction between classical and modern liberalism exists. Alan Wolfe summarises this viewpoint, which:
According to William J. Novak, however, "liberalism" in the United States shifted, "between 1877 and 1937...from laissez-faire constitutionalism to New Deal statism, from classical liberalism to democratic social-welfarism".
L. T. Hobhouse, in , attributed this purported shift, which included qualified acceptance of government intervention in the economy and the collective right to equality in dealings, to an increased desire for what Hobhouse called "just consent". Hayek wrote that Hobhouse's book would have been more accurately titled "Socialism", and Hobhouse himself called his beliefs "liberal socialism".

</doc>
<doc id="6678" url="http://en.wikipedia.org/wiki?curid=6678" title="Cat">
Cat

The domestic cat (Felis catus or Felis silvestris catus) is a small, usually furry, domesticated, and carnivorous mammal. It is often called the housecat when kept as an indoor pet, or simply the cat when there is no need to distinguish it from other felids and felines. Cats are often valued by humans for companionship, and their ability to hunt vermin and household pests.
Cats are similar in anatomy to the other felids, with strong, flexible bodies, quick reflexes, sharp retractable claws, and teeth adapted to killing small prey. Cat senses fit a crepuscular and predatory ecological niche. Cats can hear sounds too faint or too high in frequency for human ears, such as those made by mice and other small animals. They can see in near darkness. Like most other mammals, cats have poorer color vision and a better sense of smell than humans.
Despite being solitary hunters, cats are a social species, and cat communication includes the use of a variety of vocalizations (mewing, purring, trilling, hissing, growling and grunting) as well as cat pheromones, and types of cat-specific body language.
Cats have a rapid breeding rate. Under controlled breeding, they can be bred and shown as registered pedigree pets, a hobby known as cat fancy. Failure to control the breeding of pet cats by neutering, and the abandonment of former household pets, has resulted in large numbers of feral cats worldwide, requiring population control.
Since cats were cult animals in ancient Egypt, they were commonly believed to have been domesticated there, but there may have been instances of domestication as early as the Neolithic from around 9500 years ago (7500 BC).
A genetic study in 2007 concluded that domestic cats are descended from African wildcats ("Felis silvestris lybica") c. 8000 BC, in West Asia. Cats are the most popular pet in the world, and are now found in almost every place where humans live.
Nomenclature and etymology.
The English word "cat" (Old English "catt") is in origin a loanword, introduced to many languages of Europe from Latin "cattus" and Byzantine Greek , including Portuguese and Spanish "gato", French "chat", German "Katze", Lithuanian "katė" and Old Church Slavonic "kotka", among others. The ultimate source of the word is Afroasiatic, presumably from Late Egyptian "čaute", the feminine of "čaus" "wildcat". The word was introduced, together with the domestic animal itself, to the Roman Republic by the 1st century BC.
An alternative word with cognates in many languages is English "puss" ("pussycat"). Attested only from the 16th century, it may have been introduced from Dutch "poes" or from Low German "puuskatte", related to Swedish "kattepus", or Norwegian "pus", "pusekatt". Similar forms exist in Lithuanian "puižė" and Irish "puisín". The etymology of this word is unknown, but it may have simply arisen from a sound used to attract a cat.
A group of cats is referred to as a "clowder" or a "glaring", a male cat is called a "tom" or "tomcat" (or a "gib", if neutered), an unaltered female is called a "queen", and a pre-pubescent juvenile is referred to as a "kitten". Although spayed females have no commonly used name, in some rare instances immature or spayed females are referred to as a "molly". The male progenitor of a cat, especially a pedigreed cat, is its "sire", and its female progenitor is its "dam". In Early Modern English, the word "kitten" was interchangeable with the now-obsolete word "catling".
A pedigreed cat is one whose ancestry is recorded by a cat fancier organization. A purebred cat is one whose ancestry contains only individuals of the same breed. Many pedigreed and especially purebred cats are exhibited as show cats. Cats of unrecorded, mixed ancestry are referred to as domestic short-haired or domestic long-haired cats, by coat type, or commonly as random-bred, moggies (chiefly British), or (using terms borrowed from dog breeding) mongrels or mutt-cats.
While the African wildcat is the ancestral subspecies from which domestic cats are descended, and wildcats and domestic cats can completely interbreed, there are several intermediate stages between domestic pet and pedigree cats on the one hand and those entirely wild animals on the other. The semi-feral cat is a mostly outdoor cat that is not owned by any one individual, but is generally friendly to people and may be fed by several households. Feral cats are associated with human habitation areas and may be fed by people or forage in rubbish, but are typically wary of human interaction.
Taxonomy and evolution.
The felids are a rapidly evolving family of mammals that share a common ancestor only 10–15 million years ago, and include, in addition to the domestic cat, lions, tigers, cougars, and many others. Within this family, domestic cats ("Felis catus") are part of the genus "Felis", which is a group of small cats containing approximately seven species (depending upon classification scheme). Members of the genus are found worldwide and include the jungle cat ("Felis chaus") of southeast Asia, European wildcat ("F. silvestris silvestris"), African wildcat ("F. s. lybica"), the Chinese mountain cat ("F. bieti"), and the Arabian sand cat ("F. margarita"), among others.
All the cats in this genus share a common ancestor that probably lived around 6–7 million years ago in Asia. The exact relationships within the Felidae are close but still uncertain, e.g. the Chinese mountain cat is sometimes classified (under the name "Felis silvestris bieti") as a subspecies of the wildcat, like the North African variety "F. s. lybica". As domestic cats are little altered from wildcats, they can readily interbreed. This hybridization poses a danger to the genetic distinctiveness of wildcat populations, particularly in Scotland and Hungary, and possibly also the Iberian Peninsula.
The domestic cat was first classified as "Felis catus" by Carolus Linnaeus in the tenth edition of his "Systema Naturae" in 1758. However, because of modern phylogenetics, domestic cats are now usually regarded as another subspecies of the wildcat, "Felis silvestris". This has resulted in mixed usage of the terms, as the domestic cat can be called by its subspecies name, "Felis silvestris catus". Wildcats have also been referred to as various subspecies of "F. catus", but in 2003 the International Commission on Zoological Nomenclature fixed the name for wildcats as "F. silvestris". The most common name in use for the domestic cat remains "F. catus", following a convention for domesticated animals of using the earliest (the senior) synonym proposed. Sometimes the domestic cat has been called "Felis domesticus" or "Felis domestica", as proposed by German naturalist J. C. P. Erxleben in 1777, but these are not valid taxonomic names and have been used only rarely in scientific literature, because Linnaeus's binomial takes precedence.
Cats have either a mutualistic or commensal relationship with humans. However, in comparison to dogs, cats have not undergone major changes during the domestication process, as the form and behavior of the domestic cat are not radically different from those of wildcats, and domestic cats are perfectly capable of surviving in the wild. This limited evolution during domestication means that domestic cats tend to interbreed freely with wild relatives, distinguishing them from other domesticated animals. Fully domesticated house cats also often interbreed with feral "F. catus" populations. However, several natural behaviors and characteristics of wildcats may have pre-adapted them for domestication as pets. These traits include their small size, social nature, obvious body language, love of play, and relatively high intelligence; they may also have an inborn tendency towards tameness.
There are two main theories about how cats were domesticated. In one, people deliberately tamed cats in a process of artificial selection, as they were useful predators of vermin. However, this has been criticized as implausible, because there may have been little reward for such an effort: cats generally do not carry out commands and, although they do eat rodents, other species such as ferrets or terriers may be better at controlling these pests. The alternative idea is that cats were simply tolerated by people and gradually diverged from their wild relatives through natural selection, as they adapted to hunting the vermin found around humans in towns and villages.
There is a population of Transcaucasian Black feral cats once classified as "Felis daemon" (Satunin, 1904), but now this population is considered to be a part of domestic cat.
Genetics.
The domesticated cat and its closest wild ancestor are both diploid organisms that possess 38 chromosomes and roughly 20,000 genes. About 250 heritable genetic disorders have been identified in cats, many similar to human inborn errors. The high level of similarity among the metabolisms of mammals allows many of these feline diseases to be diagnosed using genetic tests that were originally developed for use in humans, as well as the use of cats as animal models in the study of the human diseases.
Anatomy.
Domestic cats are similar in size to the other members of the genus "Felis", typically weighing between . However, some breeds, such as the Maine Coon, can occasionally exceed 11 kg (25 lb). Conversely, very small cats (less than ) have been reported. The world record for the largest cat is . The smallest adult cat ever officially recorded weighed around . Feral cats tend to be lighter as they have more limited access to food than house cats. In the Boston area, the average feral adult male will scale and average feral female . Cats average about 23–25 cm (9–10 in) in height and 46 cm (18.1 in) in head/body length (males being larger than females), with tails averaging 30 cm (11.8 in) in length.
Cats have seven cervical vertebrae as do almost all mammals; 13 thoracic vertebrae (humans have 12); seven lumbar vertebrae (humans have five); three sacral vertebrae like most mammals (humans have five because of their bipedal posture); and a variable number of caudal vertebrae in the tail (humans retain three to five caudal vertebrae, fused into an internal coccyx). The extra lumbar and thoracic vertebrae account for the cat's spinal mobility and flexibility. Attached to the spine are 13 ribs, the shoulder, and the pelvis. Unlike human arms, cat forelimbs are attached to the shoulder by free-floating clavicle bones which allow them to pass their body through any space into which they can fit their heads.
The cat skull is unusual among mammals in having very large eye sockets and a powerful and specialized jaw. Within the jaw, cats have teeth adapted for killing prey and tearing meat. When it overpowers its prey, a cat delivers a lethal neck bite with its two long canine teeth, inserting them between two of the prey's vertebrae and severing its spinal cord, causing irreversible paralysis and death. Compared to other felines, domestic cats have narrowly spaced canine teeth, which is an adaptation to their preferred prey of small rodents, which have small vertebrae. The premolar and first molar together compose the carnassial pair on each side of the mouth, which efficiently shears meat into small pieces, like a pair of scissors. These are vital in feeding, since cats' small molars cannot chew food effectively.
Cats, like dogs, are digitigrades. They walk directly on their toes, with the bones of their feet making up the lower part of the visible leg. Cats are capable of walking very precisely, because like all felines they directly register; that is, they place each hind paw (almost) directly in the print of the corresponding forepaw, minimizing noise and visible tracks. This also provides sure footing for their hind paws when they navigate rough terrain. Unlike most mammals, when cats walk, they use a "pacing" gait; that is, they move the two legs on one side of the body before the legs on the other side. This trait is shared with camels and giraffes. As a walk speeds up into a trot, a cat's gait will change to be a "diagonal" gait, similar to that of most other mammals (and many other land animals, such as lizards): the diagonally opposite hind and forelegs will move simultaneously.
Like almost all members of the Felidae family, cats have protractable and retractable claws. In their normal, relaxed position the claws are sheathed with the skin and fur around the paw's toe pads. This keeps the claws sharp by preventing wear from contact with the ground and allows the silent stalking of prey. The claws on the forefeet are typically sharper than those on the hind feet. Cats can voluntarily extend their claws on one or more paws. They may extend their claws in hunting or self-defense, climbing, kneading, or for extra traction on soft surfaces. Most cats have five claws on their front paws, and four on their rear paws. The fifth front claw (the dewclaw) is proximal to the other claws. More proximally, there is a protrusion which appears to be a sixth "finger". This special feature of the front paws, on the inside of the wrists, is the carpal pad, also found on the paws of big cats and of dogs. It has no function in normal walking, but is thought to be an anti-skidding device used while jumping. Some breeds of cats are prone to polydactyly (extra toes and claws). These are particularly common along the northeast coast of North America.
Physiology.
As cats are familiar and easily kept animals, their physiology has been particularly well studied; it generally resembles that of other carnivorous mammals but displays several unusual features probably attributable to cats' descent from desert-dwelling species. For instance, cats are able to tolerate quite high temperatures: Humans generally start to feel uncomfortable when their skin temperature passes about 38 °C (100 °F), but cats show no discomfort until their skin reaches around 52 °C (126 °F), and can tolerate temperatures of up to 56 °C (133 °F) if they have access to water.
Cats conserve heat by reducing the flow of blood to their skin and lose heat by evaporation through their mouth. Cats have minimal ability to sweat, with glands located primarily in their paw pads, and pant for heat relief only at very high temperatures (but may also pant when stressed). A cat's body temperature does not vary throughout the day; this is part of cats' general lack of circadian rhythms and may reflect their tendency to be active both during the day and at night. Cats' feces are comparatively dry and their urine is highly concentrated, both of which are adaptations that allow cats to retain as much fluid as possible. Their kidneys are so efficient that cats can survive on a diet consisting only of meat, with no additional water, and can even rehydrate by drinking seawater.
Cats are obligate carnivores: their physiology has evolved to efficiently process meat, and they have difficulty digesting plant matter. In contrast to omnivores such as rats, which only require about 4% protein in their diet, about 20% of a cat's diet must be protein. Cats are unusually dependent on a constant supply of the amino acid arginine, and a diet lacking arginine causes marked weight loss and can be rapidly fatal. Another unusual feature is that the cat cannot produce taurine, with taurine deficiency causing macular degeneration, wherein the cat's retina slowly degenerates, causing irreversible blindness. Since cats tend to eat all of their prey, they obtain minerals by digesting animal bones, and a diet composed only of meat may cause calcium deficiency.
A cat's gastrointestinal tract is adapted to meat eating, being much shorter than that of omnivores and having low levels of several of the digestive enzymes that are needed to digest carbohydrates. These traits severely limit the cat's ability to digest and use plant-derived nutrients, as well as certain fatty acids. Despite the cat's meat-oriented physiology, several vegetarian or vegan cat foods have been marketed that are supplemented with chemically synthesized taurine and other nutrients, in attempts to produce a complete diet. However, some of these products still fail to provide all the nutrients that cats require, and diets containing no animal products pose the risk of causing severe nutritional deficiencies.
Cats do eat grass occasionally. Proposed explanations include that grass is a source of folic acid or dietary fiber.
Senses.
Cats have excellent night vision and can see at only one-sixth the light level required for human vision. This is partly the result of cat eyes having a tapetum lucidum, which reflects any light that passes through the retina back into the eye, thereby increasing the eye's sensitivity to dim light. Another adaptation to dim light is the large pupils of cats' eyes. Unlike some big cats, such as tigers, domestic cats have slit pupils. These slit pupils can focus bright light without chromatic aberration, and are needed since the domestic cat's pupils are much larger, relative to their eyes, than the pupils of the big cats. Indeed, at low light levels a cat's pupils will expand to cover most of the exposed surface of its eyes. However, domestic cats have rather poor color vision and (like most non-primate mammals) have only two types of cones, optimized for sensitivity to blue and yellowish green; they have limited ability to distinguish between red and green. A 1993 paper found a response to mid-wavelengths from a system other than the rods which might be due to a third type of cone. However, this appears to be an adaptation to low light levels rather than representing true trichromatic vision.
Cats have excellent hearing and can detect an extremely broad range of frequencies. They can hear higher-pitched sounds than either dogs or humans, detecting frequencies from 55 Hz up to 79 kHz, a range of 10.5 octaves; while humans can only hear from 31 Hz up to 18 kHz, and dogs hear from 67 Hz to 44 kHz, which are both ranges of about 9 octaves. Cats do not use this ability to hear ultrasound for communication but it is probably important in hunting, since many species of rodents make ultrasonic calls. Cat hearing is also extremely sensitive and is among the best of any mammal, being most acute in the range of 500 Hz to 32 kHz. This sensitivity is further enhanced by the cat's large movable outer ears (their "pinnae"), which both amplify sounds and help a cat sense the direction from which a noise is coming.
Cats have relatively few taste buds compared to humans. Domestic and wild cats share a gene mutation that keeps their sweet taste buds from binding to sugary molecules like carbohydrates, leaving them with no ability to taste sweetness. Their taste buds instead respond to amino acids, bitter tastes and acids.
To aid with navigation and sensation, cats have dozens of movable vibrissae (whiskers) over their body, especially their face. These provide information on the width of gaps and on the location of objects in the dark, both by touching objects directly and by sensing air currents; they also trigger protective blink reflexes to protect the eyes from damage.
Health.
The average life expectancy for male indoor cats at birth is around 12 to 14 years, with females usually living a year or two longer. However, there have been reports of cats reaching into their 30s, with the oldest known cat, Creme Puff, dying at a verified age of 38. Feline life expectancy has increased significantly in recent decades. Having a cat neutered confers some health benefits, since castrated males cannot develop testicular cancer, spayed females cannot develop uterine or ovarian cancer, and both have a reduced risk of mammary cancer. The lifespan of feral cats is hard to determine accurately, although one study reported a median age of 4.7 years, with a range between 0 to 8.3 years.
Diseases.
Cats can suffer from a wide range of health problems, including infectious diseases, parasites, injuries and chronic disease. Vaccinations are available for many of these diseases, and domestic cats are regularly given treatments to eliminate parasites such as worms and fleas.
Poisoning.
In addition to obvious dangers such as rodenticides, insecticides and herbicides, cats may be poisoned by many chemicals that are usually considered safe by their human guardians. This is because their livers are less effective at some forms of detoxification than those of many other animals, including humans and dogs. Some of the most common causes of poisoning in cats are antifreeze and rodent baits. It has also been suggested that cats may be particularly sensitive to environmental pollutants. When a cat has a sudden or prolonged serious illness without any obvious cause, it is possible that it has been exposed to a toxin.
Many human medicines should never be given to cats. For example, the painkiller paracetamol (also called acetaminophen, sold as Tylenol and Panadol) is extremely toxic to cats: even very small doses need immediate treatment and can be fatal. Even aspirin, which is sometimes used to treat arthritis in cats, is much more toxic to them than to humans and must be administered cautiously. Similarly, application of minoxidil (Rogaine) to the skin of cats, either accidentally or by well-meaning guardians attempting to counter loss of fur, has sometimes been fatal. Essential oils can be toxic to cats and there have been reported cases of serious illnesses caused by tea tree oil, including flea treatments and shampoos containing it.
Other common household substances that should be used with caution around cats include mothballs and other naphthalene products. Phenol-based products (e.g. Pine-Sol, Dettol (Lysol) or hexachlorophene) are often used for cleaning and disinfecting near cats' feeding areas or litter boxes but these can sometimes be fatal. Ethylene glycol, often used as an automotive antifreeze, is particularly appealing to cats, and as little as a teaspoonful can be fatal. Some human foods are toxic to cats; for example chocolate can cause theobromine poisoning, although (unlike dogs) few cats will eat chocolate. Large amounts of onions or garlic are also poisonous to cats. Many houseplants are also dangerous, such as "Philodendron" species and the leaves of the Easter lily (Lilium longiflorum), which can cause permanent and life-threatening kidney damage.
Behavior.
Free-ranging cats are active both day and night, although they tend to be slightly more active at night. The timing of cats' activity is quite flexible and varied, which means that house cats may be more active in the morning and evening (crepuscular behavior), as a response to greater human activity at these times. Although they spend the majority of their time in the vicinity of their home, housecats can range many hundreds of meters from this central point, and are known to establish territories that vary considerably in size, in one study ranging from .
Cats conserve energy by sleeping more than most animals, especially as they grow older. The daily duration of sleep varies, usually 12–16 hours, with 13–14 being the average. Some cats can sleep as much as 20 hours in a 24-hour period. The term "cat nap" for a short rest refers to the cat's tendency to fall asleep (lightly) for a brief period. While asleep, cats experience short periods of rapid eye movement sleep often accompanied by muscle twitches, which suggests that they are dreaming.
Sociability.
Although wildcats are solitary, the social behavior of domestic cats is much more variable and ranges from widely dispersed individuals to feral cat colonies that form around a food source, based on groups of co-operating females. Within such groups one cat is usually dominant over the others. Each cat in a colony holds a distinct territory, with sexually active males having the largest territories, which are about ten times larger than those of female cats and may overlap with several females' territories. These territories are marked by urine spraying, by rubbing objects at head height with secretions from facial glands, and by defecation. Between these territories are neutral areas where cats watch and greet one another without territorial conflicts. Outside these neutral areas, territory holders usually chase away stranger cats, at first by staring, hissing, and growling, and if that does not work, by short but noisy and violent attacks. Despite some cats cohabiting in colonies, cats do not have a social survival strategy, or a pack mentality and always hunt alone.
Domestic cats use many vocalizations for communication, including purring, trilling, hissing, growling/snarling, grunting, and several different forms of meowing. By contrast, feral cats are generally silent. Their types of body language, including position of ears and tail, relaxation of whole body, and kneading of paws, are all indicators of mood. The tail and ears are particularly important social signal mechanisms in cats, e.g. with a raised tail acting as a friendly greeting, and flattened ears indicating hostility. Tail-raising also indicates the cat's position in the group's social hierarchy, with dominant individuals raising their tails less often than subordinate animals. Nose-to-nose touching is also a common greeting and may be followed by social grooming, which is solicited by one of the cats raising and tilting its head.
However, some pet cats are poorly socialized. In particular, older cats may show aggressiveness towards newly arrived kittens, which may include biting and scratching; this type of behavior is known as Feline Asocial Aggression.
Even though cats and dogs are believed to be natural enemies, they can live together if correctly socialized.
For cats, life in proximity to humans and other animals kept by them amounts to a symbiotic social adaptation. They may express great affection towards their human (and even other) companions, especially if they psychologically imprint on them at a very young age and are treated with consistent affection. It has been suggested that, ethologically, the human keeper of a cat functions as a sort of surrogate for the cat's mother, and that adult housecats live their lives in a kind of extended kittenhood, a form of behavioral neoteny. It has even been theorized that the high-pitched sounds housecats make to solicit food may mimic the cries of a hungry human infant, making them particularly hard for humans to ignore.
Grooming.
Cats are known for their cleanliness, spending many hours licking their coats. The cat's tongue has backwards-facing spines about 500 micrometers long, which are called papillae. These are quite rigid, as they contain keratin. These spines allow cats to groom themselves by licking their fur, with the rows of papillae acting like a hairbrush. Some cats, particularly longhaired cats, occasionally regurgitate hairballs of fur that have collected in their stomachs from grooming. These clumps of fur are usually sausage-shaped and about two to three centimeters long. Hairballs can be prevented with remedies that ease elimination of the hair through the gut, as well as regular grooming of the coat with a comb or stiff brush. Some cats can develop a compulsive behavior known as psychogenic alopecia, or excessive grooming.
Fighting.
Among domestic cats, males are more likely to fight than females. Among feral cats, the most common reason for cat fighting is competition between two males to mate with a female. In such cases, most fights will be won by the heavier male. Another common reason for fighting in domestic cats is the difficulty of establishing territories within a small home. Female cats will also fight over territory or to defend their kittens. Neutering will decrease or eliminate this behavior in many cases, suggesting that the behavior is linked to sex hormones.
When fighting, cats make themselves appear more impressive and threatening by raising their fur, arching their backs, and turning sideways, thus increasing their apparent size. Often, the ears are pointed down and back to avoid damage to the inner ear and potentially listen for any changes behind them while focused forward. They may also vocalize loudly and bare their teeth in an effort to further intimidate their opponent. Fights usually consist of grappling and delivering powerful slaps to the face and body with the forepaws as well as bites. Cats will also throw themselves to the ground in a defensive posture to rake their opponent's belly with their powerful hind legs.
Serious damage is rare as the fights are usually short in duration, with the loser running away with little more than a few scratches to the face and ears. However, fights for mating rights are typically more severe and injuries may include deep puncture wounds and lacerations. Normally, serious injuries from fighting will be limited to infections of scratches and bites, though these can occasionally kill cats if untreated. In addition, bites are probably the main route of transmission of feline immunodeficiency virus (FIV). Sexually active males will usually be involved in many fights during their lives, and often have decidedly battered faces with obvious scars and cuts to the ears and nose.
Hunting and feeding.
Most breeds of cat have a noted fondness for settling in high places, or perching. In the wild, a higher place may serve as a concealed site from which to hunt; domestic cats may strike prey by pouncing from such a perch as a tree branch, as does a leopard. Other possible explanations include that height gives the cat a better observation point, allowing it to survey its territory. During a fall from a high place, a cat can reflexively twist its body and right itself using its acute sense of balance and flexibility. This is known as the cat righting reflex. An individual cat always rights itself in the same way, provided it has the time to do so, during a fall. The height required for this to occur is around 90 cm (3 feet). Cats without a tail (e.g. Manx cats) also have this ability, since a cat mostly moves its hind legs and relies on conservation of angular momentum to set up for landing, and the tail is in fact little used for this feat. This leads to the proverb "a cat always lands on its feet".
One poorly understood element of cat hunting behavior is the presentation of prey to human guardians. Ethologist Paul Leyhausen proposed that cats adopt humans into their social group, and share excess kill with others in the group according to the local pecking order, in which humans are placed at or near the top. Anthropologist and zoologist Desmond Morris, in his 1986 book "Catwatching", suggests that when cats bring home mice or birds, they are attempting to teach their human to hunt, or trying to help their human as if feeding "an elderly cat, or an inept kitten". Morris's theory is inconsistent with the fact that male cats also bring home prey, despite males having no involvement with raising kittens.
Domestic cats select food based on its temperature, smell and texture, strongly disliking chilled foods and responding most strongly to moist foods rich in amino acids, which are similar to meat. Cats may reject novel flavors (a response termed neophobia) and learn quickly to avoid foods that have tasted unpleasant in the past. They may also avoid sugary foods and milk; since they are lactose intolerant, these sugars are not easily digested and may cause soft stools or diarrhea. They can also develop odd eating habits. Some cats like to eat or chew on other things, most commonly wool, but also plastic, paper, string, aluminum foil/Christmas tree tinsel, or even coal. This condition is called pica and can threaten their health, depending on the amount and toxicity of the items eaten.
Since cats cannot fully close their lips around something to create suction, they use a lapping method with the tongue to draw liquid upwards into their mouths. Lapping at a rate of four times a second, the cat touches the smooth tip of its tongue to the surface of the water, and quickly retracts it, drawing water upwards.
Play.
Domestic cats, especially young kittens, are known for their love of play. This behavior mimics hunting and is important in helping kittens learn to stalk, capture, and kill prey. Cats will also engage in play fighting, with each other and with humans. This behavior may be a way for cats to practice the skills needed for real combat, and might also reduce any fear they associate with launching attacks on other animals.
Owing to the close similarity between play and hunting, cats prefer to play with objects that resemble prey, such as small furry toys that move rapidly, but rapidly lose interest (they become habituated) in a toy they have played with before. Cats also tend to play with toys more when they are hungry. String is often used as a toy, but if it is eaten it can become caught at the base of the cat's tongue and then move into the intestines, a medical emergency which can cause serious illness, even death. Owing to the risks posed by cats eating string, it is sometimes replaced with a laser pointer's dot, which cats may chase. While concerns have been raised about the safety of these lasers, John Marshall, an ophthalmologist at St Thomas' Hospital, has stated that it would be "virtually impossible" to blind a cat with a laser pointer.
Reproduction.
Female cats are seasonally polyestrous, which means they may have many periods of heat over the course of a year, the season beginning in spring and ending in late autumn. Heat periods occur about every two weeks and last about 4 to 7 days. Multiple males will be attracted to a female in heat. The males will fight over her, and the victor wins the right to mate. At first, the female will reject the male, but eventually the female will allow the male to mate. The female will utter a loud yowl as the male pulls out of her. This is because a male cat's penis has a band of about 120–150 backwards-pointing penile spines, which are about one millimeter long; upon withdrawal of the penis, the spines rake the walls of the female's vagina, which is a trigger for ovulation. This act also occurs to clear the vagina of other sperm in the context of a second (or more) mating, thus giving the later males a larger chance of conception.
After mating, the female will wash her vulva thoroughly. If a male attempts to mate with her at this point, the female will attack him. After about 20 to 30 minutes, once the female is finished grooming, the cycle will repeat.
Because ovulation is not always triggered by a single mating, females may not be impregnated by the first male with which they mate. Furthermore, cats are superfecund; that is, a female may mate with more than one male when she is in heat, with the result that different kittens in a litter may have different fathers.
At 124 hours post conception the morula forms. At 148 hours early blastocysts form. At 10–12 days implantation occurs.
The gestation period for cats is between 64 and 67 days, with an average length of 66 days. The size of a litter averages three to five kittens, with the first litter usually smaller than subsequent litters. Kittens are weaned at between six and seven weeks, and cats normally reach sexual maturity at 5–10 months (females) and to 5–7 months (males), although this can vary depending on breed. Females can have two to three litters per year, so may produce up to 150 kittens in their breeding span of around ten years.
Cats are ready to go to new homes at about 12 weeks of age, when they are ready to leave their mother. Cats can be surgically sterilized (spayed or castrated) as early as 7 weeks to limit unwanted reproduction. This surgery also prevents undesirable sex-related behavior, such as aggression, territory marking (spraying urine) in males and yowling (calling) in females. Traditionally, this surgery was performed at around six to nine months of age, but it is increasingly being performed prior to puberty, at about three to six months. In the US, approximately 80% of household cats are neutered.
Vocalizations.
The cat is a very vocal animal. Known for its trademark purring, it also produces a wide variety of other sounds.
The mechanism by which cats purr is elusive. The cat has no unique anatomical feature that is clearly responsible for the sound. It was, until recent times, believed that only the cats of the "Felis" genus could purr. However, felids of the "Panthera" genus (tiger, lion, jaguar and leopard) also produce sounds similar to purring, but only when exhaling.
Ecology.
Habitats.
Cats are a cosmopolitan species and are found across much of the world. Geneticist Stephen James O'Brien, of the National Cancer Institute in Frederick, Maryland, remarked on how successful cats have been in evolutionary terms: "Cats are one of evolution's most charismatic creatures. They can live on the highest mountains and in the hottest deserts." They are extremely adaptable and are now present on all continents except Antarctica, and on 118 of the 131 main groups of islands—even on sub-Antarctic islands such as the Kerguelen Islands.
Feral cats can live in forests, grasslands, tundra, coastal areas, agricultural land, scrublands, urban areas and wetlands. Their habitats even include small oceanic islands with no human inhabitants. Further, the close relatives of domestic cats, the African wildcat ("Felis silvestris lybica") and the Arabian sand cat ("Felis margarita") both inhabit desert environments, and domestic cats still show similar adaptations and behaviors. The cat's ability to thrive in almost any terrestrial habitat has led to its designation as one of the world's worst invasive species.
Impact on prey species.
To date, there are few scientific data available to assess the impact of cat predation on prey populations. Even well-fed domestic cats may hunt and kill, mainly catching small mammals, but also birds, amphibians, reptiles, fish and invertebrates. Hunting by domestic cats may be contributing to the decline in the numbers of birds in urban areas, although the importance of this effect remains controversial. In the wild, the introduction of feral cats during human settlement can threaten native species with extinction. In many cases controlling or eliminating the populations of non-native cats can produce a rapid recovery in native animals. However, the ecological role of introduced cats can be more complicated. For example, cats can control the numbers of rats, which also prey on birds' eggs and young, so a cat population can protect an endangered bird species by suppressing mesopredators.
In the Southern Hemisphere, cats are a particular problem in landmasses such as Australasia, where cat species have never been native and there were few equivalent native medium-sized mammalian predators. Native species such as the New Zealand Kakapo and the Australian Bettong, for example, tend to be more ecologically vulnerable and behaviorally "naive" to predation by feral cats. Feral cats have had a major impact on these native species and have played a leading role in the endangerment and extinction of many animals.
Cat numbers in the UK are growing and their abundance is far above the "natural" carrying capacity, because their population sizes are independent of their prey's dynamics: i.e. cats are "recreational" hunters, with other food sources. Population densities can be as high as 2,000 individuals per km2 and the trend is an increase of 0.5 million cats annually.
Impact on birds.
The domestic cat is probably a significant predator of birds. UK assessments indicate that they may be accountable for an estimated 64.8 million bird deaths each year. Certain species appear more susceptible than others; for example, 30% of house sparrow mortality is linked to the domestic cat. In the recovery of ringed robins ("Erithacus rubecula") and dunnocks ("Prunella modularis"), it was also concluded that 31% of deaths were a result of cat predation. The presence of larger carnivores such as coyotes which prey on cats and other small predators reduces the effect of predation by cats and other small predators such as opossums and raccoons on bird numbers and variety. The proposal that cat populations will increase when the numbers of these top predators decline is called the mesopredator release hypothesis. However a new study suggests that cats are a much greater menace than previously thought and that feral cats kill several billion birds each year in the United States.
On islands, birds can contribute as much as 60% of a cat's diet. In nearly all cases, however, the cat cannot be identified as the sole cause for reducing the numbers of island birds, and in some instances eradication of cats has caused a 'mesopredator release' effect; where the suppression of top carnivores creates an abundance of smaller predators that cause a severe decline in their shared prey. Domestic cats are, however, known to be a contributing factor to the decline of many species; a factor that has ultimately led, in some cases, to extinction. The South Island Piopio, Chatham Islands Rail, the Auckland Islands Merganser, and the common diving petrel are a few from a long list, with the most extreme case being the flightless Stephens Island Wren, which was driven to extinction only a few years after its discovery.
Some of the same factors that have promoted adaptive radiation of island avifauna over evolutionary time appear to promote vulnerability to non-native species in modern time. The susceptibility of many island birds is undoubtedly due to evolution in the absence of mainland predators, competitors, diseases and parasites, in addition to lower reproductive rates and extended incubation periods. The loss of flight, or reduced flying ability is also characteristic of many island endemics. These biological aspects have increased vulnerability to extinction in the presence of introduced species, such as the domestic cat. Equally, behavioral traits exhibited by island species, such as "predatory naivety" and ground-nesting, have also contributed to their susceptibility.
Cats and humans.
Cats are common pets in Europe and North America, and their worldwide population exceeds 500 million. Although cat guardianship has commonly been associated with women, a 2007 Gallup poll reported that men and women were equally likely to own a cat.
According to the Humane Society of the United States, as well as being kept as pets, cats are also used in the international fur trade, for making coats, gloves, hats, shoes, blankets and stuffed toys. About 24 cats are needed to make a cat fur coat. This use has now been outlawed in several countries, including the United States, Australia and the European Union. However, some cat furs are still made into blankets in Switzerland as folk remedies that are believed to help rheumatism.
Census.
There are approximately 220 million domestic cats in the world, according to the International Federation for Animal Health Europe (IFAH).
A few attempts to build a cat census have been made over the years, both through associations or national and international organization (such as the Canadian Federation of Humane Societies's one) and over the net, but such a task does not seem so simple to achieve.
Feral cats.
Feral cats are domestic cats that were born in or have reverted to a wild state. They are unfamiliar with and wary of humans and roam freely in urban and rural areas. The numbers of feral cats is not known, but estimates of the US feral population range from 25 to 60 million. Feral cats may live alone, but most are found in large groups called feral colonies, which occupy a specific territory and are usually associated with a source of food. Famous feral cat colonies are found in Rome around the Colosseum and Forum Romanum, with cats at some of these sites being fed and given medical attention by volunteers.
Public attitudes towards feral cats vary widely: ranging from seeing them as free-ranging pets, to regarding them as vermin. One common approach to reducing the feral cat population is termed "trap-neuter-return", where the cats are trapped, neutered, immunized against rabies and the feline leukemia virus, and then released. Before releasing them back into their feral colonies, the attending veterinarian often nips the tip off one ear to mark the feral as neutered and inoculated, since these cats may be trapped again. Volunteers continue to feed and give care to these cats throughout their lives. Given this support, their lifespan is increased, and behavior and nuisance problems caused by competition for food are reduced.
History and mythology.
Traditionally, historians tended to think that ancient Egypt was the site of cat domestication, owing to the clear depictions of house cats in Egyptian paintings about 3,600 years old. However, in 2004, a Neolithic grave was excavated in Shillourokambos, Cyprus, that contained the skeletons, laid close to one another, of both a human and a cat. The grave is estimated to be 9,500 years old, pushing back the earliest known feline–human association significantly. The cat specimen is large and closely resembles the African wildcat ("Felis silvestris lybica"), rather than present-day domestic cats. This discovery, combined with genetic studies, suggest that cats were probably domesticated in the Middle East, in the Fertile Crescent around the time of the development of agriculture and then they were brought to Cyprus and Egypt.
Direct evidence for the domestication of cats 5,300 years ago in Quanhucun in China has been published by researchers. The cats are believed to have been attracted to the village by rodents which in turn were attracted by grain cultivated and stored by humans.
In ancient Egypt cats were sacred animals, with the goddess Bastet often depicted in cat form, sometimes taking on the warlike aspect of a lioness. The Romans are often credited with introducing the domestic cat from Egypt to Europe; in Roman Aquitaine, a 1st or 2nd century epitaph of a young girl holding a cat is one of two earliest depictions of the Roman domesticated cat. However, it is possible that cats were already kept in Europe prior to the Roman Empire, as they may have already been present in Britain in the late Iron Age. Domestic cats were spread throughout much of the rest of the world during the Age of Discovery, as they were carried on sailing ships to control shipboard rodents and as good-luck charms.
Several ancient religions believed that cats are exalted souls, companions or guides for humans, that they are all-knowing but are mute so they cannot influence decisions made by humans. In Japan, the "maneki neko" is a cat that is a symbol of good fortune. Although there are no sacred species in Islam, cats are revered by Muslims. Some writers have stated that Muhammad had a favorite cat, Muezza. He is reported to have loved cats so much that "he would do without his cloak rather than disturb one that was sleeping on it".
Freyja—the goddess of love, beauty, and fertility in Norse mythology—is depicted as riding a chariot drawn by cats.
Many cultures have negative superstitions about cats. An example would be the belief that a black cat "crossing one's path" leads to bad luck, or that cats are witches' familiars used to augment a witch's powers and skills. The killing of cats in Medieval Ypres, Belgium is commemorated in the innocuous present-day Kattenstoet (cat parade).
According to a myth in many cultures, cats have multiple lives. In many countries, they are believed to have nine lives, but in Italy, Germany, Greece and some Spanish-speaking regions they are said to have seven lives, while in Turkish and Arabic traditions the number of lives is six. The myth is attributed to the natural suppleness and swiftness cats exhibit to escape life-threatening situations. Also lending credence to this myth is the fact that falling cats often land on their feet, using an instinctive righting reflex to twist their bodies around. Nonetheless, cats can still be injured or killed by a high fall.

</doc>
<doc id="6681" url="http://en.wikipedia.org/wiki?curid=6681" title="Crank">
Crank

Crank may refer to:

</doc>
<doc id="6682" url="http://en.wikipedia.org/wiki?curid=6682" title="Clade">
Clade

A clade (from Ancient Greek , "klados", "branch") or monophylum (see monophyletic) is a group consisting of an ancestor and all its descendants, a single "branch" on the "tree of life". The ancestor may be an individual, a population or even a species (extinct or extant). Many familiar groups, rodents and insects for example, are clades; others, like lizards and monkeys, are not (lizards excludes snakes, monkeys excludes apes and humans). Increasingly, taxonomists try to avoid naming taxa that are not clades.
Etymology.
The term "clade" was coined in 1957 by the biologist Julian Huxley to refer to the result of cladogenesis, a concept Huxley borrowed from Bernhard Rensch.
Definitions.
Clade and ancestor.
A clade is by definition monophyletic, meaning it contains one ancestor (which can be an organism, a population, or a species) and all its descendants. The ancestor can be known or unknown; any and all members of a clade can be extant or extinct.
Clades and phylogenetic trees.
The science that tries to reconstruct phylogenetic trees and thus discover clades is called phylogenetics or cladistics, the latter term being derived from "clade" by Ernst Mayr (1965). The results of phylogenetic/cladistic analyses are tree-shaped diagrams called "cladograms"; they, and all their branches, are phylogenetic hypotheses.
Three methods of defining clades are featured in phylogenetic nomenclature: node-, stem-, and apomorphy-based (see here for detailed definitions).
Terminology.
The relationship between clades can be described in several ways:
Nomenclature and taxonomy.
The idea of a clade did not exist in pre-Darwinian Linnaean taxonomy, which was based by necessity only on internal or external morphological similarities between organisms – although as it happens, many of the better known animal groups in Linnaeus' original Systema Naturae (notably among the vertebrate groups) do represent clades. The phenomenon of convergent evolution is however responsible for many cases where there are misleading similarities in the morphology of groups that evolved from different lineages.
With the publication of Darwin's theory of evolution in 1859, the idea was born that groups used in a system of classification should represent branches on the evolutionary tree of life. In the century and a half since then, taxonomists have increasingly worked to make the taxonomic system reflect evolution. When it comes to naming, however, this principle is not always compatible with the traditional rank-based nomenclature. In the latter, only taxa associated with a rank can be named, yet there are not enough ranks to name a long series of nested clades; also, taxon names cannot be defined in a way that guarantees them to refer to clades. For these and other reasons, phylogenetic nomenclature has been developed; it is still controversial.

</doc>
<doc id="6684" url="http://en.wikipedia.org/wiki?curid=6684" title="Communications in Afghanistan">
Communications in Afghanistan

Communications in Afghanistan has grown considerably in the last decade, and has embarked on wireless companies, internet, radio stations and television channels. The Afghan Ministry of Communications signed a $64.5 agreement in 2006 with China's ZTE on the establishment of a countrywide optical fiber cable network. The project was intended to improve telephone, internet, television and radio broadcast services throughout Afghanistan. As of 2012, about 85% of the country's population has access to communication services.
There are about 18 million mobile phone users in Afghanistan. Etisalat, Roshan, Afghan Wireless and MTN are the leading telecom companies. Etisalat became the first company to launch 4G services in 2013. It is predicted that over 50% of the population would have access to the internet by 2015. In the meantime, Afghan officials announced that they plan to send its own satellite into space.
Telephone.
There are about 18 million GSM mobile phone subscribers in Afghanistan as of 2009, with over 75,000 fixed-telephone-lines and little over 190,000 CDMA subscribers. Mobile communications have improved because of the introduction of wireless carriers into this developing country. The first was Afghan Wireless, which is US based that was founded by Ehsan Bayat. The second was Roshan, which began providing services to all major cities within Afghanistan. There are also a number of VSAT stations in major cities such as Kabul, Kandahar, Herat, Mazari Sharif, and Jalalabad, providing international and domestic voice/data connectivity. The international calling code for Afghanistan is +93. The following is a partial list of mobile phone companies in the country:
All the companies providing communication services are obligated to deliver 2.5% of their income to the communication development fund annually. According to the Ministry of Communication and Information Technology there are 4760 active towers throughout the country which covers 85% of the population. The Ministry of Communication and Information Technology plans to expand its services in remote parts of the country where the remaining 15% of the population will be covered with the installation of 700 new towers.
Phone calls in Afghanistan have been monitored by the National Security Agency according to Wikileaks.
Internet.
Afghanistan was given legal control of the ".af" domain in 2003, and the Afghanistan Network Information Center (AFGNIC) was established to administer domain names. As of 2010, there are at least 46 internet service providers (ISPs) in the country. Internet in Afghanistan is also at the peak with 1 million users as of 2009.
According to the Ministry of Communications, the following are some of the different ISPs operating in Afghanistan:
Television.
There are over 50 Afghan television channels worldwide, many of which are based inside Afghanistan while others are broadcast from North America and Europe. Selected foreign channels are also shown to the public in Afghanistan, but with the use of the internet, over 3,500 international TV channels may be accessed in Afghanistan.
Radio.
As of 2007, there are an estimated 50 private radio stations throughout the country. Broadcasts are in Dari, Pashto, English, Uzbeki and many other languages.
The number of radio listeners are decreasing and are being slowly outnumbered by television. Of Afghanistan's 6 main cities, Kandahar and Khost have a lot of radio listeners. Kabul and Jalalabad have moderate number of listeners. However, Mazar-e-Sharif and especially Herat have very few radio listeners.
Postal service.
In 1870, a central post office was established at Bala Hissar in Kabul and a post office in the capital of each province. The service was slowly being expanded over the years as more postal offices were established in each large city by 1918. Afghanistan became a member of the Universal Postal Union in 1928, and the postal administration elevated to the Ministry of Communication in 1934. Civil war caused a disruption in issuing official stamps during the 1980s-90s war but in 1999 postal service was operating again. Postal services to/from Kabul worked remarkably well all throughout the war years. Postal services to/from Herat resumed in 1997. The Afghan government has reported to the UPU several times about illegal stamps being issued and sold in 2003 and 2007.
Afghanistan Post has been reorganizing the postal service in 2000s with the help of Pakistan Post. The Afghanistan Postal commission was formed to prepare a written policy for the development of the postal sector, which will form the basis of a new postal services law governing licensing of postal services providers. The project was expected to finish by 2008.
Satellite.
The Afghan Ministry of Communications announced that they plan to send its own satellite into space. The satellite will be launched at a position of 50 degrees east, due to geographical position of Afghanistan, the satellite will be able to cover Asia, Europe, Australia and Africa. According to the Afghan Ministry of Communications and Information Technology (MCIT), the geographical existence of Afghanistan is very valuable to connect all communications to this satellite.
The satellite is expected to improve the country's television and internet coverage. In addition, it will save Afghanistan money in fees; Afghanistan currently pays around 100 million dollars a year to provide communication services. Based on statistics from the MCIT, Afghanistan needs around 1,700 megabytes for its communications per year. It will take at least three years to launch the satellite, with the total cost ranging between 200 to 300 million, major international countries have shown interest in sharing the costs with the Afghan government.

</doc>
<doc id="6689" url="http://en.wikipedia.org/wiki?curid=6689" title="Christian of Oliva">
Christian of Oliva

Christian of Oliva (), also Christian of Prussia () (died 4 December(?) 1245) was the first missionary bishop of Prussia. 
Christian was born about 1180 in the Duchy of Pomerania, possibly in the area of Chociwel (according to Johannes Voigt). Probably as a juvenile he joined the Cistercian Order at newly established Kołbacz ("Kolbatz") Abbey and in 1209 entered Oliwa Abbey near Gdańsk, founded in 1178 by the Samboride dukes of Pomerelia. At this time the Piast duke Konrad I of Masovia with the consent of Pope Innocent III had started the first of several unsuccessful Prussian Crusades into the adjacent Chełmno Land and Christian acted as a missionary among the Prussians east of the Vistula River. In 1215 he is recorded as abbot of Łekno Abbey near Gniezno in Greater Poland, most but not all authors identify him with Godfrey of Łękno. 
In 1209, Christian was commissioned by the Pope to be responsible for the Prussian missions between the Vistula and Neman Rivers and in 1212 he was appointed bishop. In 1215 he went to Rome in order to report to the Curia on the condition and prospects of his mission, and was consecrated first "Bishop of Prussia" at the Fourth Council of the Lateran. His seat as a bishop remained at Oliwa Abbey on the western side of the Vistula, whereas the pagan Prussian (later East Prussian) territory was on the eastern side of it.
The attempts by Konrad of Masovia to subdue the Prussian lands had picked long-term and intense border quarrels, whereby the Polish lands of Masovia, Cuyavia and even Greater Poland became subject to continuous Prussian raids. Bishop Christian asked the new Pope Honorius III for the consent to start another Crusade, however a first campaign in 1217 proved a failure and even the joint efforts by Duke Konrad with the Polish High Duke Leszek I the White and Duke Henry I the Bearded of Silesia in 122/23 only led to the reconquest of Chełmno Land but did not stop the Prussian invasions. At least Christian was able to establish the Diocese of Chełmno east of the Vistula, adopting the episcopal rights from the Masovian Bishop of Płock, confirmed by both Duke Konrad and the Pope.
Duke Konrad of Masovia still was not capable to end the Prussian attacks on his territory and in 1224 began to conduct negotiations with the Teutonic Knights under Grand Master Hermann von Salza in order to strengthen his forces. As von Salza initially hesitated to offer his services, Christian created the military Order of Dobrzyń ("Fratres Milites Christi") in 1228, however to little avail.
Meanwhile von Salza had to abandon his hope to establish an Order's State in the Burzenland region of Transylvania, which had led to an éclat with King Andrew II of Hungary. He obtained a charter by Emperor Frederick II issued in the 1226 Golden Bull of Rimini, whereby Chełmno Land would be the unshared possession of the Teutonic Knights, which was confirmed by Duke Konrad of Masovia in the 1230 Treaty of Kruszwica. Christian ceded his possessions to the new State of the Teutonic Order and in turn was appointed Bishop of Chełmno the next year.
Bishop Christian continued his mission in Sambia ("Samland"), where from 1233 to 1239 he was held captive by pagan Prussians, and freed in trade for five other hostages who then in turn were released for a ransom of 800 Marks, granted to him by Pope Gregory IX. He had to deal with the constant cut-back of his autonomy by the Knights and asked the Roman Curia for mediation. In 1243, the Papal legate William of Modena divided the Prussian lands of the Order's State into four dioceses, whereby the bishops retained the secular rule over about on third of the diocesan territory:
all suffragan dioceses under the Archbishopric of Riga. Christian was supposed to choose one of them, but did not agree to the division. He possibly retired to the Cistercians Abbey in Sulejów, where he died before the conflict was solved.

</doc>
<doc id="6690" url="http://en.wikipedia.org/wiki?curid=6690" title="Coca-Cola">
Coca-Cola

Coca-Cola is a carbonated soft drink sold in stores, restaurants, and vending machines throughout the world. It is produced by The Coca-Cola Company of Atlanta, Georgia, and is often referred to simply as Coke (a registered trademark of The Coca-Cola Company in the United States since March 27, 1944). Originally intended as a patent medicine when it was invented in the late 19th century by John Pemberton, Coca-Cola was bought out by businessman Asa Griggs Candler, whose marketing tactics led Coke to its dominance of the world soft-drink market throughout the 20th century.
The company produces concentrate, which is then sold to licensed Coca-Cola bottlers throughout the world. The bottlers, who hold territorially exclusive contracts with the company, produce finished product in cans and bottles from the concentrate in combination with filtered water and sweeteners. The bottlers then sell, distribute and merchandise Coca-Cola to retail stores and vending machines. The Coca-Cola Company also sells concentrate for soda fountains to major restaurants and food service distributors.
The Coca-Cola Company has, on occasion, introduced other cola drinks under the Coke brand name. The most common of these is Diet Coke, with others including Caffeine-Free Coca-Cola, Diet Coke Caffeine-Free, Coca-Cola Cherry, Coca-Cola Zero, Coca-Cola Vanilla, and special versions with lemon, lime or coffee. In 2013, Coke products could be found in over 200 countries worldwide, with consumers downing more than 1.8 billion company beverage servings each day.
Based on Interbrand's best global brand study of 2011, Coca-Cola was the world's most valuable brand.
History.
19th century historical origins.
Colonel John Pemberton was wounded in the Civil War, became addicted to morphine, and began a quest to find a substitute for the dangerous opiate. The prototype Coca-Cola recipe was formulated at Pemberton's Eagle Drug and Chemical House, a drugstore in Columbus, Georgia, originally as a coca wine. He may have been inspired by the formidable success of Vin Mariani, a European coca wine.
In 1885, Pemberton registered his French Wine Coca nerve tonic. In 1886, when Atlanta and Fulton County passed prohibition legislation, Pemberton responded by developing Coca-Cola, essentially a nonalcoholic version of French Wine Coca.
The first sales were at Jacob's Pharmacy in Atlanta, Georgia, on May 8, 1886. It was initially sold as a patent medicine for five cents a glass at soda fountains, which were popular in the United States at the time due to the belief that carbonated water was good for the health. Pemberton claimed Coca-Cola cured many diseases, including morphine addiction, dyspepsia, neurasthenia, headache, and impotence. Pemberton ran the first advertisement for the beverage on May 29 of the same year in the "Atlanta Journal".
By 1888, three versions of Coca-Cola – sold by three separate businesses – were on the market. A copartnership had been formed on January 14, 1888 between Pemberton and four Atlanta businessmen: J.C. Mayfield, A.O. Murphey; C.O. Mullahy and E.H. Bloodworth. Not codified by any signed document, a verbal statement given by Asa Candler years later asserted under testimony that he had acquired a stake in Pemberton's company as early as 1887. John Pemberton declared that the "name" "Coca-Cola" belonged to his son, Charley, but the other two manufacturers could continue to use the "formula".
Charley Pemberton's record of control over the "Coca-Cola" name was the underlying factor that allowed for him to participate as a major shareholder in the March 1888 Coca-Cola Company incorporation filing made in his father's place. Charley's exclusive control over the "Coca Cola" name became a continual thorn in Asa Candler's side.
Candler's oldest son, Charles Howard Candler, authored a book in 1950 published by Emory University. In this definitive biography about his father, Candler specifically states: "..., on April 14, 1888, the young druggist Griggs Candler purchased a one-third interest in the formula of an almost completely unknown proprietary elixir known as Coca-Cola."
The deal was actually between John Pemberton's son Charley and Walker, Candler & Co. – with John Pemberton acting as cosigner for his son. For $50 down and $500 in 30 days, Walker, Candler & Co. obtained all of the one-third interest in the Coca-Cola Company that Charley held, all while Charley still held on to the name. After the April 14 deal, on April 17, 1888, one-half of the Walker/Dozier interest shares were acquired by Candler for an additional $750.
The Coca-Cola Company.
In 1892, Candler set out to incorporate a second company; "The Coca-Cola Company" (the current corporation). When Candler had the earliest records of the "Coca-Cola Company" burned in 1910, the action was claimed to have been made during a move to new corporation offices around this time.
After Candler had gained a better foothold on Coca-Cola in April 1888, he nevertheless was forced to sell the beverage he produced with the recipe he had under the names "Yum Yum" and "Koke". This was while Charley Pemberton was selling the elixir, although a cruder mixture, under the name "Coca-Cola", all with his father's blessing. After both names failed to catch on for Candler, by the summer of 1888, the Atlanta pharmacist was quite anxious to establish a firmer legal claim to Coca-Cola, and hoped he could force his two competitors, Walker and Dozier, completely out of the business, as well.
When Dr. John Stith Pemberton suddenly died on August 16, 1888, Asa G. Candler now sought to move swiftly forward to attain his vision of taking full control of the whole Coca-Cola operation.
Charley Pemberton, an alcoholic, was the one obstacle who unnerved Asa Candler more than anyone else. Candler is said to have quickly maneuvered to purchase the exclusive rights to the name "Coca-Cola" from Pemberton's son Charley right after Dr. Pemberton's death. One of several stories was that Candler bought the title to the name from Charley's mother for $300; approaching her at Dr. Pemberton's funeral. Eventually, Charley Pemberton was found on June 23, 1894, unconscious, with a stick of opium by his side. Ten days later, Charley died at Atlanta's Grady Hospital at the age of 40.
In Charles Howard Candler's 1950 book about his father, he stated: "On August 30th {1888}, he {Asa Candler} became sole proprietor of Coca-Cola, a fact which was stated on letterheads, invoice blanks and advertising copy."
With this action on August 30, 1888, Candler's sole control became technically all true. Candler had negotiated with Margaret Dozier and her brother Woolfolk Walker a full payment amounting to $1,000, which all agreed Candler could pay off with a series of notes over a specified time span. By May 1, 1889, Candler was now claiming full ownership of the Coca-Cola beverage, with a total investment outlay by Candler for the drink enterprise over the years amounting to $2,300.
In 1914, Margaret Dozier, as co-owner of the original Coca-Cola Company in 1888, came forward to claim that her signature on the 1888 Coca-Cola Company bill of sale had been forged. Subsequent analysis of certain similar transfer documents had also indicated John Pemberton's signature was most likely a forgery, as well, which some accounts claim was precipitated by his son Charley.
Origins of bottling.
The first bottling of Coca-Cola occurred in Vicksburg, Mississippi, at the Biedenharn Candy Company in 1891. The proprietor of the bottling works was Joseph A. Biedenharn. The original bottles were Biedenharn bottles, very different from the much later hobble-skirt design of 1915 now so familiar.
It was then a few years later that two entrepreneurs from Chattanooga, Tennessee, namely; Benjamin F. Thomas and Joseph B. Whitehead, proposed the idea of bottling and were so persuasive that Candler signed a contract giving them control of the procedure for only one dollar. Candler never collected his dollar, but in 1899, Chattanooga became the site of the first Coca-Cola bottling company. Candler remained very content just selling his company's syrup. The loosely termed contract proved to be problematic for The Coca-Cola Company for decades to come. Legal matters were not helped by the decision of the bottlers to subcontract to other companies, effectively becoming parent bottlers.
The first outdoor wall advertisement that promoted the Coca-Cola drink was painted in 1894 in Cartersville, Georgia.
Cola syrup is sold as an over-the-counter dietary supplement for upset stomach.
20th century landmarks.
By the time of its 50th anniversary, the soft drink had reached the status of a national icon in the USA. In 1935, it was certified kosher by Atlanta Rabbi Tobias Geffen, after the company made minor changes in the sourcing of some ingredients.
The longest running commercial Coca-Cola soda fountain anywhere was Atlanta's Fleeman's Pharmacy, which first opened its doors in 1914. Jack Fleeman took over the pharmacy from his father and ran it until 1995; closing it after 81 years.
On July 12, 1944, the one-billionth gallon of Coca-Cola syrup was manufactured by The Coca-Cola Company.
Cans of Coke first appeared in 1955.
New Coke.
On April 23, 1985, Coca-Cola, amid much publicity, attempted to change the formula of the drink with "New Coke". Follow-up taste tests revealed most consumers preferred the taste of New Coke to both Coke and Pepsi, but Coca-Cola management was unprepared for the public's nostalgia for the old drink, leading to a backlash. The company gave in to protests and returned to a variation of the old formula using high fructose corn syrup instead of cane sugar as the main sweetener, under the name Coca-Cola Classic, on July 10, 1985.
21st century.
On July 5, 2005, it was revealed that Coca-Cola would resume operations in Iraq for the first time since the Arab League boycotted the company in 1968.
In April 2007, in Canada, the name "Coca-Cola Classic" was changed back to "Coca-Cola". The word "Classic" was removed because "New Coke" was no longer in production, eliminating the need to differentiate between the two. The formula remained unchanged.
In January 2009, Coca-Cola stopped printing the word "Classic" on the labels of bottles sold in parts of the southeastern United States. The change is part of a larger strategy to rejuvenate the product's image. The word "Classic" was removed from all Coca-Cola products by 2011.
In November 2009, due to a dispute over wholesale prices of Coca-Cola products, Costco stopped restocking its shelves with Coke and Diet Coke. However, some Costco locations (such as the ones in Tucson, Arizona), sell imported Coca-Cola from Mexico.
Coca-Cola introduced the 7.5-ounce mini-can in 2009, and on September 22, 2011, the company announced price reductions, asking retailers to sell eight-packs for $2.99. That same day, Coca-Cola announced the 12.5-ounce bottle, to sell for 89 cents. A 16-ounce bottle has sold well at 99 cents since being re-introduced, but the price was going up to $1.19.
In 2012, Coca-Cola would resume business in Myanmar after 60 years of absence due to U.S.-imposed investment sanctions against the country. Coca-Cola's bottling plant will be located in Yangon and is part of the company's five-year plan and $200 million investment in Myanmar.
Coca-Cola with its partners is to invest USD 5 billion in its operations in India by 2020.
In 2013, it was announced that Coca-Cola Life would be introduced in Argentina that would contain stevia and sugar.
Production.
Ingredients.
A can of Coke (12 fl ounces/355 ml) has 39 grams of carbohydrates (all from sugar, approximately 10 teaspoons), 50 mg of sodium, 0 grams fat, 0 grams potassium, and 140 calories.
Formula of natural flavorings.
The exact formula of Coca-Cola's natural flavorings (but not its other ingredients, which are listed on the side of the bottle or can) is a trade secret. The original copy of the formula was held in SunTrust Bank's main vault in Atlanta for 86 years. Its predecessor, the Trust Company, was the underwriter for the Coca-Cola Company's initial public offering in 1919. On December 8, 2011, the original secret formula was moved from the vault at SunTrust Banks to a new vault containing the formula which will be on display for visitors to its World of Coca-Cola museum in downtown Atlanta.
A popular myth states that only two executives have access to the formula, with each executive having only half the formula. The truth is that while Coca-Cola does have a rule restricting access to only two executives, each knows the entire formula and others, in addition to the prescribed duo, have known the formulation process.
On February 11, 2011, Ira Glass revealed on his PRI radio show, "This American Life", that the secret formula to Coca-Cola had been uncovered in a 1979 newspaper. The formula found basically matched the formula found in Pemberton's diary.
Use of stimulants in formula.
When launched, Coca-Cola's two key ingredients were cocaine and caffeine. The cocaine was derived from the coca leaf and the caffeine from kola nut, leading to the name Coca-Cola (the "K" in Kola was replaced with a "C" for marketing purposes).
Coca – cocaine.
Pemberton called for five ounces of coca leaf per gallon of syrup, a significant dose; in 1891, Candler claimed his formula (altered extensively from Pemberton's original) contained only a tenth of this amount. Coca-Cola once contained an estimated nine milligrams of cocaine per glass. In 1903, it was removed.
After 1904, instead of using fresh leaves, Coca-Cola started using "spent" leaves – the leftovers of the cocaine-extraction process with trace levels of cocaine. Coca-Cola now uses a cocaine-free coca leaf extract prepared at a Stepan Company plant in Maywood, New Jersey.
In the United States, the Stepan Company is the only manufacturing plant authorized by the Federal Government to import and process the coca plant, which it obtains mainly from Peru and, to a lesser extent, Bolivia. Besides producing the coca flavoring agent for Coca-Cola, the Stepan Company extracts cocaine from the coca leaves, which it sells to Mallinckrodt, a St. Louis, Missouri, pharmaceutical manufacturer that is the only company in the United States licensed to purify cocaine for medicinal use.
Kola nuts – caffeine.
Kola nuts act as a flavoring and the source of caffeine in Coca-Cola. In Britain, for example, the ingredient label states "Flavourings (Including Caffeine)." Kola nuts contain about 2.0 to 3.5% caffeine, are of bitter flavor and are commonly used in cola soft drinks. In 1911, the U.S. government initiated "United States v. Forty Barrels and Twenty Kegs of Coca-Cola", hoping to force Coca-Cola to remove caffeine from its formula. The case was decided in favor of Coca-Cola. Subsequently, in 1912, the U.S. Pure Food and Drug Act was amended, adding caffeine to the list of "habit-forming" and "deleterious" substances which must be listed on a product's label.
Coca-Cola contains 34 mg of caffeine per 12 fluid ounces (9.8 mg per 100 ml).
Franchised production model.
The actual production and distribution of Coca-Cola follows a franchising model. The Coca-Cola Company only produces a syrup concentrate, which it sells to bottlers throughout the world, who hold Coca-Cola franchises for one or more geographical areas. The bottlers produce the final drink by mixing the syrup with filtered water and sweeteners, and then carbonate it before putting it in cans and bottles, which the bottlers then sell and distribute to retail stores, vending machines, restaurants and food service distributors.
The Coca-Cola Company owns minority shares in some of its largest franchises, such as Coca-Cola Enterprises, Coca-Cola Amatil, Coca-Cola Hellenic Bottling Company and Coca-Cola FEMSA, but fully independent bottlers produce almost half of the volume sold in the world.
Independent bottlers are allowed to sweeten the drink according to local tastes.
The bottling plant in Skopje, Macedonia, received the 2009 award for "Best Bottling Company".
On May 5, 2014, Coca-Cola said they are working to remove a controversial ingredient, brominated vegetable oil, from all of their drinks.
Geographic spread.
Since it announced its intention to begin distribution in Burma in June 2012, Coca-Cola has been officially available in every country in the world except Cuba and North Korea. However, it is reported to be available in both countries as a grey import.
Coca-Cola has been a point of legal discussion in the Middle East. In the early 20th century, a fatwa was created in Egypt to discuss the question of "whether Muslims were permitted to drink Coca-Cola and Pepsi cola." The fatwa states: "According to the Muslim Hanefite, Shafi'ite, etc., the rule in Islamic law of forbidding or allowing foods and beverages is based on the presumption that such things are permitted unless it can be shown that they are forbidden on the basis of the Qur'an." The Muslim jurists stated that, unless the Qu'ran specifically prohibits the consumption of a particular product, it is permissible to consume. Another clause was discussed, whereby the same rules apply if a person is unaware of the condition or ingredients of the item in question.
Brand portfolio.
This is a list of variants of Coca-Cola introduced around the world. In addition to the caffeine-free version of the original, additional fruit flavors have been included over the years. Not included here are versions of Diet Coke and Coca-Cola Zero; variant versions of those no-calorie colas can be found at their respective articles.
Logo design.
The Coca-Cola logo was created by John Pemberton's bookkeeper, Frank Mason Robinson, in 1885. Robinson came up with the name and chose the logo's distinctive cursive script. The typeface used, known as Spencerian script, was developed in the mid-19th century and was the dominant form of formal handwriting in the United States during that period.
Robinson also played a significant role in early Coca-Cola advertising. His promotional suggestions to Pemberton included giving away thousands of free drink coupons and plastering the city of Atlanta with publicity banners and streetcar signs.
Contour bottle design.
The Coca-Cola bottle, called the "contour bottle" within the company, was created by bottle designer Earl R. Dean. In 1915, the Coca-Cola Company launched a competition among its bottle suppliers to create a new bottle for their beverage that would distinguish it from other beverage bottles, "a bottle which a person could recognize even if they felt it in the dark, and so shaped that, even if broken, a person could tell at a glance what it was."
Chapman J. Root, president of the Root Glass Company of Terre Haute, Indiana, turned the project over to members of his supervisory staff, including company auditor T. Clyde Edwards, plant superintendent Alexander Samuelsson, and Earl R. Dean, bottle designer and supervisor of the bottle molding room. Root and his subordinates decided to base the bottle's design on one of the soda's two ingredients, the coca leaf or the kola nut, but were unaware of what either ingredient looked like. Dean and Edwards went to the Emeline Fairbanks Memorial Library and were unable to find any information about coca or kola. Instead, Dean was inspired by a picture of the gourd-shaped cocoa pod in the Encyclopædia Britannica. Dean made a rough sketch of the pod and returned to the plant to show Root. He explained to Root how he could transform the shape of the pod into a bottle. Root gave Dean his approval.
Faced with the upcoming scheduled maintenance of the mold-making machinery, over the next 24 hours Dean sketched out a concept drawing which was approved by Root the next morning. Dean then proceeded to create a bottle mold and produced a small number of bottles before the glass-molding machinery was turned off.
Chapman Root approved the prototype bottle and a design patent was issued on the bottle in November 1915. The prototype never made it to production since its middle diameter was larger than its base, making it unstable on conveyor belts. Dean resolved this issue by decreasing the bottle's middle diameter. During the 1916 bottler's convention, Dean's contour bottle was chosen over other entries and was on the market the same year. By 1920, the contour bottle became the standard for the Coca-Cola Company. Today, the contour Coca-Cola bottle is one of the most recognized packages on the planet..."even in the dark!".
As a reward for his efforts, Dean was offered a choice between a $500 bonus or a lifetime job at the Root Glass Company. He chose the lifetime job and kept it until the Owens-Illinois Glass Company bought out the Root Glass Company in the mid-1930s. Dean went on to work in other Midwestern glass factories.
One alternative depiction has Raymond Loewy as the inventor of the unique design, but, while Loewy did serve as a designer of Coke cans and bottles in later years, he was in the French Army the year the bottle was invented and did not emigrate to the United States until 1919. Others have attributed inspiration for the design not to the cocoa pod, but to a Victorian hooped dress.
In 1944, Associate Justice Roger J. Traynor of the Supreme Court of California took advantage of a case involving a waitress injured by an exploding Coca-Cola bottle to articulate the doctrine of strict liability for defective products. Traynor's concurring opinion in "Escola v. Coca-Cola Bottling Co." is widely recognized as a landmark case in U.S. law today.
In 2007, the company's logo on cans and bottles changed. The cans and bottles retained the red color and familiar typeface, but the design was simplified, leaving only the logo and a plain white swirl (the "dynamic ribbon").
Designer bottles.
Karl Lagerfeld is the latest designer to have created a collection of aluminum bottles for Coca-Cola. Lagerfeld is not the first fashion designer to create a special version of the famous Coca-Cola Contour bottle. A number of other limited edition bottles by fashion designers for Coca Cola Light soda have been created in the last few years.
In 2009, in Italy, Coca-Cola Light had a Tribute to Fashion to celebrate 100 years of the recognizable contour bottle. Well known Italian designers Alberta Ferretti, Blumarine, Etro, Fendi, Marni, Missoni, Moschino, and Versace each designed limited edition bottles.
Competitors.
Pepsi, the flagship product of PepsiCo, The Coca-Cola Company's main rival in the soft drink industry, is usually second to Coke in sales, and outsells Coca-Cola in some markets. RC Cola, now owned by the Dr Pepper Snapple Group, the third largest soft drink manufacturer, is also widely available.
Around the world, many local brands compete with Coke. In South and Central America Kola Real, known as Big Cola in Mexico, is a growing competitor to Coca-Cola. On the French island of Corsica, Corsica Cola, made by brewers of the local Pietra beer, is a growing competitor to Coca-Cola. In the French region of Brittany, Breizh Cola is available. In Peru, Inca Kola outsells Coca-Cola, which led The Coca-Cola Company to purchase the brand in 1999. In Sweden, Julmust outsells Coca-Cola during the Christmas season. In Scotland, the locally produced Irn-Bru was more popular than Coca-Cola until 2005, when Coca-Cola and Diet Coke began to outpace its sales.
In India, Coca-Cola ranked third behind the leader, Pepsi-Cola, and local drink Thums Up. The Coca-Cola Company purchased Thums Up in 1993. As of 2004, Coca-Cola held a 60.9% market-share in India. Tropicola, a domestic drink, is served in Cuba instead of Coca-Cola, due to a United States embargo. French brand Mecca Cola and British brand Qibla Cola are competitors to Coca-Cola in the Middle East.
In Turkey, Cola Turka, in Iran and the Middle East, Zam Zam Cola and Parsi Cola, in some parts of China, China Cola, in Slovenia, Cockta and the inexpensive Mercator Cola, sold only in the country's biggest supermarket chain, Mercator, are some of the brand's competitors. Classiko Cola, made by Tiko Group, the largest manufacturing company in Madagascar, is a serious competitor to Coca-Cola in many regions. Laranjada is the top-selling soft drink on Madeira.
Advertising.
Coca-Cola's advertising has significantly affected American culture, and it is frequently credited with inventing the modern image of Santa Claus as an old man in a red-and-white suit. Although the company did start using the red-and-white Santa image in the 1930s, with its winter advertising campaigns illustrated by Haddon Sundblom, the motif was already common. Coca-Cola was not even the first soft drink company to use the modern image of Santa Claus in its advertising: White Rock Beverages used Santa in advertisements for its ginger ale in 1923, after first using him to sell mineral water in 1915. Before Santa Claus, Coca-Cola relied on images of smartly dressed young women to sell its beverages. Coca-Cola's first such advertisement appeared in 1895, featuring the young Bostonian actress Hilda Clark as its spokeswoman.
1941 saw the first use of the nickname "Coke" as an official trademark for the product, with a series of advertisements informing consumers that "Coke means Coca-Cola". In 1971 a song from a Coca-Cola commercial called "I'd Like to Teach the World to Sing", produced by Billy Davis, became a hit single.
Coke's advertising is pervasive, as one of Woodruff's stated goals was to ensure that everyone on Earth drank Coca-Cola as their preferred beverage. This is especially true in southern areas of the United States, such as Atlanta, where Coke was born.
Some Coca-Cola television commercials between 1960 through 1986 were written and produced by former Atlanta radio veteran Don Naylor (WGST 1936–1950, WAGA 1951–1959) during his career as a producer for the McCann Erickson advertising agency. Many of these early television commercials for Coca-Cola featured movie stars, sports heroes and popular singers.
During the 1980s, Pepsi-Cola ran a series of television advertisements showing people participating in taste tests demonstrating that, according to the commercials, "fifty percent of the participants who said they preferred Coke "actually" chose the Pepsi." Statisticians pointed out the problematic nature of a 50/50 result: most likely, the taste tests showed that in blind tests, most people cannot tell the difference between Pepsi and Coke. Coca-Cola ran ads to combat Pepsi's ads in an incident sometimes referred to as the "cola wars"; one of Coke's ads compared the so-called Pepsi challenge to two chimpanzees deciding which tennis ball was furrier. Thereafter, Coca-Cola regained its leadership in the market.
Selena was a spokesperson for Coca-Cola from 1989 till the time of her death. She filmed three commercials for the company. During 1994, to commemorate her five years with the company, Coca-Cola issued special Selena coke bottles.
The Coca-Cola Company purchased Columbia Pictures in 1982, and began inserting Coke-product images into many of its films. After a few early successes during Coca-Cola's ownership, Columbia began to under-perform, and the studio was sold to Sony in 1989.
Coca-Cola has gone through a number of different advertising slogans in its long history, including "The pause that refreshes," "I'd like to buy the world a Coke," and "Coke is it" (see Coca-Cola slogans).
In 2006, Coca-Cola introduced My Coke Rewards, a customer loyalty campaign where consumers earn points by entering codes from specially marked packages of Coca-Cola products into a website. These points can be redeemed for various prizes or sweepstakes entries.
In Australia in 2011, Coca-Cola began the "share a Coke" campaign, where the Coca-Cola logo was replaced on the bottles and replaced with first names. Coca-Cola used the 150 most popular names in Australia to print on the bottles. The campaign was paired with a website page, Facebook page and an online "share a virtual Coke". The same campaign was introduced to Coca-Cola, Diet Coke & Coke Zero bottles and cans in the UK in 2013.
Coca-Cola has also advertised its product to be consumed as a breakfast beverage, instead of coffee or tea for the morning caffeine.
Holiday campaigns.
The "Holidays are coming!" advertisement features a train of red delivery trucks, emblazoned with the Coca-Cola name and decorated with Christmas lights, driving through a snowy landscape and causing everything that they pass to light up and people to watch as they pass through.
The advertisement fell into disuse in 2001, as the Coca-Cola company restructured its advertising campaigns so that advertising around the world was produced locally in each country, rather than centrally in the company's headquarters in Atlanta, Georgia. In 2007, the company brought back the campaign after, according to the company, many consumers telephoned its information center saying that they considered it to mark the beginning of Christmas. The advertisement was created by U.S. advertising agency Doner, and has been part of the company's global advertising campaign for many years.
Keith Law, a producer and writer of commercials for Belfast CityBeat, was not convinced by Coca-Cola's reintroduction of the advertisement in 2007, saying that "I don't think there's anything Christmassy about HGVs and the commercial is too generic."
In 2001, singer Melanie Thornton recorded the campaign's advertising jingle as a single, "Wonderful Dream (Holidays are Coming)", which entered the pop-music charts in Germany at no. 9. In 2005, Coca-Cola expanded the advertising campaign to radio, employing several variations of the jingle.
In 2011, Coca-Cola launched a campaign for the Indian holiday Diwali. The campaign included commercials, a song and an integration with Shah Rukh Khan’s film Ra.One.
Sports sponsorship.
Coca-Cola was the first commercial sponsor of the Olympic games, at the 1928 games in Amsterdam, and has been an Olympics sponsor ever since. This corporate sponsorship included the 1996 Summer Olympics hosted in Atlanta, which allowed Coca-Cola to spotlight its hometown. Most recently, Coca-Cola has released localized commercials for the 2010 Winter Olympics in Vancouver; one Canadian commercial referred to Canada's hockey heritage and was modified after Canada won the gold medal game on February 28, 2010 by changing the ending line of the commercial to say "Now they know whose game they're playing".
Since 1978, Coca-Cola has sponsored the FIFA World Cup, and other competitions organised by FIFA. One FIFA tournament trophy, the FIFA World Youth Championship from Tunisia in 1977 to Malaysia in 1997, was called "FIFA — Coca Cola Cup". In addition, Coca-Cola sponsors the annual Coca-Cola 600 and Coke Zero 400 for the NASCAR Sprint Cup Series at Charlotte Motor Speedway in Concord, North Carolina and Daytona International Speedway in Daytona, Florida.
Coca-Cola has a long history of sports marketing relationships, which over the years have included Major League Baseball, the National Football League, the National Basketball Association, and the National Hockey League, as well as with many teams within those leagues. Coca-Cola has had a longtime relationship with the NFL's Pittsburgh Steelers, due in part to the now-famous 1979 television commercial featuring "Mean Joe" Greene, leading to the two opening the Coca-Cola Great Hall at Heinz Field in 2001 and a more recent Coca-Cola Zero commercial featuring Troy Polamalu.
Coca-Cola is the official soft drink of many collegiate football teams throughout the nation, partly due to Coca-Cola providing those schools with upgraded athletic facilities in exchange for Coca-Cola's sponsorship. This is especially prevalent at the high school level, which is more dependent on such contracts due to tighter budgets.
Coca-Cola was one of the official sponsors of the 1996 Cricket World Cup held on the Indian subcontinent. Coca Cola is also one of the associate sponsor of Delhi Daredevils in Indian Premier League.
In England, Coca-Cola was the main sponsor of The Football League between 2004 and 2010, a name given to the three professional divisions below the Premier League in football (soccer). In 2005, Coca-Cola launched a competition for the 72 clubs of the football league — it was called "Win a Player". This allowed fans to place one vote per day for their favorite club, with one entry being chosen at random earning £250,000 for the club; this was repeated in 2006. The "Win A Player" competition was very controversial, as at the end of the 2 competitions, Leeds United A.F.C. had the most votes by more than double, yet they did not win any money to spend on a new player for the club. In 2007, the competition changed to "Buy a Player". This competition allowed fans to buy a bottle of Coca-Cola or Coca-Cola Zero and submit the code on the wrapper on the Coca-Cola website. This code could then earn anything from 50p to £100,000 for a club of their choice. This competition was favored over the old "Win a Player" competition, as it allowed all clubs to win some money. Between 1992 and 1998, Coca-Cola was the title sponsor of the Football League Cup (Coca-Cola Cup), the secondary cup tournament of England.
Introduced March 1, 2010, in Canada, to celebrate the 2010 Winter Olympics, Coca Cola sold gold colored cans in packs of 12 355 mL each, in select stores.
In 2012, Coca-Cola (Philippines) hosted/sponsored the Coca-Cola PBA Youngstars in the Philippines.
In mass media.
Coca-Cola has been prominently featured in countless films and television programs. Since its creation, it remains as one of the most important elements of the popular culture. It was a major plot element in films such as "One, Two, Three", "The Coca-Cola Kid", and "The Gods Must Be Crazy" among many others. It provides a setting for comical corporate shenanigans in the novel "Syrup" by Maxx Barry. And in music, in The Beatles' song, "Come Together", the lyrics said, "He shoot Coca-Cola, he say...". The Beach Boys also referenced Coca-Cola in their 1964 song "All Summer Long" (i.e. 'Member when you spilled Coke all over your blouse?)
Also, the best selling artist of all time and worldwide cultural icon, Elvis Presley, promoted Coca-Cola during his last tour of 1977. The Coca-Cola Company used Elvis' image to promote the product. For example, the company used a song performed by Presley, A Little Less Conversation, in a Japanese Coca-Cola commercial.
Other artists that promoted Coca-Cola include The Beatles, David Bowie, George Michael, Elton John and Whitney Houston, who appeared in the Diet Coca-Cola commercial, among many others.
Not all musical references to Coca-Cola went well. A line in "Lola" by The Kinks was originally recorded as "You drink champagne and it tastes just like Coca-Cola." When the British Broadcasting Corporation refused to play the song because of the commercial reference, lead singer Ray Davies was forced to fly from New York to London and re-record the lyric as "it tastes just like cherry cola" to get airplay for the song.
Political cartoonist Michel Kichka satirized a Coca-Cola billboard in his 1982 poster "And I Love New York." On the billboard, the lettering and script above the Coca-Cola wave read "Enjoy Cocaine."
Criticism.
Coca-Cola has been criticized for alleged adverse health effects, its aggressive marketing to children, exploitative labor practices, high levels of pesticides in its products, building plants in Nazi Germany which employed slave labor, environmental destruction, monopolistic business practices, and hiring paramilitary units to murder trade union leaders. In October 2009, in an effort to improve their image, Coca-Cola partnered with the American Academy of Family Physicians, providing a $500,000 grant to help promote healthy-lifestyle education; the partnership spawned sharp criticism of both Coca-Cola and the AAFP by physicians and nutritionists.
Bolivia has been reported to consider banning Coca-Cola prior to January 2013.
Health effects.
Studies indicate "soda and sweetened drinks are the main source of calories in American diet", so most nutritionists advise that Coca-Cola and other soft drinks can be harmful if consumed excessively, particularly to young children whose soft drink consumption competes with, rather than complements, a balanced diet. Studies have shown that regular soft drink users have a lower intake of calcium, magnesium, ascorbic acid, riboflavin, and vitamin A. The drink has also aroused criticism for its use of caffeine, which can cause physical dependence (caffeine addiction). A link has been shown between long-term regular cola intake and osteoporosis in older women (but not men). This was thought to be due to the presence of phosphoric acid, and the risk was found to be same for caffeinated and noncaffeinated colas, as well as the same for diet and sugared colas.
A common criticism of Coke based on its allegedly toxic acidity levels has been found to be baseless by researchers; lawsuits based on these notions have been dismissed by several American courts for this reason. Although numerous court cases have been filed against The Coca-Cola Company since the 1920s, alleging that the acidity of the drink is dangerous, no evidence corroborating this claim has been found. Under normal conditions, scientific evidence indicates Coca-Cola's acidity causes no immediate harm.
Since 1980 in the U.S., Coke has been made with high-fructose corn syrup (HFCS) as an ingredient. Originally it was used in combination with more expensive cane-sugar, but by late 1984 the formulation was sweetened entirely with HFCS. Some nutritionists caution against consumption of HFCS because it may aggravate obesity and type-2 diabetes more than cane sugar.
In India, there is a controversy whether there are pesticides and other harmful chemicals in bottled products, including Coca-Cola. In 2003 the Centre for Science and Environment (CSE), a non-governmental organization in New Delhi, said aerated waters produced by soft drinks manufacturers in India, including multinational giants PepsiCo and Coca-Cola, contained toxins including lindane, DDT, malathion and chlorpyrifos — pesticides that can contribute to cancer and a breakdown of the immune system. CSE found that the Indian-produced Pepsi's soft drink products had 36 times the level of pesticide residues permitted under European Union regulations; Coca-Cola's soft drink was found to have 30 times the permitted amount. CSE said it had tested the same products sold in the U.S. and found no such residues.
After the pesticide allegations were made in 2003, Coca-Cola sales in India declined by 15 percent. In 2004 an Indian parliamentary committee backed up CSE's findings and a government-appointed committee was tasked with developing the world's first pesticide standards for soft drinks. The Coca-Cola Company has responded that its plants filter water to remove potential contaminants and that its products are tested for pesticides and must meet minimum health standards before they are distributed. In the Indian state of Kerala sale and production of Coca-Cola, along with other soft drinks, was initially banned after the allegations, until the High Court in Kerala overturned ruled that only the federal government can ban food products. Coca-Cola has also been accused of excessive water usage in India.
The 2008 Ig Nobel Prize (a parody of the Nobel Prizes) in Chemistry was awarded to Sheree Umpierre, Joseph Hill, and Deborah Anderson, for discovering that Coca-Cola is an effective spermicide, and to C.Y. Hong, C.C. Shieh, P. Wu, and B.N. Chiang for proving it is not.
Allergy.
Coca Cola has been implicated in certain allergic reactions. In a case study in Switzerland, a woman who was allergic to Balsam of Peru was allergic to her boyfriend's semen following intercourse, after he drank large amounts of Coca Cola.
Use as political and corporate symbol.
Coca-Cola has a high degree of identification with the United States, being considered by some an "American Brand" or as an item representing America. During World War II, this gave rise to brief production of the White Coke as a neutral brand.
The identification with the spread of American culture has led to the pun "Coca-Colanization".
The drink is also often a metonym for the Coca-Cola Company.
There are some consumer boycotts of Coca-Cola in Arab countries due to Coke's early investment in Israel during the Arab League boycott of Israel (its competitor Pepsi stayed out of Israel).
Mecca Cola and Pepsi have been successful alternatives in the Middle East.
A Coca-Cola fountain dispenser (officially a Fluids Generic Bioprocessing Apparatus-2 or FGBA-2) was developed for use on the Space Shuttle as a test bed to determine if carbonated beverages can be produced from separately stored carbon dioxide, water and flavored syrups and determine if the resulting fluids can be made available for consumption without bubble nucleation and resulting foam formation.
The unit flew in 1996 aboard STS-77 and held 1.65 liters each of Coca-Cola and Diet Coke.
Social causes.
In 2012, Coca-Cola is listed as a partner of the (RED) campaign, together with other brands such as Nike, Girl, American Express and Converse. The campaign's mission is to prevent the transmission of the HIV virus from mother to child by 2015 (the campaign's byline is "Fighting For An AIDS Free Generation").

</doc>
<doc id="6693" url="http://en.wikipedia.org/wiki?curid=6693" title="Cofinality">
Cofinality

In mathematics, especially in order theory, the cofinality cf("A") of a partially ordered set "A" is the least of the cardinalities of the cofinal subsets of "A".
This definition of cofinality relies on the axiom of choice, as it uses the fact that every non-empty set of cardinal numbers has a least member. The cofinality of a partially ordered set "A" can alternatively be defined as the least ordinal "x" such that there is a function from "x" to "A" with cofinal image. This second definition makes sense without the axiom of choice. If the axiom of choice is assumed, as will be the case in the rest of this article, then the two definitions are equivalent.
Cofinality can be similarly defined for a directed set and is used to generalize the notion of a subsequence in a net.
Properties.
If "A" admits a totally ordered cofinal subset, then we can find a subset "B" which is well-ordered and cofinal in "A". Any subset of "B" is also well-ordered. If two cofinal subsets of "B" have minimal cardinality (i.e. their cardinality is the cofinality of "B"), then they are order isomorphic to each other.
Cofinality of ordinals and other well-ordered sets.
The cofinality of an ordinal α is the smallest ordinal δ which is the order type of a cofinal subset of α. The cofinality of a set of ordinals or any other well-ordered set is the cofinality of the order type of that set.
Thus for a limit ordinal, there exists a δ-indexed strictly increasing sequence with limit α. For example, the cofinality of ω² is ω, because the sequence ω·"m" (where "m" ranges over the natural numbers) tends to ω²; but, more generally, any countable limit ordinal has cofinality ω. An uncountable limit ordinal may have either cofinality ω as does ωω or an uncountable cofinality.
The cofinality of 0 is 0. The cofinality of any successor ordinal is 1. The cofinality of any limit ordinal is at least ω.
Regular and singular ordinals.
A regular ordinal is an ordinal which is equal to its cofinality. A singular ordinal is any ordinal which is not regular.
Every regular ordinal is the initial ordinal of a cardinal. Any limit of regular ordinals is a limit of initial ordinals and thus is also initial but need not be regular. Assuming the Axiom of choice, formula_1 is regular for each α. In this case, the ordinals 0, 1, formula_2, formula_3, and formula_4 are regular, whereas 2, 3, formula_5, and ωω·2 are initial ordinals which are not regular.
The cofinality of any ordinal "α" is a regular ordinal, i.e. the cofinality of the cofinality of "α" is the same as the cofinality of "α". So the cofinality operation is idempotent.
Cofinality of cardinals.
If κ is an infinite cardinal number, then cf(κ) is the least cardinal such that there is an unbounded function from it to κ; and cf(κ) = the cardinality of the smallest collection of sets of strictly smaller cardinals such that their sum is κ; more precisely
That the set above is nonempty comes from the fact that
i.e. the disjoint union of κ singleton sets. This implies immediately that cf(κ) ≤ κ.
The cofinality of any totally ordered set is regular, so one has cf(κ) = cf(cf(κ)).
Using König's theorem, one can prove κ < κcf(κ) and κ < cf(2κ) for any infinite cardinal κ.
The last inequality implies that the cofinality of the cardinality of the continuum must be uncountable. On the other hand,
the ordinal number ω being the first infinite ordinal, so that the cofinality of formula_9 is card(ω) = formula_10. (In particular, formula_9 is singular.) Therefore,
Generalizing this argument, one can prove that for a limit ordinal δ

</doc>
<doc id="6695" url="http://en.wikipedia.org/wiki?curid=6695" title="Citadel">
Citadel

A citadel is a fortress protecting a town, sometimes incorporating a castle. The term derives from the same Latin root as the word "city", "civis", meaning citizen.
In a fortification with bastions, the citadel is the strongest part of the system, sometimes well inside the outer walls and bastions, but often forming part of the outer wall for the sake of economy. It is positioned to be the last line of defence should the enemy breach the other components of the fortification system. A citadel is also a term of the third part of a medieval castle, with higher walls than the rest. It was to be the last line of defence before the keep itself.
In various countries, the citadels gained a specific name such as "Kremlin" in Russia or "Alcázar" in the Iberian Peninsula. In European cities, the term "Citadel" and "City Castle" are often used interchangeably. The term "tower" is also used in some cases such as the Tower of London and Jerusalem's Tower of David. However, the Haitian citadel, which is the largest citadel in the Western Hemisphere, is called Citadelle Laferrière or simply the 'Citadel' in English.
History.
8000 BC–600 AD.
In Ancient Greece, the Acropolis (literally: "peak of the city"), placed on a commanding eminence, was important in the life of the people, serving as a refuge and stronghold in peril and containing military and food supplies, the shrine of the god and a royal palace. The most well-known is the Acropolis of Athens, but nearly every Greek city-state had one - the Acrocorinth famed as a particularly strong fortress. In a much later period, when Greece was ruled by the Latin Empire, the same strongpoints were used by the new feudal rulers for much the same purpose.
167–160 BC.
Rebels who took power in the city but with the citadel still held by the former rulers could by no means regard their tenure of power as secure. One such incident played an important part in the history of the Maccabean Revolt against the Seleucid Empire. The Hellenistic garrison of Jerusalem and local supporters of the Seleucids held out for many years in the Acra citadel, making Maccabean rule in the rest of Jerusalem precarious. When finally gaining possession of the place, the Maccabeans pointedly destroyed and razed the Acra, though they constructed another citadel for their own use in a different part of Jerusalem.
3300–1300 BC.
Some of the oldest known structures which have served as citadels were built by the Indus Valley Civilization, where the citadel represented a centralised authority. The main citadel in Indus Valley was almost 12 meters tall. The purpose of these structures, however, remains debated. Though the structures found in the ruins of Mohenjo-daro were walled, it is far from clear that these structures were defensive against enemy attacks. Rather, they may have been built to divert flood waters.
500–1500 AD.
At various periods, and particularly during the Middle Ages, the citadel - having its own fortifications, independent of the city walls - was the last defence of a besieged army, often held after the town had been conquered. A city where the citadel held out against an invading army was not considered conquered. For example, in the 1543 Siege of Nice the Ottoman forces led by Barbarossa conquered and pillaged the town itself and took many captives - but the city castle held out, due to which the townspeople were accounted the victors.
1600–2000 AD.
As late as the 19th century, a similar situation developed at Antwerp, where a Dutch garrison under General David Hendrik Chassé held out in the city's citadel between 1830 and 1832, while the city itself had already become part of the independent Belgium.
In time of war the citadel in many cases afforded retreat to the people living in the areas around the town. However, Citadels were often used also to protect a garrison or political power from the inhabitants of the town where it was located, being designed to ensure loyalty from the town that they defended.
For example Barcelona had a great citadel built in 1714 to intimidate the Catalans against repeating their mid-17th- and early-18th-century rebellions against the Spanish central government. In the 19th century, when the political climate had liberalised enough to permit it, the people of Barcelona had the citadel torn down, and replaced it with the city's main central park, the Parc de la Ciutadella. A similar example is the Citadella in Budapest, Hungary.
The attack on the Bastille in the French Revolution - though afterwards remembered mainly for the release of the handful of prisoners incarcerated there - was to considerable degree motivated by the structure being a Royal citadel in the midst of revolutionary Paris.
Similarly, after Garibaldi's overthrow of Bourbon rule in Palermo, during the 1860 Unification of Italy, Palermo's Castellamare Citadel - symbol of the hated and oppressive former rule - was ceremoniously demolished.
The Siege of the Alcázar in the Spanish Civil War, in which the Nationalists held out against a much larger Republican force for two months until relieved, shows that in some cases a citadel can be effective even in modern warfare; a similar case is the Battle of Huế during the Vietnam war, where a North Vietnamese Army division held the citadel of Huế for 26 days against roughly their own numbers of much better-equipped US and South Vietnamese troops.
2000–Present AD.
The Citadelle of Québec (construction started 1673, completed 1820) still survives as the largest citadel still in official military operation in North America. It is home to the Royal 22nd Regiment of Canada; and forms part of the fortified walls of Vieux-Québec dating back to 1608.
Modern usage.
Citadels since the mid 20th century, are commonly military commanded and control centres built to resist attack commonly aerial or nuclear bombardment. The Military citadels under London such as the massive underground complex beneath the Ministry of Defence called Pindar is one such example, as is the Cheyenne Mountain nuclear bunker in the US.
Naval term.
On armored warships, the heavily armored section of the ship that protects the ammunition and machinery spaces is called the citadel.
The safe room on a ship is also called a citadel.

</doc>
<doc id="6696" url="http://en.wikipedia.org/wiki?curid=6696" title="Mail (armour)">
Mail (armour)

Mail (chainmail, maille) is a type of armour consisting of small metal rings linked together in a pattern to form a mesh.
History.
The earliest example of mail was found in a Celtic chieftain's burial located in Ciumeşti, Romania. Its invention is commonly credited to the Celts, but there are examples of Etruscan pattern mail dating from at least the 4th century BC. Mail may have been inspired by the much earlier scale armour. Mail spread to North Africa, the Middle East, Central Asia, India, Tibet, Korea and Japan.
Mail continues to be used in the 21st century as a component of stab-resistant body armour, cut-resistant gloves for butchers and woodworkers, shark-resistant wetsuits for defense against shark bites, and a number of other applications.
Etymology.
The origins of the word “mail” are not fully known. One theory is that it originally derives from the Latin word "macula", meaning "spot" or “opacity” (as in macula of retina). Another theory relates the word to the old French “maillier”, meaning “to hammer” (a cognate of the modern English word “malleable”).
The first attestations of the word “mail” are in Old French and Anglo-Norman: “maille” “maile”, or “male” or other variants, which became “mailye” “maille” “maile”, “male”, or “meile” in Middle English.
The modern usage of terms for mail armour is highly contested in popular and, to a lesser degree, academic culture. Medieval sources referred to armour of this type simply as “mail”, however “chain-mail” has become a commonly-used, if incorrect neologism first attested in Sir Walter Scott’s 1822 novel "The Fortunes of Nigel". Since then the word “mail” has been commonly, if incorrectly, applied to other types of armour, such as in “plate-mail” (first attested in 1835). The more correct term is “plate armour”.
Civilizations that used mail invented specific terms for each garment made from it. The standard terms for European mail armour derive from French: leggings are called chausses, a hood is a coif, and mittens, mitons. A mail collar hanging from a helmet is a camail or aventail. A shirt made from mail is a hauberk if knee-length and a haubergeon if mid-thigh length. A layer (or layers) of mail sandwiched between layers of fabric is called a jazerant. 
A waist-length coat in medieval Europe was called a byrnie, although the exact construction of a byrnie is unclear, including whether it was constructed of mail or other armour-types. Noting that the byrnie was the "most highly valued piece of armour" to the Carolingian soldier, Bennet, Bradbury, DeVries, Dickie, and Jestice indicate that:
There is some dispute among historians as to what exactly constituted the Carolingian byrnie. Relying... only on artistic and some literary sources because of the lack of archaeological examples, some believe that it was a heavy leather jacket with metal scales sewn onto it. It was also quite long, reaching below the hips and covering most of the arms. Other historians claim instead that the Carolingian byrnie was nothing more than a coat of mail, but longer and perhaps heavier than traditional early medieval mail. Without more certain evidence, this dispute will continue.
Mail armour in Europe.
The use of mail as battlefield armour was common during the Iron Age and the Middle Ages, becoming less common over the course of the 16th and 17th centuries. It is believed that the Roman Republic first came into contact with mail fighting the Gauls in Cisalpine Gaul, now Northern Italy, but even earlier in time, a different pattern of mail was already in use among the Etruscans. The Roman army adopted the technology for their troops in the form of the lorica hamata which was used as a primary form of armour through the Imperial period.
After the fall of the Western Empire much of the infrastructure needed to create plate armour diminished. Eventually the word "mail" came to be synonymous with armour. It was typically an extremely prized commodity as it was expensive and time consuming to produce and could mean the difference between life and death in a battle. Mail from dead combatants was frequently looted and was used by the new owner or sold for a lucrative price. As time went on and infrastructure improved it came to be used by more soldiers. Eventually with the rise of the lanced cavalry charge, impact warfare, and high-powered crossbows, mail came to be used as a secondary armour to plate for the mounted nobility.
By the 14th century, plate armour was commonly used to supplement mail. Eventually mail was supplanted by plate for the most part as it provided greater protection against windlass crossbows, bludgeoning weapons, and lance charges. However, mail was still widely used by many soldiers as well as brigandines and padded jacks. These three types of armour made up the bulk of the equipment used by soldiers with mail being the most expensive. It was sometimes more expensive than plate armour. Mail typically persisted longer in less technologically advanced areas such as Eastern Europe but was in use everywhere into the 16th century.
During the late 19th and early 20th century mail was used as a material for bulletproof vests, most notably by the Wilkinson Sword Company. Results were unsatisfactory; Wilkinson mail worn by the Khedive of Egypt's regiment of "Iron Men" was manufactured from split rings which proved to be too brittle, and the rings would fragment when struck by bullets and aggravate the damage. The riveted mail armour worn by the opposing Sudanese Madhists did not have the same problem but also proved to be relatively useless against the firearms of British forces at the battle of Omdurman. During World War I Wilkinson Sword transitioned from mail to a lamellar design which was the precursor to the flak jacket.
Also during World War I a mail fringe, designed by Captain Cruise of the British Infantry, was added to helmets to protect the face. This proved unpopular with soldiers, in spite of being proven to defend against a three-ounce (100 g) shrapnel round fired at a distance of one hundred yards (90 m).
Mail armour in Asia.
Mail Armour was introduced to the Middle East and Asia through the Romans and was adopted by the Sassanid Persians starting in the 3rd century AD, where it was supplemental to the scale and lamellar armours already used. Mail was commonly also used as horse armour for cataphracts and heavy cavalry as well as armour for the soldiers themselves. Asian mail was typically lighter than the European variety and sometimes had prayer symbols stamped on the rings as a sign of their craftsmanship as well as for divine protection. Indeed, mail armour is mentioned in the Koran as being a gift revealed by Allah to David:
21:80 "It was We Who taught him the making of coats of mail for your benefit, to guard you from each other's violence: will ye then be grateful?" (Yusuf Ali's translation).
From the Middle East mail was quickly adopted in Central Asia by the Sogdians and by India in the South. It was not commonly used in Mongol armies due to its weight and the difficulty of its maintenance, but it eventually became the armour of choice in India. Indian mail was often used with plate protection. Plated mail was in common use in India until the Battle of Plassey and the subsequent British conquest of the sub-continent.
The Ottoman Empire used plated mail widely and it was used in their armies until the 18th century by heavy cavalry and elite units such as the Janissaries. They spread its use into North Africa where it was adopted by Mamluk Egyptians and the Sudanese who produced it until the early 20th century.
Mail was introduced to China when its allies in Central Asia paid tribute to the Tang Emperor in 718 by giving him a coat of "link armour" assumed to be mail. China first encountered the armour in 384 when its allies in the nation of Kuchi arrived wearing "armour similar to chains". Once in China mail was imported but was not produced widely. Due to its flexibility and comfort, it was typically the armour of high-ranking guards and those who could afford the import rather than the armour of the rank and file, who used the easier to produce and maintain brigandine and lamellar types. However, it was one of the only military products that China imported from foreigners. Mail spread to Korea slightly later where it was imported as the armour of imperial guards and generals.
Mail armour (kusari) in Japan.
The Japanese had more varieties of mail than all the rest of the world put together. In Japan mail is called ' which means chain. When the word "kusari" is used in conjunction with an armoured item it usually means that the "kusari" makes up the majority of the armour defence. An example of this would be "kusari gusoku" which means chain armour. "Kusari" ', ', ', ', ', shoulder, ', and other armoured clothing were produced, even ' socks.
"" was used in samurai armour at least from the time of the Mongol invasion (1270s) but particularly from the Nambokucho period (1336–1392). The Japanese used many different weave methods including: a square 4-in-1 pattern ("so gusari"), a hexagonal 6-in-1 pattern ("hana gusari") and a European 4-in-1 ("nanban gusari"). Kusari was typically made with rings that were much smaller than their European counterparts, and patches of kusari were used to link together plates and to drape over vulnerable areas such as the underarm.
Riveted kusari was known and used in Japan. On page 58 of the book "Japanese Arms & Armor: Introduction" by H. Russell Robinson, there is a picture of Japanese riveted kusari, and
this quote from the translated reference of Sakakibara Kozan's 1800 book, "The Manufacture of Armour and Helmets in Sixteenth Century Japan", shows that the Japanese not only knew of and used riveted kusari but that they manufactured it as well.
... karakuri-namban (riveted namban), with stout links each closed by a rivet. Its invention is credited to Fukushima Dembei Kunitaka, pupil, of Hojo Awa no Kami Ujifusa, but it is also said to be derived directly from foreign models. It is heavy because the links are tinned (biakuro-nagashi) and these are also sharp edged because they are punched out of iron plate
Butted and or split (twisted) links made up the majority of "kusari" links used by the Japanese. Links were either "butted" together meaning that the ends touched each other and were not riveted, or the "kusari" was constructed with links where the wire was turned or twisted two or more times, these split links are similar to the modern split ring commonly used on keychains. The rings were lacquered black to prevent rusting, and were always stitched onto a backing of cloth or leather. The kusari was sometimes concealed entirely between layers of cloth.
Kusari gusoku or chain armour was commonly used during the Edo period 1603 to 1868 as a stand alone defence. According to George Cameron Stone
Entire suits of mail "kusari gusoku" were worn on occasions, sometimes under the ordinary clothing
Ian Bottomley in his book "Arms and Armor of the Samurai: The History of Weaponry in Ancient Japan" shows a picture of a kusari armour and mentions "" (chain jackets) with detachable arms being worn by samurai police officials during the Edo period. The end of the samurai era in the 1860s, along with the 1876 ban on wearing swords in public, marked the end of any practical use for mail and other armour in Japan. Japan turned to a conscription army and uniforms replaced armour.
Effectiveness.
Mail armour provided an effective defence against slashing blows by an edged weapon and penetration by thrusting and piercing weapons; in fact, a study conducted at the Royal Armouries at Leeds concluded that "it is almost impossible to penetrate using any conventional medieval weapon" Generally speaking, mail's resistance to weapons is determined by four factors: linkage type (riveted, butted, or welded), material used (iron versus bronze or steel), weave density (a tighter weave needs a thinner weapon to surpass), and ring thickness (generally ranging from 18 to 14 gauge in most examples). Mail, if a warrior could afford it, provided a significant advantage to a warrior when combined with competent fighting techniques. When the mail was not riveted, a well placed thrust from a spear or thin sword could penetrate, and a pollaxe or halberd blow could break through the armour. In India, punching daggers known as katars were developed that could pierce the light butted mail used in the area. Some evidence indicates that during armoured combat, the intention was to actually get around the armour rather than through it—according to a study of skeletons found in Visby, Sweden, a majority of the skeletons showed wounds on less well protected legs.
The flexibility of mail meant that a blow would often injure the wearer, potentially causing serious bruising or fractures, and it was a poor defence against head trauma. Mail-clad warriors typically wore separate rigid helms over their mail coifs for head protection. Likewise, blunt weapons such as maces and warhammers could harm the wearer by their impact without penetrating the armour; usually a soft armour, such as gambeson, was worn under the hauberk. Medieval surgeons were very well capable of setting and caring for bone fractures resulting from blunt weapons. With the poor understanding of hygiene however, cuts that could get infected were much more of a problem. Thus mail armour proved to be sufficient protection in most situations.
Manufacture.
Several patterns of linking the rings together have been known since ancient times, with the most common being the 4-to-1 pattern (where each ring is linked with four others). In Europe, the 4-to-1 pattern was completely dominant. Mail was also common in East Asia, primarily Japan, with several more patterns being utilised and an entire nomenclature developing around them.
Historically, in Europe, from the pre-Roman period on, the rings composing a piece of mail would be riveted closed to reduce the chance of the rings splitting open when subjected to a thrusting attack or a hit by an arrow.
Up until the 14th century European mail was made of alternating rows of riveted rings and solid rings. After that point mail was almost all made from riveted rings only. Both were commonly made of wrought iron, but some later pieces were made of heat-treated steel. Wire for the riveted rings was formed by either of two methods. One was to hammer out wrought iron into plates and cut or slit the plates. These thin pieces were then pulled through a draw plate repeatedly until the desired diameter was achieved. Waterwheel powered drawing mills are pictured in several period manuscripts. Another method was to simply forge down an iron billet into a rod and then proceed to draw it out into wire. The solid links would have been made by punching from a sheet. Guild marks were often stamped on the rings to show their origin and craftsmanship. Forge welding was also used to create solid links, but there are few possible examples known, the only well documented example from Europe is that of the camail (mail neck-defence) of the 7th century Coppergate helmet. Outside of Europe this practice was more common such as "theta" links from India. Very few examples of historic butted mail have been found and it is generally accepted that butted mail was never in wide use historically except in Japan where mail ("kusari") was commonly made from "butted" links.
Modern uses.
Practical uses.
Mail is used as protective clothing for butchers against meat-packing equipment. Workers may wear up to of mail under their white coats. Butchers also commonly wear a single mail glove to protect themselves from self-inflicted injury while cutting meat.
Woodcarvers sometimes use similar mail gloves to protect their hands from cuts and punctures.
The British police use mail gloves for dealing with knife-armed aggressors.
Scuba divers use mail to protect them from sharkbite, as do animal control officers for protection against the animals they handle. Shark expert and underwater filmmaker Valerie Taylor was among the first to develop and test the mail suit in 1979 while diving with sharks.
Mail is widely used in industrial settings as shrapnel guards and splash guards in metal working operations.
Electrical applications for mail include RF leakage testing and being worn as a faraday cage suit by tesla coil enthusiasts and high voltage electrical workers.
Stab-proof vests.
Conventional textile based ballistic vests are designed to stop soft nosed bullets but offer little defense from knife attacks. Knife resistant armours are designed to defend against knife attacks, some of these use layers of metal plates, mail and metallic wires.
Historical re-enactment.
Many historical reenactment groups, especially those whose focus is Antiquity or the Middle Ages, commonly use mail both as practical armour and for costuming. Mail is especially popular amongst those groups which use steel weapons. Depending on their fitness, a fighter wearing hauberk and chausses can run, lie, stand up, jump, do somersaults (or even cartwheels), and even swim wearing full armour. A modern hauberk made from 1.5 mm diameter wire with 10 mm inner diameter rings weighs roughly and contains 15,000–45,000 rings. Mail can be used under everyday clothes and many reenactors wear a hauberk under their regular clothes to accustom themselves to it.
One of the real drawbacks of mail is the uneven weight distribution; the stress falls mainly on shoulders. Weight can be better distributed by wearing a belt over the mail, which provides another point of support.
Mail worn today for re-enactment and recreational use can be made in a variety of styles and materials. Most recreational mail today is made of butted links which are galvanized or stainless steel, this is historically inaccurate but is much less expensive to procure and maintain than historically accurate reproductions. Mail can also be made of titanium, aluminium, bronze, or copper. Riveted mail offers significantly better protection ability as well as historical accuracy than mail constructed with butted links, at the same time riveted mail can be more labour-intensive and expensive to manufacture. Some television shows have incorrectly portrayed butted mail as having been used historically in Europe or the Middle East (in reality, Japanese mail ("kusari") is one of the few historically correct examples of mail being constructed with such "butted links").
Decorative uses.
Mail remained in use as a decorative and possibly high-status symbol with military overtones long after its practical usefulness had passed. It was frequently used for the epaulettes of military uniforms. It is still used in this form by the British Territorial Army, and the Royal Canadian Armoured Corps of the Canadian Army.
Mail has applications in sculpture and jewellery, especially when made out of precious metals or colourful anodized metals. Mail artwork includes headdresses, Christmas ornaments, chess sets, and jewellery. For these non-traditional applications, hundreds of weaves or patterns have been invented. Public forums have been created where "mail" practitioners can show and discuss techniques and weaves and display their creations. M.A.I.L. (Maille Artisans International League) and the Ring Lord community forum are two of the most popular.
In film.
In some films, knitted string spray-painted with a metallic paint is used instead of actual mail in order to cut down on cost (an example being "Monty Python and the Holy Grail", which was filmed on a very small budget). Films more dedicated to costume accuracy often use ABS plastic rings, for the lower cost and weight. Such ABS mail coats were made for "The Lord of the Rings" film trilogy, in addition to many metal coats. The metal coats are used rarely because of their weight, except in close-up filming where the appearance of ABS rings is distinguishable. For the movie "Mad Max Beyond Thunderdome", Tina Turner is said to have been wearing an actual mail and she complained how heavy this was.

</doc>
<doc id="6697" url="http://en.wikipedia.org/wiki?curid=6697" title="Cerberus">
Cerberus

Cerberus (; "Kerberos" ) in Greek and Roman mythology, is a multi-headed (usually three-headed) dog, or "hellhound" with a serpent's tail, a mane of snakes, and a lion's claws. He guards the entrance of the underworld to prevent the dead from escaping and the living from entering. Cerberus is featured in many works of ancient Greek and Roman literature and in works of both ancient and modern art and architecture, although the depiction of Cerberus differs across various renditions. The most notable difference is the number of his heads: Most sources describe or depict three heads; others show Cerberus with two or even just one; a smaller number of sources show a variable number, sometimes as many as 50 or even 100.
In mythology.
Cerberus is the offspring of Echidna, a hybrid half-woman and half-serpent, and Typhon, a gigantic monster even the Greek gods feared. Its siblings are the Lernaean Hydra; Orthrus, a two-headed hellhound; and the Chimera, a three-headed monster. The common depiction of Cerberus in Greek mythology and art is as having three heads. In most works, the three heads each respectively see and represent the past, the present, and the future, while other sources suggest the heads represent birth, youth, and old age. Each of Cerberus' heads is said to have an appetite only for live meat and thus allow only the spirits of the dead to freely enter the underworld, but allow none to leave. Cerberus was always employed as Hades' loyal watchdog, and guarded the gates that granted access and exit to the underworld.
The Twelfth Labour of Heracles.
Capturing Cerberus, without using weapons, was the final labour assigned to Heracles (Hercules) by King Eurystheus, in recompense for the killing of his own children by Megara after he was driven insane by Hera, and therefore was the most dangerous and difficult.
After having been given the task, Heracles went to Eleusis to be initiated in the Eleusinian Mysteries so he could learn how to enter and exit the underworld alive, and in passing absolve himself for killing centaurs. He found the entrance to the underworld at Tanaerum, and Athena and Hermes helped him to traverse the entrance in each direction. He passed Charon with Hestia's assistance and his own heavy and fierce frowning.
Whilst in the underworld, Heracles met Theseus and Pirithous. The two companions had been imprisoned by Hades for attempting to kidnap Persephone. One tradition tells of snakes coiling around their legs then turning into stone; another tells that Hades feigned hospitality and prepared a feast inviting them to sit. They unknowingly sat in chairs of forgetfulness and were permanently ensnared. When Heracles had pulled Theseus first from his chair, some of his thigh stuck to it (this explains the supposedly lean thighs of Athenians), but the earth shook at the attempt to liberate Pirithous, whose desire to have the wife of a god for himself was so insulting, he was doomed to stay behind.
Heracles found Hades and asked permission to bring Cerberus to the surface, to which Hades agreed if Heracles could overpower the beast without using weapons. Heracles was able to overpower Cerberus and proceeded to sling the beast over his back, dragging it out of the underworld through a cavern entrance in the Peloponnese and bringing it to Eurystheus.
In literature.
Cerberus featured in many prominent works of Greek and Roman literature, most famously in Virgil's "Aeneid", Peisandros of Rhodes' epic poem the "Labours of Hercules", the story of Orpheus in Plato's "Symposium", and in Homer's "Iliad", which is the only known reference to one of Heracles' labours which first appeared in a literary source.
The depiction of Cerberus is relatively consistent between different works and authors, the common theme of the mane of serpents is kept across works, as is the serpent's tail, most literary works of the era describe Cerberus as having three heads with the only notable exception being Hesiod's "Theogony" in which he had 50 heads.
Most occurrences in ancient literature revolve around the basis of the threat of Cerberus being overcome to allow a living being access to the underworld; in the "Aeneid" Cerberus was lulled to sleep after being tricked into eating drugged honeycakes and Orpheus put the creature to sleep with his music. Capturing Cerberus alive was the twelfth and final labour of Heracles. In Dante Alighieri's "Inferno", Canto VI, the "great worm" Cerberus is found in the Third Circle of Hell, where he oversees and rends to pieces those who have succumbed to gluttony, one of the seven deadly sins.
In the constellation Cerberus introduced by Johannes Hevelius in 1687, Cerberus is sometimes substituted for the "branch from the tree of the golden apples" fetched by Atlas from the garden of the Hesperides. This branch is the literary source of the "golden bough" in the "Aeneid" by Virgil.
In "Paradise Lost" 11.65, Cerberean hounds are mentioned in Hell: "A cry of Hell Hounds never ceasing bark'd With wide Cerberean mouths full loud".
In Harry Potter and the sorcerer's stone, Cereberean guards the entrance into the secret passage to find the stone for eternal life.
In art.
Numerous references to Cerberus have appeared in ancient Greek and Roman art, found in archaeological ruins and often including in statues and architecture, inspired by the mythology of the creature. Cerberus' depiction in ancient art is not as definitive as in literature; the poets and linguists of ancient Greece and Rome mostly agreed on the physical appearance (with the notable exception in Hesiod's "Theogony" in which he had 50 heads). His depiction in classical art mostly shows the recurring motif of serpents, but the number of heads differs. A statue in the Galleria Borghese depicts Cerberus with three heads sitting by the side of Hades, while a bronze sculpture depicting Heracles' twelfth labour shows the demi-god leading a two-headed Cerberus from the underworld. The majority of vases depicting the twelfth task also show Cerberus as having two heads. Classical critics have identified one of the earliest works of Cerberus as "the most imaginative," that being a Laconian vase created around 560 BC in which Cerberus is shown with three-heads and with rows of serpents covering his body and heads.
Etymology.
The name "Cerberus" is a Latinised version of the Greek "Kerberos". The etymology of this name prior to Greek is disputed. It has been claimed to be related to the Sanskrit word सर्वरा "sarvarā", used as an epithet of one of the dogs of Yama, from a Proto-Indo-European word *"k̑érberos", meaning "spotted" 
The use of a dog is uncertain, although mythologists have speculated that the association was first made in the city of Trikarenos in Phliasia. Bruce Lincoln (1991), among others, critiques this etymology; and Ogden (2013) refers to it as "dismissed".
Lincoln notes a similarity between Cerberus and the Norse mythological dog Garmr, relating both names to a Proto-Indo-European root "*ger-" "to growl" (perhaps with the suffixes "-*m/*b" and "-*r"). However, as Ogden observes, this analysis actually requires "Cerberus" and "Garmr" to be derived from two "different" Indo-European roots (*"ger-" and *"gher-" respectively), and so does not actually establish a relationship between the two names.
Explanations.
There have been many attempts to explain the depiction of Cerberus. A 2nd century CE Greek known as Heraclitus the paradoxographer - not to be confused with the 5th century BCE Greek philosopher Heraclitus - claimed euhemeristically that Cerberus had two pups that were never away from their father, and that Cerberus was in fact a normal (though very large) dog, but that artists incorporating the two pups into their work made it appear as if his two children were in fact extra heads. Classical historians have dismissed Heraclitus the paradoxographer's explanation as "feeble". Mythologers have speculated that if Cerberus were given his name in Trikarenos it could be interpreted as "three karenos". Certain experts believe that the monster was inspired by the golden jackal.

</doc>
<doc id="6698" url="http://en.wikipedia.org/wiki?curid=6698" title="CamelCase">
CamelCase

CamelCase (camel case, camel caps or medial capitals) is the practice of writing compound words or phrases such that each next word or abbreviation begins with a capital letter. Camel case may start with a capital or, especially in programming languages, with a lowercase letter. Common examples are PowerPoint or iPhone. 
In Microsoft documentation, camel case always starts with a lower case letter (e.g. backColor), and it is contrasted with Pascal case which always begins with a capital letter (e.g. BackColor).
Variations and synonyms.
Although the first letter of a camel case compound word may or may not be capitalized, the term camel case generally implies lowercase first letter. For clarity, this article calls the two alternatives upper camel case and lower camel case. Some people and organizations use the term "camel case" only for lower camel case. Other synonyms include:
StudlyCaps encompasses all such variations, and more, including even random mixed capitalization, as in "MiXeD CaPitALiZaTioN" (typically a stereotyped allusion to online culture).
Camel case is also distinct from title case, which is traditionally used for book titles and headlines. Title case capitalizes most of the words yet retains the spaces between the words. Camel case is also distinct from Tall Man lettering, which uses capitals to emphasize the differences between similar-looking words.
History.
Chemical formulae.
The first systematic and widespread use of medial capitals for technical purposes was the notation for chemical formulae invented by the Swedish chemist Berzelius in 1813. To replace the multitude of naming and symbol conventions used by chemists until that time, he proposed to indicate each chemical element by a symbol of one or two letters, the first one being capitalized. The capitalization allowed formulae like 'NaCl' to be written without spaces and still be parsed without ambiguity.
Berzelius's system remains in use to this day, augmented with three-letter symbols like 'Uut' for unnamed elements and abbreviations for some common substituents (especially in the field of organic chemistry, for instance 'Et' for 'ethyl-'). This has been further extended to describe the amino acid sequences of proteins and other similar domains.
"The King's English".
In their English style guide "The King's English", first published in 1906, H. W. Fowler and F. G. Fowler suggested that medial capitals could be used in triple compound words where hyphens would cause ambiguity—the examples they give are "KingMark-like" (as against "King Mark-like") and "Anglo-SouthAmerican" (as against "Anglo-South American"). However, they described the system as "too hopelessly contrary to use at present."
Early use in trademarks.
Since the early 20th century, medial capitals have occasionally been used for corporate names and product trademarks, such as
Computer programming.
In the 1970s and 1980s, medial capitals were adopted as a standard or alternative naming convention for multi-word identifiers in several programming languages. The origin of this convention has not yet been settled. However, a 1954 conference proceedings informally referred to IBM's Speedcoding system as "SpeedCo". Christopher Strachey's paper on GPM (1965), shows a program that includes some medial capital identifiers, including "NextCh" and "WriteSymbol".
Background: multi-word identifiers.
Computer programmers often need to write descriptive (hence multi-word) identifiers, like "end of file" or "char table", in order to improve the readability of their code. However, most popular programming languages forbid the use of spaces inside identifiers, since they are interpreted as delimiters between tokens. The alternative of writing the words together as in "endoffile" or "chartable" is not satisfactory, since the word boundaries may be quite difficult to discern in the result or it may even be misleading (e.g. "chartable" may be used to mean that something can be displayed in a chart).
Some early programming languages, notably Lisp (1958) and COBOL (1959), addressed this problem by allowing a hyphen ("-") to be used between words of compound identifiers, as in "END-OF-FILE"—Lisp because it worked well with prefix notation; a Lisp parser would not treat a hyphen in the middle of a symbol as a subtraction operator; COBOL because its operators were English words. However, this solution was not adequate for algebra-oriented languages such as FORTRAN (1955) and ALGOL (1958), which used the hyphen as an intuitively obvious subtraction operator. (FORTRAN also restricted identifiers to six characters or fewer at the time, preventing multi-word identifiers except those made of very short words.) Since the common punched card character sets of the time had no lower-case letters and no other special character that would be adequate for the purpose, those early languages had to do without multi-word identifiers.
It was only in the late 1960s that the widespread adoption of the ASCII character set made both lower case and the underscore character "_" universally available. Some languages, notably C, promptly adopted underscores as word separators; and underscore-separated compounds like "end_of_file" are still prevalent in C programs and libraries. However, some languages and programmers chose to avoid underscores, among other reasons to prevent confusing them with whitespace, and adopted camel case instead. Two accounts are commonly given for the origin of this convention.
"Lazy programmer" theory.
One theory for the origin of the camel case convention holds that C programmers and hackers simply found it more convenient than the snake case style.
The underscore key is inconveniently placed on American QWERTY keyboards. Furthermore, early compilers severely restricted the length of identifiers (e.g., to 8 or 14 letters) or silently truncated all identifiers to that length (for example, FORTRAN 77 limited identifiers to 6 characters; even in C99, characters after the first 31 could be ignored. Finally, the small size of computer displays available in the 1970s (e.g., 80-character by 24-line VT52 and similar terminals) encouraged the use of short identifiers. Some programmers opted to use camel case instead of underscores to get legible compound names with fewer keystrokes and fewer characters.
"Alto Keyboard" theory.
Another account claims that the camel case style first became popular at Xerox PARC around 1978, with the Mesa programming language developed for the Xerox Alto computer. This machine lacked an underscore key, and the hyphen and space characters were not permitted in identifiers, leaving camel case as the only viable scheme for readable multiword names. The PARC Mesa Language Manual (1979) included a coding standard with specific rules for Upper- and lowerCamelCase that was strictly followed by the Mesa libraries and the Alto operating system.
The Smalltalk language, which was developed originally on the Alto and became quite popular in the early 1980s, may have been instrumental in spreading the style outside PARC. Camel case was also used by convention for many names in the PostScript page description language (invented by Adobe Systems founder and ex-PARC scientist John Warnock), as well as for the language itself. A further boost was provided by Niklaus Wirth (the inventor of Pascal) who acquired a taste for camel case during a sabbatical at PARC and used it in Modula, his next programming language.
Spread to mainstream usage.
Whatever its origins within the computing world, the practice spread in the 1980s and 1990s, when the advent of the personal computer exposed hacker culture to the world. Camel case then became fashionable for corporate trade names, initially in technical fields; mainstream usage was well established by 1990:
During the dot-com bubble of the late 1990s, the lowercase prefixes "e" (for "electronic") and "i" (for "Internet", "information", "intelligent", etc.) became quite common, giving rise to names like Apple's iMac and the eBox software platform.
In 1998, Dave Yost suggested that chemists use medial capitals to aid readability of long chemical names, e.g. write AmidoPhosphoRibosylTransferase instead of amidophosphoribosyltransferase. This usage was still rare in 2012.
The practice is sometimes used for abbreviated names of certain neighborhoods, e.g. New York City neighborhoods "SoHo" ("So"uth of "Ho"uston Street) and "TriBeCa" ("Tri"angle "Be"low "Ca"nal Street) and San Francisco's "SoMa" ("So"uth of "Ma"rket). Such usages erode quickly, so the neighborhoods are now rendered as "Soho", "Tribeca", and "Soma".
Internal capitalization has also been used for other technical codes like HeLa (1983).
History of the name "camel case".
The original name of the practice, used in media studies, grammars and the "Oxford English Dictionary", was "medial capitals". The fancier names such as "InterCaps", "CamelCase" and variations thereof are relatively recent and seem more common in computer-related communities.
The earliest known occurrence of the term "InterCaps" on Usenet is in an April 1990 post to the group alt.folklore.computers by Avi Rappoport, with "BiCapitalization" appearing slightly later in a 1991 post by Eric S. Raymond to the same group. The earliest use of the name "CamelCase" occurs in 1995, in a post by Newton Love. ""With the advent of programming languages having these sorts of constructs, the humpiness of the style made me call it HumpyCase at first, before I settled on CamelCase. I had been calling it CamelCase for years,"" said Love, ""The citation above was just the first time I had used the name on USENET.""
The name "CamelCase" is not related to the "Camel Book" ("Programming Perl"), which uses all-lowercase identifiers with underscores in its sample code. However, in Perl programming CamelCase is also commonly used.
Current usage in computing.
Programming and coding.
The use of medial caps for compound identifiers is recommended by the coding style guidelines of many organizations or software projects. For some languages (such as Mesa, Pascal, Modula, Java and Microsoft's .NET) this practice is recommended by the language developers or by authoritative manuals and has therefore become part of the language's "culture".
Style guidelines often distinguish between upper and lower camel case, typically specifying which variety should be used for specific kinds of entities: variables, record fields, methods, procedures, types, etc. These rules are sometimes supported by static analysis tools that check source code for adherence.
The original Hungarian notation for programming, for example, specifies that a lowercase abbreviation for the "usage type" (not data type) should prefix all variable names, with the remainder of the name in upper camel case; as such it is a form of lower camel case.
Programming identifiers often need to contain acronyms and initialisms that are already in upper case, such as "old HTML file". By analogy with the title case rules, the natural camel case rendering would have the abbreviation all in upper case, namely "oldHTMLFile". However, this approach is problematic when two acronyms occur together (e.g., "parse DBM XML" would become "parseDBMXML") or when the standard mandates lower camel case but the name begins with an abbreviation (e.g. "SQL server" would become "sQLServer"). For this reason, some programmers prefer to treat abbreviations as if they were lower case words and write "oldHtmlFile", "parseDbmXml" or "sqlServer".
Wiki link markup.
Camel case is used in some wiki markup languages for terms that should be automatically linked to other wiki pages. This convention was originally used in Ward Cunningham's original wiki software, WikiWikiWeb, and can be activated in most other wikis. Some wiki engines such as TiddlyWiki, Trac and PMWiki make use of it in the default settings, but usually also provide a configuration mechanism or plugin to disable it. Wikipedia formerly used camel case linking as well, but switched to explicit link markup using square brackets and many other wiki sites have done the same. Some wikis that do not use camel case linking may still use the camel case as a naming convention, such as AboutUs.
Other uses.
The NIEM registry requires that XML data elements use upper camel case and XML attributes use lower camel case.
Most popular command-line interfaces and scripting languages cannot easily handle file names that contain embedded spaces (usually requiring the name to be put in quotes). Therefore, users of those systems often resort to camel case (or underscores, hyphens and other "safe" characters) for compound file names like MyJobResume.pdf.
Microblogging and social networking sites that limit the number of characters in a message (most famously Twitter, where the 140-character limit can be quite restrictive in languages that rely on alphabets, including English) are potential outlets for medial capitals. Using CamelCase between words reduces the number of spaces, and thus the number of characters, in a given message, allowing more content to fit into the limited space.
In website URLs, spaces are percent-encoded as "%20", making the address longer and less human readable. By omitting spaces, CamelCase does not have this problem.
Current usage in natural languages.
Camel case has been used in languages other than English for a variety of purposes, including the ones below:
Orthographic markings.
Camel case is sometimes used in the transcription of certain scripts, to differentiate letters or markings. An example is the rendering of Tibetan proper names like "rLobsang": the "r" here stands for a prefix glyph in the original script that functions as tone marker rather than a normal letter. Another example is "tsIurku", a Latin transcription of the Chechen term for the capping stone of the characteristic Medieval defensive towers of Chechenia and Ingushetia; the capital letter "I" here denoting a phoneme distinct from the one transcribed as "i".
Inflection prefixes.
Camel case may also be used when writing proper names in languages that inflect words by attaching prefixes to them. In some of those languages, the custom is to leave the prefix in lower case and capitalize the root.
This convention is used in Irish orthography as well as Scots Gaelic orthography; e.g., ("in Galway"), from ("Galway"); ("the Scottish person"), from ("Scottish person"); ("to Ireland"), from ("Ireland).
Similarly, in transliteration of the Hebrew language, "haIvri" means "the Hebrew person" and "biYerushalayim" means "in Jerusalem".
This convention is also used by several Bantu languages (e.g., "kiSwahili" = "Swahili language", "isiZulu" = "Zulu language") and several indigenous languages of Mexico (e.g. Nahuatl, Totonacan, Mixe–Zoque and some Oto-Manguean languages).
In abbreviations and acronyms.
Abbreviations of some academic qualifications are sometimes presented in camel case without punctuation, e.g. PhD or BSc.
In French, camel case acronyms such as OuLiPo (1960) were favored for a time as alternatives to initialisms.
Camel case is often used to transliterate initialisms into alphabets where two letters may be required to represent a single character of the original alphabet, e.g., DShK from Cyrillic ДШК. 
Honorifics within compound words.
In several languages, including English, pronouns and possessives may be capitalized to indicate respect, e.g., when referring to the reader of a formal letter or to God. In some of those languages, the capitalization is customarily retained even when those words occur within compound words or suffixed to a verb. For example, in Italian one would write ("offering to You respectful salutations") or ("adore Him").
Other uses.
In German, many nouns carry a grammatical gender—which, for roles or job titles, is felt usually as masculine. Since the feminist movement of the 1980s, some writers and publishers have been using the feminine title suffixes "-in" (singular) and "-innen" (plural) to emphasize the inclusion of females; but written with a capital 'I', to indicate that males are not excluded. Example: ("co-workers, male or female") instead of ("co-workers", masculine grammatical gender) or ("female co-workers"). This use is analogous to the use of parentheses in English, for example in the phrase "congress(wo)man."
In German, the names to statutes are abbreviated using embedded capitals, e.g. StGB (Strafgesetzbuch) for criminal code, PatG (Patentgesetz) for Patent Act or the very common GmbH (Gesellschaft mit beschränkter Haftung) for Company with Limited Liability.
Criticism.
CamelCase has been criticised as negatively impacting readability due to the removing of spaces and upcasing of every word. One natural language study found that replacing spaces between words with letters or digits made it harder to recognise individual words, which resulted in increased reading times. However, a study that specifically compared under_score style and CamelCase found that camel case identifiers could be recognised with higher accuracy among both programmers and non-programmers, and that programmers already trained in CamelCase were able to recognise CamelCase identifiers faster than underscored identifiers.
Use of CamelCase can conflict with the regular use of uppercase letters for all caps acronyms e.g. to represent a concept like "the TCP IP socket ID" the writer must choose to either retain the capitalisation of the acronyms ("TCPIPSocketID"), which harms readability, or to retain capitalisation of only the first letter ("TcpIpSocketId"), which makes it harder to recognise that a given word is intended as an acronym. An alternative is to follow any instance of acronymic capitalization with a re-initialization of lower case camel, as TCPIPsocketID. This has the effect of enforcing the lower camel case standard.

</doc>
<doc id="6700" url="http://en.wikipedia.org/wiki?curid=6700" title="Cereal">
Cereal

A cereal is a grass, a member of the monocot family Poaceae, cultivated for the edible components of its grain (botanically, a type of fruit called a caryopsis), composed of the endosperm, germ, and bran. Cereal grains are grown in greater quantities and provide more food energy worldwide than any other type of crop; they are therefore staple crops.
In their natural form (as in "whole grain"), they are a rich source of vitamins, minerals, carbohydrates, fats, oils, and protein. However, when refined by the removal of the bran and germ, the remaining endosperm is mostly carbohydrate and lacks the majority of the other nutrients. In some developing nations, grain in the form of rice, wheat, millet, or maize constitutes a majority of daily sustenance. In developed nations, cereal consumption is moderate and varied but still substantial.
The word "cereal" derives from "Ceres", the name of the Roman goddess of harvest and agriculture.
History.
The first cereal grains were domesticated about 12,000 years ago by ancient farming communities in the Fertile Crescent region. Emmer wheat, einkorn wheat, and barley were three of the so-called Neolithic founder crops in the development of agriculture.
Production.
The following table shows the annual production of cereals in 1961, 2010, 2011, and 2012 ranked by 2012 production. All but buckwheat and quinoa are true grasses (these two are pseudocereals).
Maize, wheat, and rice together accounted for 89% of all cereal production worldwide in 2012, and 43% of all food calories in 2009, while the production of oats and triticale have drastically fallen from their 1960s levels.
Other grains that are important in some places, but that have little production globally (and are not included in FAO statistics), include:
Several other species of wheat have also been domesticated, some very early in the history of agriculture:
In 2013 global cereal production reached a record 2,521 million tonnes. A slight dip to 2,498 million tonnes was forecast for 2014 by the FAO in July 2014.
Farming.
While each individual species has its own peculiarities, the cultivation of all cereal crops is similar. Most are annual plants; consequently one planting yields one harvest. Wheat, rye, triticale, oats, barley, and spelt are the "cool-season" cereals. These are hardy plants that grow well in moderate weather and cease to grow in hot weather (approximately 30 °C, but this varies by species and variety). The "warm-season" cereals are tender and prefer hot weather. Barley and rye are the hardiest cereals, able to overwinter in the subarctic and Siberia. Many cool-season cereals are grown in the tropics. However, some are only grown in cooler highlands, where it may be possible to grow multiple crops in a year.
For a few decades, there has also been increasing interest in perennial grain plants. This interest developed due to advantages in erosion control, reduced need of fertiliser, and potential lowered costs to the farmer. Though research is still in early stages, The Land Institute in Salina, Kansas has been able to create a few cultivars that produce a fairly good crop yield.
Planting.
The warm-season cereals are grown in tropical lowlands year-round and in temperate climates during the frost-free season. Rice is commonly grown in flooded fields, though some strains are grown on dry land. Other warm climate cereals, such as sorghum, are adapted to arid conditions.
Cool-season cereals are well-adapted to temperate climates. Most varieties of a particular species are either winter or spring types. Winter varieties are sown in the autumn, germinate and grow vegetatively, then become dormant during winter. They resume growing in the springtime and mature in late spring or early summer. This cultivation system makes optimal use of water and frees the land for another crop early in the growing season.
Winter varieties do not flower until springtime because they require vernalization: exposure to low temperatures for a genetically determined length of time. Where winters are too warm for vernalization or exceed the hardiness of the crop (which varies by species and variety), farmers grow spring varieties. Spring cereals are planted in early springtime and mature later that same summer, without vernalization. Spring cereals typically require more irrigation and yield less than winter cereals.
Period.
Once the cereal plants have grown their seeds, they have completed their life cycle. The plants die and become brown and dry. As soon as the parent plants and their seed kernels are reasonably dry, harvest can begin.
In developed countries, cereal crops are universally machine-harvested, typically using a combine harvester, which cuts, threshes, and winnows the grain during a single pass across the field. In developing countries, a variety of harvesting methods are in use, depending on the cost of labor, from combines to hand tools such as the scythe or cradle.
If a crop is harvested during wet weather, the grain may not dry adequately in the field to prevent spoilage during its storage. In this case, the grain is sent to a dehydrating facility, where artificial heat dries it.
In North America, farmers commonly deliver their newly harvested grain to a grain elevator, a large storage facility that consolidates the crops of many farmers. The farmer may sell the grain at the time of delivery or maintain ownership of a share of grain in the pool for later sale. Storage facilities should be protected from small grain pests, rodents and birds.
Nutritional facts.
Some grains are deficient in the essential amino acid lysine. That is why many vegetarian cultures, in order to get a balanced diet, combine their diet of grains with legumes. Many legumes, on the other hand, are deficient in the essential amino acid methionine, which grains contain. Thus, a combination of legumes with grains forms a well-balanced diet for vegetarians. Common examples of such combinations are dal (lentils) with rice by South Indians and Bengalis, dal with wheat in Pakistan and North India, and beans with corn tortillas, tofu with rice, and peanut butter with wheat bread (as sandwiches) in several other cultures, including Americans. The amount of crude protein found in grain is measured as the grain crude protein concentration.
Standardization.
The ISO has published a series of standards regarding cereal products which are covered by ICS 67.060.

</doc>
