<doc id="5666" url="http://en.wikipedia.org/wiki?curid=5666" title="Central bank">
Central bank

A central bank, reserve bank, or monetary authority is an institution that manages a state's currency, money supply, and interest rates. Central banks also usually oversee the commercial banking system of their respective countries. In contrast to a commercial bank, a central bank possesses a monopoly on increasing the amount of money in the nation, and usually also prints the national currency, which usually serves as the nation's legal tender. Examples include the European Central Bank (ECB) and the Federal Reserve of the United States.
The primary function of a central bank is to manage the nation's money supply (monetary policy), through active duties such as managing interest rates, setting the reserve requirement, and acting as a lender of last resort to the banking sector during times of bank insolvency or financial crisis. Central banks usually also have supervisory powers, intended to prevent bank runs and to reduce the risk that commercial banks and other financial institutions engage in reckless or fraudulent behavior. Central banks in most developed nations are institutionally designed to be independent from political interference. Still, limited control by the executive and legislative bodies usually exists.
The chief executive of a central bank is normally known as the Governor, President or Chairman.
History.
Prior to the 17th century most money was commodity money, typically gold or silver. However, promises to pay were widely circulated and accepted as value at least five hundred years earlier in both Europe and Asia. The Song Dynasty was the first to issue generally circulating paper currency, while the Yuan Dynasty was the first to use notes as the predominant circulating medium. In 1455, in an effort to control inflation, the succeeding Ming Dynasty ended the use of paper money and closed much of Chinese trade. The medieval European Knights Templar ran an early prototype of a central banking system, as their promises to pay were widely respected, and many regard their activities as having laid the basis for the modern banking system.
As the first public bank to "offer accounts not directly convertible to coin", the Bank of Amsterdam established in 1609 is considered to be the precursor to modern central banks. The central bank of Sweden ("Sveriges Riksbank" or simply "Riksbanken") was founded in Stockholm from the remains of the failed bank Stockholms Banco in 1664 and answered to the parliament ("Riksdag of the Estates"). One role of the Swedish central bank was lending money to the government.
Bank of England.
In England in the 1690s, public funds were in short supply and were needed to finance the ongoing conflict with France. The credit of William III's government was so low in London that it was impossible for it to borrow the £1,200,000 (at 8 per cent) that the government wanted. In order to induce subscription to the loan, the subscribers were to be incorporated by the name of the Governor and Company of the Bank of England. The bank was given exclusive possession of the government's balances, and was the only limited-liability corporation allowed to issue banknotes. The lenders would give the government cash (bullion) and also issue notes against the government bonds, which can be lent again. The £1.2M was raised in 12 days; half of this was used to rebuild the Navy.
The establishment of the Bank of England, the model on which most modern central banks have been based, was devised by Charles Montagu, 1st Earl of Halifax, in 1694, to the plan which had been proposed by William Paterson three years before, but had not been acted upon. He proposed a loan of £1.2M to the government; in return the subscribers would be incorporated as "The Governor and Company of the Bank of England" with long-term banking privileges including the issue of notes. The Royal Charter was granted on 27 July through the passage of the Tonnage Act 1694.
Although some would point to the 1694 establishment Bank of England as the origin of central banking, it did not have the functions as a modern central bank, namely, to regulate the value of the national currency, to finance the government, to be the sole authorised distributor of banknotes, and to function as a 'lender of last resort' to banks suffering a liquidity crisis. The modern central bank evolved slowly through the 18th and 19th centuries to reach its current form.
Although the Bank was originally a private institution, by the end of the 18th century it was increasingly being regarded as a public authority with civic responsibility toward the upkeep of a healthy financial system. The currency crisis of 1797, caused by panicked depositors withdrawing from the Bank led to the government suspending convertibility of notes into specie payment. The bank was soon accused by the bullionists of causing the exchange rate to fall from over issuing banknotes, a charge which the Bank denied. Nevertheless, it was clear that the Bank was being treated as an organ of the state. 
Henry Thornton, a merchant banker and monetary theorist has been described as the father of the modern central bank. An opponent of the real bills doctrine, he was a defender of the bullionist position and a significant figure in monetary theory, his process of monetary expansion anticipating the theories of Knut Wicksell regarding the "cumulative process which restates the Quantity Theory in a theoretically coherent form". As a response 1797 currency crisis, Thornton wrote in 1802 "An Enquiry into the Nature and Effects of the Paper Credit of Great Britain", in which he argued that the increase in paper credit did not cause the crisis. The book also gives a detailed account of the British monetary system as well as a detailed examination of the ways in which the Bank of England should act to counteract fluctuations in the value of the pound.
Until the mid-nineteenth century, commercial banks were able to issue their own banknotes, and notes issued by provincial banking companies were commonly in circulation. Many consider the origins of the central bank to lie with the passage of the Bank Charter Act of 1844. Under this law, authorisation to issue new banknotes was restricted to the Bank of England. At the same time, the Bank of England was restricted to issue new banknotes only if they were 100% backed by gold or up to £14 million in government debt. The Act served to restrict the supply of new notes reaching circulation, and gave the Bank of England an effective monopoly on the printing of new notes.
The Bank accepted the role of 'lender of last resort' in the 1870s after criticism of its' lacklustre response to the Overend-Gurney crisis. The journalist Walter Bagehot wrote an influential work on the subject "", in which he advocated for the Bank to officially become a lender of last resort during a credit crunch (sometimes referred to as "Bagehot's dictum"). Paul Tucker phrased the dictum as follows:
Spread around the world.
Central banks were established in many European countries during the 19th century. The War of the Second Coalition led to the creation of the Banque de France in 1800, in an effort to improve the public financing of the war.
Although central banks today are generally associated with fiat money, the 19th and early 20th centuries central banks in most of Europe and Japan developed under the international gold standard, elsewhere free banking or currency boards were more usual at this time. Problems with collapses of banks during downturns, however, lead to wider support for central banks in those nations which did not as yet possess them, most notably in Australia.
The US Federal Reserve was created by the U.S. Congress through the passing of The Federal Reserve Act in the Senate and its signing by President Woodrow Wilson on the same day, December 23, 1913. Australia established its first central bank in 1920, Colombia in 1923, Mexico and Chile in 1925 and Canada and New Zealand in the aftermath of the Great Depression in 1934. By 1935, the only significant independent nation that did not possess a central bank was Brazil, which subsequently developed a precursor thereto in 1945 and the present central bank twenty years later. Having gained independence, African and Asian countries also established central banks or monetary unions.
The People's Bank of China evolved its role as a central bank starting in about 1979 with the introduction of market reforms, which accelerated in 1989 when the country adopted a generally capitalist approach to its export economy. Evolving further partly in response to the European Central Bank, the People's Bank of China has by 2000 become a modern central bank. The most recent bank model, was introduced together with the euro, involves coordination of the European national banks, which continue to manage their respective economies separately in all respects other than currency exchange and base interest rates.
Naming of central banks.
There is no standard terminology for the name of a central bank, but many countries use the "Bank of Country" form (for example: Bank of England (which is in fact the central bank of the United Kingdom as a whole), Bank of Canada, Bank of Mexico; But the Bank of India is a (government-owned) commercial bank and not a central bank). Some are styled "national" banks, such as the National Bank of Ukraine, although the term national bank is also used for private commercial banks in some countries. In other cases, central banks may incorporate the word "Central" (for example, European Central Bank, Central Bank of Ireland, Central Bank of Brazil); but the Central Bank of India is a (government-owned) commercial bank and not a central bank. The word "Reserve" is also often included, such as the Reserve Bank of India, Reserve Bank of Australia, Reserve Bank of New Zealand, the South African Reserve Bank, and U.S. Federal Reserve System. Other central banks are known as monetary authorities such as the Monetary Authority of Singapore, Maldives Monetary Authority and Cayman Islands Monetary Authority. Many countries have state-owned banks or other quasi-government entities that have entirely separate functions, such as financing imports and exports.
In some countries, particularly in some Communist countries, the term national bank may be used to indicate both the monetary authority and the leading banking entity, such as the Soviet Union's Gosbank (state bank). In other countries, the term national bank may be used to indicate that the central bank's goals are broader than monetary stability, such as full employment, industrial development, or other goals.
Activities and responsibilities.
Functions of a central bank may include:
Monetary policy.
Central banks implement a country's chosen monetary policy. At the most basic level, this involves establishing what form of currency the country may have, whether a fiat currency, gold-backed currency (disallowed for countries with membership of the International Monetary Fund), currency board or a currency union. When a country has its own national currency, this involves the issue of some form of standardized currency, which is essentially a form of promissory note: a promise to exchange the note for "money" under certain circumstances. Historically, this was often a promise to exchange the money for precious metals in some fixed amount. Now, when many currencies are fiat money, the "promise to pay" consists of the promise to accept that currency to pay for taxes.
A central bank may use another country's currency either directly (in a currency union), or indirectly (a currency board). In the latter case, exemplified by Bulgaria, Hong Kong and Latvia, the local currency is backed at a fixed rate by the central bank's holdings of a foreign currency.
The expression "monetary policy" may also refer more narrowly to the interest-rate targets and other active measures undertaken by the monetary authority.
Goals of monetary policy.
High employment:
Frictional unemployment is the time period between jobs when a worker is searching for, or transitioning from one job to another. Unemployment beyond frictional unemployment is classified as unintended unemployment.
For example, structural unemployment is a form of unemployment resulting from a mismatch between demand in the labour market and the skills and locations of the workers seeking employment. Macroeconomic policy generally aims to reduce unintended unemployment.
Keynes labeled any jobs that would be created by a rise in wage-goods (i.e., a decrease in real-wages) as involuntary unemployment:
Price stability:
Inflation is defined either as the devaluation of a currency or equivalently the rise of prices relative to a currency.
Since inflation lowers real wages, Keynesians view inflation as the solution to involuntary unemployment. However, "unanticipated" inflation leads to lender losses as the real interest rate will be lower than expected. Thus, Keynesian monetary policy aims for a steady rate of inflation.
Economic growth:
Economic growth can be enhanced by investment in capital, such as more or better machinery. A low interest rate implies that firms can loan money to invest in their capital stock and pay less interest for it. Lowering the interest is therefore considered to encourage economic growth and is often used to alleviate times of low economic growth. On the other hand, raising the interest rate is often used in times of high economic growth as a contra-cyclical device to keep the economy from overheating and avoid market bubbles.
Interest rate stability
Financial market stability
Foreign exchange market stability
Conflicts among goals:
Goals frequently cannot be separated from each other and often conflict. Costs must therefore be carefully weighed before policy implementation.
Currency issuance.
Similar to commercial banks, central banks hold assets (government bonds, foreign exchange, gold, and other financial assets) and incur liabilities (currency outstanding). Central banks create money by issuing interest-free currency notes and selling them to the public in exchange for interest-bearing assets such as government bonds. When a central bank wishes to purchase more bonds than their respective national governments make available, they may purchase private bonds or assets denominated in foreign currencies.
The European Central Bank remits its interest income to the central banks of the member countries of the European Union. The US Federal Reserve remits all its profits to the U.S. Treasury. This income, derived from the power to issue currency, is referred to as seigniorage, and usually belongs to the national government. The state-sanctioned power to create currency is called the Right of Issuance. Throughout history there have been disagreements over this power, since whoever controls the creation of currency controls the seigniorage income.
Interest rate interventions.
Typically a central bank controls certain types of short-term interest rates. These influence the stock- and bond markets as well as mortgage and other interest rates. The European Central Bank for example announces its interest rate at the meeting of its Governing Council; in the case of the U.S. Federal Reserve, the Federal Reserve Board of Governors.
Both the Federal Reserve and the ECB are composed of one or more central bodies that are responsible for the main decisions about interest rates and the size and type of open market operations, and several branches to execute its policies. In the case of the Federal Reserve, they are the local Federal Reserve Banks; for the ECB they are the national central banks.
Limits on policy effects.
Although the perception by the public may be that the "central bank" controls some or all interest rates and currency rates, economic theory (and substantial empirical evidence) shows that it is impossible to do both at once in an open economy. Robert Mundell's "impossible trinity" is the most famous formulation of these limited powers, and postulates that it is impossible to target monetary policy (broadly, interest rates), the exchange rate (through a fixed rate) and maintain free capital movement. Since most Western economies are now considered "open" with free capital movement, this essentially means that central banks may target interest rates or exchange rates with credibility, but not both at once.
In the most famous case of policy failure, Black Wednesday, George Soros arbitraged the pound sterling's relationship to the ECU and (after making $2 billion himself and forcing the UK to spend over $8bn defending the pound) forced it to abandon its policy. Since then he has been a harsh critic of clumsy bank policies and argued that no one should be able to do what he did.
The most complex relationships are those between the yuan and the US dollar, and between the euro and its neighbours. The situation in Cuba is so exceptional as to require the Cuban peso to be dealt with simply as an exception, since the United States forbids direct trade with Cuba. US dollars were ubiquitous in Cuba's economy after its legalization in 1991, but were officially removed from circulation in 2004 and replaced by the convertible peso.
Policy instruments.
The main monetary policy instruments available to central banks are open market operation, bank reserve requirement, interest rate policy, re-lending and re-discount (including using the term repurchase market), and credit policy (often coordinated with trade policy). While capital adequacy is important, it is defined and regulated by the Bank for International Settlements, and central banks in practice generally do not apply stricter rules.
To enable open market operations, a central bank must hold foreign exchange reserves (usually in the form of government bonds) and official gold reserves. It will often have some influence over any official or mandated exchange rates: Some exchange rates are managed, some are market based (free float) and many are somewhere in between ("managed float" or "dirty float").
Interest rates.
By far the most visible and obvious power of many modern central banks is to influence market interest rates; contrary to popular belief, they rarely "set" rates to a fixed number. Although the mechanism differs from country to country, most use a similar mechanism based on a central bank's ability to create as much fiat money as required.
The mechanism to move the market towards a 'target rate' (whichever specific rate is used) is generally to lend money or borrow money in theoretically unlimited quantities, until the targeted market rate is sufficiently close to the target. Central banks may do so by lending money to and borrowing money from (taking deposits from) a limited number of qualified banks, or by purchasing and selling bonds. As an example of how this functions, the Bank of Canada sets a target overnight rate, and a band of plus or minus 0.25%. Qualified banks borrow from each other within this band, but never above or below, because the central bank will always lend to them at the top of the band, and take deposits at the bottom of the band; in principle, the capacity to borrow and lend at the extremes of the band are unlimited. Other central banks use similar mechanisms.
It is also notable that the target rates are generally short-term rates. The actual rate that borrowers and lenders receive on the market will depend on (perceived) credit risk, maturity and other factors. For example, a central bank might set a target rate for overnight lending of 4.5%, but rates for (equivalent risk) five-year bonds might be 5%, 4.75%, or, in cases of inverted yield curves, even below the short-term rate. Many central banks have one primary "headline" rate that is quoted as the "central bank rate". In practice, they will have other tools and rates that are used, but only one that is rigorously targeted and enforced.
"The rate at which the central bank lends money can indeed be chosen at will by the central bank; this is the rate that makes the financial headlines." – Henry C.K. Liu. Liu explains further that "the U.S. central-bank lending rate is known as the Fed funds rate. The Fed sets a target for the Fed funds rate, which its Open Market Committee tries to match by lending or borrowing in the money market ... a fiat money system set by command of the central bank. The Fed is the head of the central-bank because the U.S. dollar is the key reserve currency for international trade. The global money market is a USA dollar market. All other currencies markets revolve around the U.S. dollar market." Accordingly the U.S. situation is not typical of central banks in general.
A typical central bank has several interest rates or monetary policy tools it can set to influence markets.
These rates directly affect the rates in the money market, the market for short term loans.
Open market operations.
Through open market operations, a central bank influences the money supply in an economy. Each time it buys securities (such as a government bond or treasury bill), it in effect creates money. The central bank exchanges money for the security, increasing the money supply while lowering the supply of the specific security. Conversely, selling of securities by the central bank reduces the money supply.
Open market operations usually take the form of:
All of these interventions can also influence the foreign exchange market and thus the exchange rate. For example the People's Bank of China and the Bank of Japan have on occasion bought several hundred billions of U.S. Treasuries, presumably in order to stop the decline of the U.S. dollar versus the renminbi and the yen.
Capital requirements.
All banks are required to hold a certain percentage of their assets as capital, a rate which may be established by the central bank or the banking supervisor. For international banks, including the 55 member central banks of the Bank for International Settlements, the threshold is 8% (see the Basel Capital Accords) of risk-adjusted assets, whereby certain assets (such as government bonds) are considered to have lower risk and are either partially or fully excluded from total assets for the purposes of calculating capital adequacy. Partly due to concerns about asset inflation and repurchase agreements, capital requirements may be considered more effective than reserve requirements in preventing indefinite lending: when at the threshold, a bank cannot extend another loan without acquiring further capital on its balance sheet.
Reserve requirements.
Historically, bank reserves have formed only a small fraction of deposits, a system called fractional reserve banking. Banks would hold only a small percentage of their assets in the form of cash reserves as insurance against bank runs. Over time this process has been regulated and insured by central banks. Such legal reserve requirements were introduced in the 19th century as an attempt to reduce the risk of banks overextending themselves and suffering from bank runs, as this could lead to knock-on effects on other overextended banks. "See also money multiplier."
As the early 20th century gold standard was undermined by inflation and the late 20th century fiat dollar hegemony evolved, and as banks proliferated and engaged in more complex transactions and were able to profit from dealings globally on a moment's notice, these practices became mandatory, if only to ensure that there was some limit on the ballooning of money supply. Such limits have become harder to enforce. The People's Bank of China retains (and uses) more powers over reserves because the yuan that it manages is a non-convertible currency.
Loan activity by banks plays a fundamental role in determining the money supply. The central-bank money after aggregate settlement – "final money" – can take only one of two forms:
The currency component of the money supply is far smaller than the deposit component. Currency, bank reserves and institutional loan agreements together make up the monetary base, called M1, M2 and M3. The Federal Reserve Bank stopped publishing M3 and counting it as part of the money supply in 2006.
Exchange requirements.
To influence the money supply, some central banks may require that some or all foreign exchange receipts (generally from exports) be exchanged for the local currency. The rate that is used to purchase local currency may be market-based or arbitrarily set by the bank. This tool is generally used in countries with non-convertible currencies or partially convertible currencies. The recipient of the local currency may be allowed to freely dispose of the funds, required to hold the funds with the central bank for some period of time, or allowed to use the funds subject to certain restrictions. In other cases, the ability to hold or use the foreign exchange may be otherwise limited.
In this method, money supply is increased by the central bank when it purchases the foreign currency by issuing (selling) the local currency. The central bank may subsequently reduce the money supply by various means, including selling bonds or foreign exchange interventions.
Margin requirements and other tools.
In some countries, central banks may have other tools that work indirectly to limit lending practices and otherwise restrict or regulate capital markets. For example, a central bank may regulate margin lending, whereby individuals or companies may borrow against pledged securities. The margin requirement establishes a minimum ratio of the value of the securities to the amount borrowed.
Central banks often have requirements for the quality of assets that may be held by financial institutions; these requirements may act as a limit on the amount of risk and leverage created by the financial system. These requirements may be direct, such as requiring certain assets to bear certain minimum credit ratings, or indirect, by the central bank lending to counterparties only when security of a certain quality is pledged as collateral.
Banking supervision and other activities.
In some countries a central bank through its subsidiaries controls and monitors the banking sector. In other countries banking supervision is carried out by a government department such as the UK Treasury, or an independent government agency (for example, UK's Financial Conduct Authority). It examines the banks' balance sheets and behaviour and policies toward consumers. Apart from refinancing, it also provides banks with services such as transfer of funds, bank notes and coins or foreign currency. Thus it is often described as the "bank of banks".
Many countries such as the United States will monitor and control the banking sector through different agencies and for different purposes, although there is usually significant cooperation between the agencies. For example, money center banks, deposit-taking institutions, and other types of financial institutions may be subject to different (and occasionally overlapping) regulation. Some types of banking regulation may be delegated to other levels of government, such as state or provincial governments.
Any cartel of banks is particularly closely watched and controlled. Most countries control bank mergers and are wary of concentration in this industry due to the danger of groupthink and runaway lending bubbles based on a single point of failure, the credit culture of the few large banks.
Independence.
Over the past decade, there has been a trend towards increasing the independence of central banks as a way of improving long-term economic performance. However, while a large volume of economic research has been done to define the relationship between central bank independence and economic performance, the results are ambiguous.
Advocates of central bank independence argue that a central bank which is too susceptible to political direction or pressure may encourage economic cycles ("boom and bust"), as politicians may be tempted to boost economic activity in advance of an election, to the detriment of the long-term health of the economy and the country. In this context, independence is usually defined as the central bank's operational and management independence from the government.
The literature on central bank independence has defined a number of types of independence.
It is argued that an independent central bank can run a more credible monetary policy, making market expectations more responsive to signals from the central bank. Recently, both the Bank of England (1997) and the European Central Bank have been made independent and follow a set of published inflation targets so that markets know what to expect. Even the People's Bank of China has been accorded great latitude due to the difficulty of problems it faces, though in the People's Republic of China the official role of the bank remains that of a national bank rather than a central bank, underlined by the official refusal to "unpeg" the yuan or to revalue it "under pressure". The People's Bank of China's independence can thus be read more as independence from the USA which rules the financial markets, than from the Communist Party of China which rules the country. The fact that the Communist Party is not elected also relieves the pressure to please people, increasing its independence.
Governments generally have some degree of influence over even "independent" central banks; the aim of independence is primarily to prevent short-term interference. For example, the Board of Governors of the U.S. Federal Reserve are nominated by the President of the U.S. and confirmed by the Senate. The Chairman and other Federal Reserve officials often testify before the Congress.
International organizations such as the World Bank, the Bank for International Settlements (BIS) and the International Monetary Fund (IMF) are strong supporters of central bank independence. This results, in part, from a belief in the intrinsic merits of increased independence. The support for independence from the international organizations also derives partly from the connection between increased independence for the central bank and increased transparency in the policy-making process. The IMF's Financial Services Action Plan (FSAP) review self-assessment, for example, includes a number of questions about central bank independence in the transparency section. An independent central bank will score higher in the review than one that is not independent.

</doc>
<doc id="5667" url="http://en.wikipedia.org/wiki?curid=5667" title="Chlorine">
Chlorine

Chlorine is a chemical element with symbol Cl and atomic number 17. Chlorine is in the halogen group (17) and is the second lightest halogen following fluorine. The element is a yellow-green gas under standard conditions, where it forms diatomic molecules. Chlorine has the highest electron affinity and the fourth highest electronegativity of all the reactive elements. For this reason, chlorine is a strong oxidizing agent. Free chlorine is rare on Earth, and is usually a result of direct or indirect oxidation by oxygen.
The most common compound of chlorine, sodium chloride (common salt), has been known since ancient times. Around 1630 chlorine gas was first synthesized in a chemical reaction, but not recognized as a fundamentally important substance. Characterization of chlorine gas was made in 1774 by Carl Wilhelm Scheele, who supposed it to be an oxide of a new element. In 1809 chemists suggested that the gas might be a pure element, and this was confirmed by Sir Humphry Davy in 1810, who named it from .
Nearly all chlorine in the Earth's crust occurs as chloride in various ionic compounds, including table salt. It is the second most abundant halogen and 21st most abundant chemical element in Earth's crust. Elemental chlorine is commercially produced from brine by electrolysis. The high oxidizing potential of elemental chlorine led commercially to free chlorine's bleaching and disinfectant uses, as well as its many uses of an essential reagent in the chemical industry. Chlorine is used in the manufacture of a wide range of consumer products, about two-thirds of them organic chemicals such as polyvinyl chloride, as well as many intermediates for production of plastics and other end products which do not contain the element. As a common disinfectant, elemental chlorine and chlorine-generating compounds are used more directly in swimming pools to keep them clean and sanitary.
In the form of chloride ions, chlorine is necessary to all known species of life. Other types of chlorine compounds are rare in living organisms, and artificially produced chlorinated organics range from inert to toxic. In the upper atmosphere, chlorine-containing organic molecules such as chlorofluorocarbons have been implicated in ozone depletion. Small quantities of elemental chlorine are generated by oxidation of chloride to hypochlorite in neutrophils, as part of the immune response against bacteria. Elemental chlorine at high concentrations is extremely dangerous and poisonous for all living organisms, and was used in World War I as the first gaseous chemical warfare agent.
Characteristics.
Physical characteristics of chlorine and its compounds.
At standard temperature and pressure, two chlorine atoms form the diatomic molecule Cl2. This is a yellow-green gas that has a distinctive strong odor, familiar to most from common household bleach. The bonding between the two atoms is relatively weak (only 242.580 ± 0.004 kJ/mol), which makes the Cl2 molecule highly reactive. The boiling point at standard pressure is around −34 ˚C, but it can be liquefied at room temperature with pressures above 740 kPa (107 psi).
Although elemental chlorine is yellow-green, chloride ion, in common with other halide ions, has no color in either minerals or solutions (example, table salt). Similarly, (again as with other halogens) chlorine atoms impart no color to organic chlorides when they replace hydrogen atoms in colorless organic compounds, such as tetrachloromethane. The melting point and density of these compounds is increased by substitution of hydrogen in place of chlorine. Compounds of chlorine with other halogens, however, as well as many chlorine oxides, are visibly colored.
Chemical characteristics.
Along with fluorine, bromine, iodine, and astatine, chlorine is a member of the halogen series that forms the group 17 (formerly VII, VIIA, or VIIB) of the periodic table. Chlorine forms compounds with almost all of the elements to give compounds that are usually called chlorides. Chlorine gas reacts with most organic compounds, and will even sluggishly support the combustion of hydrocarbons.
Hydrolysis of free chlorine or disproportionation in water.
At 25 °C and atmospheric pressure, one liter of water dissolves 3.26 g or 1.125 L of gaseous chlorine. Solutions of chlorine in water contain chlorine (Cl2), hydrochloric acid, and hypochlorous acid:
This conversion to the right is called disproportionation, because the ingredient chlorine both increases and decreases in formal oxidation state. The solubility of chlorine in water is increased if the water contains dissolved alkali hydroxide, and in this way, chlorine bleach is produced.
Chlorine gas only exists in a neutral or acidic solution.
Chemistry and compounds.
Chlorine exists in all odd numbered oxidation states from −1 to +7, as well as the elemental state of zero and four in chlorine dioxide (see table below, and also structures in chlorite). Chlorine typically has a −1 oxidation state in compounds, except for compounds containing fluorine, oxygen and nitrogen, all of which are even more electronegative than chlorine. Progressing through the states, hydrochloric acid can be oxidized using manganese dioxide, or hydrogen chloride gas oxidized catalytically by air to form elemental chlorine gas.
Chlorides.
Chlorine combines with almost all elements to give chlorides. Compounds with oxygen, nitrogen, xenon, and krypton are known, but do not form by direct reaction of the elements. Chloride is one of the most common anions in nature. Hydrogen chloride and its aqueous solution, hydrochloric acid, are produced on megaton scale annually both as valued intermediates but sometimes as undesirable pollutants.
Chlorine oxides.
Chlorine forms a variety of oxides, as seen above: chlorine dioxide (ClO2), dichlorine monoxide (Cl2O), dichlorine hexoxide (Cl2O6), dichlorine heptoxide (Cl2O7). The anionic derivatives of these same oxides are also well known including chlorate (), chlorite (), hypochlorite (ClO−), and perchlorate (). The acid derivatives of these anions are hypochlorous acid (HOCl), chloric acid (HClO3) and perchloric acid (HClO4). The chloroxy cation chloryl (ClO2+) is known and has the same structure as chlorite but with a positive charge and chlorine in the +5 oxidation state. The compound "chlorine trioxide" does not occur, but rather in gas form is found as the dimeric dichlorine hexoxide (Cl2O6) with a +6 oxidation state. This compound in liquid or solid form disproportionates to a mixture of +5 and +7 oxidation states, occurring as the ionic compound chloryl perchlorate, .
In hot concentrated alkali solution hypochlorite disproportionates:
Sodium chlorate and potassium chlorate can be crystallized from solutions formed by the above reactions. If their crystals are heated to a high temperature, they undergo a further, final disproportionation:
This same progression from chloride to perchlorate can be accomplished by electrolysis. The anode reaction progression is:
Each step is accompanied at the cathode by
Interhalogen compounds.
Chlorine oxidizes bromide and iodide salts to bromine and iodine, respectively. However, it cannot oxidize fluoride salts to fluorine. It makes a variety of interhalogen compounds, such as the chlorine fluorides, chlorine monofluoride (), chlorine trifluoride (), chlorine pentafluoride (). Chlorides of bromine and iodine are also known.
Organochlorine compounds.
Chlorine is used extensively in organic chemistry in substitution and addition reactions. Chlorine often imparts many desired properties to an organic compound, in part owing to its electronegativity.
Like the other halides, chlorine undergoes electrophilic addition reactions, the most notable one being the chlorination of alkenes and aromatic compounds with a Lewis acid catalyst. Organic chlorine compounds tend to be less reactive in nucleophilic substitution reactions than the corresponding bromine or iodine derivatives, but they tend to be cheaper. They may be activated for reaction by substituting with a tosylate group, or by the use of a catalytic amount of sodium iodide.
Occurrence.
In the interstellar medium, chlorine is produced in supernovae via the r-process.
In meteorites and on Earth, chlorine is found primarily as the chloride ion which occurs in minerals. In the Earth's crust, chlorine is present at average concentrations of about 126 parts per million, predominantly in such minerals as "halite" (sodium chloride), "sylvite" (potassium chloride), and "carnallite" (potassium magnesium chloride hexahydrate).
Chloride is a component of the salt that is deposited in the earth or dissolved in the oceans — about 1.9% of the mass of seawater is chloride ions. Even higher concentrations of chloride are found in the Dead Sea and in underground brine deposits. Most chloride salts are soluble in water, thus, chloride-containing minerals are usually only found in abundance in dry climates or deep underground.
Over 2000 naturally occurring organic chlorine compounds are known.
Isotopes.
Chlorine has a wide range of isotopes. The two stable isotopes are 35Cl (75.77%) and 37Cl (24.23%). Together they give chlorine an atomic weight of 35.4527 g/mol. The half-integer value for chlorine's weight caused some confusion in the early days of chemistry, when it had been postulated that atoms were composed of even units of hydrogen (see Proust's law), and the existence of chemical isotopes was unsuspected.
Trace amounts of radioactive 36Cl exist in the environment, in a ratio of about 7x10−13 to 1 with stable isotopes. 36Cl is produced in the atmosphere by spallation of 36Ar by interactions with cosmic ray protons. In the subsurface environment, 36Cl is generated primarily as a result of neutron capture by 35Cl or muon capture by 40Ca. 36Cl decays to 36S and to 36Ar, with a combined half-life of 308,000 years. The half-life of this hydrophilic nonreactive isotope makes it suitable for geologic dating in the range of 60,000 to 1 million years. Additionally, large amounts of 36Cl were produced by irradiation of seawater during atmospheric detonations of nuclear weapons between 1952 and 1958. The residence time of 36Cl in the atmosphere is about 1 week. Thus, as an event marker of 1950s water in soil and groundwater, 36Cl is also useful for dating waters less than 50 years before the present. 36Cl has seen use in other areas of the geological sciences, including dating ice and sediments.
History.
The most common compound of chlorine, sodium chloride, has been known since ancient times; archaeologists have found evidence that rock salt was used as early as 3000 BC and brine as early as 6000 BC. Around 1630, chlorine was recognized as a gas by the Belgian chemist and physician Jan Baptist van Helmont.
Elemental chlorine was first prepared and studied in 1774 by Swedish chemist Carl Wilhelm Scheele, and, therefore, he is credited for its discovery. He called it ""dephlogisticated muriatic acid air"" since it is a gas (then called "airs") and it came from hydrochloric acid (then known as "muriatic acid"). However, he failed to establish chlorine as an element, mistakenly thinking that it was the oxide obtained from the hydrochloric acid (see phlogiston theory). He named the new element within this oxide as "muriaticum". Regardless of what he thought, Scheele did isolate chlorine by reacting MnO2 (as the mineral pyrolusite) with HCl:
Scheele observed several of the properties of chlorine: the bleaching effect on litmus, the deadly effect on insects, the yellow green color, and the smell similar to aqua regia.
At the time, common chemical theory was: any acid is a compound that contains oxygen (still sounding in the German and Dutch names of oxygen: "sauerstoff" or "zuurstof", both translating into English as "acid stuff"), so a number of chemists, including Claude Berthollet, suggested that Scheele's "dephlogisticated muriatic acid air" must be a combination of oxygen and the yet undiscovered element, "muriaticum".
In 1809, Joseph Louis Gay-Lussac and Louis-Jacques Thénard tried to decompose "dephlogisticated muriatic acid air" by reacting it with charcoal to release the free element "muriaticum" (and carbon dioxide). They did not succeed and published a report in which they considered the possibility that "dephlogisticated muriatic acid air" is an element, but were not convinced.
In 1810, Sir Humphry Davy tried the same experiment again, and concluded that it is an element, and not a compound. He named this new element as chlorine, from the Greek word χλωρος ("chlōros"), meaning green-yellow. The name halogen, meaning "salt producer," was originally used for chlorine in 1811 by Johann Salomo Christoph Schweigger. However, this term was later used as a generic term to describe all the elements in the chlorine family (fluorine, bromine, iodine), after a suggestion by Jöns Jakob Berzelius in 1842. In 1823, Michael Faraday liquefied chlorine for the first time, and demonstrated that what was then known as "solid chlorine" had a structure of chlorine hydrate (Cl2·H2O).
Chlorine gas was first used by French chemist Claude Berthollet to bleach textiles in 1785. Modern bleaches resulted from further work by Berthollet, who first produced sodium hypochlorite in 1789 in his laboratory in the town of Javel (now part of Paris, France), by passing chlorine gas through a solution of sodium carbonate. The resulting liquid, known as ""Eau de Javel"" ("Javel water"), was a weak solution of sodium hypochlorite. However, this process was not very efficient, and alternative production methods were sought. Scottish chemist and industrialist Charles Tennant first produced a solution of calcium hypochlorite ("chlorinated lime"), then solid calcium hypochlorite (bleaching powder). These compounds produced low levels of elemental chlorine, and could be more efficiently transported than sodium hypochlorite, which remained as dilute solutions because when purified to eliminate water, it became a dangerously powerful and unstable oxidizer. Near the end of the nineteenth century, E. S. Smith patented a method of sodium hypochlorite production involving electrolysis of brine to produce sodium hydroxide and chlorine gas, which then mixed to form sodium hypochlorite. This is known as the chloralkali process, first introduced on an industrial scale in 1892, and now the source of essentially all modern elemental chlorine and sodium hydroxide production (a related low-temperature electrolysis reaction, the Hooker process, is now responsible for bleach and sodium hypochlorite production).
Elemental chlorine solutions dissolved in chemically basic water (sodium and calcium hypochlorite) were first used as anti-putrification agents and disinfectants in the 1820s, in France, long before the establishment of the germ theory of disease. This work is mainly due to Antoine-Germain Labarraque, who adapted Berthollet's "Javel water" bleach and other chlorine preparations for the purpose (for a more complete history, see below). Elemental chlorine has since served a continuous function in topical antisepsis (wound irrigation solutions and the like) as well as public sanitation (especially of swimming and drinking water).
In 1826, silver chloride was used to produce photographic images for the first time. Chloroform was first used as an anesthetic in 1847.
Polyvinyl chloride (PVC) was invented in 1912, initially without a purpose.
Chlorine gas was first introduced as a weapon on April 22, 1915, at Ypres by the German Army, and the results of this weapon were disastrous because gas masks had not been mass distributed and were tricky to get on quickly.
Production.
In industry, elemental chlorine is usually produced by the electrolysis of sodium chloride dissolved in water. This method, the chloralkali process industrialized in 1892, now provides essentially all industrial chlorine gas. Along with chlorine, the method yields hydrogen gas and sodium hydroxide (with sodium hydroxide actually being the most crucial of the three industrial products produced by the process). The process proceeds according to the following chemical equation:
The electrolysis of chloride solutions all proceed according to the following equations:
Overall process: 2 NaCl (or KCl) + 2 H2O → Cl2 + H2 + 2 NaOH (or KOH)
In diaphragm cell electrolysis, an asbestos (or polymer-fiber) diaphragm separates a cathode and an anode, preventing the chlorine forming at the anode from re-mixing with the sodium hydroxide and the hydrogen formed at the cathode. The salt solution (brine) is continuously fed to the anode compartment and flows through the diaphragm to the cathode compartment, where the caustic alkali is produced and the brine is partially depleted. Diaphragm methods produce dilute and slightly impure alkali but they are not burdened with the problem of preventing mercury discharge into the environment and they are more energy efficient. Membrane cell electrolysis employ permeable membrane as an ion exchanger. Saturated sodium (or potassium) chloride solution is passed through the anode compartment, leaving at a lower concentration. This method is more efficient than the diaphragm cell and produces very pure sodium (or potassium) hydroxide at about 32% concentration, but requires very pure brine.
Laboratory methods.
Small amounts of chlorine gas can be made in the laboratory by combining hydrochloric acid and manganese dioxide. Alternatively a strong acid such as sulfuric acid or hydrochloric acid reacts with sodium hypochlorite solution to release chlorine gas but reacts with sodium chlorate to produce chlorine gas and chlorine dioxide gas as well. In the home, accidents occur when hypochlorite bleach solutions are combined with certain acidic drain-cleaners.
Applications.
Production of industrial and consumer products.
Principal applications of chlorine are in the production of a wide range of industrial and consumer products. For example, it is used in making plastics, solvents for dry cleaning and metal degreasing, textiles, agrochemicals and pharmaceuticals, insecticides, dyestuffs, household cleaning products, etc.
Many important industrial products are produced via organochlorine intermediates. Examples include polycarbonates, polyurethanes, silicones, polytetrafluoroethylene, carboxymethyl cellulose, and propylene oxide. Like the other halogens, chlorine participates in free-radical substitution reactions with hydrogen-containing organic compounds. When applied to organic substrates, reaction is often—but not invariably—non-regioselective, and, hence, may result in a mixture of isomeric products. It is often difficult to control the degree of substitution as well, so multiple substitutions are common. If the different reaction products are easily separated, e.g., by distillation, substitutive free-radical chlorination (in some cases accompanied by concurrent thermal dehydrochlorination) may be a useful synthetic route. Industrial examples of this are the production of methyl chloride, methylene chloride, chloroform, and carbon tetrachloride from methane, allyl chloride from propylene, and trichloroethylene, and tetrachloroethylene from 1,2-dichloroethane.
Quantitatively, about 63% and 18% of all elemental chlorine produced is used in the manufacture of organic and inorganic chlorine compounds, respectively. About 15,000 chlorine compounds are being used commercially. The remaining 19% is used for bleaches and disinfection products. The most significant of organic compounds in terms of production volume are 1,2-dichloroethane and vinyl chloride, intermediates in the production of PVC. Other particularly important organochlorines are methyl chloride, methylene chloride, chloroform, vinylidene chloride, trichloroethylene, perchloroethylene, allyl chloride, epichlorohydrin, chlorobenzene, dichlorobenzenes, and trichlorobenzenes. The major inorganic compounds include HCl, Cl2O, HOCl, NaClO3, chlorinated isocyanurates, AlCl3, SiCl4, SnCl4, PCl3, PCl5, POCl3, AsCl3, SbCl3, SbCl5, BiCl3, S2Cl2, SCl2, SOCI2, ClF3, ICl, ICl3, TiCl3, TiCl4, MoCl5, FeCl3, ZnCl2, etc.
Public sanitation, disinfection, and antisepsis.
Combating putrefaction.
In France (as elsewhere) there was a need to process animal guts in order to make musical instrument strings, Goldbeater's skin and other products. This was carried out in "gut factories" ("boyauderies") as an odiferous and unhealthy business. In or about 1820, the Société d'encouragement pour l'industrie nationale offered a prize for the discovery of a method, chemical or mechanical, that could be used to separate the peritoneal membrane of animal intestines without causing putrefaction. It was won by Antoine-Germain Labarraque, a 44 year-old French chemist and pharmacist who had discovered that Berthollet's chlorinated bleaching solutions (""Eau de Javel"") not only destroyed the smell of putrefaction of animal tissue decomposition, but also retarded the decomposition process itself.
Labarraque's research resulted in chlorides and hypochlorites of lime (calcium hypochlorite) and of sodium (sodium hypochlorite) being employed not only in the "boyauderies" but also for the routine disinfection and deodorisation of latrines, sewers, markets, abattoirs, anatomical theatres and morgues. They were also used, with success, in hospitals, lazarets, prisons, infirmaries (both on land and at sea), magnaneries, stables, cattle-sheds, etc.; and for exhumations, embalming, during outbreaks of epidemic illness, fever, blackleg in cattle, etc.
Against infection and contagion.
Labarraque's chlorinated lime and soda solutions have been advocated since 1828 to prevent infection (called "contagious infection", and presumed to be transmitted by "miasmas") and also to treat putrefaction of existing wounds, including septic wounds. In this 1828 work, Labarraque recommended for the doctor to breathe chlorine, wash his hands with chlorinated lime, and even sprinkle chlorinated lime about the patient's bed, in cases of "contagious infection." In 1828, it was well known that some infections were contagious, even though the agency of the microbe was not to be realized or discovered for more than half a century.
During the Paris cholera outbreak of 1832, large quantities of so-called "chloride of lime" were used to disinfect the capital. This was not simply modern calcium chloride, but contained chlorine gas dissolved in lime-water (dilute calcium hydroxide) to form calcium hypochlorite (chlorinated lime). Labarraque's discovery helped to remove the terrible stench of decay from hospitals and dissecting rooms, and, by doing so, effectively deodorised the Latin Quarter of Paris. These "putrid miasmas" were thought by many to be responsible for the spread of "contagion" and "infection" – both words used before the germ theory of infection. The use of chloride of lime was based on destruction of odors and "putrid matter." One source has claimed that chloride of lime was used by Dr. John Snow to disinfect water from the cholera-contaminated well feeding the Broad Street pump in 1854 London. Three reputable sources that described the famous Broad Street pump cholera epidemic do not mention Snow performing any disinfection of water from that well. Instead, one reference makes it clear that chloride of lime was used to disinfect the offal and filth in the streets surrounding the Broad Street pump—a common practice in mid-nineteenth century England.
Semmelweis and experiments with antisepsis.
Perhaps the most famous application of Labarraque's chlorine and chemical base solutions was in 1847, when Ignaz Semmelweis used (first) chlorine-water (simply chlorine dissolved in pure water), then cheaper chlorinated lime solutions, to deodorize the hands of Austrian doctors, which Semmelweis noticed still carried the stench of decomposition from the dissection rooms to the patient examination rooms. Semmelweis, still long before the germ theory of disease, had theorized that "cadaveric particles" were somehow transmitting decay from fresh medical cadavers to living patients, and he used the well-known "Labarraque's solutions" as the only known method to remove the smell of decay and tissue decomposition (which he found that soap did not). The solutions proved to be far more effective germicide antiseptics than soap (Semmelweis was also aware of their greater efficacy, but not the reason), and this resulted in Semmelweis's (later) celebrated success in stopping the transmission of childbed fever ("puerperal fever") in the maternity wards of Vienna General Hospital in Austria in 1847.
Much later, during World War I in 1916, a standardized and diluted modification of Labarraque's solution, containing hypochlorite (0.5%) and boric acid as an acidic stabilizer, was developed by Henry Drysdale Dakin (who gave full credit to Labarraque's prior work in this area). Called Dakin's solution, the method of wound irrigation with chlorinated solutions allowed antiseptic treatment of a wide variety of open wounds, long before the modern antibiotic era. A modified version of this solution continues to be employed in wound irrigation in the modern era, where it remains effective against multiply antibiotic resistant bacteria (see Century Pharmaceuticals).
Public sanitation.
By 1918, the US Department of Treasury called for all drinking water to be disinfected with chlorine. Chlorine is presently an important chemical for water purification (such as in water treatment plants), in disinfectants, and in bleach. Chlorine in water is more than three times as effective as a disinfectant against "Escherichia coli" than an equivalent concentration of bromine, and is more than six times more effective than an equivalent concentration of iodine.
Chlorine is usually used (in the form of hypochlorous acid) to kill bacteria and other microbes in drinking water supplies and public swimming pools. In most private swimming pools, chlorine itself is not used, but rather sodium hypochlorite, formed from chlorine and sodium hydroxide, or solid tablets of chlorinated isocyanurates. The drawback of using chlorine in swimming pools is that the chlorine reacts with the proteins in human hair and skin (see Hypochlorous acid). Once the chlorine reacts with the hair and skin, it becomes chemically bonded. Even small water supplies are now routinely chlorinated.
It is often impractical to store and use poisonous chlorine gas for water treatment, so alternative methods of adding chlorine are used. These include hypochlorite solutions, which gradually release chlorine into the water, and compounds like sodium dichloro-s-triazinetrione (dihydrate or anhydrous), sometimes referred to as "dichlor", and trichloro-s-triazinetrione, sometimes referred to as "trichlor". These compounds are stable while solid and may be used in powdered, granular, or tablet form. When added in small amounts to pool water or industrial water systems, the chlorine atoms hydrolyze from the rest of the molecule forming hypochlorous acid (HOCl), which acts as a general biocide, killing germs, micro-organisms, algae, and so on.
Use as a weapon.
World War I.
Chlorine gas, also known as bertholite, was first used as a weapon in World War I by Germany on April 22, 1915 in the Second Battle of Ypres. As described by the soldiers it had a distinctive smell of a mixture between pepper and pineapple. It also tasted metallic and stung the back of the throat and chest. Chlorine can react with water in the mucosa of the lungs to form hydrochloric acid, an irritant that can be lethal. The damage done by chlorine gas can be prevented by the activated charcoal commonly found in gas masks, or other filtration methods, which makes the overall chance of death by chlorine gas much lower than those of other chemical weapons. It was pioneered by a German scientist later to be a Nobel laureate, Fritz Haber of the Kaiser Wilhelm Institute in Berlin, in collaboration with the German chemical conglomerate IG Farben, who developed methods for discharging chlorine gas against an entrenched enemy. It is alleged that Haber's role in the use of chlorine as a deadly weapon drove his wife, Clara Immerwahr, to suicide. After its first use, chlorine was utilized by both sides as a chemical weapon, but it was soon replaced by the more deadly phosgene and mustard gas.
Iraq War.
Chlorine gas has also been used by insurgents against the local population and coalition forces in the Iraq War in the form of chlorine bombs. On March 17, 2007, for example, three chlorine-filled trucks were detonated in the Anbar province killing two and sickening over 350. Other chlorine bomb attacks resulted in higher death tolls, with more than 30 deaths on two separate occasions. Most of the deaths were caused by the force of the explosions rather than the effects of chlorine, since the toxic gas is readily dispersed and diluted in the atmosphere by the blast. The Iraqi authorities have tightened security for elemental chlorine, which is essential for providing safe drinking water to the population.
Syrian Civil War.
There have been allegations of chlorine gas attacks during the Syrian Civil War such as the 2014 Kafr Zita chemical attack.
Health effects of the free element and hazards.
Chlorine is a toxic gas that irritates the respiratory system. Because it is heavier than air, it tends to accumulate at the bottom of poorly ventilated spaces. Chlorine gas is a strong oxidizer, which may react with flammable materials.
Chlorine is detectable with measuring devices in concentrations of as low as 0.2 parts per million (ppm), and by smell at 3 ppm. Coughing and vomiting may occur at 30 ppm and lung damage at 60 ppm. About 1000 ppm can be fatal after a few deep breaths of the gas. Breathing lower concentrations can aggravate the respiratory system, and exposure to the gas can irritate the eyes.
The toxicity of chlorine comes from its oxidizing power. When chlorine is inhaled at concentrations above 30 ppm, it begins to react with water and cells, which change it into hydrochloric acid (HCl) and hypochlorous acid (HClO).
When used at specified levels for water disinfection, the reaction of chlorine with water is not a major concern for human health. Other materials present in the water may generate disinfection by-products that are associated with negative effects on human health, however, the health risk is far lower than drinking undisinfected water.
Chlorine induced cracking in structural materials.
The element is widely used for purifying water owing to its powerful oxidizing properties, especially potable water supplies and water used in swimming pools. Several catastrophic collapses of swimming pool ceilings have occurred owing to chlorine induced stress corrosion cracking of stainless steel rods used to suspend them. Some polymers are also sensitive to attack, including acetal resin and polybutene. Both materials were used in hot and cold water domestic supplies, and stress corrosion cracking caused widespread failures in the USA in the 1980s and 1990s. The picture on the right shows an acetal joint in a water supply system, which, when it fractured, caused substantial physical damage to computers in the labs below the supply. The cracks started at injection molding defects in the joint and slowly grew until finally triggered. The fracture surface shows iron and calcium salts that were deposited in the leaking joint from the water supply before failure.
Chlorine-iron fire.
The element iron can combine with chlorine at high temperatures in a strong exothermic reaction, creating a "chlorine-iron fire". Chlorine-iron fires are a risk in chemical process plants, where much of the pipework used to carry chlorine gas is made of steel.
Organochlorine compounds as pollutants.
Some organochlorine compounds are serious pollutants. These are produced either as by-products or end products of industrial processes which are persistent in the environment, such as certain chlorinated pesticides and chlorofluorocarbons. Chlorine is added both to pesticides and pharmaceuticals to make the molecules more resistant to enzymatic degradation by bacteria, insects, and mammals, but this property also has the effect of prolonging the residence time of these compounds when they enter the environment. In this respect chlorinated organics have some resemblance to fluorinated organics.

</doc>
<doc id="5668" url="http://en.wikipedia.org/wiki?curid=5668" title="Calcium">
Calcium

Calcium is the chemical element with symbol Ca and atomic number 20. Calcium is a soft gray alkaline earth metal, and is the fifth-most-abundant element by mass in the Earth's crust. Calcium is also the fifth-most-abundant dissolved ion in seawater by both molarity and mass, after sodium, chloride, magnesium, and sulfate.
Calcium is essential for living organisms, in particular in cell physiology, where movement of the calcium ion Ca2+ into and out of the cytoplasm functions as a signal for many cellular processes. As a major material used in mineralization of bone, teeth and shells, calcium is the most abundant metal by mass in many animals.
Notable characteristics.
In chemical terms, calcium is reactive and soft for a metal (though harder than lead, it can be cut with a knife with difficulty). It is a silvery metallic element that must be extracted by electrolysis from a fused salt like calcium chloride. Once produced, it rapidly forms a gray-white oxide and nitride coating when exposed to air. In bulk form (typically as chips or "turnings"), the metal is somewhat difficult to ignite, more so even than magnesium chips; but, when lit, the metal burns in air with a brilliant high-intensity orange-red light. Calcium metal reacts with water, generating hydrogen gas at a rate rapid enough to be noticeable, but not fast enough at room temperature to generate much heat, making it useful for generating hydrogen. In powdered form, however, the reaction with water is extremely rapid, as the increased surface area of the powder accelerates the reaction with the water. Part of the reason for the slowness of the calcium–water reaction is a result of the metal being partly protected by insoluble white calcium hydroxide. In water solutions of acids, where this salt is soluble, calcium reacts vigorously.
With a density of 1.55 g/cm3, calcium is the lightest of the alkaline earth metals; magnesium (specific gravity 1.74) and beryllium (1.84) are denser though lighter in atomic mass. From strontium onward, the alkali earth metals become denser with increasing atomic mass.
Calcium has two allotropes.
Calcium has a higher electrical resistivity than copper or aluminium, yet weight-for-weight, due to its much lower density, it is a rather better conductor than either. However, its use in terrestrial applications is usually limited by its high reactivity with air.
Calcium salts are colorless from any contribution of the calcium, and ionic solutions of calcium (Ca2+) are colorless as well. As with magnesium salts and other alkaline earth metal salts, calcium salts are often quite soluble in water. Notable exceptions include calcium hydroxide, calcium sulfate (unusual for sulfate salts), calcium carbonate and tricalcium phosphate. With the exception of calcium sulfate, even the insoluble calcium salts listed are in general more soluble than the transition metal counterparts. When in solution, the calcium ion varies remarkably to the human taste, being reported as mildly salty, sour, "mineral-like" or even "soothing." It is apparent that many animals can taste, or develop a taste, for calcium, and use this sense to detect the mineral in salt licks or other sources. In human nutrition, soluble calcium salts may be added to tart juices without much effect to the average palate.
Calcium is the fifth-most-abundant element by mass in the human body, where it is an important cellular ionic messenger with many functions. Calcium also serves as a structural element in bone. It is the relatively high-atomic-number calcium in the skeleton that causes bone to be radio-opaque. Of the human body's solid components after drying and burning of organics (as for example, after cremation), about a third of the total "mineral" mass remaining is the approximately one kilogram of calcium that composes the average skeleton (the remainder being mostly phosphorus and oxygen).
H and K lines.
Visible spectra of many stars, including the Sun, exhibit strong absorption lines of singly ionized calcium. Prominent among these are the H-line at 3968.5 Å and the K line at 3933.7 Å of singly ionized calcium, or Ca II. When observing the Sun, or stars with low temperatures, the prominence of the H and K lines in the visible spectra can be an indication of strong magnetic activity in the chromosphere. Measurement of periodic variations of these active regions can also be used to deduce the rotation periods of these stars.
Compounds.
Calcium, combined with phosphate, forming hydroxylapatite, is the mineral portion of human and animal bones and teeth. The mineral portion of some corals can also be transformed into hydroxylapatite.
Calcium hydroxide (slaked lime) is used in many chemical refinery processes and is made by heating limestone at high temperature (above 825 °C) and then carefully adding water to it. When lime is mixed with sand, it hardens into a mortar and is turned into plaster by carbon dioxide uptake. Mixed with other compounds, lime forms an important part of Portland cement.
Calcium carbonate (CaCO3) is one of the common compounds of calcium. It is heated to form quicklime (CaO), which is then added to water (H2O). This forms another material known as slaked lime (Ca(OH)2), which is an inexpensive base material used throughout the chemical industry. Chalk, marble, and limestone are all forms of calcium carbonate.
When water percolates through limestone or other soluble carbonate rocks, it partially dissolves the rock and causes cave formation with their characteristic stalactites and stalagmites, and also forms hard water. Other important calcium compounds are calcium nitrate, calcium sulfide, calcium chloride, calcium carbide, calcium cyanamide and calcium hypochlorite.
A few calcium compounds where calcium is in the oxidation state +1 have also been investigated recently.
Isotopes.
Calcium has five stable isotopes (40Ca, 42Ca, 43Ca, 44Ca and 46Ca), plus one more isotope (48Ca) that has such a long half-life that for all practical purposes it can also be considered stable. The 20% range in relative mass among naturally occurring calcium isotopes is greater than for any other element except hydrogen and helium. Calcium also has a cosmogenic isotope, radioactive 41Ca, which has a half-life of 103,000 years. Unlike cosmogenic isotopes that are produced in the atmosphere, 41Ca is produced by neutron activation of 40Ca. Most of its production is in the upper metre or so of the soil column, where the cosmogenic neutron flux is still sufficiently strong. 41Ca has received much attention in stellar studies because it decays to 41K, a critical indicator of solar-system anomalies.
Ninety-seven percent of naturally occurring calcium is in the form of 40Ca. 40Ca is one of the daughter products of 40K decay, along with 40Ar. While K–Ar dating has been used extensively in the geological sciences, the prevalence of 40Ca in nature has impeded its use in dating. Techniques using mass spectrometry and a double spike isotope dilution have been used for K-Ca age dating.
The most abundant isotope, 40Ca, has a nucleus of 20 protons and 20 neutrons. This is the heaviest stable isotope of any element that has equal numbers of protons and neutrons. In supernova explosions, calcium is formed from the reaction of carbon with various numbers of alpha particles (helium nuclei), until the most common calcium isotope (containing 10 helium nuclei) has been synthesized.
Isotope fractionation.
As with the isotopes of other elements, a variety of processes fractionate, or alter the relative abundance of, calcium isotopes. The best studied of these processes is the mass-dependent fractionation of calcium isotopes that accompanies the precipitation of calcium minerals, such as calcite, aragonite and apatite, from solution. Isotopically light calcium is preferentially incorporated into minerals, leaving the solution from which the mineral precipitated enriched in isotopically heavy calcium. At room temperature the magnitude of this fractionation is roughly 0.25‰ (0.025%) per atomic mass unit (AMU). Mass-dependent differences in calcium isotope composition conventionally are expressed the ratio of two isotopes (usually 44Ca/40Ca) in a sample compared to the same ratio in a standard reference material. 44Ca/40Ca varies by about 1% among common earth materials.
Calcium isotope fractionation during mineral formation has led to several applications of calcium isotopes. In particular, the 1997 observation by Skulan and DePaolo that calcium minerals are isotopically lighter than the solutions from which the minerals precipitate is the basis of analogous applications in medicine and in paleooceanography. In animals with skeletons mineralized with calcium the calcium isotopic composition of soft tissues reflects the relative rate of formation and dissolution of skeletal mineral. In humans changes in the calcium isotopic composition of urine have been shown to be related to changes in bone mineral balance. When the rate of bone formation exceeds the rate of bone resorption, soft tissue 44Ca/40Ca rises. Soft tissue 44Ca/40Ca falls when bone resorption exceeds bone formation. Because of this relationship, calcium isotopic measurements of urine or blood may be useful in the early detection of metabolic bone diseases like osteoporosis.
A similar system exists in the ocean, where seawater 44Ca/40Ca tends to rise when the rate of removal of Ca2+ from seawater by mineral precipitation exceeds the input of new calcium into the ocean, and fall when calcium input exceeds mineral precipitation. It follows that rising 44Ca/40Ca corresponds to falling seawater Ca2+ concentration, and falling 44Ca/40Ca corresponds to rising seawater Ca2+ concentration. In 1997 Skulan and DePaolo presented the first evidence of change in seawater 44Ca/40Ca over geologic time, along with a theoretical explanation of these changes. More recent papers have confirmed this observation, demonstrating that seawater Ca2+ concentration is not constant, and that the ocean probably never is in “steady state” with respect to its calcium input and output. This has important climatological implications, as the marine calcium cycle is closely tied to the carbon cycle (see below).
Geochemical cycling.
Calcium provides an important link between tectonics, climate and the carbon cycle. In the simplest terms, uplift of mountains exposes Ca-bearing rocks to chemical weathering and releases Ca2+ into surface water. This Ca2+ eventually is transported to the ocean where it reacts with dissolved CO2 to form limestone. Some of this limestone settles to the sea floor where it is incorporated into new rocks. Dissolved CO2, along with carbonate and bicarbonate ions, are referred to as dissolved inorganic carbon (DIC).
The actual reaction is more complicated and involves the bicarbonate ion (HCO3−) that forms when CO2 reacts with water at seawater pH:
Note that at ocean pH most of the CO2 produced in this reaction is immediately converted back into . The reaction results in a net transport of one molecule of CO2 from the ocean/atmosphere into the lithosphere.
The result is that each Ca2+ ion released by chemical weathering ultimately removes one CO2 molecule from the surficial system (atmosphere, ocean, soils and living organisms), storing it in carbonate rocks where it is likely to stay for hundreds of millions of years. The weathering of calcium from rocks thus scrubs CO2 from the ocean and atmosphere, exerting a strong long-term effect on climate.
Analogous cycles involving magnesium, and to a much smaller extent strontium and barium, have the same effect.
As the weathering of limestone (CaCO3) liberates equimolar amounts of Ca2+ and CO2, it has no net effect on the CO2 content of the atmosphere and ocean. The weathering of silicate rocks like granite, on the other hand, is a net CO2 sink because it produces abundant Ca2+ but very little CO2.
History.
Lime as building material was used since prehistoric times going as far back as 7000 to 14000 BC. Significant statues made from lime plaster date back into the 7 millennia BC were found in 'Ain Ghazal. The first dated lime kiln dates back to 2500 BC and was found in Khafajah mesopotamia. Calcium (from Latin , genitive "calcis", meaning "lime") was known as early as the first century when the Ancient Romans prepared lime as calcium oxide. Literature dating back to 975 AD notes that plaster of paris (calcium sulfate), is useful for setting broken bones. It was not isolated until 1808 in England when Sir Humphry Davy electrolyzed a mixture of lime and mercuric oxide. Davy was trying to isolate calcium; when he heard that Swedish chemist Jöns Jakob Berzelius and Pontin prepared calcium amalgam by electrolyzing lime in mercury, he tried it himself. He worked with electrolysis throughout his life and also discovered/isolated sodium, potassium, magnesium, boron and barium. Calcium metal was not available in large scale until the beginning of the 20th century.
Occurrence.
Calcium is not naturally found in its elemental state. Calcium occurs most commonly in sedimentary rocks in the minerals calcite, dolomite and gypsum. It also occurs in igneous and metamorphic rocks chiefly in the silicate minerals: plagioclases, amphiboles, pyroxenes and garnets.
Applications.
Calcium is used
Nutrition.
Calcium is an important component of a healthy diet and a mineral necessary for life. The National Osteoporosis Foundation says, "Calcium plays an important role in building stronger, denser bones early in life and keeping bones strong and healthy later in life." Approximately 99 percent of the body's calcium is stored in the bones and teeth. The rest of the calcium in the body has other important uses, such as some exocytosis, especially neurotransmitter release, and muscle contraction. In the electrical conduction system of the heart, calcium replaces sodium as the mineral that depolarizes the cell, proliferating the action potential. In cardiac muscle, sodium influx commences an action potential, but during potassium efflux, the cardiac myocyte experiences calcium influx, prolonging the action potential and creating a plateau phase of dynamic equilibrium. Long-term calcium deficiency can lead to rickets and poor blood clotting and in case of a menopausal woman, it can lead to osteoporosis, in which the bone deteriorates and there is an increased risk of fractures. While a lifelong deficit can affect bone and tooth formation, over-retention can cause hypercalcemia (elevated levels of calcium in the blood), impaired kidney function and decreased absorption of other minerals. Several sources suggest a correlation between high calcium intake (2000 mg per day, or twice the U.S. recommended daily allowance, equivalent to six or more glasses of milk per day) and prostate cancer. High calcium intakes or high calcium absorption were previously thought to contribute to the development of kidney stones. However, a high calcium intake has been associated with a lower risk for kidney stones in more recent research. Vitamin D is needed to absorb calcium.
Dairy products, such as milk and cheese, are a well-known source of calcium. Some individuals are allergic to dairy products and even more people, in particular those of non Indo-European descent, are lactose-intolerant, leaving them unable to consume non-fermented dairy products in quantities larger than about half a liter per serving. Others, such as vegans, avoid dairy products for ethical and health reasons.
Many good vegetable sources of calcium exist, including seaweeds such as kelp, wakame and hijiki; nuts and seeds like almonds, hazelnuts, sesame, and pistachio; blackstrap molasses; beans (especially soy beans); figs; quinoa; okra; rutabaga; broccoli; dandelion leaves; and kale. In addition, several foods and drinks, such as orange juice, soy milk, tofu, breakfast cereals, and breads are often fortified with calcium.
Numerous vegetables, notably spinach, chard and rhubarb have a high calcium content, but they may also contain varying amounts of oxalic acid that binds calcium and reduces its absorption.
The same problem may to a degree affect the absorption of calcium from amaranth, collard greens, and chicory greens.
This process may also be related to the generation of calcium oxalate.
An overlooked source of calcium is eggshell, which can be ground into a powder and mixed into food or a glass of water.
The calcium content of most foods can be found in the USDA National Nutrient Database.
Dietary supplements.
Calcium supplements are used to prevent and to treat calcium deficiencies. Office of Dietary Supplements (National Institutes of Health) recommends that no more than 600 mg of supplement should be taken at a time because the percent of calcium absorbed decreases as the amount of calcium in the supplement increases. It is therefore recommended to spread doses throughout the day. Recommended daily calcium intake for adults ranges from 1000 to 1300 mg. Calcium supplements may have side effects such as bloating and constipation in some people. It is suggested that taking the supplements with food may aid in nullifying these side effects.
Vitamin D is added to some calcium supplements. Proper vitamin D status is important because vitamin D is converted to a hormone in the body, which then induces the synthesis of intestinal proteins responsible for calcium absorption.
Cardiovascular impact.
A study investigating the effects of personal calcium supplement use on cardiovascular risk in the Women’s Health Initiative Calcium/Vitamin D Supplementation Study (WHI CaD Study) found a modestly increased risk of cardiovascular events, particularly myocardial infarction in postmenopausal women. A broad recommendation of calcium/vitamin D supplements is therefore not warranted. In contrast, the authors of a 2013 literature review concluded that the benefits of calcium supplementation, such as on bone health, appear to outweigh any risk calcium supplementation may theoretically pose to the cardiovascular health.
Osteoporosis.
Such studies often do not test calcium alone, but rather combinations of calcium and vitamin D. Randomized controlled trials found both positive and negative effects. The different results may be explained by doses of calcium and underlying rates of calcium supplementation in the control groups.
Cancer.
A meta-analysis by the international Cochrane Collaboration of two randomized controlled trials found that calcium "might contribute to a moderate degree to the prevention of adenomatous colonic polyps".
More recent studies were conflicting, and one that was positive for effect (Lappe, et al.) did control for a possible anti-carcinogenic effect of vitamin D, which was found to be an independent positive influence from calcium-alone on cancer risk (see second study below).
Hazards and toxicity.
Compared with other metals, the calcium ion and most calcium compounds have low toxicity. This is not surprising given the very high natural abundance of calcium compounds in the environment and in organisms. Calcium poses few serious environmental problems, with kidney stones the most common side-effect in clinical studies. Acute calcium poisoning is rare, and difficult to achieve unless calcium compounds are administered intravenously. For example, the oral median lethal dose (LD50) for rats for calcium carbonate and calcium chloride are 6.45 and 1.4 g/kg, respectively.
Calcium metal is hazardous because of its sometimes-violent reactions with water and acids. Calcium metal is found in some drain cleaners, where it functions to generate heat and calcium hydroxide that saponifies the fats and liquefies the proteins (e.g., hair) that block drains. When swallowed calcium metal has the same effect on the mouth, esophagus and stomach, and can be fatal.
Excessive consumption of calcium carbonate antacids/dietary supplements (such as Tums) over a period of weeks or months can cause milk-alkali syndrome, with symptoms ranging from hypercalcemia to potentially fatal renal failure. What constitutes “excessive” consumption is not well known and, it is presumed, varies a great deal from person to person. Persons consuming more than 10 grams/day of CaCO3 (=4 g Ca) are at risk of developing milk-alkali syndrome, but the condition has been reported in at least one person consuming only 2.5 grams/day of CaCO3 (=1 g Ca), an amount usually considered moderate and safe.
Oral calcium supplements diminish the absorption of thyroxine when taken within four to six hours of each other. Thus, people taking both calcium and thyroxine run the risk of inadequate thyroid hormone replacement and thence hypothyroidism if they take them simultaneously or near-simultaneously.
Excessive calcium supplementation can be detrimental to cardiovascular health, especially in men.

</doc>
<doc id="5669" url="http://en.wikipedia.org/wiki?curid=5669" title="Chromium">
Chromium

Chromium is a chemical element which has the symbol Cr and atomic number 24. It is the first element in Group 6. It is a steely-gray, lustrous, hard and brittle metal which takes a high polish, resists tarnishing, and has a high melting point. The name of the element is derived from the Greek word χρῶμα, "chrōma", meaning colour, because many of its compounds are intensely coloured.
Chromium oxide was used by the Chinese in the Qin dynasty over 2,000 years ago to coat metal weapons found with the Terracotta Army. Chromium was discovered as an element after it came to the attention of the western world in the red crystalline mineral crocoite (lead(II) chromate), discovered in 1761 and initially used as a pigment. Louis Nicolas Vauquelin first isolated chromium metal from this mineral in 1797. Since Vauquelin's first production of metallic chromium, small amounts of native (free) chromium metal have been discovered in rare minerals, but these are not used commercially. Instead, nearly all chromium is commercially extracted from the single commercially viable ore chromite, which is iron chromium oxide (FeCr2O4). Chromite is also now the chief source of chromium for chromium pigments.
Chromium metal and ferrochromium alloy are commercially produced from chromite by silicothermic or aluminothermic reactions, or by roasting and leaching processes. Chromium metal has proven of high value due to its high corrosion resistance and hardness. A major development was the discovery that steel could be made highly resistant to corrosion and discoloration by adding metallic chromium to form stainless steel. This application, along with chrome plating (electroplating with chromium) currently comprise 85% of the commercial use for the element, with applications for chromium compounds forming the remainder.
Trivalent chromium (Cr(III)) ion is possibly required in trace amounts for sugar and lipid metabolism, although the issue remains in debate. In larger amounts and in different forms, chromium can be toxic and carcinogenic. The most prominent example of toxic chromium is hexavalent chromium (Cr(VI)). Abandoned chromium production sites often require environmental cleanup.
Characteristics.
Physical.
Chromium is remarkable for its magnetic properties: it is the only elemental solid which shows antiferromagnetic ordering at room temperature (and below). Above 38 °C, it transforms into a paramagnetic state.
Passivation.
Chromium metal left standing in air is passivated by oxygen, forming a thin protective oxide surface layer. This layer is a spinel structure only a few atoms thick. It is very dense, and prevents the diffusion of oxygen into the underlying material. This barrier is in contrast to iron or plain carbon steels, where the oxygen migrates into the underlying material and causes rusting. The passivation can be enhanced by short contact with oxidizing acids like nitric acid. Passivated chromium is stable against acids. The opposite effect can be achieved by treatment with a strong reducing agent that destroys the protective oxide layer on the metal. Chromium metal treated in this way readily dissolves in weak acids.
Chromium, unlike metals such as iron and nickel, does not suffer from hydrogen embrittlement. However, it does suffer from nitrogen embrittlement, reacting with nitrogen from air and forming brittle nitrides at the high temperatures necessary to work the metal parts.
Occurrence.
Chromium is the 22nd most abundant element in Earth's crust with an average concentration of 100 ppm. Chromium compounds are found in the environment, due to erosion of chromium-containing rocks and can be distributed by volcanic eruptions. The concentrations range in soil is between 1 and 300 mg/kg, in sea water 5 to 800 µg/liter, and in rivers and lakes 26 µg/liter to 5.2 mg/liter.
Chromium is mined as chromite (FeCr2O4) ore. About two-fifths of the chromite ores and concentrates in the world are produced in South Africa, while Kazakhstan, India, Russia, and Turkey are also substantial producers. Untapped chromite deposits are plentiful, but geographically concentrated in Kazakhstan and southern Africa.
Although rare, deposits of native chromium exist. The Udachnaya Pipe in Russia produces samples of the native metal. This mine is a kimberlite pipe, rich in diamonds, and the reducing environment helped produce both elemental chromium and diamond.
The relation between Cr(III) and Cr(VI) strongly depends on pH and oxidative properties of the location, but in most cases, the Cr(III) is the dominating species, although in some areas the ground water can contain up to 39 µg/liter of total chromium of which 30 µg/liter is present as Cr(VI).
Isotopes.
Naturally occurring chromium is composed of three stable isotopes; 52Cr, 53Cr and 54Cr with 52Cr being the most abundant (83.789% natural abundance). 19 radioisotopes have been characterized with the most stable being 50Cr with a half-life of (more than) 1.8 years, and 51Cr with a half-life of 27.7 days. All of the remaining radioactive isotopes have half-lives that are less than 24 hours and the majority of these have half-lives that are less than 1 minute. This element also has 2 meta states.
53Cr is the radiogenic decay product of 53Mn. Chromium isotopic contents are typically combined with manganese isotopic contents and have found application in isotope geology. Mn-Cr isotope ratios reinforce the evidence from 26Al and 107Pd for the early history of the solar system. Variations in 53Cr/52Cr and Mn/Cr ratios from several meteorites indicate an initial 53Mn/55Mn ratio that suggests Mn-Cr isotopic composition must result from in-situ decay of 53Mn in differentiated planetary bodies. Hence 53Cr provides additional evidence for nucleosynthetic processes immediately before coalescence of the solar system.
The isotopes of chromium range in atomic mass from 43 u (43Cr) to 67 u (67Cr). The primary decay mode before the most abundant stable isotope, 52Cr, is electron capture and the primary mode after is beta decay. 53Cr has been posited as a proxy for atmospheric oxygen concentration.
Compounds.
Chromium is a member of the transition metals, in group 6. Chromium(0) has an electronic configuration of 4s13d5, owing to the lower energy of the high spin configuration. Chromium exhibits a wide range of possible oxidation states, where the +3 state is most stable energetically; the +3 and +6 states are most commonly observed in chromium compounds, whereas the +1, +4 and +5 states are rare.
The following is the Pourbaix diagram for chromium in pure water, perchloric acid or sodium hydroxide:
Chromium(III).
A large number of chromium(III) compounds are known. Chromium(III) can be obtained by dissolving elemental chromium in acids like hydrochloric acid or sulfuric acid. The ion has a similar radius (63 pm) to the ion (radius 50 pm), so they can replace each other in some compounds, such as in chrome alum and alum. When a trace amount of replaces in corundum (aluminium oxide, Al2O3), the red-colored ruby is formed.
Chromium(III) ions tend to form octahedral complexes. The colors of these complexes is determined by the ligands attached to the Cr centre. The commercially available chromium(III) chloride hydrate is the dark green complex Closely related compounds have different colours: pale green [CrCl(H2O)5Cl2 and the violet [Cr(H2O)6]Cl3. If water-free green chromium(III) chloride is dissolved in water then the green solution turns violet after some time, due to the substitution of water by chloride in the inner coordination sphere. This kind of reaction is also observed with solutions of chrome alum and other water-soluble chromium(III) salts.
Chromium(III) hydroxide (Cr(OH)3) is amphoteric, dissolving in acidic solutions to form [Cr(H2O)6]3+, and in basic solutions to form . It is dehydrated by heating to form the green chromium(III) oxide (Cr2O3), which is the stable oxide with a crystal structure identical to that of corundum.
Chromium(VI).
Chromium(VI) compounds are powerful oxidants at low or neutral pH. Most important are chromate anion () and dichromate (Cr2O72-) anions, which exist in equilibrium:
Chromium(VI) halides are known also and include the hexafluoride CrF6 and chromyl chloride ().
Sodium chromate is produced industrially by the oxidative roasting of chromite ore with calcium or sodium carbonate. The dominant species is therefore, by the law of mass action, determined by the pH of the solution. The change in equilibrium is visible by a change from yellow (chromate) to orange (dichromate), such as when an acid is added to a neutral solution of potassium chromate. At yet lower pH values, further condensation to more complex oxyanions of chromium is possible.
Both the chromate and dichromate anions are strong oxidizing reagents at low pH:
They are, however, only moderately oxidizing at high pH:
Chromium(VI) compounds in solution can be detected by adding an acidic hydrogen peroxide solution. The unstable dark blue chromium(VI) peroxide (CrO5) is formed, which can be stabilized as an ether adduct .
Chromic acid has the hypothetical formula . It is a vaguely described chemical, despite many well-defined chromates and dichromates being known. The dark red chromium(VI) oxide , the acid anhydride of chromic acid, is sold industrially as "chromic acid". It can be produced by mixing sulfuric acid with dichromate, and is a strong oxidizing agent.
Chromium(V) and chromium(IV).
The oxidation state +5 is only realized in few compounds but are intermediates in many reactions involving oxidations by chromate. The only binary compound is the volatile chromium(V) fluoride (CrF5). This red solid has a melting point of 30 °C and a boiling point of 117 °C. It can be synthesized by treating chromium metal with fluorine at 400 °C and 200 bar pressure. The peroxochromate(V) is another example of the +5 oxidation state. Potassium peroxochromate (K3[Cr(O2)4]) is made by reacting potassium chromate with hydrogen peroxide at low temperatures. This red brown compound is stable at room temperature but decomposes spontaneously at 150–170 °C.
Compounds of chromium(IV) (in the +4 oxidation state) are slightly more common than those of chromium(V). The tetrahalides, CrF4, CrCl4, and CrBr4, can be produced by treating the trihalides () with the corresponding halogen at elevated temperatures. Such compounds are susceptible to disproportionation reactions and are not stable in water.
Chromium(II).
Many chromium(II) compounds are known, including the water-stable chromium(II) chloride, , which can be made by reduction of chromium(III) chloride with zinc. The resulting bright blue solution is only stable at neutral pH. Many chromous carboxylates are also known, most famously, the red chromous acetate (Cr2(O2CCH3)4), which features a quadruple bond.
Chromium(I).
Most Cr(I) compounds are obtained by oxidation of electron-rich, octahedral Cr(0) complexes. Other Cr(I) complexes contain cyclopentadienyl ligands. As verified by X-ray diffraction, a Cr-Cr quintuple bond (length 183.51(4)  pm) has also been described. Extremely bulky monodentate ligands stabilize this compound by shielding the quintuple bond from further reactions.
Chromium(0).
Many chromium(0) compounds are known. Most are derivatives of chromium hexacarbonyl or bis(benzene)chromium.
History.
Weapons found in burial pits dating from the late 3rd century B.C. Qin Dynasty of the Terracotta Army near Xi'an, China have been analyzed by archaeologists. Although buried more than 2,000 years ago, the ancient bronze tips of crossbow bolts and swords found at the site showed unexpectedly little corrosion, possibly because the bronze was deliberately coated with a thin layer of chromium oxide. However, this oxide layer was not chromium metal or chrome plating as we know it.
Chromium minerals as pigments came to the attention of the west in the 18th century. On 26 July 1761, Johann Gottlob Lehmann found an orange-red mineral in the Beryozovskoye mines in the Ural Mountains which he named "Siberian red lead". Though misidentified as a lead compound with selenium and iron components, the mineral was in fact crocoite ("lead chromate") with a formula of PbCrO4.
In 1770, Peter Simon Pallas visited the same site as Lehmann and found a red lead mineral that had useful properties as a pigment in paints. The use of Siberian red lead as a paint pigment then developed rapidly. A bright yellow pigment made from crocoite also became fashionable.
In 1797, Louis Nicolas Vauquelin received samples of crocoite ore. He produced chromium trioxide (CrO3) by mixing crocoite with hydrochloric acid. In 1798, Vauquelin discovered that he could isolate metallic chromium by heating the oxide in a charcoal oven, making him the discoverer of the element. Vauquelin was also able to detect traces of chromium in precious gemstones, such as ruby or emerald.
During the 1800s, chromium was primarily used as a component of paints and in tanning salts. At first, crocoite from Russia was the main source, but in 1827, a larger chromite deposit was discovered near Baltimore, United States. This made the United States the largest producer of chromium products till 1848 when large deposits of chromite were found near Bursa, Turkey.
Chromium is also known for its luster when polished. It is used as a protective and decorative coating on car parts, plumbing fixtures, furniture parts and many other items, usually applied by electroplating. Chromium was used for electroplating as early as 1848, but this use only became widespread with the development of an improved process in 1924.
Metal alloys now account for 85% of the use of chromium. The remainder is used in the chemical industry and refractory and foundry industries.
Production.
Approximately 23.3 million metric tons (Mt) of marketable chromite ore were produced in 2011, and converted into 9.5 Mt of ferrochromium. According to John F. Papp, writing for the USGS, "Ferrochromium is the leading end use of chromite ore, stainless steel is the leading end use of ferrochromium."
The largest producers of chromium ore have been South Africa (44%) India (18%), Kazakhstan (16%) Zimbabwe (5%), Finland (4%) Iran (4%) and Brazil (2%) with several other countries producing the rest of less than 10% of the world production.
The two main products of chromium ore refining are ferrochromium and metallic chromium. For those products the ore smelter process differs considerably. For the production of ferrochromium, the chromite ore (FeCr2O4) is reduced in large scale in electric arc furnace or in smaller smelters with either aluminium or silicon in an aluminothermic reaction.
For the production of pure chromium, the iron has to be separated from the chromium in a two step roasting and leaching process. The chromite ore is heated with a mixture of calcium carbonate and sodium carbonate in the presence of air. The chromium is oxidized to the hexavalent form, while the iron forms the stable Fe2O3. The subsequent leaching at higher elevated temperatures dissolves the chromates and leaves the insoluble iron oxide. The chromate is converted by sulfuric acid into the dichromate.
The dichromate is converted to the chromium(III) oxide by reduction with carbon and then reduced in an aluminothermic reaction to chromium.
Applications.
Metallurgy.
The strengthening effect of forming stable metal carbides at the grain boundaries and the strong increase in corrosion resistance made chromium an important alloying material for steel. The high-speed tool steels contain between 3 and 5% chromium. Stainless steel, the main corrosion-proof metal alloy, is formed when chromium is added to iron in sufficient concentrations, usually above 11%. For its formation, ferrochromium is added to the molten iron. Also nickel-based alloys increase in strength due to the formation of discrete, stable metal carbide particles at the grain boundaries. For example, Inconel 718 contains 18.6% chromium. Because of the excellent high-temperature properties of these nickel superalloys, they are used in jet engines and gas turbines in lieu of common structural materials.
The relative high hardness and corrosion resistance of unalloyed chromium makes it a good surface coating, being still the most "popular" metal coating with unparalleled combined durability. A thin layer of chromium is deposited on pretreated metallic surfaces by electroplating techniques. There are two deposition methods: Thin, below 1 µm thickness, layers are deposited by chrome plating, and are used for decorative surfaces. If wear-resistant surfaces are needed then thicker chromium layers are deposited. Both methods normally use acidic chromate or dichromate solutions. To prevent the energy-consuming change in oxidation state, the use of chromium(III) sulfate is under development, but for most applications, the established process is used.
In the chromate conversion coating process, the strong oxidative properties of chromates are used to deposit a protective oxide layer on metals like aluminium, zinc and cadmium. This passivation and the self-healing properties by the chromate stored in the chromate conversion coating, which is able to migrate to local defects, are the benefits of this coating method. Because of environmental and health regulations on chromates, alternative coating method are under development.
Anodizing of aluminium is another electrochemical process, which does not lead to the deposition of chromium, but uses chromic acid as electrolyte in the solution. During anodization, an oxide layer is formed on the aluminium. The use of chromic acid, instead of the normally used sulfuric acid, leads to a slight difference of these oxide layers.
The high toxicity of Cr(VI) compounds, used in the established chromium electroplating process, and the strengthening of safety and environmental regulations demand a search for substitutes for chromium or at least a change to less toxic chromium(III) compounds.
Dye and pigment.
The mineral crocoite (lead chromate PbCrO4) was used as a yellow pigment shortly after its discovery. After a synthesis method became available starting from the more abundant chromite, chrome yellow was, together with cadmium yellow, one of the most used yellow pigments. The pigment does not photodegrade, but it tends to darken due to the formation of chromium(III) oxide. It has a strong color, and was used for school buses in the US and for Postal Service (for example Deutsche Post) in Europe. The use of chrome yellow declined due to environmental and safety concerns and was replaced by organic pigments or alternatives free from lead and chromium. Other pigments based on chromium are, for example, the bright red pigment chrome red, which is a basic lead chromate (PbCrO4·Pb(OH)2). A very important chromate pigment, which was used widely in metal primer formulations, was zinc chromate, now replaced by zinc phosphate. A wash primer was formulated to replace the dangerous practice of pretreating aluminium aircraft bodies with a phosphoric acid solution. This used zinc tetroxychromate dispersed in a solution of polyvinyl butyral. An 8% solution of phosphoric acid in solvent was added just before application. It was found that an easily oxidized alcohol was an essential ingredient. A thin layer of about 10–15 µm was applied, which turned from yellow to dark green when it was cured. There is still a question as to the correct mechanism. Chrome green is a mixture of Prussian blue and chrome yellow, while the chrome oxide green is chromium(III) oxide.
Chromium oxides are also used as a green color in glassmaking and as a glaze in ceramics. Green chromium oxide is extremely light-fast and as such is used in cladding coatings. It is also the main ingredient in IR reflecting paints, used by the armed forces, to paint vehicles, to give them the same IR reflectance as green leaves.
Synthetic ruby and the first laser.
Natural rubies are corundum (aluminum oxide) crystals that are colored red (the rarest type) due to chromium (III) ions (other colors of corundum gems are termed sapphires). A red-colored artificial ruby may also be achieved by doping chromium(III) into artificial corundum crystals, thus making chromium a requirement for making synthetic rubies. Such a synthetic ruby crystal was the basis for the first laser, produced in 1960, which relied on stimulated emission of light from the chromium atoms in such a crystal.
Wood preservative.
Because of their toxicity, chromium(VI) salts are used for the preservation of wood. For example, chromated copper arsenate (CCA) is used in timber treatment to protect wood from decay fungi, wood attacking insects, including termites, and marine borers. The formulations contain chromium based on the oxide CrO3 between 35.3% and 65.5%. In the United States, 65,300 metric tons of CCA solution have been used in 1996.
Tanning.
Chromium(III) salts, especially chrome alum and chromium(III) sulfate, are used in the tanning of leather. The chromium(III) stabilizes the leather by cross linking the collagen fibers. Chromium tanned leather can contain between 4 and 5% of chromium, which is tightly bound to the proteins. Although the form of chromium used for tanning is not the toxic hexavalent variety, there remains interest in management of chromium in the tanning industry such as recovery and reuse, direct/indirect recycling, use of less chromium or "chrome-less" tanning are practiced to better manage chromium in tanning.
Refractory material.
The high heat resistivity and high melting point makes chromite and chromium(III) oxide a material for high temperature refractory applications, like blast furnaces, cement kilns, molds for the firing of bricks and as foundry sands for the casting of metals. In these applications, the refractory materials are made from mixtures of chromite and magnesite. The use is declining because of the environmental regulations due to the possibility of the formation of chromium(VI). 
Catalysts.
Several chromium compounds are used as catalysts for processing hydrocarbons. For example the Phillips catalysts for the production of polyethylene are mixtures of chromium and silicon dioxide or mixtures of chromium and titanium and aluminium oxide. Fe-Cr mixed oxides are employed as high-temperature catalysts for the water gas shift reaction. Copper chromite is a useful hydrogenation catalyst.
Biological role.
Trivalent chromium (Cr(III) or Cr3+) occurs in trace amounts in foods and waters, and appears to be benign. In contrast, hexavalent chromium (Cr(VI) or Cr6+) is very toxic and mutagenic when inhaled. Cr(VI) has not been established as a carcinogen when in solution, although it may cause allergic contact dermatitis (ACD).
Chromium deficiency, involving a lack of Cr(III) in the body, or perhaps some complex of it, such as glucose tolerance factor is controversial, or is at least extremely rare. Chromium has no verified biological role and has been classified by some as "not" essential for mammals. However, other reviews have regarded it as an essential trace element in humans. 
Chromium deficiency has been attributed to only three people on long-term parenteral nutrition, which is when a patient is fed a liquid diet through intravenous drips for long periods of time.
Although no biological role for chromium has ever been demonstrated, dietary supplements for chromium include chromium(III) picolinate, chromium(III) polynicotinate, and related materials. The benefit of those supplements is questioned by some studies. The use of chromium-containing dietary supplements is controversial, owing to the absence of any verified biological role, the expense of these supplements, and the complex effects of their use. The popular dietary supplement chromium picolinate complex generates chromosome damage in hamster cells (due to the picolinate ligand). In the United States the dietary guidelines for daily chromium uptake were lowered in 2001 from 50–200 µg for an adult to 35 µg (adult male) and to 25 µg (adult female).
No comprehensive, reliable database of chromium content of food currently exists. Data reported prior to 1980 is unreliable due to analytical error. Chromium content of food varies widely due to differences in soil mineral content, growing season, plant cultivar, and contamination during processing. In addition, large amounts of chromium (and nickel) leech into food cooked in stainless steel.
Precautions.
Water insoluble chromium(III) compounds and chromium metal are not considered a health hazard, while the toxicity and carcinogenic properties of chromium(VI) have been known for a long time. Because of the specific transport mechanisms, only limited amounts of chromium(III) enter the cells. Several "in vitro" studies indicated that high concentrations of chromium(III) in the cell can lead to DNA damage. Acute oral toxicity ranges between 1.5 and 3.3 mg/kg. The proposed beneficial effects of chromium(III) and the use as dietary supplements yielded some controversial results, but recent reviews suggest that moderate uptake of chromium(III) through dietary supplements poses no risk.
Cr(VI).
The acute oral toxicity for chromium(VI) ranges between 50 and 150 µg/kg. In the body, chromium(VI) is reduced by several mechanisms to chromium(III) already in the blood before it enters the cells. The chromium(III) is excreted from the body, whereas the chromate ion is transferred into the cell by a transport mechanism, by which also sulfate and phosphate ions enter the cell. The acute toxicity of chromium(VI) is due to its strong oxidational properties. After it reaches the blood stream, it damages the kidneys, the liver and blood cells through oxidation reactions. Hemolysis, renal and liver failure are the results of these damages. Aggressive dialysis can improve the situation.
The carcinogenity of chromate dust is known for a long time, and in 1890 the first publication described the elevated cancer risk of workers in a chromate dye company. Three mechanisms have been proposed to describe the genotoxicity of chromium(VI). The first mechanism includes highly reactive hydroxyl radicals and other reactive radicals which are by products of the reduction of chromium(VI) to chromium(III). The second process includes the direct binding of chromium(V), produced by reduction in the cell, and chromium(IV) compounds to the DNA. The last mechanism attributed the genotoxicity to the binding to the DNA of the end product of the chromium(III) reduction.
Chromium salts (chromates) are also the cause of allergic reactions in some people. Chromates are often used to manufacture, amongst other things, leather products, paints, cement, mortar and anti-corrosives. Contact with products containing chromates can lead to allergic contact dermatitis and irritant dermatitis, resulting in ulceration of the skin, sometimes referred to as "chrome ulcers". This condition is often found in workers that have been exposed to strong chromate solutions in electroplating, tanning and chrome-producing manufacturers.
Environmental issues.
As chromium compounds were used in dyes and paints and the tanning of leather, these compounds are often found in soil and groundwater at abandoned industrial sites, now needing environmental cleanup and remediation per the treatment of brownfield land. Primer paint containing hexavalent chromium is still widely used for aerospace and automobile refinishing applications.
In 2010, the Environmental Working Group studied the drinking water in 35 American cities. The study was the first nationwide analysis measuring the presence of the chemical in U.S. water systems. The study found measurable hexavalent chromium in the tap water of 31 of the cities sampled, with Norman, Oklahoma, at the top of list; 25 cities had levels that exceeded California's proposed limit.
Note: Concentrations of Cr(VI) in US municipal drinking water supplies reported by EWG are within likely, natural background levels for the areas tested and not necessarily indicative of industrial pollution (CalEPA Fact Sheet), as asserted by EWG. This factor was not taken into consideration in their report.

</doc>
<doc id="5671" url="http://en.wikipedia.org/wiki?curid=5671" title="Cymbal">
Cymbal

Cymbals are a common percussion instrument. Cymbals consist of thin, normally round plates of various alloys; see cymbal making for a discussion of their manufacture. The majority of cymbals are of indefinite pitch, although small disc-shaped cymbals based on ancient designs sound a definite note (see: crotales). Cymbals are used in many ensembles ranging from the orchestra, percussion ensembles, jazz bands, heavy metal bands, and marching groups. Drum kits usually incorporate at least a crash, ride or crash/ride, and a pair of hi-hat cymbals.
Etymology.
The word cymbal is derived from the Latin "cymbalum", which is the latinisation of the Greek word κύμβαλον ("kumbalon"), "cymbal", which in turn derives from κύμβος ("kumbos"), "cup".
Anatomy.
The anatomy of the cymbal plays a large part in the sound it creates. A hole is drilled in the center of the cymbal and it is used to either mount the cymbal on a stand or straps (for hand playing). The bell, dome, or cup is the raised section immediately surrounding the hole. The bell produces a higher "pinging" pitch than the rest of the cymbal. The bow is the rest of the surface surrounding the bell. The bow is sometimes described in two areas: the ride and crash area. The ride area is the thicker section closer to the bell while the crash area is the thinner tapering section near the edge. The edge or rim is the immediate circumference of the cymbal.
Cymbals are measured by their diameter often in inches or centimeters. The size of the cymbal affects its sound, larger cymbals usually being louder and having longer sustain. The weight describes how thick the cymbal is. Cymbal weights are important to the sound they produce and how they play. Heavier cymbals have a louder volume, more cut, and better stick articulation (when using drum sticks). Thin cymbals have a fuller sound, lower pitch, and faster response.
The profile of the cymbal is the vertical distance of the bow from the bottom of the bell to the cymbal edge (higher profile cymbals are more bowl shaped). The profile affects the pitch of the cymbal: higher profile cymbals have higher pitch.
Types.
Orchestral cymbals.
Cymbals offer a composer nearly endless amounts of color and effect. Their unique timbre allows them to project even against a full orchestra and through the heaviest of orchestrations and enhance articulation and nearly any dynamic. Cymbals have been utilized historically to suggest frenzy, fury or bacchanalian revels, as seen in the Venus music in Wagner's "Tannhäuser", Grieg's "Peer Gynt suite", and Osmin's aria "O wie will ich triumphieren" from Mozart's "Die Entführung aus dem Serail".
Clash cymbals.
Orchestral clash cymbals are traditionally used in pairs, each one having a strap set in the bell of the cymbal by which they are held. Such a pair is also known as crash cymbals or plates.
The sound can be obtained by rubbing their edges together in a sliding movement for a "sizzle", striking them against each other in what is called a "clash", tapping the edge of one against the body of the other in what is called a "tap-clash", scraping the edge of one from the inside of the bell to the edge for a "scrape" or "zischen," or shutting the cymbals together and choking the sound in what is called a "hi-hat chick" or crush. A skilled player can obtain an enormous dynamic range from such a pair of cymbals. For example, in Beethoven's ninth symphony, the percussionist is employed to first play cymbals at pianissimo, adding a touch of colour rather than loud clash.
Clash cymbals are usually damped by pressing them against the player's body. A composer may write "laissez vibrer", "Let vibrate" (usually abbreviated l.v.), "secco" (dry), or equivalent indications on the score; more usually, the player must judge exactly when to damp the cymbals based on the written duration of clash and the context in which it occurs.
Clash cymbals have traditionally been accompanied by the bass drum playing an identical part. This combination, played loudly, is an effective way to accentuate a note since the two instruments together contribute to both very low and very high frequency ranges and provide a satisfying "clash-bang-wallop". In older music the composer sometimes provided just one part for this pair of instruments, writing "senza piatti" or "piatti soli" () if the bass drum is to remain silent. This came from the common practice of only having one percussionist play both instruments, using one cymbal mounted to the shell of the bass drum itself. The player would clash the cymbals with his left hand and use a mallet to strike the bass drum in his right. This method is often employed today in pit orchestras and is called for specifically by composers who desire a certain effect. Stravinsky calls for this in his ballet Petrushka and Mahler calls for this in his Titan Symphony.
The modern convention is for the instruments to have independent parts. However in kit drumming, a cymbal crash is still most often accompanied by a simultaneous kick to the bass drum, which provides both a musical effect and a support to the crash stroke.
Hi hats.
Crash cymbals evolved into the low-sock and from this to the modern hi-hat. Even in a modern drum kit, they remain paired with the bass drum as the two instruments which are played with the player's feet. However, hi-hat cymbals tend to be heavy with little taper, more similar to a ride cymbal than to a clash cymbal as found in a drum kit, and perform a ride rather than a clash function.
Suspended cymbal.
Another use of cymbals is the suspended cymbal. This instrument takes its name from the traditional method of suspending the cymbal by means of a leather strap or rope, thus allowing the cymbal to vibrate as freely as possible for maximum musical effect. Early jazz drumming pioneers borrowed this style of cymbal mounting during the early 1900s and later drummers further developed this instrument into the mounted horizontal or nearly horizontally mounted "crash" cymbals of a modern drum kit, However, most modern drum kits do not employ a leather strap suspension system. Many modern drum kits use a mount with felt or otherwise dampening fabric to act as a barrier to hold the cymbals between metal clamps: thus forming the modern day ride cymbal.
Suspended cymbals can be played with yarn, sponge or cord wrapped mallets. The first known instance of using a sponge-headed mallet on a cymbal is the final chord of Hector Berlioz' Symphonie Fantastique. Composers sometimes specifically request other types of mallets like felt mallets or timpani beaters for different attack and sustain qualities.
Suspended cymbals can produce bright and slicing tones when forcefully struck, and give an eerie transparent "windy" sound when played quietly. A tremolo, or roll (played with two mallets alternately striking on opposing sides of the cymbal) can build in volume from almost inaudible to an overwhelming climax in a satisfyingly smooth manner (as in Humperdink's Mother Goose Suite).
The edge of a suspended cymbal may be hit with shoulder of a drum stick to obtain a sound somewhat akin to that of a pair of clash cymbals. Other methods of playing include scraping a coin or a triangle beater rapidly across the ridges on the top of the cymbal, giving a "zing" sound (as some players do in the fourth movement of Dvořák's Symphony No. 9). Other effects that can be used include drawing a cello or bass bow across the edge of the cymbal for a sound not unlike squealing car brakes.
Ancient cymbals.
Ancient cymbals or tuned cymbals are much more rarely called for. Their timbre is entirely different, more like that of small hand-bells or of the notes of the keyed harmonica. They are not struck full against each other, but by one of their edges, and the note given in by them is higher in proportion as they are thicker and smaller. Berlioz's "Romeo and Juliet" calls for two pairs of cymbals, modelled on some old Pompeian instruments no larger than the hand (some are no larger than a crown piece), and tuned to F and B flat. The modern instruments descended from this line are the crotales.
List of cymbal types.
Cymbal types include:

</doc>
<doc id="5672" url="http://en.wikipedia.org/wiki?curid=5672" title="Cadmium">
Cadmium

Cadmium is a chemical element with the symbol Cd and atomic number 48. This soft, bluish-white metal is chemically similar to the two other stable metals in group 12, zinc and mercury. Like zinc, it prefers oxidation state +2 in most of its compounds and like mercury it shows a low melting point compared to transition metals. Cadmium and its congeners are not always considered transition metals, in that they do not have partly filled d or f electron shells in the elemental or common oxidation states. The average concentration of cadmium in the Earth's crust is between 0.1 and 0.5 parts per million (ppm). It was discovered in 1817 simultaneously by Stromeyer and Hermann, both in Germany, as an impurity in zinc carbonate.
Cadmium occurs as a minor component in most zinc ores and therefore is a byproduct of zinc production. It was used for a long time as a pigment and for corrosion resistant plating on steel while cadmium compounds were used to stabilize plastic. The use of cadmium is generally decreasing due to its toxicity (it is specifically listed in the European Restriction of Hazardous Substances ) and the replacement of nickel-cadmium batteries with nickel-metal hydride and lithium-ion batteries. One of its few new uses is in cadmium telluride solar panels. 
Although cadmium has no known biological function in higher organisms, a cadmium-dependent carbonic anhydrase has been found in marine diatoms.
Characteristics.
Physical properties.
Cadmium is a soft, malleable, ductile, bluish-white divalent metal. It is similar in many respects to zinc but forms complex compounds. Unlike other metals, cadmium is resistant to corrosion and as a result it is used as a protective layer when deposited on other metals. As a bulk metal, cadmium is insoluble in water and is not flammable; however, in its powdered form it may burn and release toxic fumes.
Chemical properties.
Although cadmium usually has an oxidation state of +2, it also exists in the +1 state. Cadmium and its congeners are not always considered transition metals, in that they do not have partly filled d or f electron shells in the elemental or common oxidation states. Cadmium burns in air to form brown amorphous cadmium oxide (CdO); the crystalline form of this compound is a dark red which changes color when heated, similar to zinc oxide. Hydrochloric acid, sulfuric acid and nitric acid dissolve cadmium by forming cadmium chloride (CdCl2), cadmium sulfate (CdSO4), or cadmium nitrate (Cd(NO3)2). The oxidation state +1 can be reached by dissolving cadmium in a mixture of cadmium chloride and aluminium chloride, forming the Cd22+ cation, which is similar to the Hg22+ cation in mercury(I) chloride.
The structures of many cadmium complexes with nucleobases, amino acids and vitamins have been determined.
Cadmium-selective sensors.
Cadmium-selective sensors, based on the fluorophore BODIPY have been developed for imaging and sensing of cadmium in cells
Isotopes.
Naturally occurring cadmium is composed of 8 isotopes. Two of them are naturally radioactive, and three are expected to decay but have not been experimentally confirmed to do so. The two natural radioactive isotopes are 113Cd (beta decay, half-life is 7.7 × 1015 years) and 116Cd (two-neutrino double beta decay, half-life is 2.9 × 1019 years). The other three are 106Cd, 108Cd (both double electron capture), and 114Cd (double beta decay); only lower limits on their half-life times have been set. At least three isotopes – 110Cd, 111Cd, and 112Cd – are stable. Among the isotopes that do not occur naturally, the most long-lived are 109Cd with a half-life of 462.6 days, and 115Cd with a half-life of 53.46 hours. All of the remaining radioactive isotopes have half-lives that are less than 2.5 hours, and the majority of these have half-lives that are less than 5 minutes. Cadmium has 8 known meta states, with the most stable being 113mCd (t½ = 14.1 years), 115mCd (t½ = 44.6 days), and 117mCd (t½ = 3.36 hours).
The known isotopes of cadmium range in atomic mass from 94.950 u (95Cd) to 131.946 u (132Cd). For isotopes lighter than 112 u, the primary decay mode is electron capture and the dominant decay product is element 47 (silver). Heavier isotopes decay mostly through beta emission producing element 49 (indium).
One isotope of cadmium, 113Cd, absorbs neutrons with very high probability if they have an energy below the "cadmium cut-off" and transmits them otherwise. The cadmium cut-off is about 0.5 eV. Neutrons with energy below the cut-off are deemed slow neutrons, distinguishing them from intermediate and fast neutrons.
Cadmium is created via the long s-process in low-medium mass stars with masses of 0.6 to 10 solar masses, which lasts thousands of years. It requires a silver atom to capture a neutron and then undergo beta decay.
History.
Cadmium (Latin "cadmia", Greek "καδμεία" meaning "calamine", a cadmium-bearing mixture of minerals, which was named after the Greek mythological character Κάδμος, Cadmus, the founder of Thebes) was discovered simultaneously in 1817 by Friedrich Stromeyer and Karl Samuel Leberecht Hermann, both in Germany, as an impurity in zinc carbonate. Stromeyer found the new element as an impurity in zinc carbonate (calamine), and, for 100 years, Germany remained the only important producer of the metal. The metal was named after the Latin word for calamine, since the metal was found in this zinc compound. Stromeyer noted that some impure samples of calamine changed color when heated but pure calamine did not. He was persistent in studying these results and eventually isolated cadmium metal by roasting and reduction of the sulfide. The possibility to use cadmium yellow as pigment was recognized in the 1840s but the lack of cadmium limited this application.
Even though cadmium and its compounds may be toxic in certain forms and concentrations, the British Pharmaceutical Codex from 1907 states that cadmium iodide was used as a medication to treat "enlarged joints, scrofulous glands, and chilblains".
In 1927, the International Conference on Weights and Measures redefined the meter in terms of a red cadmium spectral line (1 m = 1,553,164.13 wavelengths). This definition has since been changed (see krypton).
After the industrial scale production of cadmium started in the 1930s and 1940s, the major application of cadmium was the coating of iron and steel to prevent corrosion; in 1944, 62% and in 1956, 59% of the cadmium in the United States was for coating. In 1956, 24% of the cadmium used within the United States was used for the second application, which was for red, orange and yellow pigments based on sulfides and selenides of cadmium. The stabilizing effect of cadmium-containing chemicals like the carboxylates cadmium laureate and cadmium stearate on PVC led to an increased use of those compounds in the 1970s and 1980s. The use of cadmium in applications such as pigments, coatings, stabilizers and alloys declined due to environmental and health regulations in the 1980s and 1990s; in 2006, only 7% of total cadmium consumption was used for plating and coating and only 10% was used for pigments.
The decrease in consumption in other applications was made up by a growing demand of cadmium in nickel-cadmium batteries, which accounted for 81% of the cadmium consumption in the United States in 2006.
Occurrence.
Cadmium makes up about 0.1 ppm of the Earth's crust. Compared with the more abundant 65 ppm zinc, cadmium is rare. No significant deposits of cadmium-containing ores are known. Greenockite (CdS), the only cadmium mineral of importance, is nearly always associated with sphalerite (ZnS). This association is caused by the geochemical similarity between zinc and cadmium which makes geological separation unlikely. As a consequence, cadmium is produced mainly as a byproduct from mining, smelting, and refining sulfidic ores of zinc, and, to a lesser degree, lead and copper. Small amounts of cadmium, about 10% of consumption, are produced from secondary sources, mainly from dust generated by recycling iron and steel scrap. Production in the United States began in 1907, but it was not until after World War I that cadmium came into wide use. One place where metallic cadmium can be found is the Vilyuy River basin in Siberia.
Rocks mined to produce phosphate fertilizers contain varying amounts of cadmium, leading to a cadmium concentration of up to 300 mg/kg in the produced phosphate fertilizers and thus in the high cadmium content in agricultural soils. Coal can contain significant amounts of cadmium, which ends up mostly in the flue dust.
Production.
The British Geological Survey reports that in 2001, China was the top producer of cadmium, producing almost one-sixth of the world share, closely followed by South Korea and Japan.
Cadmium is a common impurity in zinc ores, and it is most often isolated during the production of zinc. Some zinc ores concentrates from sulfidic zinc ores contain up to 1.4% of cadmium. In 1970s, the output of cadmium was 6.5 pounds per ton of zinc. Zinc sulfide ores are roasted in the presence of oxygen, converting the zinc sulfide to the oxide. Zinc metal is produced either by smelting the oxide with carbon or by electrolysis in sulfuric acid. Cadmium is isolated from the zinc metal by vacuum distillation if the zinc is smelted, or cadmium sulfate is precipitated out of the electrolysis solution.
Applications.
Cadmium has many common industrial uses as it is a key component in battery production, is present in cadmium pigments, coatings, and is commonly used in electroplating.
Batteries.
In 2009, 86% of cadmium was used in batteries, predominantly in rechargeable nickel-cadmium batteries. Nickel-cadmium cells have a nominal cell potential of 1.2 V. The cell consists of a positive nickel hydroxide electrode and a negative cadmium electrode plate separated by an alkaline electrolyte (potassium hydroxide). The European Union set the allowed use of cadmium in electronics in 2004 to limits of 0.01%, with several exceptions, but reduced the allowed content of cadmium in batteries to 0.002%.
Electroplating.
Cadmium electroplating, consuming 6% of the global production, can be found in the aircraft industry due to the ability to resist corrosion when applied to steel components. This coating is passivated by the usage of chromate salts. A limitation of cadmium plating is hydrogen embrittlement of high-strength steels caused by the electroplating process. Therefore, steel parts heat-treated to tensile strength above 1300 MPa (200 ksi) should be coated by an alternative method (such as special low-embrittlement cadmium electroplating processes or physical vapor deposition). In addition, titanium embrittlement caused by cadmium-plated tool residues resulted in banishment of these tools (along with routine tool testing programs to detect any cadmium contamination) from the A-12/SR-71 and U-2 programs, and subsequent aircraft programs using titanium.
Nuclear fission.
Cadmium is used as a barrier to control neutrons in nuclear fission. The pressurized water reactor designed by Westinghouse Electric Company uses an alloy consisting of 80% silver, 15% indium, and 5% cadmium.
Compounds.
Cadmium oxide is used in black and white television phosphors and in the blue and green phosphors for color television picture tubes. Cadmium sulfide (CdS) is used as a photoconductive surface coating for photocopier drums.
In paint pigments, cadmium forms various salts, with CdS being the most common. This sulfide is used as a yellow pigment. Cadmium selenide can be used as red pigment, commonly called "cadmium red". To painters who work with the pigment, cadmium yellows, oranges, and reds are the most brilliant and long-lasting colors to use. In fact, during production, these colors are significantly toned down before they are ground with oils and binders, or blended into watercolors, gouaches, acrylics, and other paint and pigment formulations. Since these pigments are potentially toxic, it is recommended to use a barrier cream on the hands to prevent absorption through the skin when working with them even though the amount of cadmium absorbed into the body through the skin is usually reported to be less than 1%.
In PVC, cadmium was used as heat, light, and weathering stabilizers. Currently, cadmium stabilizers have been completely replaced with barium-zinc, calcium-zinc and organo-tin stabilizers. Cadmium is used in many kinds of solder and bearing alloys, due to a low coefficient of friction and fatigue resistance. It is also found in some of the lowest-melting alloys, such as Wood's metal.
Laboratory uses.
Helium–cadmium lasers are a common source of blue-ultraviolet laser light. They operate at either 325 or 422 nm and are used in fluorescence microscopes and various laboratory experiments. Cadmium selenide quantum dots emit bright luminescence under UV excitation (He-Cd laser, for example). The color of this luminescence can be green, yellow or red depending on the particle size. Colloidal solutions of those particles are used for imaging of biological tissues and solutions with a fluorescence microscope.
Cadmium is a component of some compound semiconductors, such as cadmium sulfide, cadmium selenide, and cadmium telluride, which can be used for light detection or solar cells. HgCdTe is sensitive to infrared light and therefore may be utilized as an infrared detector or switch for example in remote control devices.
In molecular biology, cadmium is used to block voltage-dependent calcium channels from fluxing calcium ions, as well as in hypoxia research to stimulate proteasome-dependent degradation of Hif-1α.
Biological role.
Cadmium has no known useful role in higher organisms, but a cadmium-dependent carbonic anhydrase has been found in some marine diatoms. The diatoms live in environments with very low zinc concentrations and cadmium performs the function normally carried out by zinc in other anhydrases. The discovery was made using X-ray absorption fluorescence spectroscopy (XAFS).
The highest concentration of cadmium has been found to be absorbed in the kidneys of humans, and up to about 30 mg of cadmium is commonly inhaled throughout childhood and adolescence. Cadmium can be used to block calcium channels in chicken neurons.
Analytical methods for the determination of cadmium in biological samples have been reviewed.
Environment.
The biogeochemistry of cadmium and its release to the environment has been the subject of review, as has the speciation of cadmium in the environment. 
Safety.
The bioinorganic aspects of cadmium toxicity have been reviewed.
The most dangerous form of occupational exposure to cadmium is inhalation of fine dust and fumes, or ingestion of highly soluble cadmium compounds. Inhalation of cadmium-containing fumes can result initially in metal fume fever but may progress to chemical pneumonitis, pulmonary edema, and death.
Cadmium is also an environmental hazard. Human exposures to environmental cadmium are primarily the result of fossil fuel combustion, phosphate fertilizers, natural sources, iron and steel production, cement production and related activities, nonferrous metals production, and municipal solid waste incineration. Bread, root crops, and vegetables also contribute to the cadmium in modern populations. There have been a few instances of general population toxicity as the result of long-term exposure to cadmium in contaminated food and water, and research is ongoing regarding the estrogen mimicry that may induce breast cancer. In the decades leading up to World War II, mining operations contaminated the Jinzū River in Japan with cadmium and traces of other toxic metals. As a consequence, cadmium accumulated in the rice crops growing along the riverbanks downstream of the mines. Some members of the local agricultural communities consuming the contaminated rice developed itai-itai disease and renal abnormalities, including proteinuria and glucosuria.
The victims of this poisoning were almost exclusively post-menopausal women with low iron and other mineral body stores. Similar general population cadmium exposures in other parts of the world have not resulted in the same health problems because the populations maintained sufficient iron and other mineral levels. Thus, while cadmium is a major factor in the itai-itai disease in Japan, most researchers have concluded that it was one of several factors. Cadmium is one of six substances banned by the European Union's Restriction on Hazardous Substances (RoHS) directive, which bans certain hazardous substances in electrical and electronic equipment but allows for certain exemptions and exclusions from the scope of the law.
The International Agency for Research on Cancer has classified cadmium and cadmium compounds as carcinogenic to humans. Although occupational exposure to cadmium is linked to lung and prostate cancer, there is still a substantial controversy about the carcinogenicity of cadmium in low, environmental exposure. Recent data from epidemiological studies suggest that intake of cadmium through diet associates to higher risk of endometrial, breast and prostate cancer as well as to osteoporosis in humans. 
Tobacco smoking is the most important single source of cadmium exposure in the general population. It has been estimated that about 10% of the cadmium content of a cigarette is inhaled through smoking. The absorption of cadmium from the lungs is much more effective than that from the gut, and as much as 50% of the cadmium inhaled via cigarette smoke may be absorbed.
On average, smokers have 4–5 times higher blood cadmium concentrations and 2–3 times higher kidney cadmium concentrations than non-smokers. Despite the high cadmium content in cigarette smoke, there seems to be little exposure to cadmium from passive smoking. No significant effect on blood cadmium concentrations has been detected in children exposed to environmental tobacco smoke.
Cadmium exposure is a risk factor associated with early atherosclerosis and hypertension, which can both lead to cardiovascular disease.
Regulations.
Due to the adverse effects on the environment and human health, the supply and use of cadmium is restricted in Europe under the REACH Regulation.
Product recalls.
In May 2006, a sale of the seats from Arsenal F.C.'s old stadium, Highbury in London, England was cancelled after the seats were discovered to contain trace amounts of cadmium. Reports of high levels of cadmium use in children's jewelry in 2010 led to a US Consumer Product Safety Commission investigation. The U.S. CPSC issued specific recall notices for cadmium content applying to jewelry sold by Claire's and Wal-Mart stores. In June 2010, McDonald's voluntarily recalled more than 12 million promotional "Shrek Forever After 3D" Collectable Drinking Glasses owing to concerns over cadmium levels in paint pigments used on the glassware. The glasses were manufactured by Arc International, of Millville, NJ, USA.

</doc>
<doc id="5675" url="http://en.wikipedia.org/wiki?curid=5675" title="Curium">
Curium

Curium is a transuranic radioactive chemical element with the symbol Cm and atomic number 96. This element of the actinide series was named after Marie and Pierre Curie – both were known for their research on radioactivity. Curium was first intentionally produced and identified in July 1944 by the group of Glenn T. Seaborg at the University of California, Berkeley. The discovery was kept secret and only released to the public in November 1945. Most curium is produced by bombarding uranium or plutonium with neutrons in nuclear reactors – one tonne of spent nuclear fuel contains about 20 grams of curium.
Curium is a hard, dense, silvery metal with a relatively high melting point and boiling point for an actinide. Whereas it is paramagnetic at ambient conditions, it becomes antiferromagnetic upon cooling, and other magnetic transitions are also observed for many curium compounds. In compounds, curium usually exhibits valence +3 and sometimes +4, and the +3 valence is predominant in solutions. Curium readily oxidizes, and its oxides are a dominant form of this element. It forms strongly fluorescent complexes with various organic compounds, but there is no evidence of its incorporation into bacteria and archaea. When introduced into the human body, curium accumulates in the bones, lungs and liver, where it promotes cancer.
All known isotopes of curium are radioactive and have a small critical mass for a sustained nuclear chain reaction. They predominantly emit α-particles, and the heat released in this process can potentially produce electricity in radioisotope thermoelectric generators. This application is hindered by the scarcity, high cost and radioactivity of curium isotopes. Curium is used in production of heavier actinides and of the 238Pu radionuclide for power sources in artificial pacemakers. It served as the α-source in the alpha particle X-ray spectrometers installed on the Sojourner, Mars, Mars 96, Athena, Spirit and Opportunity rovers as well as the Mars Science Laboratory to analyze the composition and structure of the rocks on the surface of Mars and the Moon. Such a spectrometer will also be used by the Philae lander of the Rosetta spacecraft to probe the surface of the 67P/Churyumov-Gerasimenko comet.
History.
Although curium had likely been produced in previous nuclear experiments, it was first intentionally synthesized, isolated and identified in 1944, at the University of California, Berkeley, by Glenn T. Seaborg, Ralph A. James, and Albert Ghiorso. In their experiments, they used a cyclotron.
Curium was chemically identified at the Metallurgical Laboratory (now Argonne National Laboratory) at the University of Chicago. It was the third transuranium element to be discovered even though it is the fourth in the series – the lighter element americium was unknown at the time.
The sample was prepared as follows: first plutonium nitrate solution was coated on a platinum foil of about 0.5 cm2 area, the solution was evaporated and the residue was converted into plutonium dioxide (PuO2) by annealing. Following cyclotron irradiation of the oxide, the coating was dissolved with nitric acid and then precipitated as the hydroxide using concentrated aqueous ammonia solution. The residue was dissolved in perchloric acid, and further separation was carried out by ion exchange to yield a certain isotope of curium. The separation of curium and americium was so painstaking that the Berkeley group initially called those elements "pandemonium" (from Greek for "all demons" or "hell") and "delirium" (from Latin for "madness").
The curium-242 isotope was produced in July–August 1944 by bombarding 239Pu with α-particles to produce curium with the release of a neutron:
Curium-242 was unambiguously identified by the characteristic energy of the α-particles emitted during the decay:
The half-life of this alpha decay was first measured as 150 days and then corrected to 162.8 days.
Another isotope 240Cm was produced in a similar reaction in March 1945:
The half-life of the 240Cm α-decay was correctly determined as 26.7 days.
The discovery of curium, as well as americium, in 1944 was closely related to the Manhattan Project, the results were confidential and declassified only in 1945. Seaborg leaked the synthesis of the elements 95 and 96 on the U.S. radio show for children, the Quiz Kids, five days before the official presentation at an American Chemical Society meeting on November 11, 1945, when one of the listeners asked whether any new transuranium element beside plutonium and neptunium had been discovered during the war. The discovery of curium (242Cm and 240Cm), their production and compounds were later patented listing only Seaborg as the inventor.
The new element was named after Marie Skłodowska-Curie and her husband Pierre Curie who are noted for discovering radium and for their work in radioactivity. It followed the example of gadolinium, a lanthanide element above curium in the periodic table, which was named after the explorer of the rare earth elements Johan Gadolin:
The first curium samples were barely visible, and were identified by their radioactivity. Louis Werner and Isadore Perlman created the first substantial sample of 30 µg curium-242 hydroxide at the University of California in 1947 by bombarding americium-241 with neutrons. Macroscopic amounts of curium fluoride were obtained in 1950 by W. W. T. Crane, J. C. Wallmann and B. B. Cunningham. Its magnetic susceptibility was very close to that of GdF3 providing the first experimental evidence for the +3 valence of curium in its compounds. Curium metal was produced only in 1951 by reduction of curium fluoride with barium.
Characteristics.
Physical.
A synthetic, radioactive element, curium is a hard dense metal with silvery-white appearance and physical and chemical properties resembling those of gadolinium. Its melting point of 1340 °C is significantly higher than that of the previous transuranic elements neptunium (637 °C), plutonium (639 °C) and americium (1173 °C). In comparison, gadolinium melts at 1312 °C. The boiling point of curium is 3110 °C. With a density of 13.52 g/cm3, curium is significantly lighter than neptunium (20.45 g/cm3) and plutonium (19.8 g/cm3), but is heavier than most other metals. Between two crystalline forms of curium, the α-Cm is more stable at ambient conditions. It has a hexagonal symmetry, space group P63/mmc, lattice parameters "a" = 365 pm and "c" = 1182 pm, and four formula units per unit cell. The crystal consists of a double-hexagonal close packing with the layer sequence ABAC and so is isotypic with α-lanthanum. At pressures above 23 GPa, at room temperature, α-Cm transforms into β-Cm, which has a face-centered cubic symmetry, space group Fmm and the lattice constant "a" = 493 pm. Upon further compression to 43 GPa, curium transforms to an orthorhombic γ-Cm structure similar to that of α-uranium, with no further transitions observed up to 52 GPa. These three curium phases are also referred to as Cm I, II and III.
Curium has peculiar magnetic properties. Whereas its neighbor element americium shows no deviation from Curie-Weiss paramagnetism in the entire temperature range, α-Cm transforms to an antiferromagnetic state upon cooling to 65–52 K, and β-Cm exhibits a ferrimagnetic transition at about 205 K. Meanwhile, curium pnictides show ferromagnetic transitions upon cooling: 244CmN and 244CmAs at 109 K, 248CmP at 73 K and 248CmSb at 162 K. Similarly, the lanthanide analogue of curium, gadolinium, as well as its pnictides also show magnetic transitions upon cooling, but the transition character is somewhat different: Gd and GdN become ferromagnetic, and GdP, GdAs and GdSb show antiferromagnetic ordering.
In accordance with magnetic data, electrical resistivity of curium increases with temperature – about twice between 4 and 60 K – and then remains nearly constant up to room temperature. There is a significant increase in resistvity over time (about 10 µΩ·cm/h) due to self-damage of the crystal lattice by alpha radiation. This makes uncertain the absolute resistivity value for curium (about 125 µΩ·cm). The resistivity of curium is similar to that of gadolinium and of the actinides plutonium and neptunium, but is significantly higher than that of americium, uranium, polonium and thorium.
Under ultraviolet illumination, curium(III) ions exhibit strong and stable yellow-orange fluorescence with a maximum in the range about 590–640 nm depending on their environment. The fluorescence originates from the transitions from the first excited state 6D7/2 and the ground state 8S7/2. Analysis of this fluorescence allows monitoring interactions between Cm(III) ions in organic and inorganic complexes.
Chemical.
Curium ions in solution almost exclusively assume the oxidation state of +3, which is the most stable oxidation state for curium. The +4 oxidation state is observed mainly in a few solid phases, such as CmO2 and CmF4. Aqueous curium(IV) is only known in the presence of strong oxidizers such as potassium persulfate, and is easily reduced to curium(III) by radiolysis and even by water. The chemical behavior of curium is different from the actinides thorium and uranium, and is similar to that of americium and many lanthanides. In aqueous solution, the Cm3+ ion is colorless to pale green, and Cm4+ ion is pale yellow. The optical absorption of Cm3+ ions contains three sharp peaks at 375.4, 381.2 and 396.5 nanometers and their strength can be directly converted into the concentration of the ions. The +6 oxidation state has only been reported once in solution in 1978, as the curyl ion (): this was prepared from the beta decay of americium-242 in the americium(V) ion . Failure to obtain Cm(VI) from oxidation of Cm(III) and Cm(IV) may be due to the high Cm4+/Cm3+ ionization potential and the instability of Cm(V).
Curium ions are hard Lewis acids and thus form most stable complexes with hard bases. The bonding is mostly ionic, with a small covalent component. Curium in its complexes commonly exhibits a 9-fold coordination environment, within a tricapped trigonal prismatic geometry.
Isotopes.
About 20 radioisotopes and 7 nuclear isomers between 233Cm and 252Cm are known for curium, and no stable isotopes. The longest half-lives have been reported for 247Cm (15.6 million years) and 248Cm (348,000 years). Other long-lived isotopes are 245Cm (half-life 8500 years), 250Cm (8,300 years) and 246Cm (4,760 years). Curium-250 is unusual in that it predominantly (about 86%) decays via spontaneous fission. The most commonly used curium isotopes are 242Cm and 244Cm with the half-lives of 162.8 days and 18.1 years, respectively.
All isotopes between 242Cm and 248Cm, as well as 250Cm, undergo a self-sustaining nuclear chain reaction and thus in principle can act as a nuclear fuel in a reactor. As in most transuranic elements, the nuclear fission cross section is especially high for the odd-mass curium isotopes243Cm, 245Cm and 247Cm. These can be used in thermal-neutron reactors, whereas a mixture of curium isotopes is only suitable for fast breeder reactors since the even-mass isotopes are not fissile in a thermal reactor and accumulate as burn-up increases. The mixed-oxide (MOX) fuel, which is to be used in power reactors, should contain little or no curium because the neutron activation of 248Cm will create californium. This is strong neutron emitter, and would pollute the back end of the fuel cycle and increase the dose to reactor personnel. Hence, if the minor actinides are to be used as fuel in a thermal neutron reactor, the curium should be excluded from the fuel or placed in special fuel rods where it is the only actinide present.
The table to the right lists the critical masses for curium isotopes for a sphere, without a moderator and reflector. With a metal reflector (30 cm of steel), the critical masses of the odd isotopes are about 3–4 kg. When using water (thickness ~20–30 cm) as the reflector, the critical mass can be as small as 59 gram for 245Cm, 155 gram for 243Cm and 1550 gram for 247Cm. There is a significant uncertainty in these critical mass values. Whereas it is usually of the order 20%, the values for 242Cm and 246Cm were listed as large as 371 kg and 70.1 kg, respectively, by some research groups.
Currently, curium is not used as a nuclear fuel owing to its low availability and high price. 245Cm and 247Cm have a very small critical mass and therefore could be used in portable nuclear weapons, but none have been reported thus far. Curium-243 is not suitable for this purpose because of its short half-life and strong α emission which would result in excessive heat. Curium-247 would be highly suitable, having a half-life 647 times that of plutonium-239.
Occurrence.
The longest-lived isotope of curium, 247Cm, has a half-life of 15.6 million years. Therefore, any primordial curium, that is curium present on the Earth during its formation, should have decayed by now. Curium is produced artificially, in small quantities for research purposes. Furthermore, it occurs in spent nuclear fuel. Curium is present in nature in certain areas used for the atmospheric nuclear weapons tests, which were conducted between 1945 and 1980. So the analysis of the debris at the testing site of the first U.S. hydrogen bomb, Ivy Mike, (1 November 1952, Enewetak Atoll), beside einsteinium, fermium, plutonium and americium also revealed isotopes of berkelium, californium and curium, in particular 245Cm, 246Cm and smaller quantities of 247Cm, 248Cm and 249Cm. For reasons of military secrecy, this result was published only in 1956.
Atmospheric curium compounds are poorly soluble in common solvents and mostly adhere to soil particles. Soil analysis revealed about 4,000 times higher concentration of curium at the sandy soil particles than in water present in the soil pores. An even higher ratio of about 18,000 was measured in loam soils.
A few atoms of curium can be produced by neutron capture reactions and beta decay in very highly concentrated uranium-bearing deposits.
Synthesis.
Isotope preparation.
Curium is produced in small quantities in nuclear reactors, and by now only kilograms of it have been accumulated for the 242Cm and 244Cm and grams or even milligrams for heavier isotopes. This explains the high price of curium, which has been be quoted at 160–185 USD per milligram, with a more recent estimate at 2,000 USD/g for 242Cm and 170 USD/g for 244Cm. In nuclear reactors, curium is formed from 238U in a series of nuclear reactions. In the first chain, 238U captures a neutron and converts into 239U, which via β– decay transforms into 239Np and 239Pu.
Further neutron capture followed by β–-decay produces the 241Am isotope of americium which further converts into 242Cm:
For research purposes, curium is obtained by irradiating not uranium but plutonium, which is available in large amounts from spent nuclear fuel. Much higher neutron flux is used for the irradiation that results in a different reaction chain and formation of 244Cm:
Curium-244 decays into 240Pu by emission of alpha particle, but it also absorbs neutrons resulting in a small amount of heavier curium isotopes. Among those, 247Cm and 248Cm are popular in scientific research because of their long half-lives. However, the production rate of 247Cm in thermal neutron reactors is relatively low because of it is prone to undergo fission induced by thermal neutrons. Synthesis of 250Cm via neutron absorption is also rather unlikely because of the short half-life of the intermediate product 249Cm (64 min), which converts by β– decay to the berkelium isotope 249Bk.
254Cf. For this however, the production rate is low as 254Cf decays mainly by spontaneous fission, and only slightly by emission of α-particles into 250Cm. -->
The above cascade of (n,γ) reactions produces a mixture of different curium isotopes. Their post-synthesis separation is cumbersome, and therefore a selective synthesis is desired. Curium-248 is favored for research purposes because of its long half-life. The most efficient preparation method of this isotope is via α-decay of the californium isotope 252Cf, which is available in relatively large quantities due to its long half-life (2.65 years). About 35–50 mg of 248Cm is being produced by this method every year. The associated reaction produces 248Cm with isotopic purity of 97%.
Another interesting for research isotope 245Cm can be obtained from the α-decay of 249Cf, and the latter isotope is produced in minute quantities from the β–-decay of the berkelium isotope 249Bk.
Metal preparation.
Most synthesis routines yield a mixture of different actinide isotopes as oxides, from which a certain isotope of curium needs to be separated. An example procedure could be to dissolve spent reactor fuel (e.g. MOX fuel) in nitric acid, and remove the bulk of the uranium and plutonium using a PUREX (Plutonium – URanium EXtraction) type extraction with tributyl phosphate in a hydrocarbon. The lanthanides and the remaining actinides are then separated from the aqueous residue (raffinate) by a diamide-based extraction to give, after stripping, a mixture of trivalent actinides and lanthanides. A curium compound is then selectively extracted using multi-step chromatographic and centrifugation techniques with an appropriate reagent. "Bis"-triazinyl bipyridine complex has been recently proposed as such reagent which is highly selective to curium. Separation of curium from a very similar americium can also be achieved by treating a slurry of their hydroxides in aqueous sodium bicarbonate with ozone at elevated temperature. Both americium and curium are present in solutions mostly in the +3 valence state; whereas americium oxidizes to soluble Am(IV) complexes, curium remains unchanged and can thus be isolated by repeated centrifugation.
Metallic curium is obtained by reduction of its compounds. Initially, curium(III) fluoride was used for this purpose. The reaction was conducted in the environment free from water and oxygen, in the apparatus made of tantalum and tungsten, using elemental barium or lithium as reducing agents.
Another possibility is the reduction of curium(IV) oxide using a magnesium-zinc alloy in a melt of magnesium chloride and magnesium fluoride.
Compounds and reactions.
Oxides.
Curium readily reacts with oxygen forming mostly Cm2O3 and CmO2 oxides, but the divalent oxide CmO is also known. Black CmO2 can be obtained by burning curium oxalate (Cm2(C2O4)3), nitrate (Cm(NO3)3) or hydroxide in pure oxygen. Upon heating to 600–650 °C in vacuum (about 0.01 Pa), it transforms into the whitish Cm2O3:
Alternatively, Cm2O3 can be obtained by reducing CmO2 with molecular hydrogen:
Furthermore, a number of ternary oxides of the type M(II)CmO3 are known, where M stands for a divalent metal, such as barium.
Thermal oxidation of trace quantities of curium hydride (CmH2–3) has been reported to produce a volatile form of CmO2 and the volatile trioxide CmO3, one of the two known examples of the very rare +6 state for curium. Another observed species was reported to behave similarly to plutonium tetroxide and was tentatively characterized as CmO4, with curium in the extremely rare +8 state only known in this compound.
Halides.
The colorless curium(III) fluoride (CmF3) can be produced by introducing fluoride ions into curium(III)-containing solutions. The brown tetravalent curium(IV) fluoride (CmF4) on the other hand is only obtained by reacting curium(III) fluoride with molecular fluorine:
A series of ternary fluorides are known of the form A7Cm6F31, where A stands for alkali metal.
The colorless curium(III) chloride (CmCl3) is produced in the reaction of curium(III) hydroxide (Cm(OH)3) with anhydrous hydrogen chloride gas. It can further be converted into other halides, such as curium(III) bromide (colorless to light green) and curium(III) iodide (colorless), by reacting it with the ammonia salt of the corresponding halide at elevated temperature of about 400–450 °C:
An alternative procedure is heating curium oxide to about 600 °C with the corresponding acid (such as hydrobromic for curium bromide). Vapor phase hydrolysis of curium(III) chloride results in curium oxychloride:
Chalcogenides and pnictides.
Sulfides, selenides and tellurides of curium have been obtained by treating curium with gaseous sulfur, selenium or tellurium in vacuum at elevated temperature. The pnictides of curium of the type CmX are known for the elements nitrogen, phosphorus, arsenic and antimony. They can be prepared by reacting either curium(III) hydride (CmH3) or metallic curium with these elements at elevated temperatures.
Organocurium compounds and biological aspects.
Organometallic complexes analogous to uranocene are known also for other actinides, such as thorium, protactinium, neptunium, plutonium and americium. Molecular orbital theory predicts a stable "curocene" complex (η8-C8H8)2Cm, but it has not been reported experimentally yet.
Formation of the complexes of the type Cm(n-C3H7-BTP)3, where BTP stands for 2,6-di(1,2,4-triazin-3-yl)pyridine, in solutions containing n-C3H7-BTP and Cm3+ ions has been confirmed by EXAFS. Some of these BTP-type complexes selectively interact with curium and therefore are useful in its selective separation from lanthanides and another actinides. Dissolved Cm3+ ions bind with many organic compounds, such as hydroxamic acid, urea, fluorescein and adenosine triphosphate. Many of these compounds are related to biological activity of various microorganisms. The resulting complexes exhibit strong yellow-orange emission under UV light excitation, which is convenient not only for their detection, but also for studying the interactions between the Cm3+ ion and the ligands via changes in the half-life (of the order ~0.1 ms) and spectrum of the fluorescence.
Curium has no biological significance. There are a few reports on biosorption of Cm3+ by bacteria and archaea, however no evidence for incorporation of curium into them.
Applications.
Radionuclides.
Curium is one of the most radioactive isolable elements. Its two most common isotopes 242Cm and 244Cm are strong alpha emitters (energy 6 MeV); they have relatively short half-lives of 162.8 days and 18.1 years, and produce as much as 120 W/g and 3 W/g of thermal energy, respectively. Therefore, curium can be used in its common oxide form in radioisotope thermoelectric generators like those in spacecraft. This application has been studied for the 244Cm isotope, while 242Cm was abandoned due to its prohibitive price of around 2000 USD/g. Curium-243 with a ~30 year half-life and good energy yield of ~1.6 W/g could make for a suitable fuel, but it produces significant amounts of harmful gamma and beta radiation from radioactive decay products. Though as an α-emitter, 244Cm requires a much thinner radiation protection shielding, it has a high spontaneous fission rate, and thus the neutron and gamma radiation rate are relatively strong. As compared to a competing thermoelectric generator isotope such as 238Pu, 244Cm emits a 500 time greater fluence of neutrons, and its higher gamma emission requires a shield that is 20 times thicker — about 2 inches of lead for a 1 kW source, as compared to 0.1 in for 238Pu. Therefore this application of curium is currently considered impractical.
A more promising application of 242Cm is to produce 238Pu, a more suitable radioisotope for thermoelectric generators such as in cardiac pacemakers. The alternative routes to 238Pu use the (n,γ) reaction of 237Np, or the deuteron bombardment of uranium, which both always produce 236Pu as an undesired by-product — since the latter decays to 208Tl with strong gamma emission.
Curium is also a common starting material for the production of higher transuranic elements and transactinides. Thus, bombardment of 248Cm with oxygen (18O) or magnesium (26Mg) yielded certain isotopes of seaborgium (265Sg) and hassium (269Hs and 270Hs). Californium was discovered when a microgram-sized target of curium-242 was irradiated with 35 MeV alpha particles using the cyclotron at Berkeley:
Only about 5,000 atoms of californium were produced in this experiment.
X-ray spectrometer.
The most practical application of 244Cm — though rather limited in total volume — is as α-particle source in the alpha particle X-ray spectrometers (APXS). These instruments were installed on the Sojourner, Mars, Mars 96, Spirit, Athena and Opportunity rovers, as well as the Mars Science Laboratory to analyze the composition and structure of the rocks on the surface of planet Mars. APXS was also used in the Surveyor 5–7 moon probes but with a 242Cm source.
An elaborated APXS setup is equipped with a sensor head containing six curium sources having the total radioactive decay rate of several tens of millicuries (roughly a gigabecquerel). The sources are collimated on the sample, and the energy spectra of the alpha particles and protons scattered from the sample are analyzed (the proton analysis is implemented only in some spectrometers). These spectra contain quantitative information on all major elements in the samples except for hydrogen, helium and lithium. An APXS will also be used by the Philae lander of the Rosetta spacecraft to probe the surface of the 67P/Churyumov-Gerasimenko comet.
Safety.
Owing to its high radioactivity, curium and its compounds must be handled in appropriate laboratories under special arrangements. Whereas curium itself mostly emits α-particles which are absorbed by thin layers of common materials, some of its decay products emit significant fractions of beta and gamma radiation, which require a more elaborate protection. If consumed, curium is excreted within a few days and only 0.05% is absorbed in the blood. From there, about 45% goes to the liver, 45% to the bones, and the remaining 10% is excreted. In the bone, curium accumulates on the inside of the interfaces to the bone marrow and does not significantly redistribute with time; its radiation destroys bone marrow and thus stops red blood cell creation. The biological half-life of curium is about 20 years in the liver and 50 years in the bones. Curium is absorbed in the body much more strongly via inhalation, and the allowed total dose of 244Cm in soluble form is 0.3 μC. Intravenous injection of 242Cm and 244Cm containing solutions to rats increased the incidence of bone tumor, and inhalation promoted pulmonary and liver cancer.
Curium isotopes are inevitably present in spent nuclear fuel with a concentration of about 20 g/tonne. Among them, the 245Cm–248Cm isotopes have decay times of thousands of years and need to be removed to neutralize the fuel for disposal. The associated procedure involves several steps, where curium is first separated and then converted by neutron bombardment in special reactors to short-lived nuclides. This procedure, nuclear transmutation, while well documented for other elements, is still being developed for curium.

</doc>
<doc id="5676" url="http://en.wikipedia.org/wiki?curid=5676" title="Californium">
Californium

Californium is a radioactive metallic chemical element with the symbol Cf and atomic number 98. The element was first made in 1950 at the University of California Radiation Laboratory in Berkeley, by bombarding curium with alpha particles (helium-4 ions). It is an actinide element, the sixth transuranium element to be synthesized, and has the second-highest atomic mass of all the elements that have been produced in amounts large enough to see with the unaided eye (after einsteinium). The element was named after the university and the state of California. It is the heaviest element to occur naturally on Earth; heavier elements can only be produced by synthesis.
Two crystalline forms exist for californium under normal pressure: one above and one below . A third form exists at high pressure. Californium slowly tarnishes in air at room temperature. Compounds of californium are dominated by a chemical form of the element, designated californium(III), that can participate in three chemical bonds. The most stable of californium's twenty known isotopes is californium-251, which has a half-life of 898 years. This short half-life means the element is not found in significant quantities in the Earth's crust. Californium-252, with a half-life of about 2.64 years, is the most common isotope used and is produced at the Oak Ridge National Laboratory in the United States and the Research Institute of Atomic Reactors in Russia.
Californium is one of the few transuranium elements that have practical applications. Most of these applications exploit the property of certain isotopes of californium to emit neutrons. For example, californium can be used to help start up nuclear reactors, and it is employed as a source of neutrons when studying materials with neutron diffraction and neutron spectroscopy. Californium can also be used in nuclear synthesis of higher mass elements; ununoctium (element 118) was synthesized by bombarding californium-249 atoms with calcium-48 ions. Use of californium must take into account radiological concerns and the element's ability to disrupt the formation of red blood cells by bioaccumulating in skeletal tissue.
Characteristics.
Physical properties.
Californium is a silvery white actinide metal with a melting point of and an estimated boiling point of . The pure metal is malleable and is easily cut with a razor blade. Californium metal starts to vaporize above when exposed to a vacuum. Below 51 K (−220 °C) californium metal is either ferromagnetic or ferrimagnetic (it acts like a magnet), between 48 and 66 K it is antiferromagnetic (an intermediate state), and above it is paramagnetic (external magnetic fields can make it magnetic). It forms alloys with lanthanide metals but little is known about them.
The element has two crystalline forms under 1 standard atmosphere of pressure: A double-hexagonal close-packed form dubbed alpha (α) and a face-centered cubic form designated beta (β). The α form exists below with a density of 15.10 g/cm3 and the β form exists above 900 °C with a density of 8.74 g/cm3. At 48 GPa of pressure the β form changes into an orthorhombic crystal system due to de-localization of the atom's 5f electrons, which frees them to bond.
The bulk modulus of a material is a measure of its resistance to uniform pressure. Californium's bulk modulus is 50 ± 5 GPa, which is similar to trivalent lanthanide metals but smaller than more familiar metals, such as aluminium (70 GPa).
Chemical properties and compounds.
Californium exhibits valences of 4, 3, or 2; indicating the number of chemical bonds one atom of this element can form. Its chemical properties are predicted to be similar to other primarily 3+ valence actinide elements and the element dysprosium, which is the lanthanide above californium in the periodic table. The element slowly tarnishes in air at room temperature, with the rate increasing when moisture is added. Californium reacts when heated with hydrogen, nitrogen, or a chalcogen (oxygen family element); reactions with dry hydrogen and aqueous mineral acids are rapid. 
Californium is only water soluble as the californium(III) cation. Attempts to reduce or oxidize the +3 ion in solution have failed. The element forms a water-soluble chloride, nitrate, perchlorate, and sulfate and is precipitated as a fluoride, oxalate, or hydroxide.
Isotopes.
Twenty radioisotopes of californium have been characterized, the most stable being californium-251 with a half-life of 898 years, californium-249 with a half-life of 351 years, californium-250 with a half-life of 13.08 years, and californium-252 with a half-life of 2.645 years. All the remaining isotopes have half-lives shorter than a year, and the majority of these have half-lives shorter than 20 minutes. The isotopes of californium range in mass number from 237 to 256.
Californium-249 is formed from the beta decay of berkelium-249, and most other californium isotopes are made by subjecting berkelium to intense neutron radiation in a nuclear reactor. Although californium-251 has the longest half-life, its production yield is only 10% due to its tendency to collect neutrons (high neutron capture) and its tendency to interact with other particles (high neutron cross-section).
Californium-252 is a very strong neutron emitter, which makes it extremely radioactive and harmful. Californium-252 undergoes alpha decay (the loss of two protons and two neutrons) 96.9% of the time to form curium-248 while the remaining 3.1% of decays are spontaneous fission. One microgram (µg) of californium-252 emits 2.3 million neutrons per second, an average of 3.7 neutrons per spontaneous fission. Most of the other isotopes of californium decay to isotopes of curium (atomic number 96) via alpha decay.
History.
Californium was first synthesized at the University of California Radiation Laboratory in Berkeley, by the physics researchers Stanley G. Thompson, Kenneth Street, Jr., Albert Ghiorso, and Glenn T. Seaborg on or about February 9, 1950. It was the sixth transuranium element to be discovered; the team announced its discovery on March 17, 1950.
To produce californium, a microgram-sized target of curium-242 () was bombarded with 35 MeV-alpha particles () in the cyclotron at Berkeley, which produced californium-245 () plus one free neutron ().
Only about 5,000 atoms of californium were produced in this experiment, and these atoms had a half-life of 44 minutes.
The discoverers named the new element after the university and the state. This was a break from the convention used for elements 95 to 97, which drew inspiration from how the elements directly above them in the periodic table were named. However, the element directly above element 98 in the periodic table, dysprosium, has a name that simply means "hard to get at" so the researchers decided to set aside the informal naming convention. They added that "the best we can do is to point out ... searchers a century ago found it difficult to get to California."
Weighable quantities of californium were first produced by the irradiation of plutonium targets at the Materials Testing Reactor at the National Reactor Testing Station in eastern Idaho; and these findings were reported in 1954. The high spontaneous fission rate of californium-252 was observed in these samples. The first experiment with californium in concentrated form occurred in 1958. The isotopes californium-249 to californium-252 were isolated that same year from a sample of plutonium-239 that had been irradiated with neutrons in a nuclear reactor for five years. Two years later, in 1960, Burris Cunningham and James Wallman of the Lawrence Radiation Laboratory of the University of California created the first californium compounds—californium trichloride, californium oxychloride, and californium oxide—by treating californium with steam and hydrochloric acid.
The High Flux Isotope Reactor (HFIR) at the Oak Ridge National Laboratory (ORNL) in Oak Ridge, Tennessee, started producing small batches of californium in the 1960s. By 1995, the HFIR nominally produced of californium annually. Plutonium supplied by the United Kingdom to the United States under the 1958 US-UK Mutual Defence Agreement was used for californium production.
The Atomic Energy Commission sold californium-252 to industrial and academic customers in the early 1970s for $10 per microgram and an average of of californium-252 were shipped each year from 1970 to 1990. Californium metal was first prepared in 1974 by Haire and Baybarz who reduced californium(III) oxide with lanthanum metal to obtain microgram amounts of sub-micrometer thick films.
Occurrence.
Very minute amounts of californium have been found to exist on Earth due to neutron capture reactions and beta decay in very highly concentrated uranium-bearing deposits. Traces of californium can be found near facilities that use the element in mineral prospecting and in medical treatments. The element is fairly insoluble in water, but it adheres well to ordinary soil; and concentrations of it in the soil can be 500 times higher than in the water surrounding the soil particles.
Fallout from atmospheric nuclear testing prior to 1980 contributed a small amount of californium to the environment. Californium isotopes with mass numbers 249, 252, 253, and 254 have been observed in the radioactive dust collected from the air after a nuclear explosion. Californium is not a major radionuclide at United States Department of Energy legacy sites since it was not produced in large quantities.
Californium was once believed to be produced in supernovas, as their decay matches the 60 day half-life of 254Cf. However, subsequent studies failed to demonstrate any californium spectra, and supernova light curves are now thought to follow the decay of nickel-56.
Production.
Californium is produced in nuclear reactors and particle accelerators. Californium-250 is made by bombarding berkelium-249 () with neutrons, forming berkelium-250 () via neutron capture (n,γ) which, in turn, quickly beta decays (β−) to californium-250 () in the following reaction:
Bombardment of californium-250 with neutrons produces californium-251 and californium-252.
Prolonged irradiation of americium, curium, and plutonium with neutrons produces milligram amounts of californium-252 and microgram amounts of californium-249. As of 2006, curium isotopes 244 to 248 are irradiated by neutrons in special reactors to produce primarily californium-252 with lesser amounts of isotopes 249 to 255.
Microgram quantities of californium-252 are available for commercial use through the U.S. Nuclear Regulatory Commission. Only two sites produce californium-252 – the Oak Ridge National Laboratory in the United States, and the Research Institute of Atomic Reactors in Dimitrovgrad, Russia. As of 2003, the two sites produce 0.25 grams and 0.025 grams of californium-252 per year, respectively.
Three californium isotopes with significant half-lives are produced, requiring a total of 15 neutron captures by uranium-238 without nuclear fission or alpha decay occurring during the process. Californium-253 is at the end of a production chain that starts with uranium-238, includes several isotopes of plutonium, americium, curium, berkelium, and the californium isotopes 249 to 253 (see diagram).
Applications.
Californium-252 has a number of specialized applications as a strong neutron emitter, and each microgram of fresh californium produces 139 million neutrons per minute. This property makes californium useful as a neutron startup source for some nuclear reactors and as a portable (non-reactor based) neutron source for neutron activation analysis to detect trace amounts of elements in samples. Neutrons from californium are employed as a treatment of certain cervical and brain cancers where other radiation therapy is ineffective. It has been used in educational applications since 1969 when the Georgia Institute of Technology received a loan of 119 µg of californium-252 from the Savannah River Plant. It is also used with online elemental coal analyzers and bulk material analyzers in the coal and cement industries.
Neutron penetration into materials makes californium useful in detection instruments such as fuel rod scanners; neutron radiography of aircraft and weapons components to detect corrosion, bad welds, cracks and trapped moisture; and in portable metal detectors. Neutron moisture gauges use californium-252 to find water and petroleum layers in oil wells, as a portable neutron source for gold and silver prospecting for on-the-spot analysis, and to detect ground water movement. The major uses of californium-252 in 1982 were, in order of use, reactor start-up (48.3%), fuel rod scanning (25.3%), and activation analysis (19.4%). By 1994 most californium-252 was used in neutron radiography (77.4%), with fuel rod scanning (12.1%) and reactor start-up (6.9%) as important but distant secondary uses.
Californium-251 has a very small calculated critical mass (about ), high lethality, and a relatively short period of toxic environmental irradiation. The low critical mass of californium led to some exaggerated claims about possible uses for the element.
In October 2006, researchers announced that three atoms of ununoctium (element 118) had been identified at the Joint Institute for Nuclear Research in Dubna, Russia, as the product of bombardment of californium-249 with calcium-48, making it the heaviest element ever synthesized. The target for this experiment contained about 10 mg of californium-249 deposited on a titanium foil of 32 cm2 area. Californium has also been used to produce other transuranium elements; for example, element 103 (later named lawrencium) was first synthesized in 1961 by bombarding californium with boron nuclei.
Precautions.
Californium that bioaccumulates in skeletal tissue releases radiation that disrupts the body's ability to form red blood cells. The element plays no natural biological role in any organism due to its intense radioactivity and low concentration in the environment.
Californium can enter the body from ingesting contaminated food or drinks or by breathing air with suspended particles of the element. Once in the body, only 0.05% of the californium will reach the bloodstream. About 65% of that californium will be deposited in the skeleton, 25% in the liver, and the rest in other organs, or excreted, mainly in urine. Half of the californium deposited in the skeleton and liver are gone in 50 and 20 years, respectively. Californium in the skeleton adheres to bone surfaces before slowly migrating throughout the bone.
The element is most dangerous if taken into the body. In addition, californium-249 and californium-251 can cause tissue damage externally, through gamma ray emission. Ionizing radiation emitted by californium on bone and in the liver can cause cancer.

</doc>
<doc id="5679" url="http://en.wikipedia.org/wiki?curid=5679" title="Christian Social Union in Bavaria">
Christian Social Union in Bavaria

The Christian Social Union in Bavaria () is a Christian democratic and conservative political party in Germany. It operates only in Bavaria, while its larger sister party, the Christian Democratic Union (CDU), operates in the other fifteen states of Germany. The CSU has 45 seats in the Bundestag, making it the smallest of the five parties represented.
The CSU was founded in some ways as a continuation of the Weimar-era Catholic Bavarian People's Party. At the federal level, the CSU forms a common 'CDU/CSU' faction in the Bundestag with the CDU, which is frequently referred to as the Union Faction ("die Unionsfraktion"). Until the 2013 election, the CSU governed at the federal level along with the CDU in a coalition government with the liberal Free Democratic Party (FDP). In the state of Bavaria, the CSU governed as the major party in a coalition government with the FDP from 2008 to 2013. Since the 2013 Bavarian state election the CSU governs alone with an absolute majority.
The CSU is a member of the European People's Party (EPP) and the International Democrat Union. The CSU currently has three ministers in the cabinet of Germany of the federal government in Berlin, while party leader Horst Seehofer serves as Minister-President of Bavaria: a position that CSU representatives have held since 1957.
History.
Franz Josef Strauß (1915–1988) had left behind the strongest legacy as a leader of the party, having led the party from 1961 until his death in 1988. His political career in the federal cabinet was unique in that he had served four ministerial posts in the years between 1953 and 1969. From 1978 until his death in 1988, Strauß served as the Minister-president of Bavaria. Strauß was the first leader of the CSU to be a candidate for the German chancellery, in 1980. In the 1980 federal election Strauß ran against the incumbent Helmut Schmidt of the Social Democratic Party of Germany (SPD), but lost thereafter, as the SPD and the Free Democratic Party (FDP) managed to secure an absolute majority together, forming a Social-liberal coalition.
The CSU has led the Bavarian state government since it came into existence in 1946, save from 1950 to 1953 when the Bavaria Party formed a state government in coalition with the state branches of the SPD and FDP. Before the 2008 elections in Bavaria, the CSU perennially achieved absolute majorities at the state level by itself. This level of dominance is unique among Germany's 16 states. Edmund Stoiber took over the CSU leadership in 1999. He ran for Chancellor of Germany in 2002, but his preferred CDU/CSU–FDP coalition lost against the SPD candidate Gerhard Schröder's SPD-Green alliance. In the 2003 Bavarian state election, the CSU was as the Bavarian government with a majority (60.7% and 124 of 180 seats in the state parliament). On 18 January 2007, Stoiber announced his decision to step down from the posts of Minister-President and CSU chairman by 30 September of that year.
On 28 September 2008, the CSU failed to gain an absolute majority, attaining 43%, of the vote in the 2008 Bavarian state election for the first time since 1966 on a percentage basis and was forced into a coalition with the FDP. Even after the 2009 general election, the CDU/CSU emerged as the largest party in Germany, yet both lost votes predominantly to the FDP. The CSU received only 42.5% of the vote in Bavaria in the 2009 election, which constitutes its weakest showing in the party's history. They have three ministers in Berlin: Hans-Peter Friedrich (Federal Ministry of the Interior), Peter Ramsauer (Federal Minister of Transport, Building and Urban Affairs) and Ilse Aigner (Minister of Food, Agriculture and Consumer Protection).
Relationship with the CDU.
The CSU is the sister party of the Christian Democratic Union (CDU). Together, they are called 'The Union'. The CSU operates only within Bavaria, and the CDU operates in all other states, but not Bavaria. While virtually independent, at the federal level, the parties form a common CDU/CSU faction. No Chancellor has ever come from the CSU, although Strauß and Edmund Stoiber were CDU/CSU candidates for Chancellor in the 1980 federal election and the 2002 federal election, respectively, which were both won by the Social Democratic Party of Germany (SPD). Below the federal level, the parties are entirely independent.
Since its formation, the CSU has been more conservative than the CDU. The CSU and the state of Bavaria decided not to sign the GrundungsVertrag of the Federal Republic of Germany, as they could not agree with the division of Germany into two states, after World War 2. Although Bavaria has a separate police and justice system (distinctive and non-federal), the CSU has actively participated in all political affairs of the German Parliament, the German Government, the German Bundesrat, the parliamentary elections of the German President, the European Parliament, and meetings with Gorbachev in Russia.
Leaders.
Ministers-President.
The CSU has contributed eleven of the twelve Ministers-President of Bavaria since 1945, with only Wilhelm Hoegner (1945–46, 1954–57) of the SPD also holding the office.
Politicians.
See: List of Bavarian Christian Social Union politicians
Notes and references.
http://allstates-flag.com/fotw/flags/de%7Dcsu.html<br>
http://www.deutschland.de/link.php?lang=2&category2=190&link_id=1002

</doc>
<doc id="5681" url="http://en.wikipedia.org/wiki?curid=5681" title="Corporate title">
Corporate title

Corporate titles or business titles on company officials as a means of identifying their function and responsibility in the organization. It is used in publicly and privately held for-profit corporations. In addition, many non-profit organizations, educational institutions, partnerships, and sole proprietorships also confer corporate titles.
The highest-level executives in senior management are usually called "C-level" or part of the "C-suite", referring to the 3-letter initials starting with "C" and ending with "O" (for "chief ... officer"); the traditional three such officers are chief executive officer (CEO), chief operations officer (COO), and chief financial officer (CFO). Depending on the management structure, C-titles may exist instead of or blended/overlapped with other traditional executive titles, such as "president", various designations of "vice presidents" (e.g. VP of marketing), and "general managers" or "directors" of various divisions (e.g. director of marketing) - the latter may or may not imply membership of the "board of directors".
Certain other prominent C-level positions have emerged, some of which are sector-specific. For example, CEO and chief risk officer (CRO) positions are often found in many types of financial services companies. Technology companies of all sorts now tend to have a chief technology officer (CTO) to manage technology development. A chief information officer (CIO) oversees Information Technology (IT) matters, either in companies that specialize in IT or in any kind of company that relies on it for supporting infrastructure.
Many companies now also have a chief marketing officer (CMO) particularly in mature companies in competitive sectors, where brand management is a high priority. In creative/design industries, there is sometimes a chief creative officer (CCO), responsible for keeping the overall look and feel of different products, otherwise headed by different teams, constant throughout a brand. A chief administrative officer may be found in many large complex organizations having various departments or divisions to be coordinated. Additionally, many companies now term their top diversity leadership position as chief diversity officer (CDO). However, this and many other nontraditional and/or lower-ranking C-level titles (included below) are not universally recognized as corporate officers - and tend to be specific to particular organizational cultures or preferences as employees.
Variations.
There are considerable variations in the composition and responsibilities of corporate titles.
Within the corporate office or corporate center of a company, some companies have a Chairman and CEO as the top ranking executive, while the number two is the President and COO; other companies have a President and CEO but no official deputy. Typically, C-level managers are "higher" than Vice Presidents, although many times a C-level officer may also hold a vice president title, such as Executive Vice President and CFO. The board of directors is technically not part of management itself, although its chairman may be considered part of the corporate office if he or she is an executive chairman.
A corporation often consists of different businesses, whose senior executives report directly to the CEO or COO. If organized as a division then the top manager is often known as an Executive Vice President (for example, Todd Bradley who heads the Personal Systems Group in Hewlett Packard). If that business is a subsidiary which has considerably more independence, then the title might be Chairman and CEO (for example, Philip I. Kent of Turner Broadcasting System in Time Warner).
In many countries, particularly in Europe and Asia, there is a separate executive board for day-to-day business and supervisory board (elected by shareholders) for control purposes. In these countries, the CEO presides over the executive board and the chairman presides over the supervisory board, and these two roles will always be held by different people. This ensures a distinction between management by the executive board and governance by the supervisory board. This seemingly allows for clear lines of authority. There is a strong parallel here with the structure of government, which tends to separate the political cabinet from the management civil service.
In the United States and other countries that follow a single-board corporate structure, the board of directors (elected by the shareholders) is often equivalent to the European/Asian supervisory board, while the functions of the executive board may be vested either in the board of directors or in a separate committee, which may be called an operating committee (J.P. Morgan Chase), management committee (Goldman Sachs), executive committee (Lehman Brothers), or executive council (Hewlett-Packard), composed of the division/subsidiary heads and C-level officers that report directly to the CEO.
United States.
State laws in the United States traditionally required certain positions to be created within every corporation, such as President, Secretary and Treasurer. Today, the approach under the Model Business Corporation Act, which is employed in many states, is to grant companies discretion in determining which titles to have, with the only mandated organ being the Board of Directors. 
Some states that do not employ the MBCA continue to require that certain offices be established. Under the law of Delaware, where most large US corporations are established, stock certificates must be signed by two officers with titles specified by law (e.g. a president and secretary or a president and treasurer). Every corporation incorporated in California must have a chairman of the board or a president (or both), as well as a secretary and a chief financial officer. 
LLC-structured companies are generally run directly by their members (shareholders), but the members can agree to appoint officers such as a CEO, or to appoint "managers" to operate the company.
American companies are generally led by a chief executive officer (CEO). In some companies, the CEO also has the title of president. In other companies, the president is responsible for internal management of the company while the CEO is responsible for external relations. Many companies also have a chief financial officer (CFO), chief operating officer (COO) and other "C-level" positions that report to the president and CEO. The next level of middle management may be called vice president, director or manager, depending on the company.
Britain and Commonwealth.
In British English, the title of managing director is generally synonymous with that of chief executive officer. Managing directors do not have any particular authority under the Companies Act in the UK, but do have implied authority based on the general understanding of what their position entails, as well as any authority expressly delegated by the board of directors.
Japan and South Korea.
In Japan, corporate titles are roughly standardized across companies and organizations; although there is variation from company to company, corporate titles within a company are always consistent, and the large companies in Japan generally follow the same outline. These titles are the formal titles that are used on business cards. Korean corporate titles are similar to those of Japan, as the South Korean corporate structure had been influenced by the Japanese model.
Legally, Japanese and Korean companies are only required to have a board of directors with at least one representative director. In Japanese, a company director is called a "torishimariyaku" (取締役) and the representative director is called a "daihyo torishimariyaku" (代表取締役). The equivalent Korean titles are "isa" (이사, 理事) and "daepyo-isa" (대표이사, 代表理事). These titles are often combined with lower titles, e.g. "senmu torishimariyaku" or "jomu torishimariyaku" for Japanese executives who are also board members. Most Japanese companies also have statutory auditors, who operate alongside the board of directors in a supervisory role.
The typical structure of executive titles in large companies includes the following:
The top management group, comprising "jomu"/"sangmu" and above, is often referred to collectively as "senior management" (幹部 or 重役; "kambu" or "juyaku" in Japanese; "ganbu" or "jungyŏk" in Korean).
Some Japanese and Korean companies have also adopted American-style C-level titles, but these are not yet widespread and their usage varies. For example, although there is a Korean translation for chief operating officer ("최고운영책임자, choego unyŏng chaegimja"), not many companies have yet adopted it with an exception of a few multi-national companies such as Samsung and CJ, while the chief financial officer title is often used alongside other titles such as "bu-sajang" (SEVP) or "Jŏnmu" (EVP).
Since the late 1990s, many Japanese companies have introduced the title of "shikko yakuin" (執行役員) or "officer," seeking to emulate the separation of directors and officers found in American companies. In 2002, the statutory title of "shikko yaku" (執行役) was introduced for use in companies that introduced a three-committee structure in their board of directors. The titles are frequently given to "bucho" and higher-level personnel. Although the two titles are very similar in intent and usage, there are several legal distinctions: "shikko yaku" make their own decisions in the course of performing work delegated to them by the board of directors, and are considered managers of the company rather than employees, with a legal status similar to that of directors. "Shikko yakuin" are considered employees of the company that follow the decisions of the board of directors, although in some cases directors may have the "shikko yakuin" title as well.
Corporate titles.
Exempt and non-exempt.
Other corporate employee classifications, in US organizations, include:
Non-employees.
Most modern corporations also have non-employee workers. These are usually 'temps' (temporary workers) or consultants who, depending on the project and their experience, might be brought on to lead a task for which the skill-set did not exist within the company, or in the case of a temp, in the vernacular sense, to perform busy-work or an otherwise low-skilled repetitive task for which an employee is deemed too valuable to perform.
Non-employees generally are employed by outside agencies or firms, but perform their duties within a corporation or similar entity. They do not have the same benefits as employees of that company, such as pay-grades, health insurance, or sick days.
Some high-skilled consultants, however, may garner some benefits such as a bonus, sick leave, or food and travel expenses, since they usually charge a high flat-fee for their services, or otherwise garner high hourly wages. An example of high-skilled consultants include lawyers, lobbyists, and accountants who may not be employed by a corporation, but have their own firms or practices. Most temps, however, are compensated strictly for the hours they work, and are generally non-exempt.

</doc>
<doc id="5683" url="http://en.wikipedia.org/wiki?curid=5683" title="Computer expo">
Computer expo

A computer expo or computer show is a trade fair or exposition for computers and electronics. Expos usually include company or organization booths where products and technologies are demonstrated; talks and lectures; and general mixing of people with common interests.

</doc>
<doc id="5685" url="http://en.wikipedia.org/wiki?curid=5685" title="Cambridge, Massachusetts">
Cambridge, Massachusetts

Cambridge is a city in Middlesex County, Massachusetts, United States, situated directly north of Boston, across the Charles River. It was named in honor of the University of Cambridge in England, an important center of the Puritan theology embraced by the town's founders. Cambridge is home to two of the world's most prominent universities, Harvard University and the Massachusetts Institute of Technology. Cambridge has also been home to Radcliffe College, once one of the leading colleges for women in the United States before it merged with Harvard. According to the 2010 Census, the city's population was 105,162. It is the fifth most populous city in the state, behind Boston, Worcester, Springfield, and Lowell. Cambridge was one of the two seats of Middlesex County prior to the abolition of county government in 1997; Lowell was the other.
History.
The site for what would become Cambridge was chosen in December 1630, because it was located safely upriver from Boston Harbor, which made it easily defensible from attacks by enemy ships. Thomas Dudley, his daughter Anne Bradstreet and her husband Simon were among the first settlers of the town. The first houses were built in the spring of 1631. The settlement was initially referred to as "the newe towne." Official Massachusetts records show the name capitalized as Newe Towne by 1632. Located at the first convenient Charles River crossing west of Boston, Newe Towne was one of a number of towns (including Boston, Dorchester, Watertown, and Weymouth) founded by the 700 original Puritan colonists of the Massachusetts Bay Colony under governor John Winthrop. The original village site is in the heart of today's Harvard Square. The marketplace where farmers brought in crops from surrounding towns to sell survives today as the small park at the corner of John F. Kennedy (J.F.K.) and Winthrop Streets, then at the edge of a salt marsh, since filled. The town included a much larger area than the present city, with various outlying parts becoming independent towns over the years: Newton (originally Cambridge Village, then Newtown) in 1688, Lexington (Cambridge Farms) in 1712, and both West Cambridge (originally Menotomy) and Brighton (Little Cambridge) in 1807. Part of West Cambridge joined the new town of Belmont in 1859, and the rest of West Cambridge was renamed Arlington in 1867; Brighton was annexed by Boston in 1874. In the late 19th century, various schemes for annexing Cambridge itself to the city of Boston were pursued and rejected.
In 1636, the Newe College (later renamed Harvard College, after benefactor John Harvard), was founded by the colony to train ministers. Newe Towne (later named Cambridge) was chosen for the site of the new college by the Great and General Court (the Massachusetts legislature)...primarily, according to testimony by Cotton Mather, to be near the highly respected, popular Puritan preacher Thomas Shepard. By 1638, the name "Newe Towne" had "compacted by usage into 'Newtowne'." In May 1638 the name was changed to Cambridge in honor of the university in Cambridge, England. Thomas Shepard, the minister of Cambridge's church...Harvard's first president (Henry Dunster)... its first benefactor (John Harvard)... and the first schoolmaster (Nathaniel Eaton) were all Cambridge University alumni...as was the then ruling (and first) governor of the Massachusetts Bay Colony, John Winthrop. In 1629, Winthrop had led the signing of the founding document of the city of Boston, which was known as the Cambridge Agreement, after the university. It was Governor Thomas Dudley who, in 1650, signed the charter creating the corporation which still governs Harvard College.
Cambridge grew slowly as an agricultural village eight miles (13 km) by road from Boston, the capital of the colony. By the American Revolution, most residents lived near the Common and Harvard College, with farms and estates comprising most of the town. Most of the inhabitants were descendants of the original Puritan colonists, but there was also a small elite of Anglican "worthies" who were not involved in village life, who made their livings from estates, investments, and trade, and lived in mansions along "the Road to Watertown" (today's Brattle Street, still known as Tory Row). In 1775, George Washington came up from Virginia to take command of fledgling volunteer American soldiers camped on the Cambridge Common—today called the birthplace of the U.S. Army. (The name of today's nearby Sheraton Commander Hotel refers to that event.) Most of the Tory estates were confiscated after the Revolution. On January 24, 1776, Henry Knox arrived with artillery captured from Fort Ticonderoga, which enabled Washington to drive the British army out of Boston.
Between 1790 and 1840, Cambridge began to grow rapidly, with the construction of the West Boston Bridge in 1792, that connected Cambridge directly to Boston, making it no longer necessary to travel eight miles (13 km) through the Boston Neck, Roxbury, and Brookline to cross the Charles River. A second bridge, the Canal Bridge, opened in 1809 alongside the new Middlesex Canal. The new bridges and roads made what were formerly estates and marshland into prime industrial and residential districts.
In the mid-19th century, Cambridge was the center of a literary revolution when it gave the country a new identity through poetry and literature. Cambridge was home to some of the famous Fireside Poets—so called because their poems would often be read aloud by families in front of their evening fires. In their day, the Fireside Poets—Henry Wadsworth Longfellow, James Russell Lowell, and Oliver Wendell Holmes—were as popular and influential as rock stars are today.
Soon after, turnpikes were built: the Cambridge and Concord Turnpike (today's Broadway and Concord Ave.), the Middlesex Turnpike (Hampshire St. and Massachusetts Ave. northwest of Porter Square), and what are today's Cambridge, Main, and Harvard Streets were roads to connect various areas of Cambridge to the bridges. In addition, railroads crisscrossed the town during the same era, leading to the development of Porter Square as well as the creation of neighboring town Somerville from the formerly rural parts of Charlestown.
Cambridge was incorporated as a city in 1846. This was despite noticeable tensions between East Cambridge, Cambridgeport, and Old Cambridge that stemmed from differences in each area's culture, sources of income, and the national origins of the residents. The city's commercial center began to shift from Harvard Square to Central Square, which became the downtown of the city around this time. Between 1850 and 1900, Cambridge took on much of its present character—streetcar suburban development along the turnpikes, with working-class and industrial neighborhoods focused on East Cambridge, comfortable middle-class housing being built on old estates in Cambridgeport and Mid-Cambridge, and upper-class enclaves near Harvard University and on the minor hills of the city. The coming of the railroad to North Cambridge and Northwest Cambridge then led to three major changes in the city: the development of massive brickyards and brickworks between Massachusetts Ave., Concord Ave. and Alewife Brook; the ice-cutting industry launched by Frederic Tudor on Fresh Pond; and the carving up of the last estates into residential subdivisions to provide housing to the thousands of immigrants that arrived to work in the new industries.
For many decades, the city's largest employer was the New England Glass Company, founded in 1818. By the middle of the 19th century it was the largest and most modern glassworks in the world. In 1888, all production was moved, by Edward Drummond Libbey, to Toledo, Ohio, where it continues today under the name Owens Illinois. Flint glassware with heavy lead content, produced by that company, is prized by antique glass collectors today. There is none on public display in Cambridge, but there is a large collection in the Toledo Museum of Art. There are also a few pieces in the Museum of Fine Arts, Boston and in the Sandwich (Cape Cod, MA) Glass Museum.
Among the largest businesses located in Cambridge was the firm of Carter's Ink Company, whose neon sign long adorned the Charles River and which was for many years the largest manufacturer of ink in the world.
By 1920, Cambridge was one of the main industrial cities of New England, with nearly 120,000 residents. As industry in New England began to decline during the Great Depression and after World War II, Cambridge lost much of its industrial base. It also began the transition to being an intellectual, rather than an industrial, center. Harvard University had always been important in the city (both as a landowner and as an institution), but it began to play a more dominant role in the city's life and culture. When Radcliffe College was established in 1879 the town became a mecca for some of the nation's most academically talented female students. Also, the move of the Massachusetts Institute of Technology from Boston in 1916 ensured Cambridge's status as an intellectual center of the United States.
After the 1950s, the city's population began to decline slowly, as families tended to be replaced by single people and young couples. The 1980s brought a wave of high-technology startups, creating software such as Visicalc and Lotus 1-2-3, and advanced computers, but many of these companies fell into decline with the fall of the minicomputer and DOS-based systems. However, the city continues to be home to many startups as well as a thriving biotech industry. By the end of the 20th century, Cambridge had one of the most expensive housing markets in the Northeastern United States.
While maintaining much diversity in class, race, and age, it became harder and harder for those who grew up in the city to be able to afford to stay. The end of rent control in 1994 prompted many Cambridge renters to move to housing that was more affordable, in Somerville and other communities. In 2005, a reassessment of residential property values resulted in a disproportionate number of houses owned by non-affluent people jumping in value relative to other houses, with hundreds having their property tax increased by over 100%; this forced many homeowners in Cambridge to move elsewhere.
As of 2012, Cambridge's mix of amenities and proximity to Boston has kept housing prices relatively stable despite the bursting of the United States housing bubble. Cambridge has been a sanctuary city since 1985 and reaffirmed its status as such in 2006.
Geography.
According to the United States Census Bureau, Cambridge has a total area of , of which is land and (9.82%) is water.
Adjacent municipalities.
Cambridge is located in eastern Massachusetts, bordered by:
The border between Cambridge and the neighboring city of Somerville passes through densely populated neighborhoods which are connected by the MBTA Red Line. Some of the main squares, Inman, Porter, and to a lesser extent, Harvard, are very close to the city line, as are Somerville's Union and Davis Squares.
Neighborhoods.
Squares.
Cambridge has been called the "City of Squares" by some, as most of its commercial districts are major street intersections known as squares. Each of the squares acts as a neighborhood center. These include:
Other neighborhoods.
The residential neighborhoods in Cambridge border, but are not defined by the squares. These neighborhoods include:
Parks and outdoors.
Consisting largely of densely built residential space, Cambridge lacks significant tracts of public parkland. This is partly compensated for, however, by the presence of easily accessible open space on the university campuses, including Harvard Yard, the Radcliffe Yard, and MIT's Great Lawn, as well as the considerable open space of Mount Auburn Cemetery. At the western edge of Cambridge, the cemetery is well known as the first garden cemetery, for its distinguished inhabitants, for its superb landscaping (the oldest planned landscape in the country), and as a first-rate arboretum. Although known as a Cambridge landmark, much of the cemetery lies within the bounds of Watertown. It is also a significant Important Bird Area (IBA) in the Greater Boston area.
Public parkland includes the esplanade along the Charles River, which mirrors its Boston counterpart, Cambridge Common, a busy and historic public park immediately adjacent to the Harvard campus, and the Alewife Brook Reservation and Fresh Pond in the western part of the city.
Demographics.
As of the census of 2010, there were 105,162 people, 44,032 households, and 17,420 families residing in the city. The population density was 16,422.08 people per square mile (6,341.98/km²), making Cambridge the fifth most densely populated city in the US and the second most densely populated city in Massachusetts behind neighboring Somerville. There were 47,291 housing units at an average density of 7,354.7 per square mile (2,840.3/km²). The racial makeup of the city was 66.60% White, 11.70% Black or African American, 0.20% Native American, 15.10% Asian (3.7% Chinese, 1.4% Asian Indian, 1.2% Korean, 1.0% Japanese), 0.01% Pacific Islander, 2.10% from other races, and 4.30% from two or more races. 7.60% of the population were Hispanic or Latino of any race (1.6% Puerto Rican, 1.4% Mexican, 0.6% Dominican, 0.5% Colombian, 0.5% Salvadoran, 0.4% Spaniard). Non-Hispanic Whites were 62.1% of the population in 2010, down from 89.7% in 1970. An individual resident of Cambridge is known as a Cantabrigian.
In 2010, there were 44,032 households out of which 16.9% had children under the age of 18 living with them, 28.9% were married couples living together, 8.4% had a female householder with no husband present, and 60.4% were non-families. 40.7% of all households were made up of individuals and 9.6% had someone living alone who was 65 years of age or older. The average household size was 2.00 and the average family size was 2.76.
In the city, the age distribution was as follows: 13.3% of the population was under the age of 18, 21.2% from 18 to 24, 38.6% from 25 to 44, 17.8% from 45 to 64, and 9.2% who were 65 years of age or older. The median age was 30.5 years. For every 100 females, there were 96.1 males. For every 100 females age 18 and over, there were 94.7 males.
The median income for a household in the city was $47,979, and the median income for a family was $59,423 (these figures had risen to $58,457 and $79,533 respectively ). Males had a median income of $43,825 versus $38,489 for females. The per capita income for the city was $31,156. About 8.7% of families and 12.9% of the population were below the poverty line, including 15.1% of those under age 18 and 12.9% of those age 65 or over.
Cambridge has been ranked as one of the most liberal cities in America. Locals living in and near the city jokingly refer to it as "The People's Republic of Cambridge." For 2012, the residential property tax rate in Cambridge is $8.48 per $1,000. Cambridge enjoys the highest possible bond credit rating, AAA, with all three Wall Street rating agencies.
Cambridge is the birthplace of Thai king Bhumibol Adulyadej (Rama IX), who is the world's longest reigning monarch at age 85 (early 2013), as well as the longest reigning monarch in Thai history. He is also the first king of a foreign country to be born in the United States.
In 2000, 11.0% of city residents were of Irish ancestry; 7.2% were of English, 6.9% Italian, 5.5% West Indian and 5.3% German ancestry. 69.4% spoke only English at home, while 6.9% spoke Spanish, 3.2% Chinese or Mandarin, 3.0% Portuguese, 2.9% French Creole, 2.3% French, 1.5% Korean, and 1.0% Italian.
Government.
Federal and state representation.
Cambridge is split between Massachusetts's 5th and 7th congressional districts. The 5th district seat is held by Democrat Katherine Clark, who replaced now-Senator Ed Markey in a 2013 special election; the 7th is represented by Democrat Mike Capuano, elected in 1998. The state's senior member of the United States Senate is Democrat Elizabeth Warren, elected in 2012, who lives in Cambridge. The Governor of Massachusetts is Democrat Deval Patrick, elected in 2006 and re-elected in 2010.
On the state level, Cambridge is represented in six districts in the Massachusetts House of Representatives: the 24th Middlesex (which includes parts of Belmont and Arlington), the 25th and 26th Middlesex (the latter which includes a portion of Somerville), the 29th Middlesex (which includes a small part of Watertown), and the Eighth and Ninth Suffolk (both including parts of the City of Boston). The city is represented in the Massachusetts Senate as a part of the "First Suffolk and Middlesex" district (this contains parts of Boston, Revere and Winthrop each in Suffolk County); the "Middlesex, Suffolk and Essex" district, which includes Everett and Somerville, with Boston, Chelsea, and Revere of Suffolk, and Saugus in Essex; and the "Second Suffolk and Middlesex" district, containing parts of the City of Boston in Suffolk County, and Cambridge, Belmont and Watertown in Middlesex County.
City government.
Cambridge has a city government led by a mayor and nine-member city council. There is also a six-member school committee which functions alongside the Superintendent of public schools. The councilors and school committee members are elected every two years using the single transferable vote (STV) system.
The mayor is elected by the city councilors from amongst themselves, and serves as the chair of city council meetings. The mayor also sits on the school committee. However, the mayor is not the chief executive of the city. Rather, the city manager, who is appointed by the city council, serves in that capacity.
Under the city's Plan E form of government, the city council does not have the power to appoint or remove city officials who are under direction of the city manager. The city council and its individual members are also forbidden from giving orders to any subordinate of the city manager.
Richard C. Rossi is the city manager; he succeeded Robert W. Healy, who retired in June 2013 after serving 32 years in the position. In recent history, the media has highlighted the salary of the city manager as being one of the highest for a civic employee in Massachusetts.
The city council consists of:
"* = current mayor"<br>
"** = former mayor"
Police department.
In addition to the Cambridge Police Department, the city is patrolled by the Fifth (Brighton) Barracks of Troop H of the Massachusetts State Police. Due, however, to close proximity, the city also practices functional cooperation with the Fourth (Boston) Barracks of Troop H, as well. The campuses of Harvard and MIT are patrolled by the Harvard University Police Department and MIT Police Department, respectively.
Fire department.
The city of Cambridge is protected full-time by the 274 professional firefighters of the Cambridge Fire Department. The current Chief of Department is Gerald R. Reardon. The Cambridge Fire Department operates out of eight fire stations, located throughout the city, under the command of two divisions. The CFD also maintains and operates a front-line fire apparatus fleet of eight engines, four ladders, two Non-Transport Paramedic EMS units,a heavy rescue, a Haz-Mat unit, a Tactical Rescue unit, a Dive Rescue unit, two Marine units, and numerous special, support, and reserve units. Gerard Mahoney Chief of Operations, is in charge of day-to-day operation of the department. The CFD is rated as a Class 1 fire department by the Insurance Services Office (ISO), and is one of only 32 fire departments so rated, out of 37,000 departments in the United States. The other class 1 departments in New England are in Hartford, Connecticut and Milford, Connecticut. Class 1 signifies the highest level of fire protection according to various criteria. The CFD responds to approximately 15,000 emergency calls annually.
Water department.
Cambridge is unusual among cities inside Route 128 in having a non-MWRA water supply. City water is obtained from Hobbs Brook (in Lincoln and Waltham), Stony Brook (Waltham and Weston), and Fresh Pond (Cambridge). The city owns over of land in other towns that includes these reservoirs and portions of their watershed. Water is treated at Fresh Pond, then pumped uphill to an elevation of above sea level at the Payson Park Reservoir (Belmont); From there, the water is redistributed downhill via gravity to individual users in the city.
County government.
Cambridge was a county seat of Middlesex County, along with Lowell, prior to the abolition of county government. Though the county government was abolished in 1997, the county still exists as a geographical and political region. The employees of Middlesex County courts, jails, registries, and other county agencies now work directly for the state. At present, the county's registrars of Deeds and Probate remain in Cambridge; however, the Superior Court and District Attorney have had their base of operations transferred to Woburn. Third District court has shifted operations to Medford, and the Sheriff's office for the county is still awaiting a near-term relocation.
Education.
Higher education.
Cambridge is perhaps best known as an academic and intellectual center, owing to its colleges and universities, which include:
At least 129 of the world's total 780 Nobel Prize winners have been, at some point in their careers, affiliated with universities in Cambridge.
The American Academy of Arts and Sciences is also based in Cambridge.
Primary and secondary public education.
The Cambridge Public School District encompasses 12 elementary schools and 5 upper schools that follow a variety of different educational systems and philosophies.
The 12 elementary schools that offer grades K-5 are:
The 5 upper schools which are physically located in some of the same buildings as the elementary schools offer grades 6-8. They are:
There are three district public high school programs serving Cambridge students, including the Cambridge Rindge and Latin School.
Outside of the main public schools are other public charter schools including: Benjamin Banneker Charter School, which serves students in grades K-6, Community Charter School of Cambridge, which is located in Kendall Square and serves students in grades 7–12, and Prospect Hill Academy, a charter school whose upper school is in Central Square, though it is not a part of the Cambridge Public School District.
Primary and secondary private education.
There are also many private schools in the city including:
Economy.
Manufacturing was an important part of the economy in the late 19th and early 20th century, but educational institutions are the city's biggest employers today. Harvard and MIT together employ about 20,000. As a cradle of technological innovation, Cambridge was home to technology firms Analog Devices, Akamai, Bolt, Beranek, and Newman (BBN Technologies) (now part of Raytheon), General Radio (later GenRad), Lotus Development Corporation (now part of IBM), Polaroid, Symbolics, and Thinking Machines.
In 1996, Polaroid, Arthur D. Little, and Lotus were top employers with over 1,000 employees in Cambridge, but faded out a few years later. Health care and biotechnology firms such as Genzyme, Biogen Idec, Millennium Pharmaceuticals, Sanofi, Pfizer and Novartis have significant presences in the city. Though headquartered in Switzerland, Novartis continues to expand its operations in Cambridge. Other major biotech and pharmaceutical firms expanding their presence in Cambridge include GlaxoSmithKline, AstraZeneca, Shire, and Pfizer. Most Biotech firms in Cambridge are located around Kendall Square and East Cambridge, which decades ago were the city's center of manufacturing. A number of biotechnology companies are also located in University Park at MIT, a new development in another former manufacturing area.
None of the high technology firms that once dominated the economy was among the 25 largest employers in 2005, but by 2008 high tech companies Akamai and ITA Software had grown to be among the largest 25 employers. Google, IBM Research, and Microsoft Research maintain offices in Cambridge. In late January 2012—less than a year after acquiring Billerica-based analytic database management company, Vertica—Hewlett-Packard announced it would also be opening its first offices in Cambridge. Around this same time, e-commerce giants Staples and Amazon.com said they would be opening research and innovation centers in Kendall Square. Video game developer Harmonix Music Systems is based in Central Square. LabCentral also provides a shared laboratory facility for approximately 25 emerging biotech companies.
The proximity of Cambridge's universities has also made the city a center for nonprofit groups and think tanks, including the National Bureau of Economic Research, the Smithsonian Astrophysical Observatory, the Lincoln Institute of Land Policy, Cultural Survival, and One Laptop per Child.
In September 2011, an initiative by the City of Cambridge called the "Entrepreneur Walk of Fame" was launched. It seeks to highlight individuals who have made contributions to innovation in the global business community.
Top employers.
The ten largest employers in the city are:
Transportation.
Road.
Several major roads lead to Cambridge, including Route 2, Route 16 and the McGrath Highway (Route 28). The Massachusetts Turnpike does not pass through Cambridge, but provides access by an exit in nearby Allston. Both U.S. Route 1 and Interstate 93 also provide additional access on the eastern end of Cambridge at Leverett Circle in Boston. Route 2A runs the length of the city, chiefly along Massachusetts Avenue. The Charles River forms the southern border of Cambridge and is crossed by 11 bridges connecting Cambridge to Boston, including the Longfellow Bridge and the Harvard Bridge, eight of which are open to motorized road traffic.
Cambridge has an irregular street network because many of the roads date from the colonial era. Contrary to popular belief, the road system did not evolve from longstanding cow-paths. Roads connected various village settlements with each other and nearby towns, and were shaped by geographic features, most notably streams, hills, and swampy areas. Today, the major "squares" are typically connected by long, mostly straight roads, such as Massachusetts Avenue between Harvard Square and Central Square, or Hampshire Street between Kendall Square and Inman Square.
Mass transit.
Cambridge is well served by the MBTA, including the Porter Square stop on the regional Commuter Rail, the Lechmere stop on the Green Line, and five stops on the Red Line (Alewife, Porter Square, Harvard Square, Central Square, and Kendall Square/MIT). Alewife Station, the current terminus of the Red Line, has a large multi-story parking garage (at a rate of $7 per day ). The Harvard Bus Tunnel, under Harvard Square, reduces traffic congestion on the surface, and connects to the Red Line underground. This tunnel was originally opened for streetcars in 1912, and served trackless trolleys and buses as the routes were converted. The tunnel was partially reconfigured when the Red Line was extended to Alewife in the early 1980s.
Outside of the state-owned transit agency, the city is also served by the Charles River Transportation Management Agency (CRTMA) shuttles which are supported by some of the largest companies operating in city, in addition to the municipal government itself.
Cycling.
Cambridge has several bike paths, including one along the Charles River, and the Linear Park connecting the Minuteman Bikeway at Alewife with the Somerville Community Path. Bike parking is common and there are bike lanes on many streets, although concerns have been expressed regarding the suitability of many of the lanes. On several central MIT streets, bike lanes transfer onto the sidewalk. Cambridge bans cycling on certain sections of sidewalk where pedestrian traffic is heavy.
While "Bicycling Magazine" in 2006 rated Boston as one of the worst cities in the nation for bicycling, it has given Cambridge honorable mention as one of the best and was called by the magazine "Boston's Great Hope". Boston has since then followed the example of Cambridge, and made considerable efforts to improve bicycling safety and convenience.
Cambridge has an official bicycle committee. The LivableStreets Alliance, headquartered in Cambridge, is an advocacy group for bicyclists, pedestrians, and walkable neighborhoods.
Walking.
Walking is a popular activity in Cambridge. In 2000, of US communities with more than 100,000 residents, Cambridge had the highest percentage of commuters who walked to work. Cambridge receives a "Walk Score" of 100 out of 100 possible points. Cambridge's major historic squares have changed into modern walking neighborhoods, including traffic calming features based on the needs of pedestrians rather than of motorists.
Intercity.
The Boston intercity bus and train stations at South Station, Boston, and Logan International Airport in East Boston, are accessible by subway. The Fitchburg Line rail service from Porter Square connects to some western suburbs. Since October 2010, there has also been intercity bus service between Alewife Station (Cambridge) and New York City.
Media.
Newspapers.
Cambridge is served by several weekly newspapers. The most prominent is the "Cambridge Chronicle", which is also the oldest surviving weekly paper in the United States.
Radio.
Cambridge is home to the following commercially licensed and student-run radio stations:
Television.
Cambridge Community Television (CCTV) has served the Cambridge community since its inception in 1988. CCTV operates Cambridge's public access television facility and programs three television channels, 8, 9, and 96 on the Cambridge cable system (Comcast). The city has invited tenders from other cable providers; however, presently Comcast remains the only fixed television public utility for Cambridge. Services from American satellite TV providers, however, are available.
Social media.
As of 2011, a growing number of social media efforts provide means for participatory engagement with the locality of Cambridge, such as Localocracy and Foursquare.
Culture, art and architecture.
Public art.
Cambridge has a large and varied collection of permanent public art, both on city property (managed by the Cambridge Arts Council), and on the campuses of Harvard and MIT. Temporary public artworks are displayed as part of the annual Cambridge River Festival on the banks of the Charles River, during winter celebrations in Harvard and Central Squares, and at university campus sites. Experimental forms of public artistic and cultural expression include the Central Square World's Fair, the Somerville-based annual Honk! Festival, and If This House Could Talk, a neighborhood art and history event. An active tradition of street musicians and other performers in Harvard Square entertains an audience of tourists and local residents during the warmer months of the year. The performances are coordinated through a public process that has been developed collaboratively by the performers, city administrators, private organizations and business groups.
Architecture.
Despite intensive urbanization during the late 19th century and 20th century, Cambridge has several historic buildings, including some dating to the 17th century. The city also contains an abundance of innovative contemporary architecture, largely built by Harvard and MIT.
Music.
The city has an active music scene, from classical performances to the latest popular bands. Beyond performances at the colleges and universities, there are many venues in Cambridge including: The Middle East, Club Passim, The Plough and Stars, and the Nameless Coffeehouse.
International relations.
Twin towns – Sister cities.
Cambridge is twinned with:
Other official sister city relationships are inactive:
There is also an unofficial relationship with:

</doc>
<doc id="5686" url="http://en.wikipedia.org/wiki?curid=5686" title="Cambridge (disambiguation)">
Cambridge (disambiguation)

Cambridge is a city and the county town of Cambridgeshire, United Kingdom, famous for being the location of the University of Cambridge.
Cambridge may also refer to:

</doc>
<doc id="5688" url="http://en.wikipedia.org/wiki?curid=5688" title="Colin Dexter">
Colin Dexter

Norman Colin Dexter, OBE, (born 29 September 1930) is an English crime writer known for his Inspector Morse novels, which were written between 1975 and 1999 and adapted as a television series from 1987 to 2000.
Early life and career.
Dexter was born in Stamford, Lincolnshire, and was educated at Stamford School, a boys' public school. After completing his national service with the Royal Corps of Signals, he read Classics at Christ's College, Cambridge, graduating in 1953 and receiving an honorary master's degree in 1958.
In 1954, he started his teaching career in the East Midlands, becoming assistant Classics master at Wyggeston School, Leicester. A post at Loughborough Grammar School followed before he took up the position of senior Classics teacher at Corby Grammar School, Northamptonshire, in 1959.
In 1956 he married Dorothy Cooper, and they had a son and a daughter.
In 1966, he was forced by the onset of deafness to retire from teaching and took up the post of senior assistant secretary at the University of Oxford Delegacy of Local Examinations (UODLE) in Oxford, a job he held until his retirement in 1988.
Dexter featured prominently in the BBC programme "How to Solve a Cryptic Crossword" as part of the "Time Shift" series broadcast in November 2008, in which he recounted some of the crossword clues solved by Morse.
Writing career.
The first books he wrote were General Studies text books.
He started writing mysteries in 1972 during a family holiday. "We were in a little guest house halfway between Caernarfon and Pwllheli. It was a Saturday and it was raining—it's not unknown for it to rain in North Wales. The children were moaning... I was sitting at the kitchen table with nothing else to do, and I wrote the first few paragraphs of a potential detective novel." "Last Bus to Woodstock" was published in 1975 and introduced the character of Inspector Morse, the irascible detective whose penchants for cryptic crosswords, English literature, cask ale and Wagner reflect Dexter's own enthusiasms. Dexter's plots are notable for his use of false leads and other red herrings.
The success of the 33 episodes of the TV series "Inspector Morse", produced between 1987 and 2001, brought further acclaim for Dexter. In the manner of Alfred Hitchcock, he also makes a cameo appearance in almost all episodes. More recently, his character from the Morse series, the Sergeant (now Inspector) Lewis features in 27 episodes of the new ITV series "Lewis".
Dexter is currently a consultant on the TV series "Endeavour", starring Shaun Evans and Roger Allam. This series is a prequel to "Inspector Morse". As with "Morse", Dexter occasionally makes cameo appearances in "Lewis" and "Endeavour". 
Dexter selected the English poet A.E. Housman for the BBC Radio 4 programme "Great Lives" in May 2008. Dexter and Housman were both Classicists who found a popular audience in another genre of writing.
Awards and honours.
Dexter has received several Crime Writers' Association awards: two Silver Daggers for "Service of All the Dead" in 1979 and "The Dead of Jericho" in 1981; two Gold Daggers for "The Wench is Dead" in 1989 and "The Way Through the Woods" in 1992; and a Cartier Diamond Dagger for lifetime achievement in 1997. In 1996 Dexter received a Macavity Award for his short story "Evans Tries an O-Level". In 1980, he was elected a member of the by-invitation-only Detection Club.
In 2000 Dexter was appointed an Officer of the Order of the British Empire for services to literature. In September 2011, the University of Lincoln awarded Dexter an honorary Doctor of Letters degree.

</doc>
<doc id="5689" url="http://en.wikipedia.org/wiki?curid=5689" title="College">
College

A college (Latin: "collegium") is an educational institution or a constituent part of one. Usage of the word "college" varies in English-speaking nations. A college may be a degree-awarding tertiary educational institution, a part of a collegiate university, or an institution offering vocational education.
In the United States, "college" formally refers to a constituent part of a university, but generally "college" and "university" are used interchangeably, whereas in Ireland, the UK, New Zealand, Australia, Canada, Hong Kong and other former and present Commonwealth nations, "college" may refer to a secondary or high school, a college of further education, a training institution that awards trade qualifications, or a constituent part of a university. (See this comparison of British and American English educational terminology for further information.)
Etymology.
In ancient Rome a "collegium" was a club or society, a group of people living together under a common set of rules ("con-" = "together" + "leg-" = "law" or "lego" = "I choose" or "I read").
Usage.
Higher education.
Within higher education, the term can be used to refer to:
Secondary education.
In some national education systems, secondary schools may be called "colleges" or have "college" as part of their title.
In Australia the term "college" is applied to any private or independent (non-government) primary and, especially, secondary school as distinct from a state school. Melbourne Grammar School, Cranbrook School, Sydney and The King's School, Parramatta are considered colleges.
There has also been a recent trend to rename or create government secondary schools as "colleges". In the state of Victoria, some state high schools are referred to as "secondary colleges". Interestingly, the pre-eminent government secondary school for boys in Melbourne is still named Melbourne High School. In Western Australia, South Australia and the Northern Territory, "college" is used in the name of all state high schools built since the late 1990s, and also some older ones. In New South Wales, some high schools, especially multi-campus schools resulting from mergers, are known as "secondary colleges". In Queensland some newer schools which accept primary and high school students are styled "state college", but state schools offering only secondary education are called "State High School". In Tasmania and the Australian Capital Territory, "college" refers to the final two years of high school (years 11 and 12), and the institutions which provide this. In this context, "college" is a system independent of the other years of high school. Here, the expression is a shorter version of "matriculation college".
In a number of Canadian cities, many government-run secondary schools are called "collegiates" or "collegiate institutes" (C.I.), a complicated form of the word "college" which avoids the usual "post-secondary" connotation. This is because these secondary schools have traditionally focused on academic, rather than vocational, subjects and ability levels (for example, collegiates offered Latin while vocational schools offered technical courses). Some private secondary schools (such as Upper Canada College, Vancouver College) choose to use the word "college" in their names nevertheless. Some secondary schools elsewhere in the country, particularly ones within the separate school system, may also use the word "college" or "collegiate" in their names.
In New Zealand the word "college" normally refers to a secondary school for ages 13 to 17 and "college" appears as part of the name especially of private or integrated schools. "Colleges" most frequently appear in the North Island, whereas "high schools" are more common in the South Island.
In South Africa, some secondary schools, especially private schools on the English public school model, have "college" in their title. Thus no less than six of South Africa's Elite Seven high schools call themselves "college" and fit this description. A typical example of this category would be St John's College.
Private schools that specialize in improving children's marks through intensive focus on examination needs are informally called "cram-colleges".
In Sri Lanka the word "college" (known as "Vidyalaya" in "Sinhala") normally refers to a secondary school, which usually signifies above the 5th standard. During the British colonial period a limited number of exclusive secondary schools were established based on English public school model (Royal College Colombo, S. Thomas' College, Mount Lavinia, Trinity College, Kandy) these along with several Catholic schools (St. Joseph's College, Colombo, St Anthony's College, Kandy) traditionally carry their name as colleges. Following the start of free education in 1931 large group of central colleges were established to educate the rural masses. Since Sri Lanka gained Independence in 1948, many schools that have been established have been named as "college".
Other.
As well as an educational institution, the term can also refer, following its etymology, to any formal group of colleagues set up under statute or regulation; often under a Royal Charter. Examples are an electoral college, the College of Arms, a college of canons, and the College of Cardinals. Other collegiate bodies include professional associations, particularly in medicine and allied professions. In the UK these include the Royal College of Nursing and the Royal College of Physicians. Examples in the United States include the American College of Physicians, the American College of Surgeons, and the American College of Dentists. An example in Australia is the Royal Australian College of General Practitioners.
Country by country.
Australia.
In Australia a college may be an institution of tertiary education that is smaller than a university, run independently or as part of a university. Following a reform in the 1980s many of the formerly independent colleges now belong to a larger university. A notable exception is Campion College which operates in Western Sydney, following the American Liberal Arts College tradition.
Referring to parts of a university, there are "residential colleges" which provide residence for students, both undergraduate and postgraduate, called university colleges. These colleges often provide additional tutorial assistance, and some host theological study. Many colleges have strong traditions and rituals, so are a combination of dormitory style accommodation and fraternity or sorority culture. Less commonly the term "college" can refer to a superfaculty organizational unit, as in the ANU Colleges.
Most technical and further education institutions (TAFEs), which offer certificate and diploma vocational courses, are styled "TAFE colleges" or "Colleges of TAFE". Some private institutions offering TAFE certificates, university bridging courses, or theological courses of study (i.e. Bible colleges) style themselves "Institutes" or "Colleges".
In Tasmania the term is also used to describe a secondary school that only teaches the final two years of high school (years 11 and 12), e.g. Hellyer College and Hobart College. Throughout Australia many private secondary schools are called colleges.
Canada.
In Canada, the term "college" usually refers to a technical, applied arts, applied science school or community college. These are post-secondary institutions granting certificates, diplomas, associate's degree, and bachelor's degrees. In Quebec, the term is seldom used; the French acronym for public colleges, CEGEP ("College d'enseignement général et professionnel", "college of general and professional education"), is colloquially, yet incorrectly, used as an umbrella term to refer to all collegiate level institutions specific to the Quebec education system, a step that is required to continue onto university (unless one applies as a "mature" student, meaning 21 years of age or over, and out of the educational system for at least 2 years), or to learn a trade. In Ontario, British Columbia and Alberta, there are also institutions which are designated university colleges, as they only grant undergraduate degrees. This is to differentiate between universities, which have both undergraduate and graduate programs and those that do not. In contrast to usage in the United States, there is a strong distinction between "college" and "university" in Canada. In conversation, one specifically would say either "They are going to university" (i.e., studying for a three- or four-year degree at a university) or "They are going to college" (suggesting a technical or career college).
The Royal Military College of Canada, a full-fledged degree-granting university, does not follow the naming convention used by the rest of the country, nor does its sister school Royal Military College Saint-Jean or the now closed Royal Roads Military College.
The term "college" also applies to distinct entities within a university (usually referred to as "federated colleges" or "affiliated colleges"), to the residential colleges in the United Kingdom. These colleges act independently, but in affiliation or federation with the university that actually grants the degrees. For example, Trinity College was once an independent institution, but later became federated with the University of Toronto, and is now one of its residential colleges (though it remains a degree granting institution through its Faculty of Divinity). In the case of Memorial University of Newfoundland, located in St. John's, the Corner Brook campus is called Sir Wilfred Grenfell College. Occasionally, "college" refers to a subject specific faculty within a university that, while distinct, are neither "federated" nor "affiliated"—College of Education, College of Medicine, College of Dentistry, College of Biological Science among others.
There are also universities referred to as art colleges, empowered to grant academic degrees of BFA, Bdes, MFA, Mdes and sometimes collaborative PhD degrees. Some of them have "university" in their name (NSCAD University, OCAD University and Emily Carr University of Art and Design)and others do not.
Online and distance education (E-learning) use "college" in the name in the British sense, for example : Canada Capstone College.
One use of the term "college" in the American sense is by the Canadian Football League (CFL), which calls its annual entry draft the Canadian College Draft. The draft is restricted to players who qualify under CFL rules as "non-imports"—essentially, players who were raised in Canada (see the main CFL article for a more detailed definition). Because a player's designation as "non-import" is not affected by where he plays post-secondary football, the category includes former players at U.S. college football programs ("universities" in the Canadian sense) as well as CIS football programs at Canadian universities.
Chile.
In Chile, the term "college" is usually used in the name of some bilingual schools, like Santiago College, Saint George's College etc.
Georgia.
International Association of "Tourists and Travelers" – College
International association "tourists and travelers” is a non-commercial, non political and non industrial organization, which is created to develop tourism in Georgia.
Hong Kong.
In Hong Kong, the term 'college' is used by tertiary institutions as either part of their names or to refer to a constituent part of the university, such as the colleges in the collegiate The Chinese University of Hong Kong; or to a residence hall of a university, such as St. John's College, University of Hong Kong. Many older secondary schools have the term 'college' as part of their names.
India.
The modern system of education was heavily influenced by the British starting in 1835.
In India, the term "college" is commonly reserved for institutions that offer degrees at year 12 (""Junior College"", similar to American "high schools"), and those that offer the bachelor's degree. Generally, colleges are located in different parts of a state and all of them are affiliated to a regional university. The colleges offer programmes under that university. Examinations are conducted by the university at the same time for all colleges under its affiliation. There are several hundred universities and each university has affiliated colleges.
The first liberal arts and sciences college in India was C. M. S. College Kottayam, Kerala, established in 1817, and the Presidency College, Kolkata, also 1817, initially known as Hindu College. The first college for the study of Christian theology and ecumenical enquiry was Serampore College (1818). The first Missionary institution to impart Western style education in India was the Scottish Church College, Calcutta (1830). The first commerce and economics college in India was Sydenham College, Mumbai (1913).
Ireland.
In Ireland the term "college" is normally use to describe an institution of tertiary education. University students often say they attend "college" rather than "university". Until 1989, no university provided teaching or research directly; they were formally offered by a constituent college of the university.
There are number of secondary education institutions that traditionally used the word "college" in their names: these are either older, private schools (such as Gonzaga College and St. Michael's College) or what were formerly a particular kind of secondary school. These secondary schools, formerly known as "technical colleges," were renamed "community colleges," but remain secondary schools.
The country's only ancient university is the University of Dublin. Created during the reign of Elizabeth I, it is modelled on the collegiate universities of Cambridge and Oxford. However, only one constituent college was ever founded, hence the curious position of Trinity College, Dublin today; although both are usually considered one and the same, the University and College are completely distinct corporate entities with separate and parallel governing structures.
Among more modern foundations, the National University of Ireland, founded in 1908, consisted of constituent colleges and recognised colleges until 1997. The former are now referred to as constituent universities – institutions that are essentially universities in their own right. The National University can trace its existence back to 1850 and the creation of the Queen's University of Ireland and the creation of the Catholic University of Ireland in 1854. From 1880, the degree awarding roles of these two universities was taken over by the Royal University of Ireland, which remained until the creation of the National University in 1908 and the Queen's University Belfast.
The state's two new universities Dublin City University and University of Limerick were initially National Institute for Higher Education institutions. These institutions offered university level academic degrees and research from the start of their existence and were awarded university status in 1989 in recognition of this. These two universities now follow the general trend of universities having associated colleges offering their degrees.
Third level technical education in the state has been carried out in the Institutes of Technology, which were established from the 1970s as Regional Technical Colleges. These institutions have "delegated authority" which entitles them to give degrees and diplomas from the Higher Education and Training Awards Council in their own name.
A number of Private Colleges exist such as DBS, providing undergraduate and postgraduate courses validated by HETAC and in some cases by other Universities.
Other types of college include Colleges of Education, such as National College of Ireland. These are specialist institutions, often linked to a university, which provide both undergraduate and postgraduate academic degrees for people who want to train as teachers.
A number of state funded further education colleges exist - which offer vocational education and training in a range of areas from business studies, I.C.T to sports injury therapy. These courses are usually 1, 2 or less often 3 three years in duration and are validated by FETAC at levels 5 or 6 or for the BTEC Higher National Diploma award - validated by Edexcel which is a level 6/7 qualification. There are numerous private colleges (particularly in Dublin and Limerick) which offer both further and higher education qualifications. These degrees and diplomas are often certified by foreign universities/international awarding bodies and are aligned to the National Framework of Qualifications at level 6, 7 and 8.
Israel.
In Israel, any non university higher-learning facility is called a college. Institutions accredited by the Council for Higher Education in Israel (CHE) to confer a Bachelor's degree are called "Academic Colleges." These colleges (at least 4 for 2012) may also offer Masters degrees and act as Research facilities. There are also over twenty teacher training colleges or seminaries, most of which may award only a Bachelor of Education (B.Ed.) degree.
New Zealand.
The constituent colleges of the former University of New Zealand (such as Canterbury University College) have become independent universities. Some halls of residence associated with New Zealand universities retain the name of "college", particularly at the University of Otago (which although brought under the umbrella of the University of New Zealand, already possessed university status and degree awarding powers). The institutions formerly known as "Teacher-training colleges" now style themselves "College of education".
Some universities, such as the University of Canterbury, have divided their University into constituent administrative "Colleges" – the College of Arts containing departments that teach Arts, Humanities and Social Sciences, College of Science containing Science departments, and so on. This is largely modelled on the Cambridge model, discussed above.
Like the United Kingdom some professional bodies in New Zealand style themselves as "colleges", for example, the Royal Australasian College of Surgeons, the Royal Australasian College of Physicians.
Secondary school is often referred to as college and the term is used interchangeably with high school. This is reflected in the names of many secondary schools such as Rangitoto College, New Zealand's largest secondary.
Philippines.
In the Philippines, colleges usually refer to institutions of learning that grant degrees but whose scholastic fields are not as diverse as that of a university (University of Santo Tomas, University of the Philippines, Ateneo de Manila University, and De La Salle University), such as the San Beda College which specializes in law and the Central Colleges of the Philippines which specializes in engineering, or to component units within universities that do not grant degrees but rather facilitate the instruction of a particular field, such as a College of Science and College of Engineering, among many other colleges of the University of the Philippines.
A state college may not have the word "college" on its name, but may have several component colleges, or departments. Thus, the Eulogio Amang Rodriguez Institute of Science and Technology is a state college by classification.
Usually, the term "college" is also thought of as a hierarchical demarcation between the term "university", and quite a number of colleges seek to be recognized as universities as a sign of improvement in academic standards (Colegio de San Juan de Letran, San Beda College), and increase in the diversity of the offered degree programs (called "courses"). For private colleges, this may be done through a survey and evaluation by the Commission on Higher Education and accrediting organizations, as was the case of Urios College which is now the Fr. Saturnino Urios University. For state colleges, it is usually done by a legislation by the Congress or Senate. In common usage, "going to college" simply means attending school for an undergraduate degree, whether it's from an institution recognized as a college or a university.
When it comes to referring to the level of education, "college" is the term more used to be synonymous to tertiary or higher education. A student who is or has studied her undergraduate degree at either an institution with "college" or "university" in its name is considered to be going to or have gone to "college".
Singapore.
The term "college" in Singapore is generally only used for pre-university educational institutions called "Junior Colleges", which provide the final two years of secondary education (equivalent to sixth form in British terms or grades 11–12 in the American system). Since 1 January 2005, the term also refers to the three campuses of the Institute of Technical Education with the introduction of the "collegiate system", in which the three institutions are called ITE College East, ITE College Central, and ITE College West respectively.
The term "university" is used to describe higher-education institutions offering locally conferred degrees. Institutions offering diplomas are called "polytechnics", while other institutions are often referred to as "institutes" and so forth.
South Africa.
Although the term "college" is hardly used in any context at any university in South Africa, some non-university tertiary institutions call themselves colleges. These include teacher training colleges, business colleges and wildlife management colleges. See: List of universities in South Africa#Private colleges and universities; List of post secondary institutions in South Africa.
Sri Lanka.
There are several professional and vocational institutions that offer post-secondary education without granting degrees that are referred to as "colleges". This includes the Sri Lanka Law College, the many Technical Colleges and Teaching Colleges.
In the United Kingdom, "college" can refer to either sixth form in the context of secondary education, or university in the context of higher education.
United Kingdom.
A sixth form college is an educational institution in England, Wales, Northern Ireland, Belize, The Caribbean, Malta, Norway, Brunei Southern Africa, among others, where students aged 16 to 19 typically study for advanced school-level qualifications, such as A-levels and the International Baccalaureate Diploma, or school-level qualifications such as GCSEs. In Singapore and India, this is known as a junior college. The municipal government of the city of Paris uses the phrase "sixth form college" as the English name for a lycée.
In higher education a college is usually part of a university; such colleges do not award degrees. Universities with constituent colleges are collegiate universities. A college may also be a grouping of faculties or departments, notably in the University of Edinburgh, the University of Salford, the University of Birmingham and the University of Leicester.
In the University of Oxford, University of Cambridge, and University of the Arts London (and formerly in the University of Wales), colleges provide accommodation, tuition and other facilities to students of the university: the university conducts examinations and grants degrees. However the colleges of the University of London are now "de facto" universities in their own right.
In the other collegiate universities, including the University of Lancaster, University of York, University of Kent, University of St Andrews and University of Durham, the colleges only provide accommodation and pastoral care.
A university college is an independent institution which prepares students to sit as external candidates at other universities or has the authority to run courses that lead to the degrees of those universities. It may also be an independent higher education institution with the power to award degrees, but does not have university status, although it is usually working towards it.
Historically, some universities originated as university colleges. For example, the University of Reading was an external college of the University of Oxford, as University College, Reading. The University is now faculty based.
United States.
In the United States, there are over 4,400 colleges and universities. A "college" in the US formally denotes a constituent part of a university, but in popular usage, the word "college" is the generic term for any post-secondary undergraduate education. Americans go to "college" after high school, regardless of whether the specific institution is formally a college or a university. Some students choose to dual-enroll, by taking college classes while still in high school. The word and its derivatives are the standard terms used to describe the institutions and experiences associated with American post-secondary undergraduate education.
Students must pay for college before taking classes. Some borrow the money via loans, and some students fund their educations with cash, scholarships, or grants, or some combination of any two or more of those payment methods. In 2011, the state or federal government subsidized $8,000 to $100,000 for each undergraduate degree. For state-owned schools (called "public" universities), the subsidy was given to the college, with the student benefiting from lower tuition.
Colleges vary in terms of size, degree, and length of stay. Two-year colleges, also known as junior or community colleges, usually offer an associate's degree, and four-year colleges usually offer a bachelor's degree. Often, these are entirely undergraduate institutions, although some have graduate school programs.
Four-year institutions in the U.S. that emphasize a liberal arts curriculum are known as liberal arts colleges. Until the 20th century, liberal arts, law, medicine, theology, and divinity were about the only form of higher education available in the United States. These schools have traditionally emphasized instruction at the undergraduate level, although advanced research may still occur at these institutions.
While there is no national standard in the United States, the term "university" primarily designates institutions that provide undergraduate and graduate education. A university typically has as its core and its largest internal division an undergraduate college teaching a liberal arts curriculum, also culminating in a bachelor's degree. What often distinguishes a university is having, in addition, one or more graduate schools engaged in both teaching graduate classes and in research. Often these would be called a School of Law or School of Medicine, (but may also be called a college of law, or a faculty of law). An exception is Vincennes University, Indiana, which is styled and chartered as a "university" even though almost all of its academic programs lead only to two-year associate degrees. Some institutions, such as Dartmouth College and The College of William & Mary, have retained the term "college" in their names for historical reasons. In one unique case, Boston College and Boston University, both located in Boston, Massachusetts, are completely separate institutions.
Usage of the terms varies among the states. In 1996 for example, Georgia changed all of its four-year institutions previously designated as colleges to universities, and all of its vocational technology schools to technical colleges.
The terms "university" and "college" do not exhaust all possible titles for an American institution of higher education. Other options include "institute" (Massachusetts Institute of Technology), "academy" (United States Military Academy), "union" (Cooper Union), "conservatory" (New England Conservatory), and "school" (Juilliard School). In colloquial use, they are still referred to as "college" when referring to their undergraduate studies.
The term "college" is also, as in the United Kingdom, used for a constituent semi-autonomous part of a larger university but generally organized on academic rather than residential lines. For example, at many institutions, the undergraduate portion of the university can be briefly referred to as the college (such as The College of the University of Chicago, Harvard College at Harvard, or Columbia College at Columbia) while at others, such as the University of California, Berkeley, each of the faculties may be called a "college" (the "college of engineering", the "college of nursing", and so forth). There exist other variants for historical reasons; for example, Duke University, which was called Trinity College until the 1920s, still calls its main undergraduate subdivision Trinity College of Arts and Sciences. Some American universities, such as Princeton, Rice, and Yale do have residential colleges along the lines of Oxford or Cambridge, but the name was clearly adopted in homage to the British system. Unlike the Oxbridge colleges, these residential colleges are not autonomous legal entities nor are they typically much involved in education itself, being primarily concerned with room, board, and social life. At the University of Michigan, University of California, San Diego and the University of California, Santa Cruz, however, each of the residential colleges does teach its own core writing courses and has its own distinctive set of graduation requirements.
The founders of the first institutions of higher education in the United States were graduates of the University of Oxford and the University of Cambridge. The small institutions they founded would not have seemed to them like universities – they were tiny and did not offer the higher degrees in medicine and theology. Furthermore, they were not composed of several small colleges. Instead, the new institutions felt like the Oxford and Cambridge colleges they were used to – small communities, housing and feeding their students, with instruction from residential tutors (as in the United Kingdom, described above). When the first students came to be graduated, these "colleges" assumed the right to confer degrees upon them, usually with authority—for example, The College of William & Mary has a Royal Charter from the British monarchy allowing it to confer degrees while Dartmouth College has a charter permitting it to award degrees "as are usually granted in either of the universities, or any other college in our realm of Great Britain."
The leaders of Harvard College (which granted America's first degrees in 1642) might have thought of their college as the first of many residential colleges that would grow up into a New Cambridge university. However, over time, few new colleges were founded there, and Harvard grew and added higher faculties. Eventually, it changed its title to university, but the term "college" had stuck and "colleges" have arisen across the United States.
In U.S. usage, the word "college" embodies not only a particular type of school, but has historically been used to refer to the general concept of higher education when it is not necessary to specify a school, as in "going to college" or "college savings accounts" offered by banks.
In addition to private colleges and universities, the U.S. also has a system of government funded, public universities. Many were founded under the Morrill Land-Grant Colleges Act of 1862. When the Morrill Act was established, the original colleges on the east coast, primarily those of the Ivy League and several religious based colleges, were the only form of higher education available, and were often confined only to the children of the elite. A movement had arisen to bring a form of more practical higher education to the masses, as "…many politicians and educators wanted to make it possible for all young Americans to receive some sort of advanced education." The Morrill Act "…made it possible for the new western states to establish colleges for the citizens." Its goal was to make higher education more easily accessible to the citizenry of the country, specifically to improve agricultural systems by providing training and scholarship in the production and sales of agricultural products, and to provide formal education in "…agriculture, home economics, mechanical arts, and other professions that seemed practical at the time."
The act was eventually extended to allow all states that had remained with the Union during the American Civil War, and eventually all states, to establish such institutions. Most of the colleges established under the Morrill Act have since become full universities, and some are among the elite of the world.
Zimbabwe.
The term college is mainly used by private or independent secondary schools with Advanced Level (Upper 6th formers) and also Polytechnic Colleges which confer diplomas only. A student can complete secondary education (General Certificate of Secondary Education, GCSE) at 16 years and proceed straight to a poly-technical college or they can proceed to Advanced level (16 to 19 years) and obtain a General Certificate of Education (GCE) certificate which enables them to enrol at a University provided they have good grades alternatively with lower grades the GCE certificate holders will have an added advantage over their GCSE counterparts if they choose to enrol at a Poly-technical College. Some schools in Zimbabwe choose to offer the International Baccalaureate studies as an alternative to the GCSE and GCE.

</doc>
<doc id="5690" url="http://en.wikipedia.org/wiki?curid=5690" title="Chalmers University of Technology">
Chalmers University of Technology

Chalmers University of Technology (, often shortened to Chalmers) is a Swedish university located in Gothenburg that focuses on research and education in technology, natural science, architecture, maritime and other management areas.
History.
The University was founded in 1829 following a donation by William Chalmers, a director of the Swedish East India Company. He donated part of his fortune for the establishment of an "industrial school". Chalmers was run as a private institution until 1937, when the institute became a state-owned university. In 1994, the school was incorporated as an
aktiebolag under the control of the Swedish Government, the faculty and the Student Union. Chalmers is one of only three universities in Sweden which are named after a person, the other two being Karolinska Institutet and Linnaeus University.
Departments.
On 1 January 2005, the old schools were replaced by new departments:
In addition to these, Chalmers is home to six national competence centres in key fields like Mathematical Modelling, Environmental Science and Vehicle Safety (SAFER).
Students.
Approximately 40% of Sweden's graduate engineers and architects are educated at Chalmers. Each year, around 250 post graduate degrees are awarded as well as 850 graduate degrees. About 1,000 post-graduate students attend programmes at the university and many students are taking Master of Science engineering programmes and the Master of Architecture programme. From 2007, all Master's programmes are taught in English for both national and international students. This was a result of the adaptation to the Bologna process that started in 2004 at Chalmers (as the first technical university in Sweden).
Currently, about 10% of all students at Chalmers come from countries outside Sweden to enroll in a Master's or PhD program.
List of International Masters Programmes at Chalmers<br>
Previous List of International Masters Programmes at Chalmers (login now required)
Around 2,700 students also attend Bachelor of Science engineering programmes, merchant marine and other undergraduate courses at Campus Lindholmen. Chalmers also shares some students with Gothenburg University in the joint IT University project. The IT University focuses exclusively on information technology and offers Bachelor and Master programmes with degrees issued from either Chalmers or Gothenburg University, depending on the programme.
Chalmers confers honorary doctoral degrees to people outside the university who have shown great merit in their research or in society.
Organization.
Chalmers is an aktiebolag with 100 shares à 1,000 SEK, all of which is owned by a private foundation (Chalmers University of Technology Foundation) which appoints the university board and the president. The foundation has its members appointed by the Swedish government (4 to 8 seats), the departments appoints one member, the student union appoints one member and the president automatically gains one chair. Each department is led by a department head, usually a member of the faculty of that department. The faculty senate represents members of the faculty when decisions are taken.
Campuses.
In 1937, the school moved from the city center to the new Gibraltar Campus, named after the mansion which owned the grounds, where it is now located. The Lindholmen College Campus was created in the early 1990s and is located on the island of Hisingen. Campus Johanneberg and Campus Lindholmen, as they are now called, are connected by bus line number 16, but there have been numerous complaints that the campuses are too isolated from each other.
Societies and traditions.
Traditions include the pompous graduation ceremony and the Cortège procession, an annual public event.
Ties and partnerships.
Chalmers has partnerships with major industries mostly in the Gothenburg region such as Ericsson, Volvo, and SKF.
The University has general exchange agreements with many European and U.S. universities and maintains a special exchange program agreement with National Chiao Tung University (NCTU) in Taiwan where the exchange students from the two universities maintains offices for, among other things, helping local students with applying and preparing for an exchange year as well as acting as representatives (NCTU Europe – NCTU students at Chalmers, Chalmers Asia – Chalmers students at NCTU). It contributes also to the Top Industrial Managers for Europe (TIME) network.
A close collaboration between the Department of Computer Science and Engineering at Chalmers and ICVR at ETH Zurich is being established. As of 2014, Chalmers University of Technology is a member of the IDEA League network.
Rankings.
In the 2011 International Professional Ranking of Higher Education Institutions, which is established on the basis of the number of alumni holding a post of Chief executive officer (CEO) or equivalent in one of the Fortune Global 500 companies, Chalmers University of Technology ranked 38th in the world, ranking 1st in Sweden and 15th in Europe.
In the latest (2011) Academic Ranking of World Universities, the university was ranked between places 201 - 300 of all universities in the world and at the same time by the QS World University Rankings the university was ranked 202nd in the world (overall).
In the latest (2013/2014) Times Higher Education World University Rankings 2011/2012 Chalmers ranked between 276 and 300 of all global universities, down from 226-250 in the 2011/2012 rankings.
Rectors (Presidents).
Although the official Swedish title for the head is "rektor", the university now uses "President" as the English translation.

</doc>
<doc id="5691" url="http://en.wikipedia.org/wiki?curid=5691" title="Codex">
Codex

A codex (Latin "caudex" for "trunk of a tree" or "block of wood", "book"; plural "codices") is a book made up of a number of sheets of paper, vellum, papyrus, or similar, with hand-written content, usually stacked and bound by fixing one edge and with covers thicker than the sheets, but sometimes continuous and folded concertina-style. The alternative to paged codex format for a long document is the continuous scroll. Examples of folded codices are the Maya codices. Sometimes the term is used for a book-style format, including modern printed books but excluding folded books.
Developed by the Romans from wooden writing tablets, its gradual replacement of the scroll, the dominant form of book in the ancient world, has been termed the most important advance in the history of the book prior to the invention of printing. The codex altogether transformed the shape of the book itself and offered a form that lasted for centuries. The spread of the codex is often associated with the rise of Christianity, which adopted the format for the Bible early on. First described by the 1st-century AD Roman poet Martial, who praised its convenient use, the codex achieved numerical parity with the scroll around AD 300, and had completely replaced it throughout the now Christianised Greco-Roman world by the 6th century.
Advantages.
The codex holds considerable practical advantages over other book formats, such as compactness, sturdiness, ease of reference (a codex allows random access, as opposed to a scroll, which uses sequential access), and especially economy of materials; unlike the scroll, both recto and verso could be used for writing. Although the change from rolls to codices roughly coincides with the transition from papyrus to parchment as favourite writing material, the two developments are quite unconnected. In fact, any combination of codices and scrolls on the one hand with papyrus and parchment on the other is technically feasible and well attested from the historical record.
The codex began to replace the scroll (or roll), almost as soon as it was invented. For example, in Egypt by the fifth century, the codex outnumbered the scroll by ten to one based on surviving examples, and by the sixth century the scroll had almost vanished from use as a vehicle for literature.
Although technically even modern paperbacks are codices, the term is now used only for manuscript (hand-written) books which were produced from Late antiquity until the Middle Ages. The scholarly study of these manuscripts from the point of view of the bookbinding craft is called codicology; the study of ancient documents in general is called paleography.
History.
The Romans used precursors made of reusable wax-covered tablets of wood for taking notes and other informal writings. Two ancient polyptychs, a "pentatych" and "octotych", excavated at Herculaneum employed a unique connecting system that presages later sewing on thongs or cords. Julius Caesar may have been the first Roman to reduce scrolls to bound pages in the form of a note-book, possibly even as a papyrus codex. At the turn of the 1st century AD, a kind of folded parchment notebook called "pugillares membranei" in Latin became commonly used for writing in the Roman Empire. This term was used by both the Classical Latin poet Martial and the Christian apostle Paul. Martial used the term with reference to gifts of literature exchanged by Romans during the festival of Saturnalia. According to T.C. Skeat “…in at least three cases and probably in all, in the form of codices" and he theorized that this form of notebook was invented in Rome and then “…must have spread rapidly to the Near East…” In his discussion of one of the earliest parchment codices to survive from Oxyrhynchus in Egypt, Eric Turner seems to challenge Skeat’s notion when stating “…its mere existence is evidence that this book form had a prehistory” and that “early experiments with this book form may well have taken place outside of Egypt.” Early codices of parchment or papyrus appear to have been widely used as personal notebooks, for instance in recording copies of letters sent (Cicero "Fam." 9.26.1). The pages of parchment notebooks were commonly washed or scraped for re-use, called a palimpsest; and consequently writings in a codex were considered informal and impermanent.
As far back as the early 2nd century, there is evidence that the codex—usually of papyrus—was the preferred format among Christians: in the library of the Villa of the Papyri, Herculaneum (buried in AD 79), all the texts (Greek literature) are scrolls (see Herculaneum papyri); in the Nag Hammadi "library", secreted about AD 390, all the texts (Gnostic Christian) are codices. Despite this comparison, a fragment of a non-Christian parchment Codex of Demosthenes, "De Falsa Legatione" from Oxyrhynchus in Egypt demonstrates that the surviving evidence is insufficient to conclude whether Christians played a major, if not central, role in the development of early codices, or if they simply adopted the format to distinguish themselves from Jews. The earliest surviving fragments from codices come from Egypt and are variously dated (always tentatively) towards the end of the 1st century or in the first half of the 2nd. This group includes the Rylands Library Papyrus P52, containing part of St John's Gospel, and perhaps dating from between 125 and 160.
In Western culture the codex gradually replaced the scroll. From the 4th century, when the codex gained wide acceptance, to the Carolingian Renaissance in the 8th century, many works that were not converted from scroll to codex were lost to posterity. The codex was an improvement over the scroll in several ways. It could be opened flat at any page, allowing easier reading; the pages could be written on both front and back (recto and verso); and the codex, protected within its durable covers, was more compact and easier to transport.
The codex also made it easier to organize documents in a library because it had a stable spine on which the title of the book could be written. The spine could be used for the incipit, before the concept of a proper title was developed, during medieval times.
Although most early codices were made of papyrus, papyrus was fragile and supplies from Egypt, the only place where papyrus grew and was made into paper, became scanty; the more durable parchment and vellum gained favor, despite the cost.
The codices of pre-Columbian Mesoamerica had the same form as the European codex, but were instead made with long folded strips of either fig bark (amatl) or plant fibers, often with a layer of whitewash applied before writing. New World codices were written as late as the 16th century (see Maya codices and Aztec codices). Those written before the Spanish conquests seem all to have been single long sheets folded concertina-style, sometimes written on both sides of the local amatl paper.
In the Far East, the scroll remained standard for far longer than in the West. There were intermediate stages, such as scrolls folded concertina-style and pasted together at the back and books that were printed only on one side of the paper. Judaism still retains the Torah scroll, at least for ceremonial use.
Bookbinding.
Among the experiments of earlier centuries, scrolls were sometimes unrolled horizontally, as a succession of columns. (The Dead Sea Scrolls are a famous example of this format.) This made it possible to fold the scroll as an accordion. The next step was then to cut the folios, sew and glue them at their centers, making it easier to use the papyrus or vellum recto-verso as with a modern book. In traditional bookbinding, these assembled folios trimmed and curved were called "codex" in order to differentiate it from the "case" which we now know as "hard cover". Binding the codex was clearly a different procedure from binding the "case".

</doc>
<doc id="5692" url="http://en.wikipedia.org/wiki?curid=5692" title="Calf">
Calf

Calves ( or ; singular calf or ) are the young of domestic cattle. Calves are reared to become adult cattle, or are slaughtered for their meat, called veal.
Terminology.
"Calf" is the term used from birth to weaning, when it becomes known as a "weaner" or "weaner calf", though in some areas the term "calf" may be used until the animal is a yearling. The birth of a calf is known as "calving". A calf that has lost its mother is an orphan calf, also known as a "poddy" or "poddy-calf" in British English. "Bobby calves" are young calves which are to be slaughtered for human consumption. A "vealer" is a fat calf weighing less than about which is at about eight to nine months of age. A young female calf from birth until she has had a calf of her own is called a "heifer"
(). In the American Old West, a motherless or small, runty calf was sometimes referred to as a "dogie," (pronounced with a long "o") though in the classic traditional folk song, "Dogie's Lament," also known as "Git along little dogie," the "dogies" in question meant cattle strong enough to be herded from Texas to Wyoming, including weaners, yearling steers and other young, non-orphaned animals.
The term "calf" is also used for some other species. See "Other animals" below.
Early development.
Calves may be produced by natural means, or by artificial breeding using artificial insemination or embryo transfer.
Calves are born after a gestation of nine months. They usually stand within a few minutes of calving, and suckle within an hour. However, for the first few days they are not easily able to keep up with the rest of the herd, so young calves are often left hidden by their mothers, who visit them several times a day to suckle them. By a week old the calf is able to follow the mother all the time.
Some calves are ear tagged soon after birth, especially those that are stud cattle in order to correctly identify their dams (mothers), or in areas (such as the EU) where tagging is a legal requirement for cattle. A calf must have the very best of everything until it is at least eight months old if it is to reach its maximum potential. Typically when the calves are about two months old they are branded, ear marked, castrated and vaccinated.
Calf rearing systems.
The "single suckler" system of rearing calves is similar to that occurring naturally in wild cattle, where each calf is suckled by its own mother until it is weaned at about nine months old. This system is commonly used for rearing beef cattle throughout the world.
Cows kept on poor forage (as is typical in subsistence farming) produce a limited amount of milk. A calf left with such a mother all the time can easily drink all the milk, leaving none for human consumption. For dairy production under such circumstances, the calf's access to the cow must be limited, for example by penning the calf and bringing the mother to it once a day after partly milking her. The small amount of milk available for the calf under such systems may mean that it takes a longer time to rear, and in subsistence farming it is therefore common for cows to calve only in alternate years.
In more intensive dairy farming, cows can easily be bred and fed to produce far more milk than one calf can drink. In the "multi-suckler" system, several calves are fostered onto one cow in addition to her own, and these calves' mothers can then be used wholly for milk production. More commonly, calves of dairy cows are fed formula milk from a bottle or bucket from soon after birth.
Purebred female calves of dairy cows are reared as replacement dairy cows. Most purebred dairy calves are produced by artificial insemination (AI). By this method each bull can serve very many cows, so only a very few of the purebred dairy male calves are needed to provide bulls for breeding. The remainder of the male calves may be reared for beef or veal; however, some extreme dairy breeds carry so little muscle that rearing the purebred male calves may be uneconomic, and in this case they are often killed soon after birth and disposed of. Only a proportion of purebred heifers are needed to provide replacement cows, so often some of the cows in dairy herds are put to a beef bull to produce crossbred calves suitable for rearing as beef.
Veal calves may be reared entirely on milk formula and killed at about 18 or 20 weeks as "white" veal, or fed on grain and hay and killed at 22 to 35 weeks to produce red or pink veal.
Growth.
A commercial steer or bull calf is expected to put on about per month. A nine-month-old steer or bull is therefore expected to weigh about . Heifers will weigh at least at eight months of age. 
Calves are usually weaned at about eight to nine months of age, but depending on the season and condition of the dam, they might be weaned earlier. They may be paddock weaned, often next to their mothers, or weaned in stockyards. The latter system is preferred by some as it accustoms the weaners to the presence of people and they are trained to take feed other than grass. Small numbers may also be weaned with their dams with the use of weaning nose rings or nosebands which results in the mothers rejecting the calves' attempts to suckle. Many calves are also weaned when they are taken to the large weaner auction sales that are conducted in the south eastern states of Australia. Victoria and New South Wales have yardings of up to 8,000 weaners (calves) for auction sale in one day. The best of these weaners may go to the butchers. Others will be purchased by re-stockers to grow out and fatten on grass or as potential breeders. In the United States these weaners may be known as "feeders" and would be placed directly into feedlots.
At about 12 months old a beef heifer reaches puberty if she is well grown.
Diseases.
Calves suffer from few congenital abnormalities but the Akabane virus is widely distributed in temperate to tropical regions of the world. The virus is a teratogenic pathogen which causes abortions, stillbirths, premature births and congenital abnormalities, but occurs only during some years.
Uses of calves.
Calf meat for human consumption is called veal; also eaten are calf's brains and calf liver. The hide is used to make calfskin, or tanned into leather and called "Novillo," Spanish for steer. The fourth compartment of the stomach of slaughtered milk-fed calves is the source of rennet. The intestine is used to make Goldbeater's skin, and is the source of Calf Intestinal Alkaline Phosphatase (CIP).
Dairy cows can only produce milk after having calved, and so every dairy cow is allowed to produce one calf each year throughout her productive life. On average one of these calves will become a replacement dairy cow, and some of the rest may be reared for beef or veal; however some are effectively produced solely to allow the cow to produce milk.
Other animals.
In English the term "calf" is used by extension for the young of various other large species of mammal. In addition to other cattle (such as bison, yak and water buffalo), these include the young of camels, dolphins, elephants, giraffes, hippopotamuses, larger deer (such as moose, elk (wapiti) and red deer), rhinoceroses, porpoises, whales, walruses and larger seals.

</doc>
<doc id="5693" url="http://en.wikipedia.org/wiki?curid=5693" title="Claude Shannon">
Claude Shannon

Claude Elwood Shannon (April 30, 1916 – February 24, 2001) was an American mathematician, electronic engineer, and cryptographer known as "the father of information theory".
Shannon is famous for having founded information theory with a landmark paper that he published in 1948. However, he is also credited with founding both digital computer and digital circuit design theory in 1937, when, as a 21-year-old master's degree student at the Massachusetts Institute of Technology (MIT), he wrote his thesis demonstrating that electrical applications of boolean algebra could construct and resolve any logical, numerical relationship. Shannon contributed to the field of cryptanalysis for national defense during World War II, including his basic work on codebreaking and secure telecommunications.
Biography.
Shannon was born in Petoskey, Michigan. His father, Claude, Sr. (1862 – 1934), a descendant of early settlers of New Jersey, was a self-made businessman, and for a while, a Judge of Probate. Shannon's mother, Mabel Wolf Shannon (1890 – 1945), was a language teacher, and for a number of years she was the principal of Gaylord High School. Most of the first 16 years of Shannon's life were spent in Gaylord, Michigan, where he attended public school, graduating from Gaylord High School in 1932. Shannon showed an inclination towards mechanical and electrical things. His best subjects were science and mathematics, and at home he constructed such devices as models of planes, a radio-controlled model boat and a wireless telegraph system to a friend's house a half-mile away. While growing up, he also worked as a messenger for the Western Union company.
His childhood hero was Thomas Edison, whom he later learned was a distant cousin. Both were descendants of John Ogden (1609-1682), a colonial leader and an ancestor of many distinguished people.
Shannon was apolitical and an atheist.
Boolean theory and beyond.
In 1932, Shannon entered the University of Michigan, where he took a course that introduced him to the work of George Boole. He graduated in 1936 with two bachelor's degrees, one in electrical engineering and one in mathematics. He soon began his graduate studies in electrical engineering at the Massachusetts Institute of Technology (MIT), where he worked on Vannevar Bush's differential analyzer, an early analog computer.
While studying the complicated "ad hoc" circuits of the differential analyzer, Shannon saw that Boole's concepts could be used to great utility. A paper drawn from his 1937 master's degree thesis, "A Symbolic Analysis of Relay and Switching Circuits", was published in the 1938 issue of the "Transactions of the American Institute of Electrical Engineers". It also earned Shannon the Alfred Noble American Institute of American Engineers Award in 1939. Howard Gardner called Shannon's thesis "possibly the most important, and also the most famous, master's thesis of the century."
Victor Shestakov of the Moscow State University, had proposed a theory of systems of electrical switches based on Boolean logic earlier than Shannon in 1935, but the first publication of Shestakov's result was in 1941, after the publication of Shannon's thesis in America.
In this work, Shannon proved that boolean algebra and binary arithmetic could be used to simplify the arrangement of the electromechanical relays that were used then in telephone call routing switches. He next expanded this concept, and he also proved that it would be possible to use arrangements of relays to solve problems in Boolean algebra.
Using this property of electrical switches to do logic is the basic concept that underlies all electronic digital computers. Shannon's work became the foundation of practical digital circuit design when it became widely known in the electrical engineering community during and after World War II. The theoretical rigor of Shannon's work completely replaced the "ad hoc" methods that had previously prevailed.
Vannevar Bush suggested that Shannon, flush with this success, work on his dissertation at the Cold Spring Harbor Laboratory, funded by the Carnegie Institution, headed by Bush, to develop similar mathematical relationships for Mendelian genetics. This research resulted in Shannon's doctor of philosophy (Ph.D.) thesis at MIT in 1940, called "An Algebra for Theoretical Genetics."
In 1940, Shannon became a National Research Fellow at the Institute for Advanced Study in Princeton, New Jersey. In Princeton, Shannon had the opportunity to discuss his ideas with influential scientists and mathematicians such as Hermann Weyl and John von Neumann, and he also had occasional encounters with Albert Einstein and Kurt Gödel. Shannon worked freely across disciplines, and began to shape the ideas that would become Information Theory.
Wartime research.
Shannon then joined Bell Labs to work on fire-control systems and cryptography during World War II, under a contract with section D-2 (Control Systems section) of the National Defense Research Committee (NDRC).
Shannon met his wife Betty when she was a numerical analyst at Bell Labs. They were married in 1949.
For two months early in 1943, Shannon came into contact with the leading British cryptanalyst and mathematician Alan Turing. Turing had been posted to Washington to share with the U.S. Navy's cryptanalytic service the methods used by the British Government Code and Cypher School at Bletchley Park to break the ciphers used by the Kriegsmarine U-boats in the North Atlantic Ocean. He was also interested in the encipherment of speech and to this end spent time at Bell Labs. Shannon and Turing met at teatime in the cafeteria. Turing showed Shannon his 1936 paper that defined what is now known as the "Universal Turing machine"; this impressed Shannon, as many of its ideas complemented his own.
In 1945, as the war was coming to an end, the NDRC was issuing a summary of technical reports as a last step prior to its eventual closing down. Inside the volume on fire control a special essay titled "Data Smoothing and Prediction in Fire-Control Systems", coauthored by Shannon, Ralph Beebe Blackman, and Hendrik Wade Bode, formally treated the problem of smoothing the data in fire-control by analogy with "the problem of separating a signal from interfering noise in communications systems." In other words it modeled the problem in terms of data and signal processing and thus heralded the coming of the Information Age.
Shannon's work on cryptography was even more closely related to his later publications on communication theory. At the close of the war, he prepared a classified memorandum for Bell Telephone Labs entitled "A Mathematical Theory of Cryptography," dated September 1945. A declassified version of this paper was published in 1949 as "Communication Theory of Secrecy Systems" in the "Bell System Technical Journal". This paper incorporated many of the concepts and mathematical formulations that also appeared in his "A Mathematical Theory of Communication". Shannon said that his wartime insights into communication theory and cryptography developed simultaneously and that "they were so close together you couldn’t separate them". In a footnote near the beginning of the classified report, Shannon announced his intention to "develop these results ... in a forthcoming memorandum on the transmission of information." 
While he was at Bell Labs, Shannon proved that the cryptographic one-time pad is unbreakable in his classified research that was later published in October 1949. He also proved that any unbreakable system must have essentially the same characteristics as the one-time pad: the key must be truly random, as large as the plaintext, never reused in whole or part, and be kept secret.
Later on in the American Venona project, a supposed "one-time pad" system by the Soviets was partially broken by the National Security Agency, but this was because of misuses of the one-time pads by Soviet cryptographic technicians in the United States and Canada. The Soviet technicians made the mistake of using the same pads more than once sometimes, and this was noticed by American cryptanalysts.
Postwar contributions.
In 1948 the promised memorandum appeared as "A Mathematical Theory of Communication", an article in two parts in the July and October issues of the "Bell System Technical Journal". This work focuses on the problem of how best to encode the information a sender wants to transmit. In this fundamental work he used tools in probability theory, developed by Norbert Wiener, which were in their nascent stages of being applied to communication theory at that time. Shannon developed information entropy as a measure for the uncertainty in a message while essentially inventing the field of information theory.
The book, co-authored with Warren Weaver, "The Mathematical Theory of Communication", reprints Shannon's 1948 article and Weaver's popularization of it, which is accessible to the non-specialist. Warren Weaver pointed out that the word information in communication theory is not related to what you do say, but to what you could say. That is, information is a measure of one's freedom of choice when one selects a message. Shannon's concepts were also popularized, subject to his own proofreading, in John Robinson Pierce's "Symbols, Signals, and Noise".
Information theory's fundamental contribution to natural language processing and computational linguistics was further established in 1951, in his article "Prediction and Entropy of Printed English", showing upper and lower bounds of entropy on the statistics of English - giving a statistical foundation to language analysis. In addition, he proved that treating whitespace as the 27th letter of the alphabet actually lowers uncertainty in written language, providing a clear quantifiable link between cultural practice and probabilistic cognition.
Another notable paper published in 1949 is "Communication Theory of Secrecy Systems", a declassified version of his wartime work on the mathematical theory of cryptography, in which he proved that all theoretically unbreakable ciphers must have the same requirements as the one-time pad. He is also credited with the introduction of sampling theory, which is concerned with representing a continuous-time signal from a (uniform) discrete set of samples. This theory was essential in enabling telecommunications to move from analog to digital transmissions systems in the 1960s and later.
He returned to MIT to hold an endowed chair in 1956.
Hobbies and inventions.
Outside of his academic pursuits, Shannon was interested in juggling, unicycling, and chess. He also invented many devices, including rocket-powered flying discs, a motorized pogo stick, and a flame-throwing trumpet for a science exhibition. One of his more humorous devices was a box kept on his desk called the "Ultimate Machine", based on an idea by Marvin Minsky. Otherwise featureless, the box possessed a single switch on its side. When the switch was flipped, the lid of the box opened and a mechanical hand reached out, flipped off the switch, then retracted back inside the box. Renewed interest in the "Ultimate Machine" has emerged on YouTube and Thingiverse. In addition he built a device that could solve the Rubik's Cube puzzle.
He is also considered the co-inventor of the first wearable computer along with Edward O. Thorp. The device was used to improve the odds when playing roulette.
Legacy and tributes.
Shannon came to MIT in 1956 to join its faculty and to conduct work in the Research Laboratory of Electronics (RLE). He continued to serve on the MIT faculty until 1978. To commemorate his achievements, there were celebrations of his work in 2001, and there are currently six statues of Shannon sculpted by Eugene L. Daub: one at the University of Michigan; one at MIT in the Laboratory for Information and Decision Systems; one in Gaylord, Michigan; one at the University of California at San Diego; one at Bell Labs; and another at AT&T Shannon Labs. After the breakup of the Bell system, the part of Bell Labs that remained with AT&T Corporation was named Shannon Labs in his honor.
According to Neil Sloane, an AT&T Fellow who co-edited Shannon's large collection of papers in 1993, the perspective introduced by Shannon's communication theory (now called information theory) is the foundation of the digital revolution, and every device containing a microprocessor or microcontroller is a conceptual descendant of Shannon's publication in 1948: "He's one of the great men of the century. Without him, none of the things we know today would exist. The whole digital revolution started with him." The unit shannon is named after Claude Shannon.
Shannon developed Alzheimer's disease, and spent his last years in a nursing home in Massachusetts oblivious to the marvels of the digital revolution he had helped create. He was survived by his wife, Mary Elizabeth Moore Shannon, his son, Andrew Moore Shannon, his daughter, Margarita Shannon, his sister, Catherine Shannon Kay, and his two granddaughters. His wife stated in his obituary that, had it not been for Alzheimer's disease, "He would have been bemused" by it all.
Other work.
Shannon's mouse.
Theseus, created in 1950, was a magnetic mouse controlled by a relay circuit that enabled it to move around a maze of 25 squares. Its dimensions were the same as an average mouse. The maze configuration was flexible and it could be modified at will. The mouse was designed to search through the corridors until it found the target. Having travelled through the maze, the mouse would then be placed anywhere it had been before and because of its prior experience it could go directly to the target. If placed in unfamiliar territory, it was programmed to search until it reached a known location and then it would proceed to the target, adding the new knowledge to its memory thus learning. Shannon's mouse appears to have been the first artificial learning device of its kind.
Shannon's computer chess program.
In 1950 Shannon published a paper on computer chess entitled "Programming a Computer for Playing Chess". It describes how a machine or computer could be made to play a reasonable game of chess. His process for having the computer decide on which move to make is a minimax procedure, based on an evaluation function of a given chess position. Shannon gave a rough example of an evaluation function in which the value of the black position was subtracted from that of the white position. "Material" was counted according to the usual chess piece relative value (1 point for a pawn, 3 points for a knight or bishop, 5 points for a rook, and 9 points for a queen). He considered some positional factors, subtracting ½ point for each doubled pawns, backward pawn, and isolated pawn. Another positional factor in the evaluation function was "mobility", adding 0.1 point for each legal move available. Finally, he considered checkmate to be the capture of the king, and gave the king the artificial value of 200 points. Quoting from the paper:
The evaluation function is clearly for illustrative purposes, as Shannon stated. For example, according to the function, pawns that are doubled as well as isolated would have no value at all, which is clearly unrealistic.
The Las Vegas connection: information theory and its applications to game theory.
Shannon and his wife Betty also used to go on weekends to Las Vegas with MIT mathematician Ed Thorp, and made very successful forays in blackjack using game theory type methods co-developed with fellow Bell Labs associate, physicist John L. Kelly Jr. based on principles of information theory. His method, known as the High-Low method, a level 1 count methodology, works by adding 1, 0, or -1 depending on the cards that appear. Shannon and Thorp also invented a small, concealable computer to help them calculate odds while gambling. They made a fortune, as detailed in the book "Fortune's Formula" by William Poundstone and corroborated by the writings of Elwyn Berlekamp, Kelly's research assistant in 1960 and 1962. Shannon and Thorp also applied the same theory, later known as the "Kelly criterion", to the stock market with even better results. Claude Shannon's card count techniques were explained in "Bringing Down the House", the best-selling book published in 2003 about the MIT Blackjack Team by Ben Mezrich. In 2008, the book was adapted into a drama film titled "21".
Shannon's maxim.
Shannon formulated a version of Kerckhoffs' principle as "The enemy knows the system". In this form it is known as "Shannon's maxim".

</doc>
<doc id="5694" url="http://en.wikipedia.org/wiki?curid=5694" title="Cracking">
Cracking

Cracking may refer to:
In computing:

</doc>
<doc id="5695" url="http://en.wikipedia.org/wiki?curid=5695" title="Community">
Community

A community is a social unit of any size that shares common values. Although embodied or face-to-face communities are usually small, larger or more extended communities such as a national community, international community and virtual community are also studied.
In human communities, intent, belief, resources, preferences, needs, risks, and a number of other conditions may be present and common, affecting the identity of the participants and their degree of cohesiveness.
Since the advent of the Internet, the concept of community has less geographical limitation, as people can now gather virtually in an online community and share common interests regardless of physical location. Prior to the internet, virtual communities (like social or academic organizations) were far more limited by the constraints of available communication and transportation technologies.
The word "community" is derived from the Old French "comunete" which is derived from the Latin "communitas" (from Latin "communis", things held in common), a broad term for fellowship or organized society. One broad definition which incorporates all the different forms of community is 
Perspectives from various disciplines.
Sociology.
Gemeinschaft and Gesellschaft.
German sociologist Ferdinand Tönnies distinguished between two types of human association: "Gemeinschaft" (usually translated as "community") and "Gesellschaft" ("society" or "association"). In his 1887 work, "Gemeinschaft und Gesellschaft", Tönnies argued that "Gemeinschaft" is perceived to be a tighter and more cohesive social entity, due to the presence of a "unity of will." He added that family and kinship were the perfect expressions of "Gemeinschaft", but that other shared characteristics, such as place or belief, could also result in "Gemeinschaft". This paradigm of communal networks and shared social understanding has been applied to multiple cultures in many places throughout history. "Gesellschaft", on the other hand, is a group in which the individuals who make up that group are motivated to take part in the group purely by self-interest. He also proposed that in the real world, no group was either pure "Gemeinschaft" or pure "Gesellschaft," but, rather, a mixture of the two.
Social capital.
If community exists, both freedom and security may exist as well. The community then takes on a life of its own, as people become free enough to share and secure enough to get along. The sense of connectedness and formation of social networks comprise what has become known as social capital.
Social capital is defined by Robert D. Putnam as "the collective value of all social networks and species (who people know) and the inclinations that arise from these works to do things for each other (norms of reciprocity)." Social capital in action can be seen in all sorts of groups, including neighbors keeping an eye on each other's homes. However, as Putnam notes in "Bowling Alone: The Collapse and Revival of American Community" (2000), social capital has been falling in the United States. Putnam found that over the past 25 years, attendance at club meetings has fallen 58 percent, family dinners are down 33 percent, and having friends visit has fallen 45 percent.
The same patterns are also evident in many other western countries. Western cultures are thus said to be losing the spirit of community that once were found in institutions including churches and community centers. Sociologist Ray Oldenburg states in "The Great Good Place" that people need three places: 1) the home, 2) the office, and, 3) the community hangout or gathering place. With this philosophy in mind, many grassroots efforts such as The Project for Public Spaces are being started to create this "Third Place" in communities. They are taking form in independent bookstores, coffeehouses, local pubs, and through new and innovative means to create the social capital needed to foster the sense and spirit of community.
As a measure of religiosity.
According to the sociologist Mervin Verbit, community may be understood as one of the key components of religiosity. And communal involvement itself may be broken down into four dimensions:
The content of one's communal involvement may vary from person to person (or from one religious organization to the next), as will the degree of the person's participation (frequency), and the intensity and centrality of that involvement (for that person).
Psychology.
Community.
In a seminal 1986 study, McMillan and Chavis identify four elements of "sense of community":
They give the following example of the interplay between these factors:
Someone puts an announcement on the dormitory bulletin board about the formation of an intramural dormitory basketball team. People attend the organizational meeting as strangers out of their individual needs (integration and fulfillment of needs). The team is bound by place of residence (membership boundaries are set) and spends time together in practice (the contact hypothesis). They play a game and win (successful shared valent event). While playing, members exert energy on behalf of the team (personal investment in the group). As the team continues to win, team members become recognized and congratulated (gaining honor and status for being members), Influencing new members to join and continue to do the same. Someone suggests that they all buy matching shirts and shoes (common symbols) and they do so (influence).
A "Sense of Community Index" (SCI) has been developed by Chavis and colleagues and revised and adapted by others. Although originally designed to assess sense of community in neighborhoods, the index has been adapted for use in schools, the workplace, and a variety of types of communities.
Studies conducted by the APPA show substantial evidence that young adults who feel a sense of belonging in a community, particularly small communities, develop fewer psychiatric and depressive disorders than those who do not have the feeling of love and belonging.
Anthropology.
Cultural (or social) anthropology has traditionally looked at community through the lens of ethnographic fieldwork and ethnography continues to be an important methodology for study of modern communities. Other anthropological approaches that deal with various aspects of community include cross-cultural studies and the anthropology of religion. Cultures in modern society are also studied in the fields of urban anthropology, ethnic studies, ecological anthropology, and psychological anthropology. Since the 1990s, internet communities have increasingly been the subject of research in the emerging field of cyber anthropology.
The group of people having same identity is called community, Anthropologist Iahtsham Hussain Khatti
Archaeology.
In archaeological studies of social communities the term "community" is used in two ways, paralleling usage in other areas. The first is an informal definition of community as a place where people used to live. In this sense it is synonymous with the concept of an ancient settlement, whether a hamlet, village, town, or city. The second meaning is similar to the usage of the term in other social sciences: a community is a group of people living near one another who interact socially. Social interaction on a small scale can be difficult to identify with archaeological data. Most reconstructions of social communities by archaeologists rely on the principle that social interaction is conditioned by physical distance. Therefore a small village settlement likely constituted a social community, and spatial subdivisions of cities and other large settlements may have formed communities. Archaeologists typically use similarities in material culture—from house types to styles of pottery—to reconstruct communities in the past. This is based on the assumption that people or households will share more similarities in the types and styles of their material goods with other members of a social community than they will with outsiders.
Business and communications.
Organizational communication.
Effective communication practices in group and organizational settings are very important to the formation and maintenance of communities. The ways that ideas and values are communicated within communities are important to the induction of new members, the formulation of agendas, the selection of leaders and many other aspects. Organizational communication is the study of how people communicate within an organizational context and the influences and interactions within organizational structures. Group members depend on the flow of communication to establish their own identity within these structures and learn to function in the group setting. Although organizational communication, as a field of study, is usually geared toward companies and business groups, these may also be seen as communities. The principles of organizational communication can also be applied to other types of communities.
Ecology.
In ecology, a community is an assemblage of populations of different species, interacting with one another. Community ecology is the branch of ecology that studies interactions between and among species. It considers how such interactions, along with interactions between species and the abiotic environment, affect community structure and species richness, diversity and patterns of abundance. Species interact in three ways: competition, predation and mutualism. Competition typically results in a double negative—that is both species lose in the interaction. Predation is a win/lose situation with one species winning. Mutualism, on the other hand, involves both species cooperating in some way, with both winning
Interdisciplinary perspectives.
Socialization.
The process of learning to adopt the behavior patterns of the community is called socialization. The most fertile time of socialization is usually the early stages of life, during which individuals develop the skills and knowledge and learn the roles necessary to function within their culture and social environment. For some psychologists, especially those in the psychodynamic tradition, the most important period of socialization is between the ages of one and ten. But socialization also includes adults moving into a significantly different environment, where they must learn a new set of behaviors.
Socialization is influenced primarily by the family, through which children first learn community norms. Other important influences include schools, peer groups, people, mass media, the workplace, and government. The degree to which the norms of a particular society or community are adopted determines one's willingness to engage with others. The norms of tolerance, reciprocity, and trust are important "habits of the heart," as de Tocqueville put it, in an individual's involvement in community.
Community development.
Community development, often linked with community work or community planning, is often formally conducted by non-government organisations (NGOs), universities or government agencies to progress the social well-being of local, regional and, sometimes, national communities. Less formal efforts, called community building or community organizing, seek to empower individuals and groups of people by providing them with the skills they need to effect change in their own communities. These skills often assist in building political power through the formation of large social groups working for a common agenda. Community development practitioners must understand both how to work with individuals and how to affect communities' positions within the context of larger social institutions.
Formal programs conducted by universities are often used to build a knowledge base to drive curricula in sociology and community studies. The General Social Survey from the National Opinion Research Center at the University of Chicago and the Saguaro Seminar at the John F. Kennedy School of Government at Harvard University are examples of national community development in the United States. In the United Kingdom, Oxford University has led in providing extensive research in the field through its " Community Development Journal," used worldwide by sociologists and community development practitioners.
At the intersection between community "development" and community "building" are a number of programs and organizations with community development tools. One example of this is the program of the Asset Based Community Development Institute of Northwestern University. The institute makes available downloadable tools to assess community assets and make connections between non-profit groups and other organizations that can help in community building. The Institute focuses on helping communities develop by "mobilizing neighborhood assets" — building from the inside out rather than the outside in.
Community building and organizing.
In "The Different Drum: Community-Making and Peace," Scott Peck argues that the almost accidental sense of community that exists at times of crisis can be consciously built. Peck believes that conscious community building is a process of deliberate design based on the knowledge and application of certain rules. He states that this process goes through four stages:
More recently Peck remarked that building a sense of community is easy but maintaining this sense of community is difficult in the modern world. Community building can use a wide variety of practices, ranging from simple events such as potlucks and small book clubs to larger-scale efforts such as mass festivals and construction projects that involve local participants rather than outside contractors.
Community building that is geared toward citizen action is usually termed "community organizing." In these cases, organized community groups seek accountability from elected officials and increased direct representation within decision-making bodies. Where good-faith negotiations fail, these constituency-led organizations seek to pressure the decision-makers through a variety of means, including picketing, boycotting, sit-ins, petitioning, and electoral politics. The ARISE Detroit! coalition and the Toronto Public Space Committee are examples of activist networks committed to shielding local communities from government and corporate domination and inordinate influence.
Community organizing is sometimes focused on more than just resolving specific issues. Organizing often means building a widely accessible power structure, often with the end goal of distributing power equally throughout the community. Community organizers generally seek to build groups that are open and democratic in governance. Such groups facilitate and encourage consensus decision-making with a focus on the general health of the community rather than a specific interest group. The three basic types of community organizing are grassroots organizing, coalition building, and "institution-based community organizing," (also called "broad-based community organizing," an example of which is faith-based community organizing, or Congregation-based Community Organizing).
If communities are developed based on something they share in common, whether that be location or values, then one challenge for developing communities is how to incorporate individuality and differences. Indeed, as Rebekah Nathan suggests in her book, "My Freshman Year", we are actually drawn to developing communities totally based on sameness, despite stated commitments to diversity, such as those found on university websites. Nathan states that certain commonalities allow college students to cohere: "What holds students together, really, is age, pop culture, a handful of (recent) historical events, and getting a degree" (qtd. In Barrios 229). Universities may try to create community through all freshman reads, freshman seminars, and school pride; however, Nathan argues students will only form communities based on the attributes, such as age and pop culture, that they bring with them to college. Nathan's point, then, is that people come to college and don't expand their social horizons and cultural tolerance, which can prevent the development of your social community. (Barrios, Barclay. "Emerging: Contemporary Readings for Writers". New York: Bedford St. Martins, 2010.)
Community currencies.
Some communities have developed their own "Local Exchange Trading Systems" (LETS) and local currencies, such as the Ithaca Hours system, to encourage economic growth and an enhanced sense of community. Community currencies have recently proven valuable in meeting the needs of people living in various South American nations, particularly Argentina, that recently suffered as a result of the collapse of the Argentinian national currency.
Community service.
Community service is usually performed in connection with a nonprofit organization, but it may also be undertaken under the auspices of government, one or more businesses, or by individuals. It is typically unpaid and voluntary. However, it can be part of alternative sentencing approaches in a justice system and it can be required by educational institutions.
Types of community.
A number of ways to categorize types of community have been proposed; one such breakdown is:
Communities are nested; one community can contain another—for example a geographic community may contain a number of ethnic communities.
Location.
Possibly the most common usage of the word ""community"" indicates a large group living in close proximity. Examples of local community include:
Identity.
In some contexts, ""community"" indicates a group of people with a common identity other than location. Members often interact regularly. Common examples in everyday usage include:
Overlaps.
Some communities share both location and other attributes. Members choose to live near each other because of one or more common interests.
Special nature of human community.
Definitions of community as "organisms inhabiting a common environment and interacting with one another," while scientifically accurate, do not convey the richness, diversity and complexity of human communities. Their classification, likewise is almost never precise. Untidy as it may be, community is vital for humans. M. Scott Peck expresses this in the following way: "There can be no vulnerability without risk; there can be no community without vulnerability; there can be no peace, and ultimately no life, without community."

</doc>
<doc id="5696" url="http://en.wikipedia.org/wiki?curid=5696" title="Community college">
Community college

A community college is a type of educational institution. The term can have different meanings in different countries.
Australia.
In Australia, the term community college is not used. Analogous to community colleges are colleges or institutes of Technical and Further Education (TAFEs); public institutions mostly regulated at state and territory level. There are also an increasing number of private providers of varying social esteem; often these are colloquially called 'colleges'. 
TAFEs and other providers carry on the tradition of adult education, which was established in Australia around mid 19th century when evening classes were held to help adults enhance their numeracy and literacy skills. The majority of Australian universities can also be traced back to such forerunners, although obtaining a university charter has always changed their nature. In TAFEs and colleges today, courses are designed for personal development of an individual and/or for employment outcomes. Educational programs cover a variety of topics such as arts, languages, business and lifestyle; and are usually timetabled to be conducted in the evenings or weekends to accommodate people working full-time. Funding for colleges may come from government grants and course fees; and many are not-for-profit organisations. There are located in metropolitan, regional and rural locations of Australia. 
Learning offered by TAFEs and colleges has changed over the years. By the 1980s many colleges had recognised a community need for computer training and since then thousands of people have been up-skilled through IT courses. The majority of colleges by the late 20th century had also become Registered Training Organisations; recognising the need to offer individuals a nurturing, non-traditional education venue to gain skills that would better prepare them for the workplace and potential job openings. TAFEs and colleges have not traditionally offered bachelor's degrees, instead providing pathway arrangements with universities to continue towards degrees. The American innovation of the associate degree is emerging at some institutions. Certificate courses I to IV, diplomas and advanced diplomas are typically offered, the latter deemed equivalent to an undergraduate qualification, albeit typically in more vocational areas. Recently, some TAFE institutes (and private providers) have also become Higher Education providers in their own right and are now starting to offer bachelor degrees programs.
Canada.
In Canada, the term community college is not widely used. There are 150 institutions that could be roughly equivalent of the US community college in certain contexts. They are usually referred to simply as "colleges" since in common usage a degree granting institution is almost exclusively a university. In the province of Quebec, even when speaking in English, all colleges are often incorrectly called Cégeps, the French acronym for the public system: "Collège d'enseignement général et professionnel", meaning "College of General and Vocational Education". (The word College can also refer to a private High School in Quebec). 
Colleges are educational institutions providing higher education and tertiary education, granting certificates, and diplomas. Associate's degrees and bachelor's degrees are granted by universities, but, in some courses of study, there may be an agreement between colleges and universities to collaborate on the education requirements toward a degree. Only in Western Canada is the term Associates degree used as in the United States. In other parts of Canada a degree is usually attained as a 4 year study program, and to a much lesser degree now (except in Quebec, where it is the norm), in 3 years. 
Each province has its own educational system reflecting the decentralization of the Canadian provinces and therefore of the education system. However, most of the colleges began in the mid-1960s to provide education and training for the then-emerging baby boom generation and for immigrants from around the world who were starting to enter the country. 
Malaysia.
Community colleges in Malaysia are a network of educational institutions whereby vocational and technical skills training could be provided at all levels for school leavers before they entered the workforce. The community colleges also provide an infrastructure for rural communities to gain skills training through short courses as well as providing access to a post-secondary education.
At the moment, most community colleges award qualifications up to Level 3 in the Malaysian Qualifications Framework (Certificate 3) in both the Skills sector (Sijil Kemahiran Malaysia or the Malaysian Skills Certificate) as well as the Vocational and Training sector but the number of community colleges that are starting to award Level 4 qualifications (Diploma) are increasing. This is two levels below a Bachelor's degree (Level 6 in the MQF) and students within the system who intend to further their studies to that level will usually seek entry into Advanced Diploma programs in public universities, polytechnics or accredited private providers.
Philippines.
In the Philippines, a community school functions as elementary or secondary school at daytime and towards the end of the day convert into a community college. This type of institution offers night classes under the supervision of the same principal, and the same faculty members who are given part-time college teaching load.
The concept of community college dates back to the time of the former Minister of Education, Culture and Sports (MECS) that had under its wings the Bureaus of Elementary Education, Secondary Education, Higher Education and Vocational-Technical Education. MECS Secretary, Dr. Cecilio Putong, who in 1971 wrote that a community school is a school established in the community, by the community, and for the community itself. Dr. Pedro T. Orata of Pangasinan shared the same idea, hence the establishment of a Community College, now called the City College of Urdaneta.
A community college like the one in Abuyog, Leyte can operate with only PHP 124,000 annual budget in a 2-storey structure housing more than 700 students.
United Kingdom (excluding Scotland).
In the United Kingdom, except for Scotland, a community college is a school which not only provides education for the school age population (11–18) of the locality, but also additional services and education to adults and other members of the community. This education includes but is not limited to sports, adult literacy and lifestyle education. Usually at the age of 16 when students finish their secondary school studies, they move on to a sixth form college where they study for their A-levels (although some secondary schools have integrated sixth forms). After the 2 year A-level period, they may then proceed to a college of further education or a university.
United States.
In the United States, community colleges, sometimes called junior colleges, technical colleges, two-year colleges, or city colleges, are primarily two-year public institutions providing higher education and lower-level tertiary education, granting certificates, diplomas, and associate's degrees. Many also offer continuing and adult education.
After graduating from a community college, some students transfer to a four-year liberal arts college or university for two to three years to complete a bachelor's degree.
Before the 1970s, community colleges in the United States were more commonly referred to as junior colleges, and that term is still used at some institutions. However, the term "junior college" has evolved to describe private two-year institutions, whereas the term "community college" has evolved to describe publicly funded two-year institutions. The name derives from the fact that community colleges primarily attract and accept students from the local community, and are often supported by local tax revenue.
Comprehensive community colleges.
Many schools have adapted the term "comprehensive" to describe their institutions. These schools typically offer six facets of education: 
Within the transfer education category, comprehensive schools typically have articulation agreements in place that provide prearranged acceptance into specific four-year institutions. At some community colleges, the partnering four-year institution teaches the third and fourth year courses at the community college location and thereby allows a student to obtain a four-year degree without having to physically move to the four-year school.
There is a number of institutions and organizations which provide community college research to inform practice and policy.
For background on U.S. community college libraries, see "Disposed to Consolidation and Innovation: Criteria for the Community College Specialization."
India.
This community college system went through a thorough modification to suit to the Indian need. More than 500 community colleges are running in India. Various bodies recognized this system.
Research.
There are a number of research organizations and publications who focus upon the activities of community college, junior college, and technical college institutions. Many of these institutions and organizations present the most current research and practical outcomes at annual community college conferences.
Additionally, several peer-reviewed journals extensively publish research on community colleges:

</doc>
<doc id="5697" url="http://en.wikipedia.org/wiki?curid=5697" title="Civil Rights Memorial">
Civil Rights Memorial

The Civil Rights Memorial is a memorial in Montgomery, Alabama to 40 people who died in the struggle for the equal and integrated treatment of all people, regardless of race, during the Civil Rights Movement in the United States. The memorial is sponsored by the Southern Poverty Law Center.
The names included belong to those who died between 1954 and 1968. Those dates were chosen because in 1954 the U.S. Supreme Court ruled that racial segregation in schools was unlawful and 1968 is the year of Martin Luther King's assassination. The monument was created by Maya Lin who created the Vietnam Veterans Memorial in Washington, D.C. The memorial was dedicated in 1989.
The concept of Maya Lin's design is based on the soothing and healing effect of water. It was inspired by Martin Luther King, Jr.'s paraphrase ""... we will not be satisfied until justice rolls down like waters and righteousness like a mighty stream. ..."", from the "I Have a Dream" speech, delivered at the Lincoln Memorial, Washington D.C. on August 28, 1963. This passage in King's speech is a direct reference to Amos , as translated in the American Standard Version of the Bible Joshua Heschel "The Prophets". The memorial is a fountain in the form of a round stone inverted cone. A film of water flows over the base of the cone, which contains the 40 names included. It is possible to touch the smooth film of water and temporarily alter the surface film, which quickly returns to smoothness. As such, the memorial represents the aspirations of the American civil rights movement against racism.
Tours & Location.
The memorial is located downtown at 400 Washington Avenue in an open plaza in front of the Civil Rights Memorial Center, which was formerly the offices of the Southern Poverty Law Center and which moved across the street into a new building in 2001. The memorial may be visited freely 24 hours a day, 7 days a week. 
The Civil Rights Memorial Center offers guided group tours lasting approximately one hour. Tours are available by appointment, Monday through Saturday.
The memorial is within walking distance of only a few blocks from other historic sites, including the Dexter Avenue King Memorial Baptist Church, the Alabama State Capitol, the Alabama Department of Archives and History and the Rosa Parks Library and Museum.

</doc>
<doc id="5698" url="http://en.wikipedia.org/wiki?curid=5698" title="Charles Babbage">
Charles Babbage

Charles Babbage, FRS (26 December 1791 – 18 October 1871) was an English polymath. He was a mathematician, philosopher, inventor and mechanical engineer, who is best remembered now for originating the concept of a programmable computer.
Considered a "father of the computer", Babbage is credited with inventing the first mechanical computer that eventually led to more complex designs. His varied work in other fields has led him to be described as "pre-eminent" among the many polymaths of his century.
Parts of Babbage's uncompleted mechanisms are on display in the London Science Museum. In 1991, a perfectly functioning difference engine was constructed from Babbage's original plans. Built to tolerances achievable in the 19th century, the success of the finished engine indicated that Babbage's machine would have worked.
Early life.
Babbage's birthplace is disputed, but according to the "Oxford Dictionary of National Biography" he was most likely born at 44 Crosby Row, Walworth Road, London, England. A blue plaque on the junction of Larcom Street and Walworth Road commemorates the event.
His date of birth was given in his obituary in "The Times" as 26 December 1792; but then a nephew wrote to say that Babbage was born one year earlier, in 1791. The parish register of St. Mary's Newington, London, shows that Babbage was baptised on 6 January 1792, supporting a birth year of 1791.
Babbage was one of four children of Benjamin Babbage and Betsy Plumleigh Teape. His father was a banking partner of William Praed in founding Praed's & Co. of Fleet Street, London, in 1801. In 1808, the Babbage family moved into the old Rowdens house in East Teignmouth. Around the age of eight Babbage was sent to a country school in Alphington near Exeter to recover from a life-threatening fever. For a short time he attended King Edward VI Grammar School in Totnes, South Devon, but his health forced him back to private tutors for a time.
Babbage then joined the 30-student Holmwood academy, in Baker Street, Enfield, Middlesex, under the Reverend Stephen Freeman. The academy had a library that prompted Babbage's love of mathematics. He studied with two more private tutors after leaving the academy. The first was a clergyman near Cambridge; through him Babbage encountered Charles Simeon and his evangelical followers, but the tuition was not what he needed. He was brought home, to study at the Totnes school: this was at age 16 or 17. The second was an Oxford tutor, under whom Babbage reached a level in Classics sufficient to be accepted by Cambridge.
At the University of Cambridge.
Babbage arrived at Trinity College, Cambridge, in October 1810. He was already self-taught in some parts of contemporary mathematics; he had read in Robert Woodhouse, Joseph Louis Lagrange, and Marie Agnesi. As a result he was disappointed in the standard mathematical instruction available at Cambridge.
Babbage, John Herschel, George Peacock, and several other friends formed the Analytical Society in 1812; they were also close to Edward Ryan. As a student, Babbage was also a member of other societies such as The Ghost Club, concerned with investigating supernatural phenomena, and the Extractors Club, dedicated to liberating its members from the madhouse, should any be committed to one.
In 1812 Babbage transferred to Peterhouse, Cambridge. He was the top mathematician there, but did not graduate with honours. He instead received a degree without examination in 1814. He had defended a thesis that was considered blasphemous in the preliminary public disputation; but it is not known whether this fact is related to his not sitting the examination.
After Cambridge.
Considering only his reputation, Babbage quickly made progress. He lectured to the Royal Institution on astronomy in 1815, and was elected a Fellow of the Royal Society in 1816. After graduation, on the other hand, he applied for positions unsuccessfully, and had little in the way of career. In 1816 he was a candidate for a teaching job at Haileybury College; he had recommendations from James Ivory and John Playfair, but lost out to Henry Walter. In 1819, Babbage and Herschel visited Paris and the Society of Arcueil, meeting leading French mathematicians and physicists. That year Babbage applied to be professor at the University of Edinburgh, with the recommendation of Pierre Simon Laplace; the post went to William Wallace.
With Herschel, Babbage worked on the electrodynamics of Arago's rotations, publishing in 1825. Their explanations were only transitional, being picked up and broadened by Michael Faraday. The phenomena are now part of the theory of eddy currents, and Babbage and Herschel missed some of the clues to unification of electromagnetic theory, staying close to Ampère's force law.
Babbage purchased the actuarial tables of George Barrett, who died in 1821 leaving unpublished work, and surveyed the field in 1826 in "Comparative View of the Various Institutions for the Assurance of Lives". This interest followed a project to set up an insurance company, prompted by Francis Baily and mooted in 1824, but not carried out. Babbage did calculate actuarial tables for that scheme, using Equitable Society mortality data from 1762 onwards.
During this whole period Babbage depended awkwardly on his father's support, given his father's attitude to his early marriage, of 1814: he and Edward Ryan wedded the Whitmore sisters. He made a home in Marylebone in London, and founded a large family. On his father's death in 1827, Babbage inherited a large estate (value around £100,000), making him independently wealthy. After his wife's death in the same year he spent time travelling. In Italy he met Leopold II, Grand Duke of Tuscany, foreshadowing a later visit to Piedmont. In April 1828 he was in Rome, and relying on Herschel to manage the difference engine project, when he heard that he had become professor at Cambridge, a position he had three times failed to obtain (in 1820, 1823 and 1826).
Astronomical Society.
Babbage was instrumental in founding the Astronomical Society in 1820. Its initial aims were to reduce astronomical calculations to a more standard form, and to circulate data. These directions were closely connected with Babbage's ideas on computation, and in 1824 he won its Gold Medal, cited "for his invention of an engine for calculating mathematical and astronomical tables".
Babbage's motivation to overcome errors in tables by mechanisation has been a commonplace since Dionysius Lardner wrote about it in 1834 in the "Edinburgh Review" (under Babbage's guidance). The context of these developments is still debated. Babbage's own account of the origin of the difference engine begins with the Astronomical Society's wish to improve "The Nautical Almanac". Babbage and Herschel were asked to oversee a trial project, to recalculate some part of those tables. With the results to hand, discrepancies were found. This was in 1821 or 1822, and was the occasion on which Babbage formulated his idea for mechanical computation. The issue of the "Nautical Almanac" is now described as a legacy of a polarisation in British science caused by attitudes to Sir Joseph Banks, who had died in 1820.
Babbage studied the requirements to establish a modern postal system, with his friend Thomas Frederick Colby, concluding there should be a uniform rate that was put into effect with the introduction of the Uniform Fourpenny Post supplanted by the Uniform Penny Post in 1839 and 1840. Colby was another of the founding group of the Society. He was also in charge of the Survey of Ireland. Herschel and Babbage were present at a celebrated operation of that survey, the remeasuring of the Lough Foyle baseline.
British Lagrangian School.
The Analytical Society had initially been no more than an undergraduate provocation. During this period it had some more substantial achievements. In 1816 Babbage, Herschel and Peacock published a translation from French of the lectures of Sylvestre Lacroix, which was then the state-of-the-art calculus textbook.
Reference to Lagrange in calculus terms marks out the application of what are now called formal power series. British mathematicians had used them from about 1730 to 1760. As re-introduced, they were not simply applied as notations in differential calculus. They opened up the fields of functional equations (including the difference equations fundamental to the difference engine) and operator (D-module) methods for differential equations. The analogy of difference and differential equations was notationally changing Δ to D, as a "finite" difference becomes "infinitesimal". These symbolic directions became popular, as operational calculus, and pushed to the point of diminishing returns. The Cauchy concept of limit was kept at bay. Woodhouse had already founded this second "British Lagrangian School" with its treatment of Taylor series as formal.
In this context function composition is complicated to express, because the chain rule is not simply applied to second and higher derivatives. This matter was known to Woodhouse by 1803, who took from Louis François Antoine Arbogast what is now called Faà di Bruno's formula (a misnomer). In essence it was known to Abraham De Moivre (1697). Herschel found the method impressive, Babbage knew of it, and it was later noted by Lovelace as compatible with the analytical engine. In the period to 1820 Babbage worked intensively on functional equations in general, and resisted both conventional finite differences and Arbogast's approach (in which Δ and D were related by the simple additive case of the exponential map). But via Herschel he was influenced by Arbogast's ideas in the matter of iteration, i.e. composing a function with itself, possibly many times. Writing in a major paper on functional equations in the "Philosophical Transactions" (1815/6), Babbage said his starting point was work of Gaspard Monge.
Academic.
From 1828 to 1839 Babbage was Lucasian Professor of Mathematics at Cambridge. Not a conventional resident don, and inattentive to teaching, he wrote three topical books during this period of his life. He was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1832. Babbage was out of sympathy with colleagues: George Biddell Airy, his predecessor as Lucasian Professor of Mathematics at Trinity College, Cambridge, thought an issue should be made of his lack of interest in lecturing. Babbage planned to lecture in 1831 on political economy. Babbage's reforming direction looked to see university education more inclusive, universities doing more for research, a broader syllabus and more interest in applications; but William Whewell found the programme unacceptable. A controversy Babbage had with Richard Jones lasted for six years. He never did give a lecture.
It was during this period that Babbage tried to enter politics. Simon Schaffer writes that his views of the 1830s included disestablishment of the Church of England, a broader political franchise, and inclusion of manufacturers as stakeholders. He twice stood for Parliament as a candidate for the borough of Finsbury. In 1832 he came in third among five candidates, missing out by some 500 votes in the two-member constituency when two other reformist candidates, Thomas Wakley and Christopher Temple, split the vote. In his memoirs Babbage related how this election brought him the friendship of Samuel Rogers: his brother Henry Rogers wished to support Babbage again, but died within days. In 1834 Babbage finished last among four.
"Declinarians", learned societies and the BAAS.
Babbage now emerged as a polemicist. One of his biographers notes that all his books contain a "campaigning element". His "Reflections on the Decline of Science and some of its Causes" (1830) stands out, however, for its sharp attacks. It aimed to improve British science, and more particularly to oust Davies Gilbert as President of the Royal Society, which Babbage wished to reform. It was written out of pique, when Babbage hoped to become the junior secretary of the Royal Society, as Herschel was the senior, but failed because of his antagonism to Humphry Davy. Michael Faraday had a reply written, by Gerrit Moll, as "On the Alleged Decline of Science in England" (1831). On the front of the Royal Society Babbage had no impact, with the bland election of the Duke of Sussex to succeed Gilbert the same year. As a broad manifesto, on the other hand, his "Decline" led promptly to the formation in 1831 of the British Association for the Advancement of Science (BAAS).
The "Mechanics' Magazine" in 1831 identified as Declinarians the followers of Babbage. In an unsympathetic tone it pointed out David Brewster writing in the "Quarterly Review" as another leader; with the barb that both Babbage and Brewster had received public money.
In the debate of the period on statistics ("qua" data collection) and what is now statistical inference, the BAAS in its Statistical Section (which owed something also to Whewell) opted for data collection. This Section was the sixth, established in 1833 with Babbage as chairman and John Elliot Drinkwater as secretary. The foundation of the Statistical Society followed. Babbage was its public face, backed by Richard Jones and Robert Malthus.
"On the Economy of Machinery and Manufactures".
Babbage published "On the Economy of Machinery and Manufactures" (1832), on the organisation of industrial production. It was an influential early work of operational research. John Rennie the Younger in addressing the Institute of Civil Engineers on manufacturing in 1846 mentioned mostly surveys in encyclopaedias, and Babbage's book was first an article in the "Encyclopædia Metropolitana", the form in which Rennie noted it, in the company of related works by John Farey, Jr., Peter Barlow and Andrew Ure. From "An essay on the general principles which regulate the application of machinery to manufactures and the mechanical arts" (1827), which became the "Encyclopædia Metropolitana" article of 1829, Babbage developed the schematic classification of machines that, combined with discussion of factories, made up the first part of the book. The second part considered the "domestic and political economy" of manufactures.
The book sold well, and quickly went to a fourth edition (1836). Babbage represented his work as largely a result of actual observations in factories, British and abroad. It was not, in its first edition, intended to address deeper questions of political economy; the second (late 1832) did, with three further chapters including one on piece rate. The book also contained ideas on rational design in factories, and profit sharing.
"Babbage principle".
In "Economy of Machinery" was described what is now called the "Babbage principle". It pointed out commercial advantages available with more careful division of labour. As Babbage himself noted, it had already appeared in the work of Melchiorre Gioia in 1815. The term was introduced in 1974 by Harry Braverman. Related formulations are the "principle of multiples" of Philip Sargant Florence, and the "balance of processes".
What Babbage remarked is that skilled workers typically spend parts of their time performing tasks that are below their skill level. If the labour process can be divided among several workers, labour costs may be cut by assigning only high-skill tasks to high-cost workers, restricting other tasks to lower-paid workers. He also pointed out that training or apprenticeship can be taken as fixed costs; but that returns to scale are available by his approach of standardisation of tasks, therefore again favouring the factory system. His view of human capital was restricted to minimising the time period for recovery of training costs.
Publishing.
Another aspect of the work was its detailed breakdown of the cost structure of book publishing. Babbage took the unpopular line, from the publishers' perspective, of exposing the trade's profitability. He went as far as to name the organisers of the trade's restrictive practices. Twenty years later he attended a meeting hosted by John Chapman to campaign against the Booksellers Association, still a cartel.
Influence.
It has been written that "what Arthur Young was to agriculture, Charles Babbage was to the factory visit and machinery". Babbage's theories are said to have influenced the layout of the 1851 Great Exhibition, and his views had a strong effect on his contemporary George Julius Poulett Scrope. Karl Marx argued that the source of the productivity of the factory system was exactly the combination of the division of labour with machinery, building on Adam Smith, Babbage and Ure. Where Marx picked up on Babbage and disagreed with Smith was on the motivation for division of labour by the manufacturer: as Babbage did, he wrote that it was for the sake of profitability, rather than productivity, and identified an impact on the concept of a trade. John Ruskin went further, to oppose completely what manufacturing in Babbage's sense stood for. Babbage also affected the economic thinking of John Stuart Mill.
George Holyoake saw Babbage's detailed discussion of profit sharing as substantive, in the tradition of Robert Owen and Charles Fourier, if requiring the attentions of a benevolent captain of industry, and ignored at the time. The French engineer and writer on industrial organisation Léon Lalande was influenced by Babbage, but also the economist , in reducing the issues to "technology". William Jevons connected Babbage's "economy of labour" with his own labour experiments of 1870. The Babbage principle is an inherent assumption in Frederick Winslow Taylor's scientific management.
Natural theology.
In 1837, responding to the series of eight "Bridgewater Treatises", Babbage published his "Ninth Bridgewater Treatise", under the title "On the Power, Wisdom and Goodness of God, as manifested in the Creation". In this work Babbage weighed in on the side of uniformitarianism in a current debate. He preferred the conception of creation in which natural law dominated, removing the need for "contrivance". The book is a work of natural theology, and incorporates extracts from related correspondence of Herschel with Charles Lyell. It was quoted extensively in "Vestiges of the Natural History of Creation".
Babbage put forward the thesis that God had the omnipotence and foresight to create as a divine legislator. He could make laws which then produced species at the appropriate times, rather than continually interfering with "ad hoc" miracles each time a new species was required. In "Vestiges" the parallel with Babbage's computing machines is made explicit, as allowing plausibility to the theory that transmutation of species could be pre-programmed.
Babbage has been seen as influenced by Indian thought, in particular Indian logic; one possible route would be through Henry Thomas Colebrooke. Mary Everest Boole claims that Babbage was introduced to Indian thought in the 1820s by her uncle George Everest:
Some time about 1825, came to England for two or three years, and made a fast and lifelong friendship with Herschel and with Babbage, who was then quite young. I would ask any fair-minded mathematician to read Babbage's Ninth Bridgewater Treatise and compare it with the works of his contemporaries in England; and then ask himself whence came the peculiar conception of the nature of miracle which underlies Babbage's ideas of Singular Points on Curves (Chap, viii) – from European Theology or Hindu Metaphysic? Oh! how the English clergy of that day hated Babbage's book!
Later life.
The British Association was consciously modelled on the Deutsche Naturforscher-Versammlung, founded in 1822. It rejected romantic science as well as metaphysics, and started to entrench the divisions of science from literature, and professionals from amateurs. Belonging as he did to the "Wattite" faction in the BAAS, represented in particular by James Watt the younger, Babbage identified closely with industrialists. He wanted to go faster in the same directions, and had little time for the more gentlemanly component of its membership. Indeed, he subscribed to a version of conjectural history that placed industrial society as the culmination of human development (and shared this view with Herschel). A clash with Roderick Murchison led in 1838 to his withdrawal from further involvement. At the end of the same year he sent in his resignation as Lucasian professor, walking away also from the Cambridge struggle with Whewell. His interests became more focussed, on computation and metrology, and on international contacts.
Metrology programme.
A project announced by Babbage was to tabulate all physical constants (referred to as "constants of nature", a phrase in itself a neologism), and then to compile an encyclopaedic work of numerical information. He was a pioneer in the field of "absolute measurement". His ideas followed on from those of Johann Christian Poggendorff, and were mentioned to Brewster in 1832. There were to be 19 categories of constants, and Ian Hacking sees these as reflecting in part Babbage's "eccentric enthusiasms". Babbage's paper "On Tables of the Constants of Nature and Art" was reprinted by the Smithsonian Institution in 1856, with an added note that the physical tables of Arnold Henry Guyot "will form a part of the important work proposed in this article".
Exact measurement was also key to the development of machine tools. Here again Babbage is considered a pioneer, with Henry Maudslay, William Sellers, and Joseph Whitworth.
Engineer and inventor.
Through the Royal Society Babbage acquired the friendship of the engineer Marc Brunel. It was through Brunel that Babbage knew of Joseph Clement, and so came to encounter the artisans whom he observed in his work on manufactures. Babbage provided an introduction for Isambard Kingdom Brunel in 1830, for a contact with the proposed Bristol & Birmingham Railway. He carried out studies, around 1838, to show the superiority of the broad gauge for railways, used by Brunel's Great Western Railway.
In 1838, Babbage invented the pilot (also called a cow-catcher), the metal frame attached to the front of locomotives that clears the tracks of obstacles; he also constructed a dynamometer car. His eldest son, Benjamin Herschel Babbage, worked as an engineer for Brunel on the railways before emigrating to Australia in the 1850s.
Babbage also invented an ophthalmoscope, which he gave to Thomas Wharton Jones for testing. Jones, however, ignored it. The device only came into use after being independently invented by Hermann von Helmholtz.
Cryptography.
Babbage achieved notable results in cryptography, though this was still not known a century after his death. Letter frequency was category 18 of Babbage's tabulation project. Joseph Henry later defended interest in it, in the absence of the facts, as relevant to the management of movable type.
During the Crimean War of the 1850s, Babbage broke Vigenère's autokey cipher as well as the much weaker cipher that is called Vigenère cipher today. His discovery was kept a military secret, and was not published. Credit for the result was instead given to Friedrich Kasiski, a Prussian infantry officer, who made the same discovery some years later. Babbage did write to the "Journal of the Society for Arts" a short letter "Cypher Writing" which was printed on 7 December 1855. His priority wasn't established until 1985.
Public nuisances.
Babbage involved himself in well-publicised but unpopular campaigns against public nuisances. He once counted all the broken panes of glass of a factory, publishing in 1857 a "Table of the Relative Frequency of the Causes of Breakage of Plate Glass Windows": Of 464 broken panes, 14 were caused by "drunken men, women or boys".
Babbage's distaste for commoners ("the Mob") included writing "Observations of Street Nuisances" in 1864, as well as tallying up 165 "nuisances" over a period of 80 days. He especially hated street music, and in particular the music of organ grinders, against whom he railed in various venues. The following quotation is typical: 
Babbage was not alone in his campaign. A convert to the cause was the MP Michael Thomas Bass, Jr.
In the 1860s, Babbage also took up the anti-hoop-rolling campaign. He blamed hoop-rolling boys for driving their iron hoops under horses' legs, with the result that the rider is thrown and very often the horse breaks a leg. Babbage achieved a certain notoriety in this matter, being denounced in debate in Commons in 1864 for "commencing a crusade against the popular game of tip-cat and the trundling of hoops."
Death.
Babbage lived and worked for over 40 years at 1 Dorset Street, Marylebone, where he died, at the age of 79, on 18 October 1871; he was buried in London's Kensal Green Cemetery. According to Horsley, Babbage died "of renal inadequacy, secondary to cystitis." He had declined both a knighthood and baronetcy. He also argued against hereditary peerages, favouring life peerages instead.
In 1983 the autopsy report for Charles Babbage was discovered and later published by his great-great-grandson. A copy of the original is also available. Half of Babbage's brain is preserved at the Hunterian Museum in the Royal College of Surgeons in London. The other half of Babbage's brain is on display in the Science Museum, London.
Computing pioneer.
Babbage's machines were among the first mechanical computers. That they were not actually completed was largely because of funding problems and personality issues.
Babbage directed the building of some steam-powered machines that achieved some modest success, suggesting that calculations could be mechanised. For more than ten years he received government funding for his project, which amounted to £17,000, but eventually the Treasury lost confidence in him.
While Babbage's machines were mechanical and unwieldy, their basic architecture was similar to a modern computer. The data and program memory were separated, operation was instruction-based, the control unit could make conditional jumps, and the machine had a separate I/O unit.
Background on mathematical tables.
In Babbage's time, printed mathematical tables were calculated by human computers, in other words by hand. They were central to navigation, science and engineering, as well as mathematics. Mistakes were known to occur in transcription as well as calculation.
At Cambridge, Babbage saw the fallibility of this process, and the opportunity of adding mechanisation into its management. His own account of his path towards mechanical computation references a particular occasion:
In 1812 he was sitting in his rooms in the Analytical Society looking at a table of logarithms, which he knew to be full of mistakes, when the idea occurred to him of computing all tabular functions by machinery. The French government had produced several tables by a new method. Three or four of their mathematicians decided how to compute the tables, half a dozen more broke down the operations into simple stages, and the work itself, which was restricted to addition and subtraction, was done by eighty computers who knew only these two arithmetical processes. Here, for the first time, mass production was applied to arithmetic, and Babbage was seized by the idea that the labours of the unskilled computers could be taken over completely by machinery which would be quicker and more reliable.
There was another period, seven years later, when his interest was aroused by the issues around computation of mathematical tables. The French official initiative by Gaspard de Prony, and its problems of implementation, were familiar to him. After the Napoleonic Wars came to a close, scientific contacts were renewed on the level of personal contact: in 1819 Charles Blagden was in Paris looking into the printing of the stalled de Prony project, and lobbying for the support of the Royal Society. In works of the 1820s and 1830s, Babbage referred in detail to de Prony's project.
Difference engine.
Babbage began in 1822 with what he called the difference engine, made to compute values of polynomial functions. It was created to calculate a series of values automatically. By using the method of finite differences, it was possible to avoid the need for multiplication and division.
For a prototype difference engine, Babbage brought in Joseph Clement to implement the design, in 1823. Clement worked to high standards, but his machine tools were particularly elaborate. Under the standard terms of business of the time, he could charge for their construction, and would also own them. He and Babbage fell out over costs around 1831.
Some parts of the prototype survive in the Museum of the History of Science, Oxford. This prototype evolved into the "first difference engine." It remained unfinished and the finished portion is located at the Science Museum in London. This first difference engine would have been composed of around 25,000 parts, weigh fifteen tons (13,600 kg), and would have been tall. Although Babbage received ample funding for the project, it was never completed. He later (1847–1849) produced detailed drawings for an improved version,"Difference Engine No. 2", but did not receive funding from the British government. His design was finally constructed in 1989–1991, using his plans and 19th century manufacturing tolerances. It performed its first calculation at the London Science Museum, returning results to 31 digits.
Nine years later, the Science Museum completed the printer Babbage had designed for the difference engine.
Completed models.
The London Science Museum has constructed two Difference Engines according to Babbage's plans for the Difference Engine No 2. One is owned by the museum. The other, owned by the technology multimillionaire Nathan Myhrvold, went on exhibition at the Computer History Museum in Mountain View, California on 10 May 2008. The two models that have been constructed are not replicas; Myhrvold's engine is the first design by Babbage, and the London Science Museum's is a later model.
Analytical Engine.
After the attempt at making the first difference engine fell through, Babbage worked to design a more complex machine called the Analytical Engine. He hired C. G. Jarvis, who had previously worked for Clement as a draughtsman. The Analytical Engine marks the transition from mechanised arithmetic to fully-fledged general purpose computation. It is largely on it that Babbage's standing as computer pioneer rests.
The major innovation was that the Analytical Engine was to be programmed using punched cards: the Engine was intended to use loops of Jacquard's punched cards to control a mechanical calculator, which could use as input the results of preceding computations. The machine was also intended to employ several features subsequently used in modern computers, including sequential control, branching and looping. It would have been the first mechanical device to be, in principle, Turing-complete. The Engine was not a single physical machine, but rather a succession of designs that Babbage tinkered with until his death in 1871.
Ada Lovelace and Italian followers.
Babbage had no research team. Ada Lovelace corresponded with him during his development of the Analytical Engine. She is credited with developing an algorithm for the Analytical Engine to calculate a sequence of Bernoulli numbers. Although there is disagreement over how much of the ideas were Lovelace's own, she is often described as the first computer programmer. She also translated and wrote literature supporting the project. Babbage visited Turin in 1840 at the invitation of Giovanni Plana. In 1842 Charles Wheatstone approached Lovelace to translate a paper of Luigi Menabrea, who had taken notes of Babbage's Turin talks; and Babbage asked her to add something of her own. Fortunato Prandi who acted as interpreter in Turin was an Italian exile and follower of Giuseppe Mazzini.
Swedish followers.
Per Georg Scheutz wrote about the difference engine in 1830, and experimented in automated computation. After 1834 and Lardner's "Edinburgh Review" article he set up a project of his own, doubting whether Babbage's initial plan could be carried out. This he pushed through with his son, Edvard Scheutz. Another Swedish engine was that of Martin Wiberg (1860).
Legacy.
In 2011, researchers in Britain embarked on a multimillion-pound project, "Plan 28", to construct Babbage's Analytical Engine. Since Babbage's plans were continually being refined and were never completed, they will engage the public in the project and crowd-source the analysis of what should be built. It would have the equivalent of 675 bytes of memory, and run at a clock speed of about 7 Hz. They hope to complete it by the 150th anniversary of Babbage's death, in 2021.
Advances in MEMs and nanotechnology have led to recent high-tech experiments in mechanical computation. The benefits suggested include operation in high radiation or high temperature environments. These modern versions of mechanical computation were highlighted in "The Economist" in its special "end of the millennium" black cover issue in an article entitled "Babbage's Last Laugh".
Family.
On 25 July 1814, Babbage married Georgiana Whitmore at St. Michael's Church in Teignmouth, Devon; her sister Louisa married Edward Ryan. The couple lived at Dudmaston Hall, Shropshire (where Babbage engineered the central heating system), before moving to 5 Devonshire Street, Portland Place, London.
Charles and Georgiana had eight children,Also see </ref> but only four – Benjamin Herschel, Georgiana Whitmore, Dugald Bromhead and Henry Prevost – survived childhood. Charles' wife Georgiana died in Worcester on 1 September 1827, the same year as his father, their second son (also named Charles) and their newborn son Alexander.
His youngest son, Henry Prevost Babbage (1824–1918), went on to create six working difference engines based on his father's designs, one of which was sent to Harvard University where it was later discovered by Howard H. Aiken, pioneer of the Harvard Mark I. Henry Prevost's 1910 Analytical Engine Mill, previously on display at Dudmaston Hall, is now on display at the Science Museum.
Memorials.
There is a green plaque commemorating the 40 years Babbage spent at 1 Dorset Street, London. Locations, institutions and other things named after Babbage include:
In fiction and film.
Babbage frequently appears in steampunk works; he has been called an iconic figure of the genre. Other works in which Babbage appears include:

</doc>
<doc id="5700" url="http://en.wikipedia.org/wiki?curid=5700" title="Cross-dressing">
Cross-dressing

Cross-dressing refers to the act of wearing clothing and other accoutrements commonly associated with the opposite sex within a particular society. Cross-dressing has been used for purposes of disguise, comfort, and as a literary trope in modern times and throughout history. It does not, however, necessarily indicate transgender identity.
Nearly every human society throughout history has expected distinctions to be made between males and females by the style, color, or type of clothing they are expected to wear, and likewise most societies have had a set of social norms, views, guidelines, or even laws defining what type of clothing is appropriate for each gender.
The term "cross-dressing" denotes an action or a behavior without attributing or implying any specific causes for that behavior. Some people automatically connect cross-dressing behavior to transgender identity or sexual, fetishist, and homosexual behavior, but the term itself does not imply any motives.
History.
Cross-dressing has been practiced throughout much of recorded history and in many societies. There are many examples in Greek, Norse, and Hindu mythology. A reasonable number of historical figures are known to have cross-dressed to varying degrees and for a variety of reasons. There is a rich history of cross-dressing found in folklore, literature, theater, and music. Examples include Kabuki and Korean shamanism.
Varieties.
There are many different kinds of cross-dressing and many different reasons why an individual might engage in cross-dressing behavior. Some people cross-dress as a matter of comfort or style, out of personal preference for clothing associated with the opposite sex. In this case, a person's cross-dressing may or may not be apparent to other people. Some people cross-dress to shock others or challenge social norms.
Gender disguise has been used by women and girls to pass as male in society and by men and boys to pass themselves off as female. Gender disguise has also been used as a plot device in storytelling and is a recurring motif in literature, theater, and film. It is a common plot device in narrative ballads. Historically, some women have cross-dressed to take up male-dominated or male-exclusive professions, such as military service. Conversely, some men have cross-dressed to escape from mandatory military service or as a disguise to assist in political or social protest, as men did in the Rebecca Riots.
Single-sex theatrical troupes often have some performers who cross-dress to play roles written for members of the opposite sex (travesti). Cross-dressing, particularly the depiction of males wearing dresses, is often used for comic effect onstage and onscreen.
Drag is a special form of performance art based on the act of cross-dressing. A drag queen is usually a male-bodied person who performs as an exaggeratedly feminine character, in heightened costuming sometimes consisting of a showy dress, high-heeled shoes, obvious makeup, and wig. A drag queen may imitate famous female film or pop-music stars. A faux queen is a female-bodied person employing the same techniques.
A drag king is a counterpart of the drag queen but usually for much different audiences. A female-bodied person who adopt a masculine persona in performance or imitates a male film or pop-music star. Some female-bodied people undergoing gender reassignment therapy also self-identify as "drag kings" although this use of "drag king" would generally be considered inaccurate.
A transvestic fetishist is a person (typically a heterosexual male) who cross-dresses as part of a sexual fetish.
The term "underdressing" is used by male cross-dressers to describe wearing female undergarments under their male clothes. The famous low-budget filmmaker Edward D. Wood, Jr. said he often wore women's underwear under his military uniform during World War II.
Some people who cross-dress may endeavor to project a complete impression of belonging to another gender, down to mannerisms, speech patterns, and emulation of sexual characteristics. This is referred to as passing or "trying to pass" depending how successful the person is. An observer who sees through the cross-dresser's attempt to pass is said to have "read" or "clocked" them. There are videos, books, and magazines on how a man may look more like a woman.
"Female masking" is a form of cross-dressing in which men wear masks that present them as female.
Sometimes either member of a heterosexual couple will crossdress in order to arouse the other. For example, the male might wear skirts or lingerie and/or the female will wear boxers or other male clothing. (See also forced feminization)
Others may choose to take a mixed approach, adopting some feminine traits and some masculine traits in their appearance. For instance, a man might wear both a dress and a beard. This is sometimes known as "genderfuck".
Practical reasons.
Utilitarian purposes also bring about forms of crossdressing, such as:
Clothes.
The actual determination of cross-dressing is largely socially constructed. For example, in Western society, trousers have been adopted for wear by women, and it is not regarded as cross-dressing. In cultures where men have traditionally worn skirt-like garments such as the kilt or sarong these are not seen as female clothing, and wearing them is not seen as cross-dressing for men. As societies are becoming more global in nature, both men's and women's clothing are adopting styles of dress associated with other cultures.
It was once considered taboo in Western society for women to wear clothing traditionally associated with men, except in certain circumstances such as cases of necessity (as per St. Thomas Aquinas's guidelines in "Summa Theologiae II"),
While this prohibition remained in force in general throughout the middle and early modern ages, this is no longer the case and Western women are often seen wearing trousers, ties, and men's hats. Nevertheless, many cultures around the world still prohibit women from wearing trousers or other traditionally male clothing.
Cosplaying may also involve cross-dressing, for some females may wish to dress as a male, and vice versa (see Crossplay). Breast binding (for females) is not uncommon and is most likely needed to cosplay a male character.
In most parts of the world it remains socially frowned upon for men to wear clothes traditionally associated with women. Attempts are occasionally made, e.g. by fashion designers, to promote the acceptance of skirts as everyday wear for men. Cross-dressers have complained that society permits women to wear pants or jeans and other masculine clothing, while condemning any man who wants to wear clothing sold for women.
While creating a more feminine figure, male cross-dressers will often utilize different types and styles of breast forms, which are silicone prostheses traditionally used by women who have undergone mastectomies to recreate the visual appearance of a breast.
While most male cross-dressers utilize clothing associated with modern women, there are some who are involved in subcultures that involve dressing as little girls or in vintage clothing. Some such men have written that they enjoy dressing as femininely as possible, so they will wear frilly dresses with lace and ribbons, bridal gowns complete with veils, as well as multiple petticoats, corsets, girdles and/or garter belts with nylon stockings.
Social issues.
Cross-dressers may begin wearing clothing associated with the opposite sex as children, using the clothes of a sibling, parent, or friend. Some parents have said they allowed their children to cross-dress and, in many cases, the child stopped when they became older. The same pattern often continues into adulthood, where there may be confrontations with a spouse. Married cross-dressers experience considerable anxiety and guilt if their spouse objects to their behavior. Most cross-dressers have periodically disposed of all their clothing, a practice called "purging", only to start collecting other gender's clothing again.
Analyses.
The historical associations of maleness with power and femaleness with submission and frivolity mean that in the present time a woman dressing in men's clothing and a man dressing in women's clothing evoke very different responses. A woman dressing in men's clothing is considered to be a more acceptable activity.
Feminist advocacy for social change has done much to relax the constrictions of gender roles on men and women, but they are still heavily policed.
The reason it is so hard to have statistics for female-bodied crossdressers is that the line where non-crossdressing stops and crossdressing begins has become blurred, whereas the same line for men is as well defined as ever. This is one of the many issues being addressed by third wave feminism as well as the modern-day masculist movement.
Culture can be very skewed about cross-dressing. A woman who wears her husband's shirt to bed is considered attractive while a man who wears his wife's nightgown to bed is considered transgressive. Marlene Dietrich in a tuxedo was considered very erotic; Jack Lemmon in a dress was considered ridiculous. All this may result from an overall gender role rigidity for males; that is, because of the prevalent gender dynamic throughout the world, men frequently encounter discrimination when deviating from masculine gender norms, particularly violations of heteronormativity. A man's adoption of feminine clothing is often considered a going down in the gendered social order whereas a woman's adoption of what are traditionally men's clothing (at least in the English-speaking world) has less of an impact because women have been traditionally subordinate to men, unable to affect serious change through style of dress. Thus when a male cross-dresser puts on his clothes, he transforms into the quasi-female and thereby becomes an intolerable embodiment of the conflicted gender dynamic. Following the work of Butler, gender proceeds along through ritualized performances, but in male cross-dressing it becomes a performative "breaking" of the masculine and a "subversive repetition" of the feminine.
Some psychoanalysts today do not regard cross-dressing by itself as a psychological problem, unless it interferes with a person's life. "For instance," said Dr. Joseph Merlino, Senior Editor of the book "Freud at 150: 21st Century Essays on a Man of Genius", "that...I'm a cross-dresser and I don't want to keep it confined to my circle of friends, or my party circle, and I want to take that to my wife and I don't understand why she doesn't accept it, or I take it to my office and I don't understand why they don't accept it, then it's become a problem because it's interfering with my relationships and environment."
References.
Notes
Further reading

</doc>
<doc id="5702" url="http://en.wikipedia.org/wiki?curid=5702" title="Channel Tunnel">
Channel Tunnel

The Channel Tunnel (; also referred to as the Chunnel) is a rail tunnel linking Folkestone, Kent, in the United Kingdom, with Coquelles, Pas-de-Calais, near Calais in northern France, beneath the English Channel at the Strait of Dover. At its lowest point, it is deep. At , the tunnel has the longest undersea portion of any tunnel in the world, although the Seikan Tunnel in Japan is both longer overall at and deeper at below sea level.
The tunnel carries high-speed Eurostar passenger trains, the Eurotunnel Shuttle for automobiles and other road vehicles—the largest such transport in the world—and international rail freight trains. The tunnel connects end-to-end with the LGV Nord and High Speed 1 high-speed railway lines.
Ideas for a cross-Channel fixed link appeared as early as 1802, but British political and press pressure over compromised national security stalled attempts to construct a tunnel. An early attempt at building a Channel Tunnel was made in the late 19th century, on the English side "in the hope of forcing the hand of the English Government". The eventual successful project, organised by Eurotunnel, began construction in 1988 and opened in 1994. At £4.650 billion, the project came in 80% over its predicted budget. Since its construction, the tunnel has faced several problems. Fires and cold weather have both disrupted operation of the tunnel. Illegal immigrants have attempted to use the tunnel to enter the UK, causing a minor diplomatic disagreement over the siting of the Sangatte refugee camp, which was eventually closed in 2002.
Origins.
Proposals and attempts.
In 1802, Albert Mathieu, a French mining engineer, put forward a proposal to tunnel under the English Channel, with illumination from oil lamps, horse-drawn coaches, and an artificial island mid-Channel for changing horses.
In the 1830s, Aimé Thomé de Gamond, a Frenchman, performed the first geological and hydrographical surveys on the Channel, between Calais and Dover. Thomé de Gamond explored several schemes and, in 1856, he presented a proposal to Napoleon III for a mined railway tunnel from Cap Gris-Nez to Eastwater Point with a port/airshaft on the Varne sandbank at a cost of 170 million francs, or less than £7 million.
In 1865, a deputation led by George Ward Hunt proposed the idea of a tunnel to the Chancellor of the Exchequer of the day, William Ewart Gladstone.
Around 1866, William Low and Sir John Clarke Hawkshaw promoted ideas, but apart from preliminary geological studies none were implemented. An official Anglo-French protocol was established in 1876 for a cross-Channel railway tunnel. In 1881, the British railway entrepreneur Sir Edward Watkin and Alexandre Lavalley, a French Suez Canal contractor, were in the Anglo-French Submarine Railway Company that conducted exploratory work on both sides of the Channel. On the English side a diameter Beaumont-English boring machine dug a pilot tunnel from Shakespeare Cliff. On the French side, a similar machine dug from Sangatte. The project was abandoned in May 1882, owing to British political and press campaigns advocating that a tunnel would compromise Britain's national defences. These early works were encountered more than a century later during the TML project.
In 1919, during the Paris Peace Conference, the British prime minister, David Lloyd George, repeatedly brought up the idea of a Channel tunnel as a way of reassuring France about British willingness to defend against another German attack. The French did not take the idea seriously and nothing came of Lloyd George's proposal.
In 1929 there was another proposal but nothing came of this discussion and the idea was shelved. Proponents estimated construction to be about US$150 million. The engineers had addressed the concerns of both nations' military leaders by designing two sumps—one near the coast of each country—that could be flooded at will to block the tunnel. This design feature did not override the concerns of both nations' military leaders, and other concerns about hordes of undesirable tourists who would disrupt English habits of living. Military fears continued during World War II. After the fall of France, as Britain prepared for an expected German invasion, a Royal Navy officer in the Directorate of Miscellaneous Weapons Development calculated that Hitler could use slave labour to build two Channel tunnels in 18 months. The estimate caused rumours that Germany had already begun digging.
In 1955, defence arguments were accepted to be irrelevant because of the dominance of air power, and both the British and French governments supported technical and geological surveys. A detailed geological survey was carried out in 1964–65. Construction work commenced on both sides of the Channel in 1974, a government-funded project using twin tunnels on either side of a service tunnel, with capability for car shuttle wagons. In January 1975, to the dismay of the French partners, the British government cancelled the project. The government had changed to the Labour Party and there was uncertainty about EEC membership, cost estimates had ballooned to 200% and the national economy was troubled. By this time the British tunnel boring machine was ready and the Ministry of Transport was able to do a experimental drive. This short tunnel was reused as the starting and access point for tunnelling operations from the British side.
In 1979, the "Mouse-hole Project" was suggested when the Conservatives came to power in Britain. The concept was a single-track rail tunnel with a service tunnel, but without shuttle terminals. The British government took no interest in funding the project, but Margaret Thatcher, the prime minister, said she had no objection to a privately funded project. In 1981 Thatcher and François Mitterrand, the French president, agreed to set up a working group to look into a privately funded project, and in April 1985 promoters were formally invited to submit scheme proposals. Four submissions were shortlisted:
The cross-Channel ferry industry protested under the name "Flexilink". In 1975 there was no campaign protesting against a fixed link, with one of the largest ferry operators (Sealink) being state-owned. Flexilink continued rousing opposition throughout 1986 and 1987. Public opinion strongly favoured a drive-through tunnel, but ventilation issues, concerns about accident management, and fear of driver mesmerisation led to the only shortlisted rail submission, CTG/F-M, being awarded the project.
Arrangement.
The British "Channel Tunnel Group" consisted of two banks and five construction companies, while their French counterparts, "France–Manche", consisted of three banks and five construction companies. The role of the banks was to advise on financing and secure loan commitments. On 2 July 1985, the groups formed Channel Tunnel Group/France–Manche (CTG/F–M). Their submission to the British and French governments was drawn from the 1975 project, including 11 volumes and a substantial environmental impact statement.
The design and construction was done by the ten construction companies in the CTG/F-M group. The French terminal and boring from Sangatte was undertaken by the five French construction companies in the joint venture group "GIE Transmanche Construction". The English Terminal and boring from Shakespeare Cliff was undertaken by the five British construction companies in the "Translink Joint Venture". The two partnerships were linked by TransManche Link (TML), a bi-national project organisation. The Maître d'Oeuvre was a supervisory engineering body employed by Eurotunnel under the terms of the concession that monitored project activity and reported back to the governments and banks.
In France, with its long tradition of infrastructure investment, the project garnered widespread approval. In April the French National Assembly gave unanimous support and, in June 1987, after a public inquiry, the Senate gave unanimous support. In Britain, select committees examined the proposal, making history by holding hearings away from Westminster, in Kent. In February 1987, the third reading of the Channel Tunnel Bill took place in the House of Commons, and was carried by 94 votes to 22. The Channel Tunnel Act gained Royal assent and passed into law in July. Parliamentary support for the project came partly from provincial members of Parliament on the basis of promises of regional Eurostar through train services that never materialised; the promises were repeated in 1996 when the contract for construction of the Channel Tunnel Rail Link was awarded.
The tunnel is a build-own-operate-transfer (BOOT) project with a concession. TML would design and build the tunnel, but financing was through a separate legal entity, Eurotunnel. Eurotunnel absorbed CTG/F-M and signed a construction contract with TML, but the British and French governments controlled final engineering and safety decisions, now in the hands of the Channel Tunnel Safety Authority. The British and French governments gave Eurotunnel a 55- (later 65-) year operating concession to repay loans and pay dividends. A Railway Usage Agreement was signed between Eurotunnel, British Rail and the Société Nationale des Chemins de fer Français guaranteeing future revenue in exchange for the railways obtaining half of the tunnel's capacity.
Private funding for such a complex infrastructure project was of unprecedented scale. An initial equity of £45 million was raised by CTG/F-M, increased by £206 million private institutional placement, £770 million was raised in a public share offer that included press and television advertisements, a syndicated bank loan and letter of credit arranged £5 billion. Privately financed, the total investment costs at 1985 prices were £2600 million. At the 1994 completion actual costs were, in 1985 prices, £4650 million: an 80% cost overrun. The cost overrun was partly due to enhanced safety, security, and environmental demands.
Construction.
Working from both the English side and the French side of the Channel, eleven tunnel boring machines or TBMs cut through chalk marl to construct two rail tunnels and a service tunnel. The vehicle shuttle terminals are at Cheriton (part of Folkestone) and Coquelles, and are connected to the English M20 and French A16 motorways respectively.
Tunnelling commenced in 1988, and the tunnel began operating in 1994. In 1985 prices, the total construction cost was £4.650 billion (equivalent to £ billion today), an 80% cost overrun. At the peak of construction 15,000 people were employed with daily expenditure over £3 million. Ten workers, eight of them British, were killed during construction between 1987 and 1993, most in the first few months of boring.
Completion.
A two-inch (50-mm) diameter pilot hole allowed the service tunnel to break through without ceremony on 30 October 1990. On 1 December 1990, Englishman Graham Fagg and Frenchman Phillippe Cozette broke through the service tunnel with the media watching. Eurotunnel completed the tunnel on time, Following the ceremony President Mitterrand and the Queen travelled on Le Shuttle to a similar ceremony in Folkestone. A full public service did not start for several months.
The Channel Tunnel Rail Link (CTRL), now called High Speed 1, runs from St Pancras railway station in London to the tunnel portal at Folkestone in Kent. It cost £5.8 billion. On 16 September 2003 the prime minister, Tony Blair, opened the first section of High Speed 1, from Folkestone to north Kent. On 6 November 2007 the Queen officially opened High Speed 1 and St Pancras International station, replacing the original slower link to Waterloo International railway station. On High Speed 1 trains travel at up to , the journey from London to Paris taking 2 hours 15 minutes, to Brussels 1 hour 51 minutes.
In 1994, the American Society of Civil Engineers elected the tunnel as one of the seven modern Wonders of the World. In 1995, the American magazine "Popular Mechanics" published the results.
Engineering.
Surveying undertaken in the 20 years before construction confirmed earlier speculations that a tunnel could be bored through a chalk marl stratum. The chalk marl was conducive to tunnelling, with impermeability, ease of excavation and strength. On the English side the chalk marl ran along the entire length of the tunnel, but on the French a length of had variable and difficult geology. The tunnel consists of three bores: two diameter rail tunnels, apart, in length with a diameter service tunnel in between. There are also cross-passages and piston relief ducts. The service tunnel was used as a pilot tunnel, boring ahead of the main tunnels to determine the conditions. English access was provided at Shakespeare Cliff, French access from a shaft at Sangatte. The French side used five tunnel boring machines (TBMs), the English side six. The service tunnel uses Service Tunnel Transport System (STTS) and Light Service Tunnel Vehicles (LADOGS). Fire safety was a critical design issue.
Between the portals at Beussingue and Castle Hill the tunnel is long, with under land on the French side and on the UK side, and under sea. It is the second-longest rail tunnel in the world, behind the Seikan Tunnel in Japan, but with the longest under-sea section. The average depth is below the seabed. On the UK side, of the expected of spoil approximately was used for fill at the terminal site, and the remainder was deposited at Lower Shakespeare Cliff behind a seawall, reclaiming of land. This land was then made into the Samphire Hoe Country Park. Environmental impact assessment did not identify any major risks for the project, and further studies into safety, noise, and air pollution were overall positive. However, environmental objections were raised over a high-speed link to London.
Geology.
Successful tunnelling required a sound understanding of the topography and geology and the selection of the best rock strata through which to tunnel. The geology generally consists of northeasterly dipping Cretaceous strata, part of the northern limb of the Wealden-Boulonnais dome. Characteristics include:
On the English side, the strata dip is less than 5°; on the French side this increases to 20°. Jointing and faulting are present on both sides. On the English side, only minor faults of displacement less than exist; on the French side, displacements of up to are present owing to the Quenocs anticlinal fold. The faults are of limited width, filled with calcite, pyrite and remoulded clay. The increased dip and faulting restricted the selection of route on the French side. To avoid confusion, microfossil assemblages were used to classify the chalk marl. On the French side, particularly near the coast, the chalk was harder, more brittle and more fractured than on the English side. This led to the adoption of different tunnelling techniques on the two sides.
The Quaternary undersea valley Fosse Dangaered, and Castle Hill landslip at the English portal, caused concerns. Identified by the 1964–65 geophysical survey, the Fosse Dangaered is an infilled valley system extending below the seabed, south of the tunnel route in mid-channel. A 1986 survey showed that a tributary crossed the path of the tunnel, and so the tunnel route was made as far north and deep as possible. The English terminal had to be located in the Castle Hill landslip, which consists of displaced and tipping blocks of lower chalk, glauconitic marl and gault debris. Thus the area was stabilised by buttressing and inserting drainage adits. The service tunnel acted as a pilot tunnel preceding the main tunnels, so that the geology, areas of crushed rock, and zones of high water inflow could be predicted. Exploratory probing took place in the service tunnel, in the form of extensive forward probing, vertical downward probes and sideways probing.
Surveying.
Marine soundings and samplings by Thomé de Gamond were carried out during 1833–67, establishing the seabed depth at a maximum of and the continuity of geological strata (layers). Surveying continued over many years, with 166 marine and 70 land-deep boreholes being drilled and over 4,000-line-kilometres of marine geophysical survey completed. Surveys were undertaken in 1958–1959, 1964–1965, 1972–1974 and 1986–1988.
The surveying in 1958–59 catered for immersed tube and bridge designs as well as a bored tunnel, and thus a wide area was investigated. At this time marine geophysics surveying for engineering projects was in its infancy, with poor positioning and resolution from seismic profiling. The 1964–65 surveys concentrated on a northerly route that left the English coast at Dover harbour; using 70 boreholes, an area of deeply weathered rock with high permeability was located just south of Dover harbour.
Given the previous survey results and access constraints, a more southerly route was investigated in the 1972–73 survey and the route was confirmed to be feasible. Information for the tunnelling project also came from work before the 1975 cancellation. On the French side at Sangatte a deep shaft with adits was made. On the English side at Shakespeare Cliff, the government allowed of diameter tunnel to be driven. The actual tunnel alignment, method of excavation and support were essentially the same as the 1975 attempt. In the 1986–87 survey, previous findings were reinforced and the nature of the gault clay and the tunnelling medium (chalk marl that made up 85% of the route) were investigated. Geophysical techniques from the oil industry were employed.
Tunnelling.
Tunnelling was a major engineering challenge, with the only precedent being the undersea Seikan Tunnel in Japan. A serious risk with underwater tunnels is major water inflow due to the water pressure from the sea above under weak ground conditions. The tunnel also had the challenge of time: being privately funded, early financial return was paramount.
The objective was to construct two diameter rail tunnels, apart, in length; a diameter service tunnel between the two main tunnels; pairs of diameter cross-passages linking the rail tunnels to the service tunnel at spacing; piston relief ducts diameter connecting the rail tunnels at spacing; two undersea crossover caverns to connect the rail tunnels. The service tunnel always preceded the main tunnels by at least to ascertain the ground conditions. There was plenty of experience with tunnelling through chalk in the mining industry. The undersea crossover caverns were a complex engineering problem. The French cavern was based on the Mount Baker Ridge freeway tunnel in the US. The UK cavern was dug from the service tunnel ahead of the main tunnels to avoid delay.
Precast segmental linings in the main TBM drives were used, but different solutions were used on the two sides. On the French side, neoprene and grout sealed bolted linings made of cast iron or high-strength reinforced concrete were used. On the English side, the main requirement was for speed and bolting of cast-iron lining segments was only carried out in areas of poor geology. In the UK rail tunnels, eight lining segments plus a key segment were used; on the French side, five segments plus a key segment. On the French side, a diameter deep grout-curtained shaft at Sangatte was used for access. On the English side, a marshalling area was below the top of Shakespeare Cliff, and the New Austrian Tunnelling method (NATM) was first applied in the chalk marl here. On the English side, the land tunnels were driven from Shakespeare Cliff, the same place as the marine tunnels, not from Folkestone. The platform at the base of the cliff was not large enough for all of the drives and, despite environmental objections, tunnel spoil was placed behind a reinforced concrete seawall, on condition of placing the chalk in an enclosed lagoon to avoid wide dispersal of chalk fines. Owing to limited space, the precast lining factory was on the Isle of Grain in the Thames estuary, which used Scottish granite aggregate delivered by ship from the Foster Yeoman coastal super quarry at Glensanda.
On the French side, owing to the greater permeability to water, earth pressure balance TBMs with open and closed modes were used. The TBMs were of a closed nature during the initial , but then operated as open, boring through the chalk marl stratum. This minimised the impact to the ground and allowed high water pressures to be withstood, and it also alleviated the need to grout ahead of the tunnel. The French effort required five TBMs: two main marine machines, one main land machine (the short land drives of allowed one TBM to complete the first drive then reverse direction and complete the other), and two service tunnel machines. On the English side, the simpler geology allowed faster open-faced TBMs. Six machines were used, all commenced digging from Shakespeare Cliff, three marine-bound and three for the land tunnels. Towards the completion of the undersea drives, the UK TBMs were driven steeply downwards and buried clear of the tunnel. These buried TBMs were then used to provide an electrical earth. The French TBMs then completed the tunnel and were dismantled. A gauge railway was used on the English side during construction.
In contrast to the English machines, which were given alphanumeric names, the French tunnelling machines were all named after women: Brigitte, Europa, Catherine, Virginie, Pascaline, Séverine.
Railway design.
Communications.
There are three communication systems: concession radio (CR) for mobile vehicles and personnel within Eurotunnel's Concession (terminals, tunnels, coastal shafts); track-to-train radio (TTR) for secure speech and data between trains and the railway control centre; Shuttle internal radio (SIR) for communication between shuttle crew and to passengers over car radios. This service was discontinued within one year of opening because of drivers' difficulty setting their radios to the correct frequency (88.8 MHz).
Power supply.
All tunnel services run on electricity, shared equally from English and French sources. Power is delivered to the locomotives via an overhead line (catenary) at 25 kV AC railway electrification.
The traditional railway south of London initially used a 750 V DC third rail to deliver electricity, but since the opening of High Speed 1 there is no need to use the third rail system. High Speed 1, the tunnel and the LGV Nord has power provided via overhead catenary at 25 kV 50 Hz. The railways on "classic" lines in Belgium are also electrified by overhead wires, but at 3000 V DC.
Signalling.
A cab signalling system gives information directly to train drivers on a display. There is a train protection system that stops the train if the speed exceeds that indicated on the in-cab display. TVM430, as used on LGV Nord and High Speed 1, is used in the tunnel. The TVM signalling is interconnected with the signalling on the high-speed lines either side, allowing trains to enter and exit the tunnel system without stopping. The maximum speed is .
Track system.
The American Sonneville International Corporation track system was chosen, consisting of UIC60 rails on 900A grade resting on microcellular EVA pads, bolted into concrete. The tunnel and its terminal areas use a very large loading gauge to allow drive-in shuttle rolling stock. Through traffic is allowed up to European GC loading gauge via High Speed 1 to St Pancras for passenger traffic or as far as Barking in east London for freight traffic. Ballasted track was ruled out owing to maintenance constraints and a need for geometric stability.
Rolling stock.
Eurotunnel Shuttle.
Initially 38 Le Shuttle locomotives were commissioned, with one at each end of a shuttle train. The shuttles have two separate halves: single and double deck. Each half has two loading/unloading wagons and 12 carrier wagons. Eurotunnel's original order was for nine tourist shuttles.
HGV shuttles also have two halves, with each half containing one loading wagon, one unloading wagon and 14 carrier wagons. There is a club car behind the leading locomotive. Eurotunnel originally ordered six HGV shuttle rakes.
Freight locomotives.
Forty-six Class 92 locomotives for hauling freight trains and overnight passenger trains (the Nightstar project, which was abandoned) were commissioned, running on both overhead AC and third-rail DC power. However, RFF does not let these run on French railways, so there are plans to certify Alstom Prima II locomotives for use in the tunnel.
International passenger.
Thirty-one Eurostar trains, based on the French TGV, built to UK loading gauge with many modifications for safety within the tunnel, were commissioned, with ownership split between British Rail, French national railways (SNCF) and Belgian national railways (SNCB). British Rail ordered seven more for services north of London.
At the end of 2009, extensive fire-proofing requirements were dropped and Deutsche Bahn (DB) received permission to run German Intercity-Express (ICE) trains through the tunnel. On 19 October 2010 DB ran the first ICE train through the tunnel, arriving in St Pancras after evacuation tests in the tunnel were a success. Around the same time, Eurostar ordered ten trains from Siemens based on its Velaro product.
Service locomotives.
Diesel locomotives for rescue and shunting work are Eurotunnel Class 0001 and Eurotunnel Class 0031.
Operation.
Usage and services.
Services offered by the tunnel are:
Both the freight and passenger traffic forecasts that led to the construction of the tunnel were overestimated; in particular, Eurotunnel's commissioned forecasts were over-predictions. Although the captured share of Channel crossings was forecast correctly, high competition and reduced tariffs led to low revenue. Overall cross-Channel traffic was overestimated.
With the EU's liberalisation of international rail services, the tunnel and High Speed 1 have been open to competition since 2010. There have been a number of operators interested in running trains through the tunnel and along High Speed 1 to London. In June 2013, after several years, DB obtained a license to operate Frankfurt – London trains, not expected to run before 2016 because of delivery delays of the custom-made trains.
Passenger traffic volumes.
Cross-tunnel passenger traffic volumes peaked at 18.4 million in 1998, dropped to 14.9 million in 2003, then rose to 17.0 million in 2010.
At the time of the decision about building the tunnel, 15.9 million passengers were predicted for Eurostar trains in the opening year. In 1995, the first full year, actual numbers were a little over 2.9 million, growing to 7.1 million in 2000, then dropping to 6.3 million in 2003. Eurostar was limited by the lack of a high-speed connection on the British side. After the completion of High Speed 1 in two stages in 2003 and 2007, traffic increased. In 2008, Eurostar carried 9,113,371 passengers, a 10% increase over the previous year, despite traffic limitations due to the 2008 Channel Tunnel fire. Eurostar passenger numbers continued to increase, reaching 9,528,558 in 2010.
Freight traffic volumes.
Freight volumes have been erratic, with a decrease during 1997 due to a closure caused by a fire in a freight shuttle. Freight crossings increased over the period, indicating the substitutability of the tunnel by sea crossings. The tunnel has achieved a market share close to or above Eurotunnel's 1980s predictions but Eurotunnel's 1990 and 1994 predictions were overestimates.
For through freight trains, the first year prediction was 7.2 million gross tonnes; the actual 1995 figure was 1.3 million gross tonnes. Through freight volumes peaked in 1998 at 3.1 million tonnes. This fell back to 1.21 million tonnes in 2007, increasing slightly to 1.24 million tonnes in 2008. Together with that carried on freight shuttles, freight growth has occurred since opening, with 6.4 million tonnes carried in 1995, 18.4 million tonnes recorded in 2003 and 19.6 million tonnes in 2007. Numbers fell back in the wake of the 2008 fire.
Eurotunnel's freight subsidiary is Europorte 2. In September 2006 EWS, the UK's largest rail freight operator, announced that owing to cessation of UK-French government subsidies of £52 million per annum to cover the tunnel "Minimum User Charge" (a subsidy of around £13,000 per train, at a traffic level of 4,000 trains per annum), freight trains would stop running after 30 November.
Economic performance.
Shares in Eurotunnel were issued at £3.50 per share on 9 December 1987. By mid-1989 the price had risen to £11.00. Delays and cost overruns led to the price dropping; during demonstration runs in October 1994 it reached an all-time low. Eurotunnel suspended payment on its debt in September 1995 to avoid bankruptcy. In December 1997 the British and French governments extended Eurotunnel's operating concession by 34 years, to 2086. Financial restructuring of Eurotunnel occurred in mid-1998, reducing debt and financial charges. Despite the restructuring, "The Economist" reported in 1998 that to break even Eurotunnel would have to increase fares, traffic and market share for sustainability. A cost benefit analysis of the tunnel indicated that there were few impacts on the wider economy and few developments associated with the project, and that the British economy would have been better off if it had not been constructed.
Under the terms of the Concession, Eurotunnel was obliged to investigate a cross-Channel road tunnel. In December 1999 road and rail tunnel proposals were presented to the British and French governments, but it was stressed that there was not enough demand for a second tunnel. A three-way treaty between the United Kingdom, France and Belgium governs border controls, with the establishment of "control zones" wherein the officers of the other nation may exercise limited customs and law enforcement powers. For most purposes these are at either end of the tunnel, with the French border controls on the UK side of the tunnel and vice versa. For some city-to-city trains, the train is a control zone. A binational emergency plan coordinates UK and French emergency activities.
In 1999 Eurostar posted its first net profit, having made a loss of £925m in 1995.
Terminals.
The terminals sites are at Cheriton (near Folkestone in the United Kingdom) and Coquelles (near Calais in France). The terminals are designed to transfer vehicles from the motorway onto trains at a rate of 700 cars and 113 heavy vehicles per hour. The UK site uses the M20 motorway for access. The terminals are organised with the frontier controls juxtaposed with the entry to the system to allow travellers to go onto the motorway at the destination country immediately after leaving the shuttle. The area of the UK site was severely constrained and the design was challenging. The French layout was achieved more easily. To achieve design output, the shuttles accept cars on double-deck wagons; for flexibility, ramps were placed inside the shuttles to provide access to the top decks. At Folkestone there are of main-line track, 45 turnouts and eight platforms. At Calais there are of track and 44 turnouts. At the terminals the shuttle trains traverse a figure eight to reduce uneven wear on the wheels. There is a freight marshalling yard west of Cheriton at Dollands Moor Freight Yard.
Regional impact.
A 1996 report from the European Commission predicted that Kent and Nord-Pas de Calais had to face increased traffic volumes due to general growth of cross-Channel traffic and traffic attracted by the tunnel. In Kent, a high-speed rail line to London would transfer traffic from road to rail. Kent's regional development would benefit from the tunnel, but being so close to London restricts the benefits. Gains are in the traditional industries and are largely dependent on the development of Ashford International passenger station, without which Kent would be totally dependent on London's expansion. Nord-Pas-de-Calais enjoys a strong internal symbolic effect of the Tunnel which results in significant gains in manufacturing.
The removal of a bottleneck by means like the tunnel does not necessarily induce economic gains in all adjacent regions. The image of a region being connected to the European high-speed transport and active political response are more important for regional economic development. Some small-medium enterprises located in the immediate vicinity of the terminal have used the opportunity to re-brand the profile of their business with positive effect, such as "The New Inn" at Etchinghill which was able to commercially exploit its unique selling point as being 'the closest pub to the Channel Tunnel'. Tunnel-induced regional development is small compared to general economic growth. The South East of England is likely to benefit developmentally and socially from faster and cheaper transport to continental Europe, but the benefits are unlikely to be equally distributed throughout the region. The overall environmental impact is almost certainly negative.
Since the opening of the tunnel, small positive impacts on the wider economy have been felt, but it is difficult to identify major economic successes directly attributed to the tunnel. The Eurotunnel does operate profitably, offering an alternative transportation mode unaffected by poor weather. High costs of construction did delay profitability, however, and companies involved in the tunnel's construction and operation early in operation relied on government aid to deal with debts amounted. Eurotunnel has been described as being in a serious situation.
Incidents.
Fires.
There have been three fires in the tunnel, all on the heavy goods vehicle (HGV) shuttles, that were significant enough to close the tunnel, as well as other more minor incidents.
During an "invitation only" testing phase on 9 December 1994, a fire broke out in a Ford Escort car whilst its owner was loading it on to the upper deck of a tourist shuttle. The fire started at about 10:00 with the shuttle train stationary in the Folkestone terminal and was put out about 40 minutes later with no passenger injuries.
On 18 November 1996, a fire broke out on an HGV shuttle wagon in the tunnel but nobody was seriously hurt. The exact cause is unknown, although it was not a Eurotunnel equipment or rolling stock problem; it may have been due to arson of a heavy goods vehicle. It is estimated that the heart of the fire reached , with the tunnel severely damaged over , with some affected to some extent. Full operation recommenced six months after the fire.
The tunnel was closed for several hours on 21 August 2006, when a truck on an HGV shuttle train caught fire.
On 11 September 2008, a fire occurred in the Channel Tunnel at 13:57 GMT. The incident started on an HGV shuttle train travelling towards France. The event occurred from the French entrance to the tunnel. No one was killed but several people were taken to hospitals suffering from smoke inhalation, and minor cuts and bruises. The tunnel was closed to all traffic, with the undamaged South Tunnel reopening for limited services two days later. Full service resumed on 9 February 2009 after repairs costing €60 million.
The tunnel was closed for a couple of hours on 29 November 2012 after a truck on an HGV shuttle caught fire.
Train failures.
On the night of 19/20 February 1996, about 1,000 passengers became trapped in the Channel Tunnel when Eurostar trains from London broke down owing to failures of electronic circuits caused by snow and ice being deposited and then melting on the circuit boards.
On 3 August 2007, an electrical failure lasting six hours caused passengers to be trapped in the tunnel on a shuttle.
On the evening of 18 December 2009, during the December 2009 European snowfall, five London-bound Eurostar trains failed inside the tunnel, trapping 2,000 passengers for approximately 16 hours, during the coldest temperatures in eight years. A Eurotunnel spokesperson explained that snow had evaded the train's winterisation shields, and that the transition from cold air outside to the tunnel's warm atmosphere had melted the snow, resulting in electrical failures. One train was turned back before reaching the tunnel; two trains were hauled out of the tunnel by Eurotunnel Class 0001 diesel locomotives. The blocking of the tunnel led to the implementation of Operation Stack, the transformation of the M20 motorway into a linear car park.
The occasion was the first time that a Eurostar train was evacuated inside the tunnel; the failing of four at once was described as "unprecedented". The Channel Tunnel reopened the following morning. Nirj Deva, Member of the European Parliament for South East England, had called for Eurostar chief executive Richard Brown to resign over the incidents. An independent report by Christopher Garnett (former CEO of Great North Eastern Railway) and Claude Gressier (a French transport expert) on the 18/19 December 2009 incidents was issued in February 2010, making 21 recommendations.
A Brussels–London Eurostar broke down in the tunnel on 7 January 2010. The train had 236 passengers on board and was towed to Ashford; other trains that had not yet reached the tunnel were turned back.
Asylum and immigration.
Immigrants and would-be asylum seekers have used the tunnel to attempt to enter Britain. By 1997 the problem had attracted international press attention, and the French Red Cross opened a refugee centre at Sangatte in 1999, using a warehouse once used for tunnel construction; by 2002 it housed up to 1500 people at a time, most of them trying to get to the UK. In 2001, most came from Afghanistan, Iraq and Iran, but African and Eastern European countries were also represented.
Most immigrants who got into Britain found some way to ride a freight train, but others used Eurostar. Though the facilities were fenced, airtight security was deemed impossible; refugees would even jump from bridges onto moving trains. In several incidents people were injured during the crossing; others tampered with railway equipment, causing delays and requiring repairs. Eurotunnel said it was losing £5m per month because of the problem. A dozen refugees have died in crossing attempts.
In 2001 and 2002, several riots broke out at Sangatte and groups of refugees (up to 550 in a December 2001 incident) stormed the fences and attempted to enter "en masse". Immigrants have also arrived as legitimate Eurostar passengers without proper entry papers.
Local authorities in both France and the UK called for the closure of Sangatte, and Eurotunnel twice sought an injunction against the centre. The United Kingdom blamed France for allowing Sangatte to open, and France blamed the UK for its lax asylum rules and the EU for not having a uniform immigration policy. The "cause célèbre" nature of the problem even included journalists detained as they followed refugees onto railway property.
In 2002, after the European Commission told France that it was in breach of European Union rules on the free transfer of goods because of the delays and closures as a result of its poor security, a double fence was built at a cost of £5 million, reducing the numbers of refugees detected each week reaching Britain on goods trains from 250 to almost none. Other measures included CCTV cameras and increased police patrols. At the end of 2002, the Sangatte centre was closed after the UK agreed to take some of its refugees.
Safety.
The Channel Tunnel Safety Authority is responsible for some aspects of safety regulation in the tunnel; it reports to the IGC.
The service tunnel is used for access to technical equipment in cross-passages and equipment rooms, to provide fresh-air ventilation and for emergency evacuation. The Service Tunnel Transport System (STTS) allows fast access to all areas of the tunnel. The service vehicles are rubber-tyred with a buried wire guidance system. The 24 STTS vehicles are used mainly for maintenance but also for firefighting and in emergencies. "Pods" with different purposes, up to a payload of , are inserted into the side of the vehicles. The vehicles cannot turn around within the tunnel, and are driven from either end. The maximum speed is when the steering is locked. A fleet of 15 Light Service Tunnel Vehicles (LADOGS) was introduced to supplement the STTSs. The LADOGS have a short wheelbase with a turning circle, allowing two-point turns within the service tunnel. Steering cannot be locked like the STTS vehicles, and maximum speed is . Pods up to 1 tonne can be loaded onto the rear of the vehicles. Drivers in the tunnel sit on the right, and the vehicles drive on the left. Owing to the risk of French personnel driving on their native right side of the road, sensors in the vehicles alert the driver if the vehicle strays to the right side.
The three tunnels contain of air that needs to be conditioned for comfort and safety. Air is supplied from ventilation buildings at Shakespeare Cliff and Sangatte, with each building capable of providing 100% standby capacity. Supplementary ventilation also exists on either side of the tunnel. In the event of a fire, ventilation is used to keep smoke out of the service tunnel and move smoke in one direction in the main tunnel to give passengers clean air. The tunnel was the first main-line railway tunnel to have special cooling equipment. Heat is generated from traction equipment and drag. The design limit was set at , using a mechanical cooling system with refrigeration plants on both sides that run chilled water circulating in pipes within the tunnel.
Trains travelling at high speed create piston-effect pressure changes that can affect passenger comfort, ventilation systems, tunnel doors, fans and the structure of the trains, and drag on the trains. Piston relief ducts of diameter were chosen to solve the problem, with 4 ducts per kilometre to give close to optimum results. Unfortunately this design led to unacceptable lateral forces on the trains so a reduction in train speed was required and restrictors were installed in the ducts.
The safety issue of a fire on a passenger-vehicle shuttle garnered much attention, with Eurotunnel noting that fire was the risk gathering the most attention in a 1994 Safety Case for three reasons: ferry companies opposed to passengers being allowed to remain with their cars; Home Office statistics indicating that car fires had doubled in ten years; and the long length of the tunnel. Eurotunnel commissioned the UK Fire Research Station to give reports of vehicle fires, and liaised with Kent Fire Brigade to gather vehicle fire statistics over one year. Fire tests took place at the French Mines Research Establishment with a mock wagon used to investigate how cars burned. The wagon door systems are designed to withstand fire inside the wagon for 30 minutes, longer than the transit time of 27 minutes. Wagon air conditioning units help to purge dangerous fumes from inside the wagon before travel. Each wagon has a fire detection and extinguishing system, with sensing of ions or ultraviolet radiation, smoke and gases that can trigger halon gas to quench a fire. Since the HGV wagons are not covered, fire sensors are located on the loading wagon and in the tunnel. A water main in the service tunnel provides water to the main tunnels at intervals. The ventilation system can control smoke movement. Special arrival sidings accept a train that is on fire, as the train is not allowed to stop whilst on fire in the tunnel, unless continuing its journey would lead to a worse outcome. Eurotunnel has banned a wide range of hazardous goods from travelling in the tunnel. Two STTS (Service Tunnel Transportation System) vehicles with firefighting pods are on duty at all times, with a maximum delay of 10 minutes before they reach a burning train.
Mobile network coverage.
Since 2012, French operators Bouygues Telecom, Orange and SFR cover the southern part of the tunnel.
In January 2014, UK operators EE and Vodafone signed ten-year contracts with Eurotunnel. The agreements will enable both operators' subscribers to use 2G and 3G services. Both EE and Vodafone plan to offer LTE services on the route; EE said it expects to cover the route with LTE connectivity by summer 2014. EE and Vodafone will offer Channel Tunnel network coverage for travellers from the UK to France. Eurotunnel said it also held talks with O2 and 3UK but is yet to reach an agreement with either operator.
On 6 May 2014, Eurotunnel announced that they have installed equipment from Alcatel Lucent to cover the North Running Tunnel and simultaneously to provide mobile service (GSM 900/1800 MHz and UMTS 2100 MHz) by EE, O2 and Vodafone. The service of EE and Vodafone commenced on the same date as the announcement. O2 service is expected to be available soon afterwards.

</doc>
<doc id="5703" url="http://en.wikipedia.org/wiki?curid=5703" title="Cyberpunk">
Cyberpunk

Cyberpunk is a subgenre of science fiction in a near-future setting. Noted for its focus on "high tech and low life," it features advanced science, such as information technology and cybernetics, coupled with a degree of breakdown or radical change in the social order.
Cyberpunk plots often center on a conflict among hackers, artificial intelligences, and megacorporations, and tend to be set in a near-future Earth, rather than the far-future settings or galactic vistas found in novels such as Isaac Asimov's "Foundation" or Frank Herbert's "Dune". The settings are usually post-industrial dystopias but tend to be marked by extraordinary cultural ferment and the use of technology in ways never anticipated by its creators ("the street finds its own uses for things"). Much of the genre's atmosphere echoes film noir, and written works in the genre often use techniques from detective fiction.
"Classic cyberpunk characters were marginalized, alienated loners who lived on the edge of society in generally dystopic futures where daily life was impacted by rapid technological change, an ubiquitous datasphere of computerized information, and invasive modification of the human body." – Lawrence Person
Style and ethos.
Primary exponents of the cyberpunk field include William Gibson, Neal Stephenson, Bruce Sterling, Pat Cadigan, Rudy Rucker, and John Shirley.
"Blade Runner" can be seen as a quintessential example of the cyberpunk style and theme. Video games, board games, and tabletop role-playing games, such as "Cyberpunk 2020" and "Shadowrun", often feature storylines that are heavily influenced by cyberpunk writing and movies. Beginning in the early 1990s, some trends in fashion and music were also labeled as cyberpunk. Cyberpunk is also featured prominently in anime: "Akira", "Serial Experiments Lain" and "Ghost in the Shell" being among the most notable.
Setting.
Cyberpunk writers tend to use elements from hardboiled detective fiction, film noir, and postmodernist prose to describe the often nihilistic underground side of an electronic society. The genre's vision of a troubled future is often called the antithesis of the generally utopian visions of the future popular in the 1940s and 1950s. Gibson defined cyberpunk's antipathy towards utopian SF in his 1981 short story "The Gernsback Continuum," which pokes fun at and, to a certain extent, condemns utopian science fiction.
In some cyberpunk writing, much of the action takes place online, in cyberspace, blurring the border between actual and virtual reality. A typical trope in such work is a direct connection between the human brain and computer systems. Cyberpunk depicts the world as a dark, sinister place with networked computers dominating every aspect of life. Giant, multinational corporations have for the most part replaced governments as centers of political, economic, and even military power.
The economic and technological state of Japan in the 80s influenced Cyberpunk literature at the time. Of Japan's influence on the genre, William Gibson said, "Modern Japan simply was cyberpunk." Cyberpunk is often set in urbanized, artificial landscapes, and "city lights, receding" was used by Gibson as one of the genre's first metaphors for cyberspace and virtual reality.
Protagonists.
One of the cyberpunk genre's prototype characters is Case, from Gibson's "Neuromancer". Case is a "console cowboy," a brilliant hacker who had betrayed his organized criminal partners. Robbed of his talent through a crippling injury inflicted by the vengeful partners, Case unexpectedly receives a once-in-a-lifetime opportunity to be healed by expert medical care but only if he participates in another criminal enterprise with a new crew.
Like Case, many cyberpunk protagonists are manipulated, placed in situations where they have little or no choice, and although they might see things through, they do not necessarily come out any further ahead than they previously were. These anti-heroes—"criminals, outcasts, visionaries, dissenters and misfits" call to mind the private eye of detective fiction. This emphasis on the misfits and the malcontents is the "punk" component of cyberpunk.
Society and government.
Cyberpunk can be intended to disquiet readers and call them to action. It often expresses a sense of rebellion, suggesting that one could describe it as a type of culture revolution in science fiction. In the words of author and critic David Brin:
...a closer look cyberpunk authors reveals that they nearly always portray future societies in which governments have become wimpy and pathetic ...Popular science fiction tales by Gibson, Williams, Cadigan and others "do" depict Orwellian accumulations of power in the next century, but nearly always clutched in the secretive hands of a wealthy or corporate elite.
Cyberpunk stories have also been seen as fictional forecasts of the evolution of the Internet. The earliest descriptions of a global communications network came long before the World Wide Web entered popular awareness, though not before traditional science-fiction writers such as Arthur C. Clarke and some social commentators such as James Burke began predicting that such networks would eventually form.
Media.
Literature.
Probably the first novel to depict cyberspace and combat within it was John M. Ford's "Web of Angels" (1980). The science-fiction editor Gardner Dozois is generally acknowledged as the person who popularized the use of the term "cyberpunk" as a kind of literature, although Minnesota writer Bruce Bethke coined the term in 1980 for his short story "Cyberpunk," which was published in the November 1983 issue of "Amazing Science Fiction Stories". The term was quickly appropriated as a label to be applied to the works of William Gibson, Bruce Sterling, Pat Cadigan and others. Of these, Sterling became the movement's chief ideologue, thanks to his fanzine "Cheap Truth." John Shirley wrote articles on Sterling and Rucker's significance.
William Gibson with his novel "Neuromancer" (1984) is likely the most famous writer connected with the term cyberpunk. He emphasized style, a fascination with surfaces, and atmosphere over traditional science-fiction tropes. Regarded as ground-breaking and sometimes as "the archetypal cyberpunk work," "Neuromancer" was awarded the Hugo, Nebula, and Philip K. Dick Awards. "Count Zero" (1986) and "Mona Lisa Overdrive" (1988) followed after Gibson's popular debut novel. According to the Jargon File, "Gibson's near-total ignorance of computers and the present-day hacker culture enabled him to speculate about the role of computers and hackers in the future in ways hackers have since found both irritatingly naïve and tremendously stimulating."
Early on, cyberpunk was hailed as a radical departure from science-fiction standards and a new manifestation of vitality. Shortly thereafter, however, many critics arose to challenge its status as a revolutionary movement. These critics said that the SF New Wave of the 1960s was much more innovative as far as narrative techniques and styles were concerned. Furthermore, while "Neuromancer"'s narrator may have had an unusual "voice" for science fiction, much older examples can be found: Gibson's narrative voice, for example, resembles that of an updated Raymond Chandler, as in his novel "The Big Sleep" (1939). Others noted that almost all traits claimed to be uniquely cyberpunk could in fact be found in older writers' works—often citing J. G. Ballard, Philip K. Dick, Harlan Ellison, Stanisław Lem, Samuel R. Delany, and even William S. Burroughs. For example, Philip K. Dick's works contain recurring themes of social decay, artificial intelligence, paranoia, and blurred lines between objective and subjective realities, and the influential cyberpunk movie "Blade Runner" (1982) is based on his book, "Do Androids Dream of Electric Sheep?". Humans linked to machines are found in Pohl and Kornbluth's "Wolfbane" (1959) and Roger Zelazny's "Creatures of Light and Darkness" (1968).
In 1994, scholar Brian Stonehill suggested that Thomas Pynchon's 1973 novel "Gravity's Rainbow" "not only curses but precurses what we now glibly dub cyberspace." Other important predecessors include Alfred Bester's two most celebrated novels, "The Demolished Man" and "The Stars My Destination", as well as Vernor Vinge's novella "True Names".
Science-fiction writer David Brin describes cyberpunk as "the finest free promotion campaign ever waged on behalf of science fiction." It may not have attracted the "real punks," but it did ensnare many new readers, and it provided the sort of movement that postmodern literary critics found alluring. Cyberpunk made science fiction more attractive to academics, argues Brin; in addition, it made science fiction more profitable to Hollywood and to the visual arts generally. Although the "self-important rhetoric and whines of persecution" on the part of cyberpunk fans were irritating at worst and humorous at best, Brin declares that the "rebels did shake things up. We owe them a debt."
Cyberpunk further inspired many professional writers who were not among the "original" cyberpunks to incorporate cyberpunk ideas into their own works, such as George Alec Effinger's "When Gravity Fails". "Wired" magazine, created by Louis Rossetto and Jane Metcalfe, mixes new technology, art, literature, and current topics in order to interest today’s cyberpunk fans, which Paula Yoo claims "proves that hardcore hackers, multimedia junkies, cyberpunks and cellular freaks are poised to take over the world."
Film and television.
The film "Blade Runner" (1982), adapted from Philip K. Dick's "Do Androids Dream of Electric Sheep?", is set in 2019 in a dystopian future in which manufactured beings called replicants are slaves used on space colonies and are legal prey on Earth to various bounty hunters who "retire" (kill) them. Although "Blade Runner" was largely unsuccessful in its first theatrical release, it found a viewership in the home video market and became a cult film. Since the movie omits the religious and mythical elements of Dick's original novel (e.g. empathy boxes and Wilbur Mercer), it falls more strictly within the cyberpunk genre than the novel does. William Gibson would later reveal that upon first viewing the film, he was surprised at how the look of this film matched his vision when he was working on "Neuromancer". The film's tone has since been the staple of many cyberpunk movies, such as "The Matrix" (1999), which uses a wide variety of cyberpunk elements.
The number of films in the genre or at least using a few genre elements has grown steadily since "Blade Runner". Several of Philip K. Dick's works have been adapted to the silver screen. The films "Johnny Mnemonic" and "New Rose Hotel", both based upon short stories by William Gibson, flopped commercially and critically.
In addition, "tech-noir" film as a hybrid genre, means a work of combining neo-noir and science fiction or cyberpunk. It includes many cyberpunk films such as "Blade Runner", "Burst City", "The Terminator", Robocop,"12 Monkeys", "The Lawnmower Man", "Hackers", "Hardware", and "Strange Days."
Anime and manga.
Cyberpunk themes are widely visible in anime and manga. In Japan, where cosplay is popular and not only teenagers display such fashion styles, cyberpunk has been accepted and its influence is widespread. William Gibson’s "Neuromancer," whose influence dominated the early cyberpunk movement, was also set in Chiba, one of Japan’s largest industrial areas, although at the time of writing the novel Gibson did not know the location of Chiba and had no idea how perfectly it fit his vision in some ways. The exposure to cyberpunk ideas and fiction in the mid 1980s has allowed it to seep into the Japanese culture. 
Even though most anime and manga is written in Japan, the cyberpunk anime and manga have a more futuristic and therefore international feel to them so they are widely accepted by all. “The conceptualization involved in cyberpunk is more of forging ahead, looking at the new global culture. It is a culture that does not exist right now, so the Japanese concept of a cyberpunk future, seems just as valid as a Western one, especially as Western cyberpunk often incorporates many Japanese elements.” William Gibson is now a frequent visitor to Japan, and he came to see that many of his visions of Japan have become a reality:
Modern Japan simply was cyberpunk. The Japanese themselves knew it and delighted in it. I remember my first glimpse of Shibuya, when one of the young Tokyo journalists who had taken me there, his face drenched with the light of a thousand media-suns—all that towering, animated crawl of commercial information—said, "You see? You see? It is "Blade Runner" town." And it was. It so evidently was.
Cyberpunk has influenced many anime and manga including the ground-breaking "Akira", "Ghost in the Shell", "Ergo Proxy", "Battle Angel Alita", "Megazone 23", "Neo Tokyo", "Goku Midnight Eye", "Cyber City Oedo 808", "Bubblegum Crisis", "", "Angel Cop", "Extra", "Blame!", "Armitage III" and "Psycho-Pass".
Games.
Several role-playing games (RPGs) called "Cyberpunk" exist: "Cyberpunk", "Cyberpunk 2020" and "Cyberpunk v3", by R. Talsorian Games, and "GURPS Cyberpunk", published by Steve Jackson Games as a module of the GURPS family of RPGs. "Cyberpunk 2020" was designed with the settings of William Gibson's writings in mind, and to some extent with his approval, unlike the approach taken by FASA in producing the transgenre "Shadowrun" game. Both are set in the near future, in a world where cybernetics are prominent. In addition, Iron Crown Enterprises released an RPG named "Cyberspace", which was out of print for several years until recently being re-released in online PDF form. 
In 1990, in a convergence of cyberpunk art and reality, the United States Secret Service raided Steve Jackson Games's headquarters and confiscated all their computers. This was allegedly because the "GURPS Cyberpunk" sourcebook could be used to perpetrate computer crime. That was, in fact, not the main reason for the raid, but after the event it was too late to correct the public's impression. Steve Jackson Games later won a lawsuit against the Secret Service, aided by the new Electronic Frontier Foundation. This event has achieved a sort of notoriety, which has extended to the book itself as well. All published editions of "GURPS Cyberpunk" have a tagline on the front cover, which reads "The book that was seized by the U.S. Secret Service!" Inside, the book provides a summary of the raid and its aftermath.
Cyberpunk has also inspired several tabletop, miniature and board games. "Netrunner" is a collectible card game introduced in 1996, based on the "Cyberpunk 2020" role-playing game.
There are many cyberpunk video games. Popular series include the "Deus Ex" series, the "Syndicate" series, "System Shock" and its sequel. Other games, like "Blade Runner", "Ghost in the Shell", and the "Matrix" series, are based upon genre movies, or role-playing games (for instance the various "Shadowrun" games). CD Projekt RED are currently developing a "Cyberpunk" game, "Cyberpunk 2077".
Music.
Some musicians and acts have been classified as cyberpunk due to their aesthetic style and musical content. Often dealing with dystopian visions of the future or biomechanical themes, some fit more squarely in the category than others. Bands whose music has been classified as cyberpunk include Psydoll, Front Line Assembly, Atari Teenage Riot, and Sigue Sigue Sputnik. Some musicians not normally associated with cyberpunk have at times been inspired to create concept albums exploring such themes. Nine Inch Nails' concept album Year Zero fits into this category. Billy Idol's "Cyberpunk" drew heavily from cyberpunk literature and the cyberdelic counter culture in its creation. "1. Outside", a cyberpunk narrative fueled concept album by David Bowie, was warmly met by critics upon its release in 1995. Many musicians have also taken inspiration from specific cyberpunk works or authors, including Sonic Youth, whose albums Sister and Daydream Nation take influence from the works of Phillip K. Dick and William Gibson respectively. 
Industrial music can be seen as cyberpunk, as well as various electronic body music acts.
Vaporwave and synthwave are also influenced by cyberpunk. While vaporwave often relies on the commercial side of Neo-Futurists, synthwave is often based on the more gritty side of cyberpunk.
Social impact.
Art and Architecture.
Some Neo-Futurism artworks and cityscapes have been influenced by cyberpunk, such as Japan, the Sony Center in the Potsdamer Platz public square of Berlin, Germany, Cyberport in Hong Kong, and Shanghai.
Society and counterculture.
Several subcultures have been inspired by cyberpunk fiction. These include the cyberdelic counter culture of the late 80s and early 90s. Cyberdelic, whose adherents referred to themselves as "cyberpunks," attempted to blend the psychedelic art and drug movement with the technology of cyberculture. Early adherents included Timothy Leary, Mark Frauenfelder and R. U. Sirius. The movement largely faded following the dot-com bubble implosion of 2000.
Cybergoth is a fashion and dance subculture which draws its inspiration from cyberpunk fiction, as well as rave and gothic subcultures.
In addition, a distinct cyberpunk fashion of its own has emerged in recent years which rejects the raver and goth influences of cybergoth.
Several Dubstep producers, like Machine Man and Ghosthack, have found inspiration in Cyberpunk themes for their works.
Related genres.
As a wider variety of writers began to work with cyberpunk concepts, new sub-genres of science fiction emerged, some which could be considered as playing off the cyberpunk label, others which could be considered as legitimate explorations into newer territory. These focused on technology and its social effects in different ways. One prominent subgenre is "steampunk," which is set in an alternate history Victorian era that combines anachronistic technology with cyberpunk's bleak film noir world view. The term was originally coined around 1987 as a joke to describe some of the novels of Tim Powers, James P. Blaylock, and K.W. Jeter, but by the time Gibson and Sterling entered the subgenre with their collaborative novel "The Difference Engine" the term was being used earnestly as well.
Another subgenre is "biopunk" (cyberpunk themes dominated by biotechnology) from the early 1990s, a derivative style building on biotechnology rather than informational technology. In these stories, people are changed in some way not by mechanical means, but by genetic manipulation. Paul Di Filippo is seen as the most prominent biopunk writer, including his half-serious ribofunk. Bruce Sterling's Shaper/Mechanist cycle is also seen as a major influence. In addition, some people consider works such as Neal Stephenson's "The Diamond Age" to be postcyberpunk.
Cyberpunk works have been described as well-situated within postmodern literature.

</doc>
<doc id="5704" url="http://en.wikipedia.org/wiki?curid=5704" title="Comic strip">
Comic strip

A comic strip is a sequence of drawings arranged in interrelated panels to display brief humor or form a narrative, often serialized, with text in balloons and captions. 
Traditionally, throughout the 20th century and into the 21st, these were published in newspapers, with horizontal strips printed in black-and-white in daily newspapers, while Sunday newspapers offered longer sequences in special color comics sections. There were more than 200 different comic strips and daily cartoon panels in American newspapers alone each day for most of the 20th century, for a total of at least 7,300,000 episodes.
Strips are written and drawn by a comics artist or cartoonist. As the name implies, comic strips can be humorous (for example, "gag-a-day" strips such as "Blondie", "Bringing Up Father", "Marmaduke", and "Pearls Before Swine"). 
Starting in the late 1920s, comic strips expanded from their mirthful origins to feature adventure stories, as seen in "Popeye", "Captain Easy", "Buck Rogers", "Tarzan", and "The Adventures of Tintin". Soap-opera continuity strips such as "Judge Parker" and "Mary Worth" gained popularity in the 1940s. All are called, generically, comic strips, though cartoonist Will Eisner has suggested that "sequential art" would be a better name.
In the UK and the rest of Europe, comic strips are also serialized in comic book magazines, with a strip's story sometimes continuing over three pages or more. Comic strips have appeared in American magazines such as "Liberty" and "Boys' Life" and also on the front covers of magazines, such as the "Flossy Frills" series on "The American Weekly" Sunday newspaper supplement.
History.
Storytelling using a sequence of pictures has existed through history. One medieval European example in textile form is the Bayeux Tapestry. Printed examples emerged in 19th-century Germany and in 18th-century England, where some of the first satirical or humorous sequential narrative drawings were produced. William Hogarth's 18th century English cartoons include both narrative sequences, such as "A Rake's Progress", and single panels.
The Biblia pauperum ("Paupers' Bible"), a tradition of picture Bibles beginning in the later Middle Ages, sometimes depicted Biblical events with words spoken by the figures in the miniatures written on scrolls coming out of their mouths—which makes them to some extent ancestors of the modern cartoon strips.
In China, with its traditions of block printing and of the incorporation of text with image, experiments with what became "lianhuanhua" date back to 1884.
Newspapers.
The first newspaper comic strips appeared in North America in the late 19th century. "The Yellow Kid" is usually credited as the first. However, the art form combining words and pictures evolved gradually and there are many examples of proto-comic strips. 
The Swiss teacher, author and caricature artist Rodolphe Töpffer (Geneva, 1799–1846) is considered the father of the modern comic strips. His illustrated stories such as "Histoire de M. Vieux Bois" (1827), first published in the USA in 1842 as "The Adventures of Obadiah Oldbuck" or "Histoire de Monsieur Jabot" (1831), inspired subsequent generations of German and American comic artists. In 1865, the German painter, author and caricaturist Wilhelm Busch created the strip "Max and Moritz", about two trouble-making boys, which had a direct influence on the American comic strip. "Max and Moritz" was a series of severely moralistic tales in the vein of German children's stories such as "Struwwelpeter" ("Shockheaded Peter"); in one, the boys, after perpetrating some mischief, are tossed into a sack of grain, run through a mill and consumed by a flock of geese. "Max and Moritz" provided an inspiration for German immigrant Rudolph Dirks, who created the "Katzenjammer Kids" in 1897. Familiar comic-strip iconography such as stars for pain, sawing logs for snoring, speech balloons, and thought balloons originated in Dirks' strip.
Hugely popular, "Katzenjammer Kids" occasioned one of the first comic-strip copyright ownership suits in the history of the medium. When Dirks left William Randolph Hearst for the promise of a better salary under Joseph Pulitzer, it was an unusual move, since cartoonists regularly deserted Pulitzer for Hearst. In a highly unusual court decision, Hearst retained the rights to the name "Katzenjammer Kids", while creator Dirks retained the rights to the characters. Hearst promptly hired Harold Knerr to draw his own version of the strip. Dirks renamed his version "Hans and Fritz" (later, "The Captain and the Kids"). Thus, two versions distributed by rival syndicates graced the comics pages for decades. Dirks' version, eventually distributed by United Feature Syndicate, ran until 1979.
In America, the great popularity of comics sprang from the newspaper war (1887 onwards) between Pulitzer and Hearst. "The Little Bears" (1893–96) was the first American comic with recurring characters, while the first color comic supplement was published by the "Chicago Inter-Ocean" sometime in the latter half of 1892, followed by the "New York Journal"'s first color Sunday comic pages in 1897. On January 31, 1912, Hearst introduced the nation's first full daily comic page in his "New York Evening Journal". The history of this newspaper rivalry and the rapid appearance of comic strips in most major American newspapers is discussed by Ian Gordon. Numerous events in newspaper comic strips have reverberated throughout society at large, though few of these events occurred in recent years, owing mainly to the declining role of the newspaper comic strip as an entertainment form.
The longest running American comic strips are:
Newspaper comic strips come in two different types: daily strips and Sunday strips. Most newspaper comic strips are syndicated; a syndicate hires people to write and draw a strip and then distributes it to many newspapers for a fee. A few newspaper strips are exclusive to one newspaper. For example, the "Pogo" comic strip by Walt Kelly originally appeared only in the "New York Star" in 1948 and was not picked up for syndication until the following year.
In the United States, a daily strip appears in newspapers on weekdays, Monday through Saturday, as contrasted with a Sunday strip, which typically only appears on Sundays. Daily strips usually are printed in black and white, and Sunday strips are usually in color. However, a few newspapers have published daily strips in color, and some newspapers have published Sunday strips in black and white. The two conventional formats for newspaper comics are strips and single gag panels. The strips are usually displayed horizontally, wider than they are tall. Single panels are square, circular or taller than they are wide. Strips usually, but not always, are broken up into several smaller panels with continuity from panel to panel. A horizontal strip can also be used for a single panel with a single gag, as seen occasionally in Mike Peters' "Mother Goose and Grimm".
During the 1930s, the original art for a daily strip could be drawn as large as 25 inches wide by six inches high. As strips have become smaller, the number of panels have been reduced. 
The popularity and accessibility of strips meant they were often clipped and saved; authors including John Updike and Ray Bradbury have written about their childhood collections of clipped strips. Often posted on bulletin boards, clipped strips had an ancillary form of distribution when they were faxed, photocopied or mailed. The "Baltimore Sun"'s Linda White recalled, "I followed the adventures of "Winnie Winkle", "Moon Mullins" and "Dondi", and waited each fall to see how Lucy would manage to trick Charlie Brown into trying to kick that football. (After I left for college, my father would clip out that strip each year and send it to me just to make sure I didn’t miss it.)"
Proof sheets were the means by which syndicates provided newspapers with black-and-white line art for the reproduction of strips (which they arranged to have colored in the case of Sunday strips). Michigan State University Comic Art Collection librarian Randy Scott describes these as "large sheets of paper on which newspaper comics have traditionally been distributed to subscribing newspapers. Typically each sheet will have either six daily strips of a given title or one Sunday strip. Thus, a week of "Beetle Bailey" would arrive at the "Lansing State Journal" in two sheets, printed much larger than the final version and ready to be cut apart and fitted into the local comics page." Comic strip historian Allan Holtz described how strips were provided as mats (the plastic or cardboard trays in which molten metal is poured to make plates) or even plates ready to be put directly on the printing press. He also notes that with electronic means of distribution becoming more prevalent printed sheets "are definitely on their way out."
Cartoon panels.
Single panels usually, but not always, are not broken up and lack continuity. The daily "Peanuts" is a strip, and the daily "Dennis the Menace" is a single panel. J. R. Williams' long-run "Out Our Way" continued as a daily panel even after it expanded into a Sunday strip, "Out Our Way with the Willets". Jimmy Hatlo's "They'll Do It Every Time" was often displayed in a two-panel format with the first panel showing some deceptive, pretentious, unwitting or scheming human behavior and the second panel revealing the truth of the situation.
Early daily strips were large, often running the entire width of the newspaper, and were sometimes three or more inches high. Initially, a newspaper page included only a single daily strip, usually either at the top or the bottom of the page. By the 1920s, many newspapers had a comics page on which many strips were collected together. Over decades, the size of daily strips became smaller and smaller, until by the year 2000, four standard daily strips could fit in an area once occupied by a single daily strip.
NEA Syndicate experimented briefly with a two-tier daily strip, "Star Hawks", but after a few years, "Star Hawks" dropped down to a single tier.
In Flanders, the two-tier strip is the standard publication style of most daily strips like "Spike and Suzy" and "Nero". They appear Monday through Saturday; until 2003 there were no Sunday papers in Flanders. In the last decades, they have switched from black and white to color.
Sunday comics.
Sunday newspapers traditionally included a special color section. Early Sunday strips, such as "Thimble Theatre" and "Little Orphan Annie", filled an entire newspaper page, a format known to collectors as full page. Sunday pages during the 1930s and into the 1940s often carried a secondary strip by the same artist as the main strip. No matter whether it appeared above or below a main strip, the extra strip was known as the topper, such as "The Squirrel Cage" which ran along with "Room and Board", both drawn by Gene Ahern.
During the 1930s, the original art for a Sunday strip was usually drawn quite large. For example, in 1930, Russ Westover drew his "Tillie the Toiler" Sunday page at a size of 17" × 37". In 1937, the cartoonist Dudley Fisher launched the innovative "Right Around Home", drawn as a huge single panel filling an entire Sunday page.
Full-page strips were eventually replaced by strips half that size. Strips such as "The Phantom" and "Terry and the Pirates" began appearing in a format of two strips to a page in full-size newspapers, such as the "New Orleans Times Picayune", or with one strip on a tabloid page, as in the "Chicago Sun-Times". When Sunday strips began to appear in more than one format, it became necessary for the cartoonist to allow for rearranged, cropped or dropped panels. During World War II, because of paper shortages, the size of Sunday strips began to shrink. After the war, strips continued to get smaller and smaller because of increased paper and printing costs. The last full-page comic strip was the "Prince Valiant" strip for 11 April 1971.
Comic strips have also been published in Sunday newspaper magazines. Russell Patterson and Carolyn Wells' "New Adventures of Flossy Frills" was a continuing strip series seen on Sunday magazine covers. Beginning January 26, 1941, it ran on the front covers of Hearst's "American Weekly" newspaper magazine supplement, continuing until March 30 of that year. Between 1939 and 1943, four different stories featuring Flossy appeared on "American Weekly" covers.
Sunday comics sections employed offset color printing with multiple print runs imitating a wide range of colors. Printing plates were created with four or more colors—traditionally, the CMYK color model: cyan, magenta, yellow and "K" for black. With a screen of tiny dots on each printing plate, the dots allowed an image to be printed in a halftone that appears to the eye in different gradations. The semi-opaque property of ink allows halftone dots of different colors to create an optical effect of full-color imagery.
Underground comic strips.
The decade of the 1960s saw the rise of underground newspapers, which often carried comic strips, such as "Fritz the Cat" and "The Fabulous Furry Freak Brothers". "Zippy the Pinhead" initially appeared in underground publications in the 1970s before being syndicated. "Bloom County" and "Doonesbury" began as strips in college newspapers under different titles, and later moved to national syndication. Underground comic strips covered subjects that are usually taboo in newspaper strips, such as sex and drugs. Many underground artists, notably Vaughn Bode, Dan O'Neill, Gilbert Shelton, and Art Spiegelman went on to draw comic strips for magazines such as "Playboy", "National Lampoon", and Pete Millar's "CARtoons". Jay Lynch graduated from undergrounds to alternative weekly newspapers to "Mad" and children's books.
Webcomic.
"Webcomics", also known as "online comics" and "internet comics", are comics that are available to read on the Internet. Many are exclusively published online, while some are published in print but maintain a web archive for either commercial or artistic reasons. Two of the most popular are "Penny Arcade", focused primarily on video gaming, and "User Friendly", which bases its humor on the Internet and other computer-user issues. The majority of traditional newspaper comic strips have some Internet presence. King Features Syndicate and other syndicates often provide archives of recent strips on their websites. Some, such as Scott Adams, creator of "Dilbert", include an email address in each strip.
Conventions and genres.
Most comic strip characters do not age throughout the strip's life, but in some strips, like Lynn Johnston's award-winning "For Better or For Worse", the characters age as the years pass. The first strip to feature aging characters was "Gasoline Alley".
The history of comic strips also includes series that are not humorous, but tell an ongoing dramatic story. Examples include "The Phantom", "Prince Valiant", "Dick Tracy", "Mary Worth", "Modesty Blaise", "Little Orphan Annie", "Flash Gordon", and "Tarzan". Sometimes these are spin-offs from comic books, for example "Superman", "Batman", and "The Amazing Spider-Man".
A number of strips have featured animals ('funny animals') as main characters. Some are non-verbal ("Marmaduke", "The Angriest Dog in the World"), some have verbal thoughts but are not understood by humans, ("Garfield", Snoopy in "Peanuts"), and some can converse with humans ("Bloom County", "Calvin and Hobbes", "Mutts", "Citizen Dog", "Buckles", "Get Fuzzy", "Pearls Before Swine", and "Pooch Cafe"). Other strips are centered entirely on animals, as in "Pogo" and "Donald Duck". Gary Larson's "The Far Side" was unusual, as there were no central characters. Instead "The Far Side" used a wide variety of characters including humans, monsters, aliens, chickens, cows, worms, amoebas, and more. John McPherson's "Close to Home" also uses this theme, though the characters are mostly restricted to humans and real-life situations. Wiley Miller not only mixes human, animal, and fantasy characters, but also does several different comic strip continuities under one umbrella title, "Non Sequitur". Bob Thaves's "Frank & Ernest" began in 1972 and paved the way for some of these strips, as its human characters were manifest in diverse forms — as animals, vegetables, and minerals.
Social and political influence.
The comics have long held a distorted mirror to contemporary society, and almost from the beginning have been used for political or social commentary. This ranged from the conservative slant of "Little Orphan Annie" to the unabashed liberalism of "Doonesbury". "Pogo" used animals to particularly devastating effect, caricaturing many prominent politicians of the day as animal denizens of Pogo's Okeefenokee Swamp. In a fearless move, Pogo's creator Walt Kelly took on Joseph McCarthy in the 1950s, caricaturing him as a bobcat named Simple J. Malarkey, a megalomaniac who was bent on taking over the characters' birdwatching club and rooting out all undesirables. Kelly also defended the medium against possible government regulation in the McCarthy era. At a time when comic books were coming under fire for supposed sexual, violent, and subversive content, Kelly feared the same would happen to comic strips. Going before the Congressional subcommittee, he proceeded to charm the members with his drawings and the force of his personality. The comic strip was safe for satire.
During the early 20th century, comic strips were widely associated with publisher William Randolph Hearst, whose papers had the largest circulation of strips in the United States. Hearst was notorious for his practice of yellow journalism, and he was frowned on by readers of "The New York Times" and other newspapers which featured few or no comic strips. Hearst's critics often assumed that all the strips in his papers were fronts for his own political and social views. Hearst did occasionally work with or pitch ideas to cartoonists, most notably his continued support of George Herriman's "Krazy Kat". An inspiration for Bill Watterson and other cartoonists, "Krazy Kat" gained a considerable following among intellectuals during the 1920s and 1930s.
Some comic strips, such as "Doonesbury" and "The Boondocks", may be printed on the editorial or op-ed page rather than the comics page because of their regular political commentary. For example, the August 12, 1974 "Doonesbury" strip awarded a 1975 Pulitzer Prize for its depiction of the Watergate scandal. "Dilbert" is sometimes found in the business section of a newspaper instead of the comics page because of the strip's commentary about office politics, and Tank McNamara often appears on the sports page because of its subject matter. Lynn Johnston's "For Better or for Worse" created an uproar when one of its supporting characters came out of the closet and announced he was gay.
Publicity and recognition.
The world's longest comic strip is long and on display at Trafalgar Square as part of the London Comedy Festival. The London Cartoon Strip was created by 15 of Britain's best known cartoonists and depicts the history of London.
The Reuben, named for cartoonist Rube Goldberg, is the most prestigious award for U.S. comic strip artists. Reuben awards are presented annually by the National Cartoonists Society (NCS).
Today's strip artists, with the help of the NCS, enthusiastically promote the medium, which is considered to be in decline due to fewer markets (today few strips are published in newspapers outside the United States, the United Kingdom, and Canada, mainly because of the smaller interest there, with translated versions of popular strips - particularly in Spanish - are primarily read over the internet) and ever-shrinking newspaper space. One particularly humorous example of such promotional efforts is the Great Comic Strip Switcheroonie, held in 1997 on April Fool's Day, an event in which dozens of prominent artists took over each other's strips. "Garfield"’s Jim Davis, for example, switched with "Blondie"’s Stan Drake, while Scott Adams ("Dilbert") traded strips with Bil Keane ("The Family Circus"). Even the United States Postal Service got into the act, issuing a series of commemorative stamps marking the comic-strip centennial in 1996.
While the Switcheroonie was a one-time publicity stunt, for one artist to take over a feature from its originator is an old tradition in newspaper cartooning (as it is in the comic book industry). In fact, the practice has made possible the longevity of the genre's more popular strips. Examples include "Little Orphan Annie" (drawn and plotted by Harold Gray from 1924 to 1944 and thereafter by a succession of artists including Leonard Starr and Andrew Pepoy), and "Terry and The Pirates", started by Milton Caniff in 1934 and picked up by George Wunder.
A business-driven variation has sometimes led to the same feature continuing under a different name. In one case, in the early 1940s, Don Flowers' "Modest Maidens" was so admired by William Randolph Hearst that he lured Flowers away from the Associated Press and to King Features Syndicate by doubling the cartoonist's salary, and renamed the feature "Glamor Girls" to avoid legal action by the AP. The latter continued to publish "Modest Maidens", drawn by Jay Allen in Flowers' style.
Issues in U.S. newspaper comic strips.
As newspapers have declined, the changes have affected comic strips.
Size.
In the early decades of the 20th century, all Sunday comics received a full page, and daily strips were generally the width of the page. The competition between papers for having more cartoons than the rest from the mid-1920s, the growth of large-scale newspaper advertising during most of the thirties, paper rationing during World War II, the decline on news readership (as television newscasts began to be more common) and inflation (which has caused higher printing costs) beginning during the fifties and sixties led to Sunday strips being published on smaller and more diverse formats. Daily strips have suffered as well, in 1910 the strips had an unlimited amount of panels, covering the entire width page, while by 1930 most "dailies" had four or five panels covering six of the eight columns occupied by a traditional broadsheet paper, by 1958 those four panels would be narrower, and those would have half of the space a 1910 daily strip had, and by 1998 most strips would have three panels only (with a few exceptions), or even two or one on an occasional basis, apart from strips being smaller, as most papers became slightly narrower. While most cartoonist decided to follow the tide, some cartoonists have complained about this, with "Pogo" ending in 1975 as a form of protest from its creators against the practice. Since then "Calvin and Hobbes" creator Bill Watterson has written extensively on the issue, arguing that size reduction and dropped panels reduce both the potential and freedom of a cartoonist. After a lengthy battle with his syndicator, Watterson won the privilege of making half page-sized Sunday strips where he could arrange the panels any way he liked. Many newspaper publishers and a few cartoonists objected to this, and some papers continued to print "Calvin and Hobbes" at small sizes. Opus won that same privilege years after "Calvin and Hobbes" ended, while Wiley Miller circumvented further downsizings by making his "Non Sequitur" Sunday strip available only in an extremely vertical (near-page-long) arrangement. Few newspapers still run half-page strips, as with "Prince Valiant" and "Hagar the Horrible" in the front page of the "Reading Eagle" Sunday comics section. Actually Universal Uclick and United Media practically have no half-page comics, with the remaining strips from both syndicates in this format are published only as "thirds", "fourths", and "sixths" (also called "third tabs").
Format.
In an issue related to size limitations, Sunday comics are often bound to rigid formats that allow their panels to be rearranged in several different ways while remaining readable. Such formats usually include throwaway panels at the beginning, which some newspapers will omit for space. As a result, cartoonists have less incentive to put great efforts into these panels. "Garfield" and "Mutts" were known during the mid-to-late 80s and 1990s respectively for their throwaways on their Sunday strips, however both strips now run "generic" title panels.
With the success of "The Gumps" during the 1920s, it became commonplace for strips (comedy- and adventure-laden alike) to have lengthy stories spanning weeks or months. The "Monarch of Medioka" story in Floyd Gottfredson's "Mickey Mouse" comic strip ran from September 8, 1937 to May 2, 1938. Between the 1960s and the late 1980s, as television news relegated newspaper reading to an occasional basis rather than daily, syndicators were abandoning long stories and urging cartoonists to switch to simple daily gags, or week-long "storylines" (with six consecutive (mostly unrelated) strips following a same subject), with longer storylines being used mainly on adventure-based and dramatic strips. Strips begun during the mid-1980s or after (such as "Get Fuzzy", "Over the Hedge", "Monty", and others) are known for their heavy use of storylines, lasting between one and three weeks in most cases.
The writing style of comic strips changed as well after World War II. With an increase in the number of college-educated readers, there was a shift away from slapstick comedy and towards more cerebral humor. Slapstick and visual gags became more confined to Sunday strips, because as "Garfield" creator Jim Davis put it, "Children are more likely to read Sunday strips than dailies."
Second author.
Many older strips are no longer drawn by the original cartoonist, who has either died or retired. Such strips are known as "zombie strips". A cartoonist, paid by the syndicate or sometimes a relative of the original cartoonist, continues writing the strip, a tradition that became commonplace in the early half of the 20th century. "Hägar the Horrible" and "Frank and Ernest" are both drawn by the sons of the creators. Some strips which are still in affiliation with the original creator are produced by small teams or entire companies, such as Jim Davis' "Garfield", however there is some debate if these strips fall in this category.
This act is commonly criticized by modern cartoonists including Watterson and "Pearls Before Swine"'s Stephan Pastis. The issue was addressed in six consecutive "Pearls" strips in 2005. Charles Schulz, of "Peanuts" fame, requested that his strip not be continued by another cartoonist after his death. He also rejected the idea of hiring an inker or letterer, comparing it to a golfer hiring a man to make his putts. Schulz's family has honored his wishes and refused numerous proposals by syndicators to continue "Peanuts" with a new author.
Assistants.
Since the consolidation of newspaper comics by the first quarter of the 20th century, most cartoonists have used a group of assistants (with usually one of them credited). However, quite a few cartoonists (e.g.: George Herriman and Charles Schulz, among others) have done their strips almost completely by themselves; often criticizing the use of assistants for the same reasons most have about their editors hiring anyone else to continue their work after their retirement.
Rights to the strips.
Since the dawn of comic strips, the ownership of them has been a recurrent issue. Traditionally, the syndicate owned the rights to the strips. However, throughout history there have been exceptions, with "Mutt and Jeff" being an early (if not the earliest) case in which the creator owned his works. However this was later limited to adaptations of animated properties. When it started in 1970, the Universal Press Syndicate gave cartoonists a 50-percent share on the ownership of their works, while the Creators Syndicate (founded in 1987) granted artists full rights to the strips, something that Universal Press did in 1990. followed by King Features in 1995, while before 1999 both the Tribune and United Feature services began granting rights to creators over their works; however the latter three syndicates only applied this to new strips, or to ones popular enough.
Censorship.
Starting in the late 1940s, the national syndicates which distributed newspaper comic strips subjected them to very strict censorship. "Li'l Abner" was censored in September 1947 and was pulled from papers by Scripps-Howard. The controversy, as reported in "Time", centered on Capp's portrayal of the U.S. Senate. Said Edward Leech of Scripps, "We don't think it is good editing or sound citizenship to picture the Senate as an assemblage of freaks and crooks... boobs and undesirables."
Because historically comics have been considered mostly for children, they have a significantly more rigid censorship code than other media. Stephan Pastis has lamented that the "unwritten" censorship code is still "stuck somewhere in the 1950s." Generally, comics are not allowed to include such words as "damn", "sucks", "screwed", and "hell", although there have been exceptions such as the September 22, 2010 "Mother Goose and Grimm" in which an elderly man says, "This nursing home food sucks," and a pair of "Pearls Before Swine" comics from January 11, 2011 with a character named Ned using the word "crappy". Naked backsides and shooting guns cannot be shown, according to "Dilbert" cartoonist Scott Adams. Such comic strip taboos were detailed in Dave Breger's book "But That's Unprintable" (Bantam, 1955). 
Many issues such as sex, narcotics, and terrorism cannot or can very rarely be openly discussed in strips, although there are exceptions, usually for satire, as in "Bloom County". This led some cartoonists to resort to double entendre or dialogue children do not understand, as in Greg Evans' "Luann". Young cartoonists have claimed commonplace words, images, and issues should be allowed in the comics. Some of the taboo words and topics are mentioned daily on television and other forms of visual media. Web comics and comics distributed primarily to college newspapers are much freer in this respect.

</doc>
<doc id="5705" url="http://en.wikipedia.org/wiki?curid=5705" title="Continuum hypothesis">
Continuum hypothesis

In mathematics, the continuum hypothesis (abbreviated CH) is a hypothesis, advanced by Georg Cantor in 1878, about the possible sizes of infinite sets. It states:
Establishing the truth or falsehood of this hypothesis is the first of Hilbert's 23 problems presented in the year 1900. Τhe answer to this problem is independent of ZFC set theory, so that either the continuum hypothesis or its negation can be added as an axiom to ZFC set theory, with the resulting theory being consistent if and only if ZFC is consistent. This was proved in 1963 by Paul Cohen, complementing earlier work by Kurt Gödel in 1940.
The name of the hypothesis comes from the term "the continuum" for the real numbers.
Cardinality of infinite sets.
Two sets are said to have the same "cardinality" or "cardinal number" if there exists a bijection (a one-to-one correspondence) between them. Intuitively, for two sets "S" and "T" to have the same cardinality means that it is possible to "pair off" elements of "S" with elements of "T" in such a fashion that every element of "S" is paired off with exactly one element of "T" and vice versa. Hence, the set formula_1 has the same cardinality as formula_2.
With infinite sets such as the set of integers or rational numbers, this becomes more complicated to demonstrate. The rational numbers seemingly form a counterexample to the continuum hypothesis: the integers form a proper subset of the rationals, which themselves form a proper subset of the reals, so intuitively, there are more rational numbers than integers, and more real numbers than rational numbers. However, this intuitive analysis does not take account of the fact that all three sets are infinite. It turns out the rational numbers can actually be placed in one-to-one correspondence with the integers, and therefore the set of rational numbers is the same size ("cardinality") as the set of integers: they are both countable sets.
Cantor gave two proofs that the cardinality of the set of integers is strictly smaller than that of the set of real numbers (see Cantor's first uncountability proof and Cantor's diagonal argument). His proofs, however, give no indication of the extent to which the cardinality of the integers is less than that of the real numbers. Cantor proposed the continuum hypothesis as a possible solution to this question.
The hypothesis states that the set of real numbers has minimal possible cardinality which is greater than the cardinality of the set of integers. Equivalently, as the cardinality of the integers is formula_3 ("aleph-naught") and the cardinality of the real numbers is formula_4, the continuum hypothesis says that there is no set formula_5 for which
Assuming the axiom of choice, there is a smallest cardinal number formula_7 greater than formula_3, and the continuum hypothesis is in turn equivalent to the equality
There is also a generalization of the continuum hypothesis called the generalized continuum hypothesis (GCH) which says that for all ordinals formula_10
A consequence of the continuum hypothesis is that every infinite subset of the real numbers either has the same cardinality as the integers or the same cardinality as the entire set of the reals.
Independence from ZFC.
Cantor believed the continuum hypothesis to be true and tried for many years to prove it, in vain . It became the first on David Hilbert's list of important open questions that was presented at the International Congress of Mathematicians in the year 1900 in Paris. Axiomatic set theory was at that point not yet formulated.
Kurt Gödel showed in 1940 that the continuum hypothesis (CH for short) cannot be disproved from the standard Zermelo–Fraenkel set theory (ZF), even if the axiom of choice is adopted (ZFC) (). Paul Cohen showed in 1963 that CH cannot be proven from those same axioms either ( & ). Hence, CH is "independent" of ZFC. Both of these results assume that the Zermelo–Fraenkel axioms are consistent; this assumption is widely believed to be true. Cohen was awarded the Fields Medal in 1966 for his proof.
The continuum hypothesis is closely related to many statements in analysis, point set topology and measure theory. As a result of its independence, many substantial conjectures in those fields have subsequently been shown to be independent as well.
So far, CH appears to be independent of all known "large cardinal axioms" in the context of ZFC.
Gödel and Cohen's negative results are not universally accepted as disposing of the hypothesis. Hilbert's problem remains an active topic of research; see and for an overview of the current research status.
The continuum hypothesis was not the first statement shown to be independent of ZFC. An immediate consequence of Gödel's incompleteness theorem, which was published in 1931, is that there is a formal statement (one for each appropriate Gödel numbering scheme) expressing the consistency of ZFC that is independent of ZFC, assuming that ZFC is incomplete. The continuum hypothesis and the axiom of choice were among the first mathematical statements shown to be independent of ZF set theory. These proofs of independence were not completed until Paul Cohen developed forcing in the 1960s. However, they all rely on the assumption that ZF is consistent. These proofs are called proofs of relative consistency (see "Forcing (mathematics))".
Arguments for and against CH.
Gödel believed that CH is false and that his proof that CH is consistent with ZFC only shows that the Zermelo–Fraenkel axioms do not adequately characterize the universe of sets. Gödel was a platonist and therefore had no problems with asserting the truth and falsehood of statements independent of their provability. Cohen, though a formalist , also tended towards rejecting CH.
Historically, mathematicians who favored a "rich" and "large" universe of sets were against CH, while those favoring a "neat" and "controllable" universe favored CH. Parallel arguments were made for and against the axiom of constructibility, which implies CH. More recently, Matthew Foreman has pointed out that ontological maximalism can actually be used to argue in favor of CH, because among models that have the same reals, models with "more" sets of reals have a better chance of satisfying CH (Maddy 1988, p. 500).
Another viewpoint is that the conception of set is not specific enough to determine whether CH is true or false. This viewpoint was advanced as early as 1923 by Skolem, even before Gödel's first incompleteness theorem. Skolem argued on the basis of what is now known as Skolem's paradox, and it was later supported by the independence of CH from the axioms of ZFC, since these axioms are enough to establish the elementary properties of sets and cardinalities. In order to argue against this viewpoint, it would be sufficient to demonstrate new axioms that are supported by intuition and resolve CH in one direction or another. Although the axiom of constructibility does resolve CH, it is not generally considered to be intuitively true any more than CH is generally considered to be false (Kunen 1980, p. 171).
At least two other axioms have been proposed that have implications for the continuum hypothesis, although these axioms have not currently found wide acceptance in the mathematical community. In 1986, Chris Freiling presented an argument against CH by showing that the negation of CH is equivalent to Freiling's axiom of symmetry, a statement about probabilities. Freiling believes this axiom is "intuitively true" but others have disagreed. A difficult argument against CH developed by W. Hugh Woodin has attracted considerable attention since the year 2000 (Woodin 2001a, 2001b). Foreman (2003) does not reject Woodin's argument outright but urges caution.
Solomon Feferman (2011) has made a complex philosophical argument that CH is not a definite mathematical problem. He proposes a theory of "definiteness" using a semi-intuitionistic subsystem of ZF that accepts classical logic for bounded quantifiers but uses intuitionistic logic for unbounded ones, and suggests that a proposition formula_12 is mathematically "definite" if the semi-intuitionistic theory can prove formula_13. He conjectures that CH is not definite according to this notion, and proposes that CH should therefore be considered not to have a truth value. Peter Koellner (2011b) wrote a critical commentary on Feferman's article.
Joel David Hamkins proposes a multiverse approach to set theory and argues that "the continuum hypothesis is settled on the multiverse view by our extensive knowledge about how it behaves in the multiverse, and as a result it can no longer be settled in the manner formerly hoped for." (Hamkins 2012).
The generalized continuum hypothesis.
The "generalized continuum hypothesis" (GCH) states that if an infinite set's cardinality lies between that of an infinite set "S" and that of the power set of "S", then it either has the same cardinality as the set "S" or the same cardinality as the power set of "S". That is, for any infinite cardinal formula_14 there is no cardinal formula_15 such that formula_16 GCH is equivalent to:
The beth numbers provide an alternate notation for this condition: formula_19 for every ordinal formula_18
This is a generalization of the continuum hypothesis since the continuum has the same cardinality as the power set of the integers.
Like CH, GCH is also independent of ZFC, but Sierpiński proved that ZF + GCH implies the axiom of choice (AC), so choice and GCH are not independent in ZF; there are no models of ZF in which GCH holds and AC fails. To prove this, Sierpiński showed GCH implies that every cardinality n is smaller than some Aleph number, and thus can be ordered. This is done by showing that n is smaller than formula_21 which is smaller than its own Hartogs number (this uses the equality formula_22; for the full proof, see Gillman (2002). 
Kurt Gödel showed that GCH is a consequence of ZF + V=L (the axiom that every set is constructible relative to the ordinals), and is therefore consistent with ZFC. As GCH implies CH, Cohen's model in which CH fails is a model in which GCH fails, and thus GCH is not provable from ZFC. W. B. Easton used the method of forcing developed by Cohen to prove Easton's theorem, which shows it is consistent with ZFC for arbitrarily large cardinals formula_23 to fail to satisfy formula_24 Much later, Foreman and Woodin proved that (assuming the consistency of very large cardinals) it is consistent that formula_25 holds for every infinite cardinal formula_26 Later Woodin extended this by showing the consistency of formula_27 for every formula_15. A recent result of Carmi Merimovich shows that, for each "n"≥1, it is consistent with ZFC that for each κ, 2κ is the "n"th successor of κ. On the other hand, proved, that if γ is an ordinal and for each infinite cardinal κ, 2κ is the γth successor of κ, then γ is finite.
For any infinite sets A and B, if there is an injection from A to B then there is an injection from subsets of A to subsets of B. Thus for any infinite cardinals A and B,
References.
, , etc. - instead, fix new citations so that they use the existing citation style

</doc>
<doc id="5706" url="http://en.wikipedia.org/wiki?curid=5706" title="Çevik Bir">
Çevik Bir

Çevik Bir is a retired Turkish army general. He was a member of the Turkish General Staff in the 1990s. He took a major part in several important international missions in the Middle East and North Africa. He was born in Buca, Izmir Province, in 1939 and is married with one child.
He graduated from the Turkish Military Academy as an engineer officer in 1958, from the Army Staff College in 1970 and from the Armed Forces College in 1971. He graduated from NATO Defense College, Rome, Italy in 1973.
From 1973 to 1985, he served at SHAPE, NATO's headquarters in Belgium. He was promoted to brigadier general and commanded an armed brigade and division in Turkey. From 1987 to 1991, he served as major general, and then was promoted to lieutenant general.
After the dictator Siad Barre’s ousting, conflicts between the General Farah Aidid's party and other clans in Somalia had led to famine and lawlessness throughout the country. An estimated 300,000 people had died from starvation. A combined military force of United States and United Nations (under the name "UNOSOM") were deployed to Mogadishu, to monitor the ceasefire and deliver food and supplies to the starving people of Somali. Çevik Bir, who was then a lieutenant-general of Turkey, became the force commander of UNOSOM II in 1993. Despite the retreat of US and UN forces after several deaths due to local hostilities mainly led by Aidid, the introduction of a powerful military force opened the transportation routes, enabling the provision of supplies and ended the famine quickly.
He became a four-star general and served three years as vice chairman of the Turkish Armed Forces, then appointed commander of the Turkish First Army, in Istanbul. While he was vice chairman of the TAF, he signed the Turkish-Israeli Military Coordination agreement in 1996.
Çevik Bir became the Turkish army's deputy chief of general staff shortly after the Somali operation and played a vital role in establishing a Turkish-Israeli entente against the emerging fundamentalism in the Middle East.
Çevik Bir retired from the army on August 30, 1999. He is a former member of the Association for the Study of the Middle East and Africa (ASMEA).
On April 12, 2012, Bir and 30 other officers were taken in custody for their role in the 1997 military memorandum that forced the then Turkish government, led by the Refah Partisi (Welfare Party), to step down.

</doc>
<doc id="5708" url="http://en.wikipedia.org/wiki?curid=5708" title="Collectivism">
Collectivism

Collectivism is any philosophic, political, religious, economic, or social outlook that emphasizes the interdependence of every human. Collectivism is a basic cultural element that exists as the reverse of individualism in human nature (in the same way high context culture exists as the reverse of low context culture). Collectivist orientations stress the importance of cohesion within social groups (such as an "in-group", in what specific context it is defined) and in some cases, the priority of group goals over individual goals. Collectivists often focus on community, society, nation or country. It has been used as an element in many different and diverse types of government and political, economic and educational philosophies throughout history and most human societies in practice contain elements of both individualism and collectivism. Some examples of collectivist cultures include Pakistan, India and Japan. 
Collectivism can be divided into "horizontal" (or egalitarian) collectivism and "vertical" (or hierarchical) collectivism. Horizontal collectivism stresses collective decision-making among equal individuals, and is thus usually based on decentralization and egalitarianism. Vertical collectivism is based on hierarchical structures of power and on moral and cultural conformity, and is therefore based on centralization and hierarchy. A cooperative enterprise would be an example of horizontal collectivism, whereas a military hierarchy would be an example of vertical collectivism.
Typology.
Collectivism has been used to refer to a diverse range of political and economic positions, including nationalism, direct democracy, representative democracy and monarchy. In modern times, collectivism is sometimes thought to be synonymous with the political culture and system of the former Soviet Union, specifically to the ideologies of Leninism and Marxist-Leninism for their emphasis on a Vanguard party and paramilitary organizational principles, though collectivism more accurately simply means "group oriented" or "group orientation." Collectivism does not require a government or political system to exist (an example of that would be a religious organization that stresses "group goals" within it that is not backed by a government like American or Canadian society), but it can also exist within a political system rather than simply "on the ground". Primarily, collectivism describes how groups orient themselves naturally within a society. 
Collectivism can be typified as "horizontal collectivism", wherein equality is emphasized and people engage in sharing and cooperation, or "vertical collectivism", wherein hierarchy is emphasized and people submit to specific authorities. Horizontal collectivism is based on the assumption that each individual is more or less equal, while vertical collectivism assumes that individuals are fundamentally different from each other. Social anarchist Alexander Berkman, who was a horizontal collectivist, argued that equality does not imply a lack of unique individuality, but an equal amount of freedom and equal opportunity to develop one's own skills and talents,
Indeed, horizontal collectivists argue that the idea of individuals sacrificing themselves for the "group" or "greater good" is nonsensical, arguing that groups are made up of individuals (including oneself) and are not a cohesive, monolithic entity separate from the self. But most social anarchists do not see themselves as collectivists or individualists, viewing both as illusory ideologies based on fiction .
Horizontal collectivists tend to favour democratic decision-making, while vertical collectivists believe in a strict chain of command. Horizontal collectivism stresses common goals, interdependence and sociability. Vertical collectivism stresses the integrity of the in-group (e.g. the family or the nation), expects individuals to sacrifice themselves for the in-group if necessary, and promotes competition between different in-groups.
Culture and politics.
Collectivism is a basic element of human culture that exists independently of any one political system and has existed since the founding of human society ten thousand years ago. It is a feature that all societies use to some degree or another and therefore an inherent feature of human nature. For example, monarchical societies often had a system of "social ranks" which were collectivist because the social rank one had or did not have was more important than his or her individual will, and the specific rank in question could only be overridden in very limited cases. An example of collectivism in more modern times are the police and fire departments. All individuals (except in rare cases) are expected to pay taxes to these organizations and their will has been overridden in making them do so under law, thus they are collectivist institutions. We also see, that in regards to a police department, an individual can be detained whether he or she wishes to or not, overriding his or her will as an example of collectivism.
An example of a collectivist political system is representative democracy, as in such systems, after voting occurs and a leader has been chosen by the populace everyone is expected to accept that individual as their leader regardless of whether they voted for them or not. For example, in the United States Presidential election of 2012 Barack Obama received a majority of the electoral college votes cast, and the opposition was expected to submit to letting him lead them whether or not they had originally voted for him. The will of the "collective" (President Obama voters) mattered more and is considered "collectivist" because ultimately, the totality of decision by the voters in the country, expressed through the electoral college system, was more important than the will of any single individual in that context.
Though all human societies contain elements of both individualism and collectivism by definition (if not they would become unstable), some societies are on the whole more collectivist and some on the whole more individualist. In collectivist societies, the group is considered more important than any one individual and groups in such societies are expected to "take care" of their members and individuals are expected to "take care" of the group (usually called an "in-group") that they are a member of. Harmony within these groups is considered paramount. For example, it may be considered "inappropriate" for a member of an in-group to openly criticize another in public (though they are often allowed to do so in private). Collectivism does have its advantages as compared to individualist societies as people in collectivist societies almost always have access to a "group" and as such are known to be considered "happier", "less lonely", and have lower rates of mental illness in studies done by psychologists and political scientists. People in individual societies are known to feel "lonely" at some times or another compared to their collectivist counterparts. Many people also find it easier, to live in a society where social harmony is stressed and groups by definition remain more cohesive than in individualist societies where groups are observed to be inherently less stable. However, it depends on the preference of an individual if they wish to live in a collectivist society like Japan or an individualist one like the United States. One type could not be said to be better than another and both are known to come into existence naturally as a consequence of human nature.
Criticisms.
Classical liberal criticisms.
There are two main objections to collectivism from the ideas of individualism. One is that collectivism stifles individuality and diversity by insisting upon a common social identity, such as nationalism or some other group focus. The other is that collectivism is linked to statism and the diminution of freedom when political authority is used to advance collectivist goals.
Criticism of collectivism comes from liberal individualists, such as classical liberals, libertarians, Objectivists, and individualist anarchists. Perhaps the most notable modern criticism of economic collectivism is the one put forward by Friedrich Hayek in his book "The Road to Serfdom", published in 1944.
Ludwig von Mises wrote:
On the other hand the application of the basic ideas of collectivism cannot result in anything but social disintegration and the perpetuation of armed conflict. It is true that every variety of collectivism promises eternal peace starting with the day of its own decisive victory and the final overthrow and extermination of all other ideologies and their supporters. ... As soon as a faction has succeeded in winning the support of the majority of citizens and thereby attained control of the government machine, it is free to deny to the minority all those democratic rights by means of which it itself has previously carried on its own struggle for supremacy.
Socialist criticisms.
Many socialists, particularly libertarian socialists, individualist anarchists, and De Leonists criticise the concept of collectivism. Some anti-collectivists often argue that all authoritarian and totalitarian societies are (vertically) collectivist in nature. Socialists argue that modern capitalism and private property, which is based on socialized production and joint-stock or corporate ownership structures, is a form of organic collectivism that sharply contrasts with the perception that capitalism is a system of free individuals exchanging commodities. Socialists sometimes argue that true individualism can only exist when individuals are free from coercive social structures to pursue their own interests, which can only be accomplished by common ownership of socialized, productive assets and free access to the means of life so that no individual has coercive power over other individuals.
George Orwell, a dedicated democratic socialist, believed that collectivism resulted in the empowerment of a minority of individuals that led to further oppression of the majority of the population in the name of some ideal such as freedom.
It cannot be said too often – at any rate, it is not being said nearly often enough – that collectivism is not inherently democratic, but, on the contrary, gives to a tyrannical minority such powers as the Spanish Inquisitors never dreamt of.
Yet in the subsequent sentence he also warns of the tyranny of private ownership over the means of production:
... that a return to 'free' competition means for the great mass of people a tyranny probably worse, because more irresponsible, than that of the state.
Marxists criticize this use of the term "collectivism," on the grounds that all societies are based on class interests and therefore all societies could be considered "collectivist." The liberal ideal of the free individual is seen from a Marxist perspective as a smokescreen for the collective interests of the capitalist class. Social anarchists argue that "individualism" is a front for the interests of the upper class. As anarchist Emma Goldman wrote:
'rugged individualism'... is only a masked attempt to repress and defeat the individual and his individuality. So-called Individualism is the social and economic laissez-faire: the exploitation of the masses by the classes by means of legal trickery, spiritual debasement and systematic indoctrination of the servile spirit ... That corrupt and perverse 'individualism' is the straitjacket of individuality. ... [It has inevitably resulted in the greatest modern slavery, the crassest class distinctions driving millions to the breadline. 'Rugged individualism' has meant all the 'individualism' for the masters, while the people are regimented into a slave caste to serve a handful of self-seeking 'supermen.' ... Their 'rugged individualism' is simply one of the many pretenses the ruling class makes to mask unbridled business and political extortion.
In response to criticism made by various pro-capitalist groups that claim that public ownership or common ownership of the means of production is a form of collectivism, socialists maintain that common ownership over productive assets does not infringe upon the individual, but is instead a liberating force that transcends the false dichotomy of individualism and collectivism. Socialists maintain that these critiques conflate the concept of private property in the means of production with personal possessions and individual production.
Other criticisms.
Ayn Rand, creator of the ideology Objectivism and a particularly vocal opponent of collectivism, argued that it led to totalitarianism. She argued that "collectivism means the subjugation of the individual to a group," and that "throughout history, no tyrant ever rose to power except on the claim of representing "the common good"." She further claimed that "horrors which no man would dare consider for his own selfish sake are perpetrated with a clear conscience by "altruists" who justify themselves by the common good." (The "altruists" Rand refers to are not those who practice simple benevolence or charity, but rather those who believe in Auguste Comte's ethical doctrine of altruism which holds that there is "a moral and political obligation of the individual to sacrifice his own interests for the sake of a greater social good.").

</doc>
<doc id="5711" url="http://en.wikipedia.org/wiki?curid=5711" title="Nepeta">
Nepeta

Nepeta is a genus of flowering plants in the family Lamiaceae known commonly as catmints. The genus name is reportedly in reference to Nepete, an ancient Etruscan city. There are about 250 species.
The genus is native to Europe, Asia, and Africa, and has also naturalized in North America.
Some members of this group are known as catnip or catmint because of their effect on house cats – the nepetalactone contained in some "Nepeta" species binds to the olfactory receptors of cats, typically resulting in temporary euphoria.
Description.
Most of the species are herbaceous perennial plants, but some are annuals. They have sturdy stems with opposite heart-shaped, green to gray-green leaves. "Nepeta" plants are usually aromatic in foliage and flowers.
The tubular flowers can be lavender, blue, white, pink, or lilac, and spotted with tiny lavender-purple dots. The flowers are located in verticillasters grouped on spikes; or the verticillasters are arranged in opposite cymes, racemes, or panicles – toward the tip of the stems.
The calyx is tubular or campanulate, they are slightly curved or straight, and the limbs are often 2-lipped with five teeth. The lower lip is larger, with 3-lobes, and the middle lobe is the largest. The flowers have 4 hairless stamens that are nearly parallel, and they ascend under the upper lip of the corolla. Two stamen are longer and stamens of pistillate flowers are rudimentary. The style protrudes outside of the mouth of the flowers.
The fruits are nutlets, which are oblong-ovoid, ellipsoid, ovoid, or obovoid in shape. The surfaces of the nutlets can be slightly ribbed, smooth or warty.
Uses.
Cultivation.
Some "Nepeta" species are cultivated as ornamental plants. They can be drought tolerant – water conserving, often deer repellent, with long bloom periods from late spring to autumn. Some species also have repellent properties to insect pests, including aphids and squash bugs, when planted in a garden.
"Nepeta" species are used as food plants by the larvae of some Lepidoptera (butterfly and moth) species including "Coleophora albitarsella", and as nectar sources for pollinators, such as honeybees and hummingbirds.
Medicinal.
Some catmint species are also used in herbal medicine for their mild sedative effect on humans. Chemical compounds isolated from "Nepeta cataria" inhibit calcineurin "in vitro".
"Nepeta cataria" has been used as an insect repellent for humans against mosquitoes.
Teucrium chamaedrys L. and Nepeta cataria L. (Lamiaceae) can be used for the treatment of inflammation. Extracts of both species were found to inhibit calcineurin; a regulator of T-cell mediated inflammation. 

</doc>
