<doc id="3118" url="http://en.wikipedia.org/wiki?curid=3118" title="Arithmetic">
Arithmetic

Arithmetic or arithmetics (from the Greek word ἀριθμός, "arithmos" "number") is the oldest and most elementary branch of mathematics (compared to algebra, geometry, and analysis). It consists in the study of numbers, especially the properties of the traditional operations between them — addition, subtraction, multiplication and division. Arithmetic is an elementary part of number theory. However the terms "arithmetic" and "higher arithmetic" were used until the beginning of 20th century as synonyms for "number theory", and are, sometimes, still used to refer to a wider part of number theory.
History.
The prehistory of arithmetic is limited to a small number of artifacts which may indicate the conception of addition and subtraction, the best-known being the Ishango bone from central Africa, dating from somewhere between 20,000 and 18,000 BC, although its interpretation is disputed.
The earliest written records indicate the Egyptians and Babylonians used all the elementary arithmetic operations as early as 2000 BC. These artifacts do not always reveal the specific process used for solving problems, but the characteristics of the particular numeral system strongly influence the complexity of the methods. The hieroglyphic system for Egyptian numerals, like the later Roman numerals, descended from tally marks used for counting. In both cases, this origin resulted in values that used a decimal base but did not include positional notation. Complex calculations with Roman numerals required the assistance of a counting board or the Roman abacus to obtain the results.
Early number systems that included positional notation were not decimal, including the sexagesimal (base 60) system for Babylonian numerals and the vigesimal (base 20) system that defined Maya numerals. Because of this place-value concept, the ability to reuse the same digits for different values contributed to simpler and more efficient methods of calculation.
The continuous historical development of modern arithmetic starts with the Hellenistic civilization of ancient Greece, although it originated much later than the Babylonian and Egyptian examples. Prior to the works of Euclid around 300 BC, Greek studies in mathematics overlapped with philosophical and mystical beliefs. For example, Nicomachus summarized the viewpoint of the earlier Pythagorean approach to numbers, and their relationships to each other, in his "Introduction to Arithmetic".
Greek numerals were used by Archimedes, Diophantus and others in a positional notation not very different from ours. Because the ancient Greeks lacked a symbol for zero (until the Hellenistic period), they used three separate sets of symbols. One set for the unit's place, one for the ten's place, and one for the hundred's. Then for the thousand's place they would reuse the symbols for the unit's place, and so on. Their addition algorithm was identical to ours, and their multiplication algorithm was only very slightly different. Their long division algorithm was the same, and the square root algorithm that was once taught in school was known to Archimedes, who may have invented it. He preferred it to Hero's method of successive approximation because, once computed, a digit doesn't change, and the square roots of perfect squares, such as 7485696, terminate immediately as 2736. For numbers with a fractional part, such as 546.934, they used negative powers of 60 instead of negative powers of 10 for the fractional part 0.934. The ancient Chinese used a similar positional notation. Because they also lacked a symbol for zero, they had one set of symbols for the unit's place, and a second set for the ten's place. For the hundred's place they then reused the symbols for the unit's place, and so on. Their symbols were based on the ancient counting rods. It is a complicated question to determine exactly when the Chinese started calculating with positional representation, but it was definitely before 400 BC. The Bishop of Syria, Severus Sebokht (650 AD), "Indians possess a method of calculation that no word can praise enough. Their rational system of mathematics, or of their method of calculation. I mean the system using nine symbols."
Leonardo of Pisa (Fibonacci) in 1200 AD wrote in "Liber Abaci" "The method of the Indians (Modus Indoram) surpasses any known method to compute. Its a marvelous method. They do their computations using nine figures and symbol zero".
The gradual development of Hindu–Arabic numerals independently devised the place-value concept and positional notation, which combined the simpler methods for computations with a decimal base and the use of a digit representing 0. This allowed the system to consistently represent both large and small integers. This approach eventually replaced all other systems. In the early the Indian mathematician Aryabhata incorporated an existing version of this system in his work, and experimented with different notations. In the 7th century, Brahmagupta established the use of 0 as a separate number and determined the results for multiplication, division, addition and subtraction of zero and all other numbers, except for the result of division by 0. His contemporary, the Syriac bishop Severus Sebokht described the excellence of this system as "... valuable methods of calculation which surpass description". The Arabs also learned this new method and called it "hesab".
Although the Codex Vigilanus described an early form of Arabic numerals (omitting 0) by 976 AD, Fibonacci was primarily responsible for spreading their use throughout Europe after the publication of his book "Liber Abaci" in 1202. He considered the significance of this "new" representation of numbers, which he styled the "Method of the Indians" (Latin "Modus Indorum"), so fundamental that all related mathematical foundations, including the results of Pythagoras and the algorism describing the methods for performing actual calculations, were "almost a mistake" in comparison.
In the Middle Ages, arithmetic was one of the seven liberal arts taught in universities.
The flourishing of algebra in the medieval Islamic world and in Renaissance Europe was an outgrowth of the enormous simplification of computation through decimal notation.
Various types of tools exist to assist in numeric calculations. Examples include slide rules (for multiplication, division, and trigonometry) and nomographs in addition to the electrical calculator.
Arithmetic operations.
The basic arithmetic operations are addition, subtraction, multiplication and division, although this subject also includes more advanced operations, such as manipulations of percentages, square roots, exponentiation, and logarithmic functions. Arithmetic is performed according to an order of operations. Any set of objects upon which all four arithmetic operations (except division by 0) can be performed, and where these four operations obey the usual laws, is called a field.
Addition (+).
Addition is the basic operation of arithmetic. In its simplest form, addition combines two numbers, the "addends" or "terms", into a single number, the "sum" of the numbers (Such as or ).
Adding more than two numbers can be viewed as repeated addition; this procedure is known as summation and includes ways to add infinitely many numbers in an infinite series; repeated addition of the number 1 is the most basic form of counting.
Addition is commutative and associative so the order the terms are added in does not matter. The identity element of addition (the additive identity) is 0, that is, adding 0 to any number yields that same number. Also, the inverse element of addition (the additive inverse) is the opposite of any number, that is, adding the opposite of any number to the number itself yields the additive identity, 0. For example, the opposite of 7 is −7, so .
Addition can be given geometrically as in the following example:
Subtraction (−).
Subtraction is the opposite of addition. Subtraction finds the "difference" between two numbers, the "minuend" minus the "subtrahend". If the minuend is larger than the subtrahend, the difference is positive; if the minuend is smaller than the subtrahend, the difference is negative; if they are equal, the difference is 0.
Subtraction is neither commutative nor associative. For that reason, it is often helpful to look at subtraction as addition of the minuend and the opposite of the subtrahend, that is . When written as a sum, all the properties of addition hold.
There are several methods for calculating results, some of which are particularly advantageous to machine calculation. For example, digital computers employ the method of two's complement. Of great importance is the counting up method by which change is made. Suppose an amount "P" is given to pay the required amount "Q", with "P" greater than "Q". Rather than performing the subtraction and counting out that amount in change, money is counted out starting at "Q" and continuing until reaching "P". Although the amount counted out must equal the result of the subtraction , the subtraction was never really done and the value of might still be unknown to the change-maker.
Multiplication (× or · or *).
Multiplication is the second basic operation of arithmetic. Multiplication also combines two numbers into a single number, the "product". The two original numbers are called the "multiplier" and the "multiplicand", sometimes both simply called "factors".
Multiplication is best viewed as a scaling operation. If the numbers are imagined as lying in a line, multiplication by a number, say "x", greater than 1 is the same as stretching everything away from 0 uniformly, in such a way that the number 1 itself is stretched to where "x" was. Similarly, multiplying by a number less than 1 can be imagined as squeezing towards 0. (Again, in such a way that 1 goes to the multiplicand.)
Multiplication is commutative and associative; further it is distributive over addition and subtraction. The multiplicative identity is 1, that is, multiplying any number by 1 yields that same number. Also, the multiplicative inverse is the reciprocal of any number (except 0; 0 is the only number without a multiplicative inverse), that is, multiplying the reciprocal of any number by the number itself yields the multiplicative identity.
The product of "a" and "b" is written as or . When "a" or "b" are expressions not written simply with digits, it is also written by simple juxtaposition: "ab". In computer programming languages and software packages in which one can only use characters normally found on a keyboard, it is often written with an asterisk: 
Division (÷ or /).
Division is essentially the opposite of multiplication. Division finds the "quotient" of two numbers, the "dividend" divided by the "divisor". Any dividend divided by 0 is undefined. For distinct positive numbers, if the dividend is larger than the divisor, the quotient is greater than 1, otherwise it is less than 1 (a similar rule applies for negative numbers). The quotient multiplied by the divisor always yields the dividend.
Division is neither commutative nor associative. As it is helpful to look at subtraction as addition, it is helpful to look at division as multiplication of the dividend times the reciprocal of the divisor, that is When written as a product, it obeys all the properties of multiplication.
Decimal arithmetic.
Decimal representation refers exclusively, in common use, to the written numeral system employing arabic numerals as the digits for a radix 10 ("decimal") positional notation; however, any numeral system based on powers of 10, e.g., Greek, Cyrillic, Roman, or Chinese numerals may conceptually be described as "decimal notation" or "decimal representation".
Modern methods for four fundamental operations (addition, subtraction, multiplication and division) were first devised by Brahmagupta of India. This was known during medieval Europe as "Modus Indoram" or Method of the Indians. Positional notation (also known as "place-value notation") refers to the representation or encoding of numbers using the same symbol for the different orders of magnitude (e.g., the "ones place", "tens place", "hundreds place") and, with a radix point, using those same symbols to represent fractions (e.g., the "tenths place", "hundredths place"). For example, 507.36 denotes 5 hundreds (102), plus 0 tens (101), plus 7 units (100), plus 3 tenths (10−1) plus 6 hundredths (10−2).
The concept of 0 as a number comparable to the other basic digits is essential to this notation, as is the concept of 0's use as a placeholder, and as is the definition of multiplication and addition with 0. The use of 0 as a placeholder and, therefore, the use of a positional notation is first attested to in the Jain text from India entitled the "Lokavibhâga", dated 458 AD and it was only in the early 13th century that these concepts, transmitted via the scholarship of the Arabic world, were introduced into Europe by Fibonacci using the Hindu–Arabic numeral system.
Algorism comprises all of the rules for performing arithmetic computations using this type of written numeral. For example, addition produces the sum of two arbitrary numbers. The result is calculated by the repeated addition of single digits from each number that occupies the same position, proceeding from right to left. An addition table with ten rows and ten columns displays all possible values for each sum. If an individual sum exceeds the value 9, the result is represented with two digits. The rightmost digit is the value for the current position, and the result for the subsequent addition of the digits to the left increases by the value of the second (leftmost) digit, which is always one. This adjustment is termed a "carry" of the value 1.
The process for multiplying two arbitrary numbers is similar to the process for addition. A multiplication table with ten rows and ten columns lists the results for each pair of digits. If an individual product of a pair of digits exceeds 9, the "carry" adjustment increases the result of any subsequent multiplication from digits to the left by a value equal to the second (leftmost) digit, which is any value from (). Additional steps define the final result.
Similar techniques exist for subtraction and division.
The creation of a correct process for multiplication relies on the relationship between values of adjacent digits. The value for any single digit in a numeral depends on its position. Also, each position to the left represents a value ten times larger than the position to the right. In mathematical terms, the exponent for the radix (base) of 10 increases by 1 (to the left) or decreases by 1 (to the right). Therefore, the value for any arbitrary digit is multiplied by a value of the form 10"n" with integer "n". The list of values corresponding to all possible positions for a single digit is written 
Repeated multiplication of any value in this list by 10 produces another value in the list. In mathematical terminology, this characteristic is defined as closure, and the previous list is described as closed under multiplication. It is the basis for correctly finding the results of multiplication using the previous technique. This outcome is one example of the uses of number theory.
Compound unit arithmetic.
Compound unit arithmetic is the application of arithmetic operations to mixed radix quantities such as feet and inches, gallons and pints, pounds shillings and pence, and so on. Prior to the use of decimal-based systems of money and units of measure, the use of compound unit arithmetic formed a significant part of commerce and industry.
Basic arithmetic operations.
The techniques used for compound unit arithmetic were developed over many centuries and are well-documented in many textbooks in many different languages. In addition to the basic arithmetic functions encountered in decimal arithmetic, compound unit arithmetic employs three more functions:
Knowledge of the relationship between the various units of measure, their multiples and their submultiples forms an essential part of compound unit arithmetic.
Principles of compound unit arithmetic.
There are two basic approaches to compound unit arithmetic:
Operations in practice.
During the 19th and 20th centuries various aids were developed to aid the manipulation of compound units, particularly in commercial applications. The most common aids were mechanical tills which were adapted in countries such as the United Kingdom to accommodate pounds, shillings, pennies and farthings and "Ready Reckoners" – books aimed at traders that catalogued the results of various routine calculations such as the percentages or multiples of various sums of money. One typical booklet that ran to 150 pages tabulated multiples "from one to ten thousand at the various prices from one farthing to one pound".
The cumbersome nature of compound unit arithmetic has been recognized for many years – in 1586, the Flemish mathematician Simon Stevin published a small pamphlet called "De Thiende" ("the tenth") in which he declared that the universal introduction of decimal coinage, measures, and weights to be merely a question of time while in the modern era, many conversion programs, such as that embedded in the calculator supplied as a standard part of the Microsoft Windows 7 operating system display compound units in a reduced decimal format rather than using an expanded format (i.e. "2.5 ft" is displayed rather than ).
Number theory.
Until 19th century, "number theory" was a synonym of "arithmetic". The addressed problems were directly related to the basic operations and concerned primality, divisibility, and the solution of equations in integers, such as Fermat's last theorem. It appeared that most of these problems, although very elementary to state, are very difficult and may not be solved without very deep mathematics involving concepts and methods from many other branches of mathematics. This led to new branches of number theory such as analytic number theory, algebraic number theory, Diophantine geometry and arithmetic algebraic geometry. Wiles' proof of Fermat's Last Theorem is a typical example of the necessity of sophistical methods, which go far beyond the classical methods of arithmetic, for solving problems that can be stated in elementary arithmetic.
Arithmetic in education.
Primary education in mathematics often places a strong focus on algorithms for the arithmetic of natural numbers, integers, fractions, and decimals (using the decimal place-value system). This study is sometimes known as algorism.
The difficulty and unmotivated appearance of these algorithms has long led educators to question this curriculum, advocating the early teaching of more central and intuitive mathematical ideas. One notable movement in this direction was the New Math of the 1960s and 1970s, which attempted to teach arithmetic in the spirit of axiomatic development from set theory, an echo of the prevailing trend in higher mathematics.
Also, arithmetic was used by Islamic Scholars in order to teach application of the rulings related to Zakat and Irth. This was done in a book entitled "The Best of Arithmetic" by Abd-al-Fattah-al-Dumyati.
The book begins with the foundations of mathematics and proceeds to its application in the later chapters.

</doc>
<doc id="3120" url="http://en.wikipedia.org/wiki?curid=3120" title="Andersonville, Georgia">
Andersonville, Georgia

Andersonville is a city in Sumter County, Georgia, United States. As of the 2010 census, the city had a population of 255. It is located in the southwest part of the state, about southwest of Macon, Georgia on the Central of Georgia railroad. During the American Civil War, it was the site of a prisoner-of-war camp which is now Andersonville National Historic Site.
Andersonville is part of the Americus Micropolitan Statistical Area.
History.
The little hamlet of Anderson was named for Mr. John Anderson who was a Director in the South Western Railroad at the time it was extended from Oglethorpe to Americus in 1853. It was known as Anderson Station until the post office was established in November 1855 and the government changed the name of the station from “Anderson” to “Andersonville” in order to avoid confusion with the post office in Anderson, South Carolina.
During the Civil War, the Confederate army established Camp Sumter to house incoming Union prisoners of war. The town served as a supply depot during the period, and it included a post office, a depot, a blacksmith shop and stable, a couple of general stores, two saloons, a school, a Methodist church, and about a dozen houses. (Ben Dykes, who owned the land on which the prison was built, was both depot agent and postmaster.)
Until the establishment of the prison, the area was entirely dependent on agriculture, and, after the close of the prison, the town continued economically dependent on agriculture. The town changed very little over the years, until 1968 when the large scale mining of kaolin, bauxitic kaolin, and bauxite was begun by Mulcoa, Mullite Company of America, which turned of scrub oak wilderness into a massive mining and refining operation. The company now ships more than 2000 tons of refined ore from Andersonville each week.
In 1974, long-time mayor Lewis Easterlin and a group of concerned citizens decided to promote tourism in the town by turning the clock back and making Andersonville look much as it did during the American Civil War. Now today Andersonville welcomes tourists from all over the world who come for the history, museums, and to step back in time.
Demographics.
As of the census of 2000, there were 331 people, 124 households, and 86 families residing in the city. The population density was 254.1 people per square mile (98.3/km²). There were 142 housing units at an average density of 109.0 per square mile (42.2/km²). The racial makeup of the city was 65.26% white and 34.74% African American. 1.21% of the population were Hispanic or Latino.
There were 124 households out of which 34.7% had children under the age of 18 living with them, 46.0% were married couples living together, 17.7% had a female householder with no husband present, and 30.6% were non-families. 26.6% of all households were made up of individuals and 10.5% had someone living alone who was 65 years of age or older. The average household size was 2.67 and the average family size was 3.21.
In the city the population was spread out with 27.8% under the age of 18, 9.4% from 18 to 24, 31.4% from 25 to 44, 19.3% from 45 to 64, and 12.1% who were 65 years of age or older. The median age was 36 years. For every 100 females there were 105.6 males. For every 100 females age 18 and over, there were 97.5 males.
The median income for a household in the city was $29,107, and the median income for a family was $30,972. Males had a median income of $26,591 versus $20,000 for females. The per capita income for the city was $15,168. About 19.8% of families and 23.0% of the population were below the poverty line, including 29.3% of those under age 18 and 13.5% of those age 65 or over.

</doc>
<doc id="3121" url="http://en.wikipedia.org/wiki?curid=3121" title="Andersonville">
Andersonville

Andersonville may refer to:
__NOTOC__
In geography:
Other:

</doc>
<doc id="3122" url="http://en.wikipedia.org/wiki?curid=3122" title="Agra Canal">
Agra Canal

The Agra Canal is an important Indian irrigation work which starts from Okhla in Delhi. The Agra canal originates from Okhla barrage, downstream of Nizamuddin bridge. It opened in 1874.
In the beginning, it was available for navigation, in Delhi, erstwhile Gurgaon, Mathura and Agra Districts, and Bharatpur State. Later, navigation was stopped in 1904 and the canal has since then, been exclusively used for irrigation purposes only. At present the canal does not flow in district Gurgaon, but only in Faridabad, which was earlier a part of Gurgaon.
The Canal receives its water from the Yamuna River at Okhla, about 10 KM to the south of New Delhi. The weir across the Yamuna was the first attempted in Upper India upon a foundation of fine sand; it is about 800-yard long, and rises seven-feet above the summer level of the river.
From Okhla the canal follows the high land between the Khari-Nadi and the Yamuna and finally joins the Banganga river about below Agra. Navigable branches connect the canal with Mathura and Agra.
the canal irrigates about 1.5 lakh hectares in Agra, and Mathura in Uttar Pradesh, Faridabad in Haryana, Bharatpur in Rajasthan and also some parts of Delhi
References.
The Agra Canal also has many places to visit along its coast.

</doc>
<doc id="3123" url="http://en.wikipedia.org/wiki?curid=3123" title="Amakusa">
Amakusa

Amakusa (天草), which means "Heaven's Grass," is a series of islands belonging to Japan, off the west coast of Kyushu, the southernmost of the four main islands of Japan. The largest island of the Amakusa group is Shimoshima, which is 26.5 miles long and 13.5 miles in extreme width (). It is situated at 32°20'N, 130°E, separated from the rest of Kumamoto Prefecture by the Yatsushiro Sea.
It has no high mountains, but its surface is very hilly—four of the peaks rise to a height of over . The population resorts to the terrace system of cultivation to cope with the lack of flat arable land.
Amakusa, along with the neighboring Shimabara Peninsula, became the site of a Christian rebellion in the 17th century. Following the rebellion, those Christians who survived continued to practice their faith in secret, despite persecution.
Amakusa produces a little coal and fine kaolin, which was used in former times largely by the potters of Hirado and Satsuma Province. Many kilns remain on the islands today, and pottery is still an export. Hidenoshin Koyama, who built Thomas Blake Glover's House in Glover Garden, came from this island.
At present, the islands are organized as Amakusa District, Amakusa City, and Kami-amakusa City, all of which are under the administration of Kumamoto Prefecture.
Transport.
The islands are served by Amakusa Airfield, located on the north end of Shimoshima. The islands are connected to the mainland by the Five Bridges of Amakusa and by ferry from Hondo and Matsushima.
There are also ferries between the islands and the neighboring prefectures of Kagoshima Prefecture and Nagasaki Prefecture. The ferry from Oniike on the north Shimoshima to Kuchinotsu, at the southern tip of the Shimabara Peninsula, is run by the Shimabara Railway and operates hourly each day. The ferry boat from Tomioka Port in Reihoku, sailing north to Mogi in Nagasaki Prefecture, is operated by Yasuda Sangyo Kisen Co. Ltd.
Two ferries from Shinwa and Ushibuka, in the south of Shimoshima, connect Amakusa to Nagashima in Kagoshima Prefecture.

</doc>
<doc id="3124" url="http://en.wikipedia.org/wiki?curid=3124" title="Afterglow">
Afterglow

An afterglow is a broad high arch of whitish or rosy light appearing in the sky due to very fine particles of dust suspended in the high regions of the atmosphere. An afterglow may appear above the highest clouds in the hour of deepening twilight, or reflected from the high snowfields in mountain regions long after sunset. The particles produce a scattering effect upon the component parts of white light. 
After the eruption of the volcano Krakatoa in 1883, a remarkable series of red sunsets appeared worldwide. These were due to an enormous amount of exceedingly fine dust blown to a great height by the volcano's explosion, and then globally diffused by the high atmospheric currents. Edvard Munch's painting "The Scream" possibly depicts an afterglow during this period.

</doc>
<doc id="3125" url="http://en.wikipedia.org/wiki?curid=3125" title="Ammonius Grammaticus">
Ammonius Grammaticus

Ammonius Grammaticus was a 4th-century Egyptian priest who, after the destruction of the pagan temple at Alexandria (389), fled to Constantinople, where he became the tutor of the ecclesiastical historian Socrates.
Ammonius was formerly identified as the author of a treatise titled "Peri homoíōn kai diaphórōn léxeōn" (περὶ ὁμοίων καὶ διαφόρων λέξεων, "On the Differences of Synonymous Expressions").
But it seems more probable that the real author was Herennius Philo of Byblus, who was born during the reign of Nero and lived till the reign of Hadrian, and that the treatise in its present form is a revision prepared by a later Byzantine editor, whose name may have been Ammonius.

</doc>
<doc id="3129" url="http://en.wikipedia.org/wiki?curid=3129" title="Algebraic closure">
Algebraic closure

In mathematics, particularly abstract algebra, an algebraic closure of a field "K" is an algebraic extension of "K" that is algebraically closed. It is one of many closures in mathematics.
Using Zorn's lemma, it can be shown that every field has an algebraic closure, and that the algebraic closure of a field "K" is unique up to an isomorphism that fixes every member of "K". Because of this essential uniqueness, we often speak of "the" algebraic closure of "K", rather than "an" algebraic closure of "K".
The algebraic closure of a field "K" can be thought of as the largest algebraic extension of "K".
To see this, note that if "L" is any algebraic extension of "K", then the algebraic closure of "L" is also an algebraic closure of "K", and so "L" is contained within the algebraic closure of "K".
The algebraic closure of "K" is also the smallest algebraically closed field containing "K",
because if "M" is any algebraically closed field containing "K", then the elements of "M" that are algebraic over "K" form an algebraic closure of "K".
The algebraic closure of a field "K" has the same cardinality as "K" if "K" is infinite, and is countably infinite if "K" is finite.
Separable closure.
An algebraic closure "Kalg" of "K" contains a unique separable extension "Ksep" of "K" containing all (algebraic) separable extensions of "K" within "Kalg". This subextension is called a separable closure of "K". Since a separable extension of a separable extension is again separable, there are no finite separable extensions of "Ksep", of degree > 1. Saying this another way, "K" is contained in a "separably-closed" algebraic extension field. It is essentially unique (up to isomorphism).
The separable closure is the full algebraic closure if and only if "K" is a perfect field. For example, if "K" is a field of characteristic "p" and if "X" is transcendental over "K", formula_1 is a non-separable algebraic field extension.
In general, the absolute Galois group of "K" is the Galois group of "Ksep" over "K".

</doc>
<doc id="3130" url="http://en.wikipedia.org/wiki?curid=3130" title="Advanced Power Management">
Advanced Power Management

Advanced power management (APM) is an API developed by Intel and Microsoft and released in 1992 which enables an operating system running an IBM-compatible personal computer to work with the BIOS (part of the computer's firmware) to achieve power management.
Revision 1.2 was the last version of the APM specification, released in 1996. ACPI is intended as the successor to APM. Microsoft dropped support for APM in Windows Vista. The Linux kernel still mostly supports APM, with the last fully functional APM support shipping in 3.3.
Overview.
APM uses a layered approach to manage devices. APM-aware applications (which include device drivers) talk to an OS-specific APM driver. This driver communicates to the APM-aware BIOS, which controls the hardware. There is the ability to opt-out of APM control on a device-by-device basis, which can be used if a driver wants to communicate directly with a hardware device.
Communication occurs both ways; power management events are sent from the BIOS to the APM driver, and the APM driver sends information and requests to the BIOS via function calls. In this way the APM driver is an intermediary between the BIOS and the operating system.
Power management happens in two ways; through the above mentioned function calls from the APM driver to the BIOS requesting power state changes, and automatically based on device activity.
Power management events.
There are 12 power events (such as standby, suspend and resume requests, and low battery notifications), plus OEM-defined events, that can be sent from the APM BIOS to the operating system. The APM driver regularly polls for event change notifications.
Power Management Events:
Power management functions:
APM functions.
There are 21 APM function calls defined that the APM driver can use to query power management statuses, or request power state transitions. Example function calls include letting the BIOS know about current CPU usage (the BIOS may respond to such a call by placing the CPU in a low-power state, or returning it to its full-power state), retrieving the current power state of a device, or requesting a power state change.
Power states.
The APM specification defines system power states and device power states.
System power states.
APM defines five power states for the computer system:
Device power states.
APM also defines power states that APM-aware hardware can implement. There is no requirement that an APM-aware device implement all states.
The four states are:
CPU.
The CPU core (defined in APM as the CPU clock, cache, system bus and system timers) is treated specially in APM, as it is the last device to be powered down, and the first device to be powered back up. The CPU core is always controlled through the APM BIOS (there is no option to control it through a driver). Drivers can use APM function calls to notify the BIOS about CPU usage, but it is up to the BIOS to act on this information; a driver cannot directly tell the CPU to go into a power saving state.

</doc>
<doc id="3132" url="http://en.wikipedia.org/wiki?curid=3132" title="Adolphe Sax">
Adolphe Sax

Antoine-Joseph "Adolphe" Sax (6 November 1814 – c. 7 February 1894) was a Belgian musical instrument designer and musician who played the flute and clarinet, and is well known for having invented the saxophone.
Biography.
Early life.
Adolphe Sax was born in Dinant, Belgium. His father, Charles-Joseph Sax, was an instrument designer himself, who made several changes to the design of the horn. Adolphe began to make his own instruments at an early age, entering two of his flutes and a clarinet into a competition at the age of 15. He subsequently studied those two instruments at the Royal Conservatory of Brussels.
Career.
Having left the school, Sax began to experiment with new instrument designs, while his father continued to make conventional instruments to bring money into the household. Adolphe's first important invention was an improvement of the bass clarinet design, which he patented at the age of 24. Sax relocated permanently to Paris in 1841 and began working on a new set of instruments exhibited there in 1844. These were valved bugles, and although he had not invented the instrument itself, his examples were so much more successful than those of his rivals that they became known as saxhorns. They range in approximately seven different sizes, and paved the path to the creation of the flugelhorn. Saxhorns are widely used today in concert bands and sometimes in orchestras. The saxhorn also laid the groundwork for the modern euphonium.
Sax also developed the "saxotromba" family, valved brass instruments with narrower bore than the saxhorns, in 1845, though they survived only briefly.
Saxhorn instruments spread rapidly throughout the world. The saxhorn valves were accepted as state of the art and are largely unchanged today. The advances made by Adolphe Sax were soon followed by the British brass band movement which exclusively adopted the saxhorn range. The Jedforest Instrumental Band formed in 1854 and The Hawick Saxhorn Band formed in 1855, within the Scottish Borders, a decade after saxhorn models became available.
The period around 1840 saw Sax inventing the "clarinette-bourdon", an early unsuccessful design of contrabass clarinet. He developed around this time the instrument for which he is now best known, the saxophone, patented on June 28, 1846. The saxophone was invented for use in both orchestras and concert bands. Composer Hector Berlioz wrote approvingly of the new instrument in 1842. By 1846 Sax had designed, on paper, a full range of saxophones (from sopranino to subcontrabass). Although they never became standard orchestral instruments, the saxophones made his reputation and secured him a job, teaching at the Paris Conservatoire in 1857.
Sax continued to make instruments later in life and presided over a new saxophone class at the Paris Conservatoire. Rival instrument makers attacked the legitimacy of his patents and mounted a long campaign of litigation against Sax and his company. He was driven into bankruptcy in 1856 and again in 1873.
Sax suffered from lip cancer between 1853 and 1858 but made a full recovery. He died in 1894 in Paris and was interred in section 5 (Avenue de Montebello) at the Cimetière de Montmartre in Paris.

</doc>
<doc id="3134" url="http://en.wikipedia.org/wiki?curid=3134" title="Aspirated consonant">
Aspirated consonant

In phonetics, aspiration is the strong burst of air that accompanies either the release or, in the case of preaspiration, the closure of some obstruents. In English, aspirated consonants are allophones in complementary distribution with their unaspirated counterparts, but in some other languages, notably most Indian and East Asian Languages, the difference is contrastive.
To feel or see the difference between aspirated and unaspirated sounds, one can put a hand or a lit candle in front of one's mouth, and say "pin" and then "bin" . One should either feel a puff of air or see a flicker of the candle flame with "pin" that one does not get with "bin". In most dialects of English, the initial consonant is aspirated in "pin" and unaspirated in "bin".
The diacritic for aspiration in the International Phonetic Alphabet (IPA) is a superscript "h", . In Unicode, it is encoded at . Unaspirated consonants are not normally marked explicitly, but there is a diacritic for non-aspiration in the Extensions to the IPA, the superscript equal sign, .
The term "aspiration" is sometimes also used for the "replacement" of a (usually fricative) consonant with an sound, but that process is more accurately termed debuccalization.
Description.
Voiceless consonants are produced with the vocal cords open and voiced consonants are produced when the vocal folds are fractionally closed. Voiceless aspiration occurs when the vocal cords remain open after a consonant is released. An easy way to measure this is by noting the consonant's voice onset time, as the voicing of a following vowel cannot begin until the vocal cords close.
Usage patterns.
English voiceless stops are aspirated for most native speakers when they are word-initial or begin a stressed syllable, as in "pen", "ten", "Ken". They are unaspirated for almost all speakers when immediately following word-initial s, as in "spun", "stun", "skunk". After an "s" elsewhere in a word they are normally unaspirated as well, except when the cluster is heteromorphemic and the stop belongs to an unbound morpheme; compare vs. . Word-final voiceless stops optionally aspirate.
Aspirated consonants are not always followed by vowels or other voiced sounds. For example, in Eastern Armenian, aspiration is contrastive even word-finally so that տաք ('hot') contrasts with տակ ('under').
In addition to Eastern Armenian, many languages, such as Korean, Thai, Indo-Aryan languages, Dravidian languages, Icelandic, Ancient Greek, and the dialects of Chinese etc. and etc. are different phonemes altogether.
Alemannic German dialects have unaspirated as well as aspirated ; the latter series are usually viewed as consonant clusters. In Danish and most southern varieties of German, the "lenis" consonants transcribed for historical reasons as are distinguished from their fortis counterparts , mainly in their lack of aspiration.
Icelandic and Faroese have preaspirated ; some scholars interpret these as consonant clusters as well. Preaspirated stops also occur in some Sami languages; for example, in Sami, the unvoiced stop phonemes , , , are pronounced preaspirated (, ) when they occur in medial or final position.
French, Dutch, Tamil, Italian, Spanish, Modern Greek, and Latvian are languages that do not have aspirated consonants.
There are degrees of aspiration. Armenian and Cantonese have aspiration that lasts about as long as English aspirated stops, in addition to unaspirated stops. Korean has lightly aspirated stops that fall between the Armenian and Cantonese unaspirated and aspirated stops, as well as strongly aspirated stops whose aspiration lasts longer than that of Armenian or Cantonese. (See voice onset time.) An old IPA symbol for light aspiration was (that is, like a rotated ejective symbol), but this is no longer commonly used. There is no specific symbol for strong aspiration, but can be iconically doubled for, say, Korean vs. . Note however that Korean is nearly universally transcribed as vs. , with the details of voice onset time given numerically.
Aspiration also varies with place of articulation. Spanish , for example, have voice onset times (VOTs) of about 5, 10, and 30 milliseconds, whereas English have VOTs of about 60, 70, and 80 ms. Korean has been measured at 20, 25, and 50 ms for and 90, 95, and 125 for .
Although most aspirated obstruents in the world's language are stops and affricates, aspirated fricatives such as , or have been documented in Korean, in a few Tibeto-Burman languages, in some Oto-Manguean languages, and in the Siouan language Ofo. Some languages, such as Cone Tibetan, have up to four contrastive aspirated fricatives , and 
True aspirated voiced stops, as opposed to murmured voiced stops such as are extremely rare, but have been described in the Kelabit language.
Usage of the diacritic.
The word 'aspiration' and the aspiration diacritic are sometimes used with voiced stops, such as . However, such voiced aspiration, also known as breathy voice or murmur, is less ambiguously transcribed with dedicated diacritics, as or .
Some linguists restrict the double-dot subscript to murmured sonorants, such as vowels and nasals, which are murmured throughout their duration, and use the superscript hook-aitch for the breathy-voiced release of obstruents. When murmur is included under the term aspiration, as is common in Indo-Aryan linguistics, "voiceless aspiration" is called just that to avoid ambiguity.
The diacritic may be doubled to indicate especially long aspiration, as in Navajo: etc.

</doc>
<doc id="3135" url="http://en.wikipedia.org/wiki?curid=3135" title="Arteriovenous malformation">
Arteriovenous malformation

Arteriovenous malformation (AVM) is an abnormal connection between arteries and veins, bypassing the capillary system. This vascular anomaly is widely known because of its occurrence in the central nervous system, but can appear in any location. Although many AVMs are asymptomatic, they can cause intense pain or bleeding or lead to other serious medical problems.
AVMs are usually congenital and belong to the RASopathies.
The genetic transmission patterns of AVM, if any, are unknown. AVM is not generally thought to be an inherited disorder, unless in the context of a specific hereditary syndrome.
Signs and symptoms.
Symptoms of AVM vary according to the location of the malformation. Roughly 88% of people affected with AVM are asymptomatic; often the malformation is discovered as part of an autopsy or during treatment of an unrelated disorder (called in medicine "an incidental finding"); in rare cases its expansion or a micro-bleed from an AVM in the brain can cause epilepsy, deficit or pain.
The most general symptoms of a cerebral AVM include headache and epilepsy, with more specific symptoms occurring that normally depend on the location of the malformation and the individual. Such possible symptoms include:
Cerebral AVMs may present in a number of ways
Genetics.
Can occur due to autosomal dominant diseases, such as Hereditary Hemorrhagic Telangiectasia.
Pathophysiology.
In a normal functioning human body, arteries carry blood away from the heart to the lungs or the rest of the body, where the blood passes through capillaries, and veins return the blood to heart. An AVM interferes with this process by forming a direct connection of the arteries and veins. AVMs can cause intense pain and lead to serious medical problems. Although AVMs are often associated with the brain and spinal cord, they can develop in any part of the body.
Arteries and veins are part of the human cardiovascular system. Normally, the arteries in the vascular system carry oxygen-rich blood, except in the case of the pulmonary artery. Structurally, arteries divide and sub-divide repeatedly, eventually forming a sponge-like capillary bed. Blood moves through the capillaries, giving up oxygen and taking up waste products, including , from the surrounding cells. Capillaries in turn successively join together to form veins that carry blood away. The heart acts to pump blood through arteries and uptake the venous blood.
An AVM lacks the dampening effect of capillaries on the blood flow; it also causes the surrounding area to be deprived of the functions of the capillaries — removal of and delivery of nutrients to the cells. The resulting tangle of blood vessels, often called a "nidus" (Latin for "nest"), has no capillaries. It can be extremely fragile and prone to bleeding because of the abnormally direct connections between high-pressure arteries and low-pressure veins. The resultant sign, audible via stethoscope, is a rhythmic, whooshing sound caused by excessively rapid blood flow through the arteries and veins. It has been given the term "bruit", French for noise. On some occasions a patient with a brain AVM may become aware of the noise, which can compromise hearing and interfere with sleep in addition to causing psychological distress.
Diagnosis.
AVMs are diagnosed primarily by the following methods:
AVMs can occur in various parts of the body:
AVMs may occur in isolation or as a part of another disease (for example, Von Hippel-Lindau disease or hereditary hemorrhagic telangiectasia).
AVMs have been shown to be associated with Aortic Stenosis.
Bleeding from an AVM can be relatively mild or devastating. It can cause severe and less often fatal strokes. If a cerebral AVM is detected before a stroke occurs, usually the arteries feeding blood into the nidus can be closed off to avert the danger. However, interventional therapy may also be relatively risky. 
Treatment.
Treatment for brain AVMs can be symptomatic, and patients should be followed by a neurologist for any seizures, headaches or focal deficits. AVM-specific treatment may also involve endovascular embolization, neurosurgery or radiation therapy.
Embolization, that is, cutting off the blood supply to the AVM with coils or particles or glue introduced by a radiographically guided catheter, can be used in addition to either, but is rarely successful in isolation except for in smaller AVMs.
The Spetzler-Martin grading system developed at the Barrow Neurological Institute is utilized by neurosurgeons to determine operative versus nonoperative management of AVMs.
Epidemiology.
The estimated detection rate of AVM in the US general population is 1.4/100,000 per year. This is approximately one fifth to one seventh the incidence of intracranial aneurysms. An estimated 300,000 Americans have AVMs, of whom 12% (approximately 36,000) will exhibit symptoms of greatly varying severity.
History.
Emmanuel, Luschka and Virchow first described arteriovenous malformations in the mid-1800s. Olivecrona performed the first surgical excision of an intracranial AVM in 1932.
Research directions.
Despite many years of research, the central question of whether to treat AVMs has not been answered. All treatments, whether involving surgery, radiation, or drugs, have risks and side-effects. Therefore, it might be better in some cases to avoid treatment altogether and simply accept a small risk of coming to harm from the AVM itself. This question is currently being addressed in clinical trials.

</doc>
<doc id="3138" url="http://en.wikipedia.org/wiki?curid=3138" title="Atlanta">
Atlanta

Atlanta (, locally ) is the capital of and the most populous city in the U.S. state of Georgia, with an estimated 2012 population of 443,775. Atlanta is the cultural and economic center of the Atlanta metropolitan area, home to 5,457,831 people and the ninth largest metropolitan area in the United States. Atlanta is the county seat of Fulton County, and a small portion of the city extends eastward into DeKalb County.
Atlanta was established in 1837 at the intersection of two railroad lines, and the city rose from the ashes of the Civil War to become a national center of commerce. In the decades following the Civil Rights Movement, during which the city earned a reputation as "too busy to hate" for the progressive views of its citizens and leaders, Atlanta attained international prominence. Atlanta is the primary transportation hub of the Southeastern United States, via highway, railroad, and air, with Hartsfield–Jackson Atlanta International Airport being the world's busiest airport since 1998. 
Atlanta is considered an "alpha-" or "world city", and with a gross domestic product of US$270 billion, Atlanta's economy ranks 15th among world cities and sixth in the nation. Although Atlanta's economy is considered diverse, dominant sectors include logistics, professional and business services, media operations, and information technology. Topographically, Atlanta is marked by rolling hills and dense tree coverage. Revitalization of Atlanta's neighborhoods, initially spurred by the 1996 Olympics, has intensified in the 21st century, altering the city's demographics, politics, and culture.
History.
Prior to the arrival of European settlers in north Georgia, Creek and Cherokee Indians inhabited the area. Standing Peachtree, a Creek village located where Peachtree Creek flows into the Chattahoochee River, was the closest Indian settlement to what is now Atlanta. As part of the systematic removal of Native Americans from northern Georgia from 1802 to 1825, the Creek ceded the area in 1821, and white settlers arrived the following year.
In 1836, the Georgia General Assembly voted to build the Western and Atlantic Railroad in order to provide a link between the port of Savannah and the Midwest. The initial route was to run southward from Chattanooga to a terminus east of the Chattahoochee River, which would then be linked to Savannah. After engineers surveyed various possible locations for the terminus, the "zero milepost" was driven into the ground in what is now Five Points. A year later, the area around the milepost had developed into a settlement, first known as "Terminus", and later as "Thrasherville" after a local merchant who built homes and a general store in the area. By 1842, the town had six buildings and 30 residents, and was renamed "Marthasville" to honor the Governor's daughter. J. Edgar Thomson, Chief Engineer of the Georgia Railroad, suggested the town be renamed "Atlantica-Pacifica,” which was shortened to "Atlanta". The residents approved, and the town was incorporated as Atlanta on December 29, 1847.
By 1860, Atlanta's population had grown to 9,554. During the Civil War, the nexus of multiple railroads in Atlanta made the city a hub for the distribution of military supplies. In 1864, following the capture of Chattanooga, the Union Army moved southward and began its invasion of north Georgia. The region surrounding Atlanta was the location of several major army battles, culminating with the Battle of Atlanta and a four-month-long siege of the city by the Union Army under the command of General William Tecumseh Sherman. On September 1, 1864, Confederate General John Bell Hood made the decision to retreat from Atlanta, ordering all public buildings and possible assets to the Union Army destroyed. On the next day, Mayor James Calhoun surrendered Atlanta to the Union Army, and on September 7, General Sherman ordered the city's civilian population to evacuate. On November 11, 1864, in preparation of the Union Army's march to Savannah, Sherman ordered Atlanta to be burned to the ground, sparing only the city's churches and hospitals.
After the Civil War ended in 1865, Atlanta was gradually rebuilt. Due to the city's superior rail transportation network, the state capital was moved to Atlanta from Milledgeville in 1868. In the 1880 Census, Atlanta surpassed Savannah as Georgia's largest city.
Beginning in the 1880s, Henry W. Grady, the editor of the "Atlanta Constitution" newspaper, promoted Atlanta to potential investors as a city of the "New South" that would be based upon a modern economy and less reliant on agriculture. By 1885, the founding of the Georgia School of Technology (now Georgia Tech) and the city's black colleges had established the city as a center for higher education. In 1895, Atlanta hosted the Cotton States and International Exposition, which attracted nearly 800,000 attendees and successfully promoted the New South's development to the world.
During the first decades of the 20th century, Atlanta experienced a period of unprecedented growth. In three decades' time, Atlanta's population tripled as the city limits expanded to include nearby streetcar suburbs; the city's skyline emerged with the construction of the Equitable, Flatiron, Empire, and Candler buildings; and Sweet Auburn emerged as a center of black commerce. However, the period was also marked by strife and tragedy. Increased racial tensions led to the Atlanta Race Riot of 1906, which left at least 27 people dead and over 70 injured. In 1915, Leo Frank, a Jewish-American factory superintendent, convicted of murder, was hanged by a lynch mob, drawing attention to antisemitism in the United States. On May 21, 1917, the Great Atlanta Fire destroyed 1,938 buildings in what is now the Old Fourth Ward, resulting in one fatality and the displacement of 10,000 people.
On December 15, 1939, Atlanta hosted the film premiere of "Gone with the Wind", the epic film based on the best-selling novel by Atlanta's Margaret Mitchell. The film's legendary producer, David O. Selznick, as well as the film's stars Clark Gable, Vivien Leigh, and Olivia de Havilland attended the gala event at Loew's Grand Theatre, but Oscar winner Hattie McDaniel, an African American, was barred from the event due to the color of her skin.
Atlanta played a vital role in the Allied effort during World War II due the city's war-related manufacturing companies, railroad network, and military bases, leading to rapid growth in the city's population and economy. In the 1950s, the city's newly constructed freeway system allowed middle class Atlantans the ability to relocate to the suburbs. As a result, the city began to make up an ever smaller proportion of the metropolitan area's population. 
During the 1960s, Atlanta was a major organizing center of the Civil Rights Movement, with Dr. Martin Luther King, Jr., Ralph David Abernathy, and students from Atlanta's historically black colleges and universities playing major roles in the movement's leadership. While minimal compared to other cities, Atlanta was not completely free of racial strife. In 1961, the city attempted to thwart blockbusting by erecting road barriers in Cascade Heights, countering the efforts of civic and business leaders to foster Atlanta as the "city too busy to hate". Desegregation of the public sphere came in stages, with public transportation desegregated by 1959, the restaurant at Rich's department store by 1961, movie theaters by 1963, and public schools by 1973.
In 1960, whites comprised 61.7% of the city's population. By 1970, African Americans were a majority of the city's population and exercised new-found political influence by electing Atlanta's first black mayor, Maynard Jackson, in 1973. Under Mayor Jackson's tenure, Atlanta's airport was modernized, solidifying the city's role as a transportation center. The opening of the Georgia World Congress Center in 1976 heralded Atlanta's rise as a convention city. Construction of the city's subway system began in 1975, with rail service commencing in 1979. However, despite these improvements, Atlanta succumbed to the same decay afflicting major American cities during the era, and the city lost over 100,000 residents between 1970 and 1990, over 20% of its population.
In 1990, Atlanta was selected as the site for the 1996 Summer Olympic Games. Following the announcement, the city government undertook several major construction projects to improve Atlanta's parks, sporting venues, and transportation infrastructure. While the games themselves were marred by numerous organizational inefficiencies, as well as the Centennial Olympic Park bombing, they were a watershed event in Atlanta's history, initiating a fundamental transformation of the city in the decade that followed.
During the 2000s, Atlanta underwent a profound transformation demographically, physically, and culturally. Suburbanization, rising prices, a booming economy, and new migrants decreased the city's black percentage from a high of 67% in 1990 to 54% in 2010. From 2000 to 2010, Atlanta gained 22,763 white residents, 5,142 Asian residents, and 3,095 Hispanic residents, while the city's black population decreased by 31,678. Much of the city's demographic change during the decade was driven by young, college-educated professionals: from 2000 to 2009, the three-mile radius surrounding Downtown Atlanta gained 9,722 residents aged 25 to 34 holding at least a four-year degree, an increase of 61%. Between the mid-1990s and 2010, stimulated by funding from the HOPE VI program, Atlanta demolished nearly all of its public housing, a total of 17,000 units and about 10% of all housing units in the city. In 2005, the $2.8 billion BeltLine project was adopted, with the stated goals of converting a disused 22-mile freight railroad loop that surrounds the central city into an art-filled multi-use trail and increasing the city's park space by 40%. Lastly, Atlanta's cultural offerings expanded during the 2000s: the High Museum of Art doubled in size; the Alliance Theatre won a Tony Award; and numerous art galleries were established on the once-industrial Westside.
Geography.
Atlanta encompasses , of which is land and is water. The city is situated among the foothills of the Appalachian Mountains, and at above mean sea level, Atlanta has the highest elevation of major cities east of the Mississippi River. Atlanta straddles the Eastern Continental Divide, such that rainwater that falls on the south and east side of the divide flows into the Atlantic Ocean, while rainwater on the north and west side of the divide flows into the Gulf of Mexico. Atlanta sits atop a ridge south of the Chattahoochee River, which is part of the ACF River Basin. Located at the far northwestern edge of the city, much of the river's natural habitat is preserved, in part by the Chattahoochee River National Recreation Area.
Cityscape.
Most of Atlanta was burned during the Civil War, depleting the city of a large stock of its historic architecture. Yet architecturally, the city had never been particularly "southern"—because Atlanta originated as a railroad town, rather than a patrician southern seaport like Savannah or Charleston, many of the city's landmarks could have easily been erected in the Northeast or Midwest.
During the Cold War era, Atlanta embraced global modernist trends, especially regarding commercial and institutional architecture. Examples of modernist architecture include the Westin Peachtree Plaza (1976), Georgia-Pacific Tower (1982), the State of Georgia Building (1966), and the Atlanta Marriott Marquis (1985). In the latter half of the 1980s, Atlanta became one of the early adopters of postmodern designs that reintroduced classical elements to the cityscape. Many of Atlanta's tallest skyscrapers were built in the late 1980s and early 1990s, with most displaying tapering spires or otherwise ornamented crowns, such as One Atlantic Center (1987), 191 Peachtree Tower (1991), and the Four Seasons Hotel Atlanta (1992). Also completed during the era is Atlanta's tallest skyscraper, the Bank of America Plaza (1992), which, at , is the 61st-tallest building in the world and the 9th-tallest building in the United States. The Bank of America Plaza is currently the tallest building outside of New York City and Chicago, and was the last building built in the United States to be in the top 10 tallest buildings in the world until One World Trade Center was completed externally on May 2013. The city's embrace of modern architecture, however, translated into an ambivalent approach toward historic preservation, leading to the destruction of notable architectural landmarks, including the Equitable Building (1892-1971), Terminal Station (1905-1972), and the Carnegie Library (1902-1977). The Fox Theatre (1929)—Atlanta's cultural icon—would have met the same fate had it not been for a grassroots effort to save it in the mid-1970s.
Atlanta is divided into 242 officially defined neighborhoods. The city contains three major high-rise districts, which form a north-south axis along Peachtree: Downtown, Midtown, and Buckhead. Surrounding these high-density districts are leafy, low-density neighborhoods, most of which are dominated by single-family homes.
Downtown Atlanta contains the most office space in the metro area, much of it occupied by government entities. Downtown is also home to the city's sporting venues and many of its tourist attractions. Midtown Atlanta is the city's second-largest business district, containing the offices of many of the region's law firms. Midtown is also known for its art institutions, cultural attractions, institutions of higher education, and dense form. Buckhead, the city's uptown district, is eight miles (13 km) north of Downtown and the city's third-largest business district. The district is marked by an urbanized core along Peachtree Road, surrounded by suburban single-family neighborhoods situated among dense forests and rolling hills.
Surrounding Atlanta's three high-rise districts are the city's low- and medium-density neighborhoods, where the craftsman bungalow single-family home is dominant. The eastside is marked by historic streetcar suburbs built from the 1890s-1930s as havens for the upper middle class. These neighborhoods, many of which contain their own villages encircled by shaded, architecturally distinct residential streets, include the Victorian Inman Park, Bohemian East Atlanta, and eclectic Old Fourth Ward. On the westside, former warehouses and factories have been converted into housing, retail space, and art galleries, transforming the once-industrial West Midtown into a model neighborhood for smart growth, historic rehabilitation, and infill construction. In southwest Atlanta, neighborhoods closer to downtown originated as streetcar suburbs, including the historic West End, while those farther from downtown retain a postwar suburban layout, including Collier Heights and Cascade Heights, home to much of the city's affluent African American population. Northwest Atlanta, marked by Atlanta's poorest and most crime-ridden neighborhoods, has been the target of community outreach programs and economic development initiatives.
Gentrification of the city's neighborhoods is one of the more controversial and transformative forces shaping contemporary Atlanta. The gentrification of Atlanta has its origins in the 1970s, after many of Atlanta's neighborhoods had undergone the urban decay that affected other major American cities in the mid-20th century. When neighborhood opposition successfully prevented two freeways from being built through city's the east side in 1975, the area became the starting point for Atlanta's gentrification. After Atlanta was awarded the Olympic games in 1990, gentrification expanded into other parts of the city, stimulated by infrastructure improvements undertaken in preparation for the games. Gentrification was also aided by the Atlanta Housing Authority's eradication of the city's public housing. The gentrification of the city's neighborhoods has been the topic of social commentary, including "The Atlanta Way", a documentary detailing the negative effects gentrification has had on the city and its inhabitants.
Climate.
Under the Köppen classification, Atlanta has a humid subtropical climate ("Cfa") with four distinct seasons and generous precipitation year-round, typical for the inland South. Summers are hot and humid, with temperatures somewhat moderated by the city's elevation. Winters are cool but variable, with an average of 48 freezing days per year and temperatures dropping to on rare occasions. Warm air from the Gulf of Mexico can bring spring-like highs while strong Arctic air masses can push lows into the teens (≤ −7 °C). July averages , with high temperatures reaching on an average 44 days per year, though readings are not seen most years. January averages , with temperatures in the suburbs slightly cooler due largely to the urban heat island effect. Lows at or below freezing can be expected 40 nights annually, but extended stretches of high temperatures below are very rare. Extremes range from on February 13, 1899 to on June 30, 2012.
Typical of the southeastern U.S., Atlanta receives abundant rainfall that is relatively evenly distributed throughout the year, though spring and early fall are markedly drier. The average annual rainfall is , while snowfall is typically light at around per year. The heaviest single snowfall occurred on January 23, 1940, with around of snow. However, ice storms usually cause more problems than snowfall does, the most severe occurring on January 7, 1973. Tornadoes are rare in the city itself, but the March 15, 2008 EF2 tornado damaged prominent structures in downtown Atlanta.
Demographics.
The 2010 United States Census reported that Atlanta had a population of 420,003. The population density was 3,154 per square mile (1232/km2). The racial makeup and population of Atlanta was 54.0% Black or African American, 38.4% White, 3.1% Asian and 0.2% Native American. Those from some other race made up 2.2% of the city's population, while those from two or more races made up 2.0%. Hispanics of any race made up 5.2% of the city's population. The median income for a household in the city was $45,171. The per capita income for the city was $ 35,453. 22.6% percent of the population was living below the poverty line. However, compared to the rest of the country, Atlanta's cost of living is 6.00% lower than the U.S. average. Atlanta has one of the highest LGBT populations per capita, ranking third among major American cities, behind San Francisco and slightly behind Seattle, with 12.8% of the city's total population recognizing themselves as gay, lesbian, or bisexual.
In the 2010 Census, Atlanta was recorded as the nation's fourth largest majority black city, and the city has long been known as a center of African American political power, education, and culture, often called a black mecca. However, African American Atlantans have rapidly suburbanized in recent decades, and from 2000 to 2010, the city's black population decreased by 31,678 people, shrinking from 61.4% of the city's population in 2000 to 54.0% in 2010.
Atlanta has recently undergone a drastic demographic increase in its white population. Between 2000 and 2010, the proportion of whites in the city's population grew faster than that of any other U.S. city. In that decade, Atlanta's white population grew from 31% to 38% of the city's population, an absolute increase of 22,753 people, more than triple the increase that occurred between 1990 and 2000.
Out of the total population five years and older, 83.3% spoke only English at home, while 8.8% spoke Spanish, 3.9% another Indo-European language and 2.8% an Asian language. Atlanta's dialect has traditionally been a variation of Southern American English. The Chattahoochee River long formed a border between the Coastal Southern and Southern Appalachian dialects. However, by 2003, "Atlanta" magazine concluded that Atlanta had become significantly "de-Southernized", with a Southern accent considered a handicap in some circumstances. In general, Southern accents are less prevalent among residents of the city and inner suburbs and among younger people, while they are more common in the outer suburbs and among older people; this pattern coexists alongside Southern variations of African American Vernacular English.
Religion in Atlanta, while historically centered around Protestant Christianity, now involves many faiths as a result of the city and metro area's increasingly international population. While Protestant Christianity still maintains a strong presence in the city, in recent decades Catholicism has gained a strong foothold due to migration patterns. Metro Atlanta also has a considerable number of ethnic Christian congregations, including Korean and Indian churches. Large non-Christian faiths are present in the form of Judaism and Hinduism. Overall, there are over 1,000 places of worship within Atlanta.
Economy.
Encompassing $304 billion, the Atlanta metropolitan area is the eighth-largest economy in the country and 17th-largest in the world. Corporate operations comprise a large portion of the Atlanta's economy, with the city serving as the regional, national, or global headquarters for many corporations. Atlanta contains the country's third largest concentration of Fortune 500 companies, and the city is the global headquarters of corporations such as The Coca-Cola Company, The Home Depot, Delta Air Lines, AT&T Mobility, UPS, and Newell-Rubbermaid. Over 75 percent of Fortune 1000 companies conduct business operations in the Atlanta metropolitan area, and the region hosts offices of about 1,250 multinational corporations. Many corporations are drawn to Atlanta on account of the city's educated workforce; as of 2010, nearly 43% of adults in the city of Atlanta have college degrees, compared to 27% in the nation as a whole and 41% in Boston.
Atlanta began as a railroad town and logistics has remained a major component of the city's economy to this day. Atlanta is an important rail junction and contains major classification yards for Norfolk Southern and CSX. Since its construction in the 1950s, Hartsfield-Jackson Atlanta International Airport has served as a key engine of Atlanta's economic growth. Delta Air Lines, the city's largest employer and the metro area's third largest, operates the world's largest airline hub at Hartsfield-Jackson Atlanta International Airport and has helped make Hartsfield-Jackson the world's busiest airport, both in terms of passenger traffic and aircraft operations. Partly due to the airport, Atlanta has become a hub for diplomatic missions; as of 2012, the city contains 25 general consulates, the seventh-highest concentration of diplomatic missions in the United States.
Media is also an important aspect of Atlanta's economy. The city is a major cable television programming center. Ted Turner established the headquarters of both the Cable News Network (CNN) and the Turner Broadcasting System (TBS) in Atlanta. Cox Enterprises, the country's third-largest cable television service and the publisher of over a dozen major American newspapers, is headquartered in the city. The Weather Channel, owned by NBCUniversal, Bain Capital, and The Blackstone Group, is headquartered just outside of Atlanta in Cobb County.
Information technology, an economic sector that includes publishing, software development, entertainment and data processing has garnered a larger percentage of Atlanta's economic output. Indeed, Atlanta has been nicknamed the Silicon peach due to its burgeoning technology sector. As of 2013, Atlanta contains the fourth-largest concentration of information technology jobs in the United States, numbering 85,000. Atlanta also ranks as the sixth-fastest growing city for information technology jobs, with an employment growth of 4.8% in 2012 and a three-year growth near 9%, or 16,000 jobs. Information technology companies are drawn to Atlanta's lower costs and educated workforce.
Largely due to a state-wide tax incentive enacted in 2005, the Georgia Entertainment Industry Investment Act, which awards qualified productions a transferable income tax credit of 20% of all in-state costs for film and television investments of $500,000 or more, Atlanta has become a center for film and television production. Film and television production facilities in Atlanta include Turner Studios, Pinewood Studios#Pinewood Atlanta, Tyler Perry Studios, Williams Street Productions, and the EUE/Screen Gems soundstages. Film and television production injected $1 billion into Georgia's economy in 2010, with Atlanta garnering most of the projects. Atlanta has gained recognition as a center of production of horror and zombie-related productions, with "Atlanta" magazine dubbing the city the "Zombie Capital of the World".
Compared to its peer cities, Atlanta's economy has been disproportionately affected by the 2008 financial crisis and the subsequent recession. The city's economic problems are displayed in its elevated unemployment rate, declining real income levels, and depressed housing market. From 2010-2011, Atlanta saw a 0.9% contraction in employment and a meager 0.4% rise in income. As of 2012, the unemployment rate in Atlanta was over 9%, higher than the national average of 8.2%. These dismal statistics have garnered Atlanta recognition as one of the world's worst economic performers, with the city's economy earning a ranking of 189 among 200 global cities, down from a ranking of 89 during the 1990s, when the city realized 1.6% income growth and 2.6% employment growth. However, even when the 2008-2009 period is excluded, the 2001-2007 period is still one of the worst on record for Atlanta: the city never recovered the jobs it lost during the Early 2000s recession, and per capita income declined nearly 5% from 2000 to 2006, the largest decline among major U.S. cities. Thus, Atlanta's current economic crisis was only worsened, and not caused, by the Recession. Adding to the city's employment and income woes is the spectacular collapse of its housing market. Atlanta home prices fell by 2.1% in January 2012, reaching levels not seen since 1996, a decline that measured among the worst in the country. Compared with a year earlier, the average home price in Atlanta fell 17.3% in February 2012, the largest annual drop in the history of the index for any city. Atlanta home values average $85,000 as of January 2012, second-worst among major metropolitan areas, coming in just behind Detroit. This unprecedented collapse in home prices has led some economists to deem Atlanta the worst housing market in the country. Nevertheless, in August 2013, Atlanta appeared on "Forbes" magazine's list of the Best Places for Business and Careers.
Culture.
Atlanta, while located in the South, has a culture that is no longer strictly Southern. This is due to a large population of migrants from other parts of the U.S., in addition to many recent immigrants to the U.S. who have made the city their home, establishing Atlanta as one of the most multi-cultural cities in the nation. Thus, although traditional Southern culture is part of Atlanta's cultural fabric, it is mostly the backdrop to one of the nation's leading international cities. This unique cultural combination reveals itself in the arts district of Midtown, the quirky neighborhoods on the city's eastside, and the multi-ethnic enclaves found along Buford Highway.
Arts and theater.
Atlanta is one of few United States cities with permanent, professional, resident companies in all major performing arts disciplines: opera (Atlanta Opera), ballet (Atlanta Ballet), music (Atlanta Symphony Orchestra), and theater (the Alliance Theatre). Atlanta also attracts many touring Broadway acts, concerts, shows, and exhibitions catering to a variety of interests. Atlanta's performing arts district is concentrated in Midtown Atlanta at the Woodruff Arts Center, which is home to the Atlanta Symphony Orchestra and the Alliance Theatre. The city also frequently hosts touring Broadway acts, especially at The Fox Theatre, a historic landmark that is among the highest grossing theatres of its size.
As a national center for the arts, Atlanta is home to significant art museums and institutions. The renowned High Museum of Art is arguably the South's leading art museum and among the most-visited art museums in the world. The Museum of Design Atlanta (MODA), a design museum, is the only such museum in the Southeast. Contemporary art museums include the Atlanta Contemporary Art Center and the Museum of Contemporary Art of Georgia. Institutions of higher education also contribute to Atlanta's art scene, with the Savannah College of Art and Design's Atlanta campus providing the city's arts community with a steady stream of curators, and Emory University's Michael C. Carlos Museum containing the largest collection of ancient art in the Southeast.
Music.
Atlanta has played a major or contributing role in the development of various genres of American music at different points in the city's history. Beginning as early as the 1920s, Atlanta emerged as a center for country music, which was brought to the city by migrants from Appalachia. During the countercultural 1960s, Atlanta hosted the Atlanta International Pop Festival, with the 1969 festival taking place more than a month before Woodstock and featuring many of the same bands. The city was also a center for Southern rock during its 1970s heyday: the Allman Brothers Band's hit instrumental "Hot 'Lanta" is an ode to the city, while Lynyrd Skynyrd's famous live rendition of "Free Bird" was recorded at the Fox Theatre in 1976, with lead singer Ronnie Van Zant directing the band to "play it pretty for Atlanta". During the 1980s, Atlanta had an active Punk rock scene that was centered around two of the city's music venues, 688 Club and the Metroplex, and Atlanta famously played host to the Sex Pistols first U.S. show, which was performed at the Great Southeastern Music Hall. The 1990s saw the birth of Atlanta hip hop, a sub-genre that gained relevance following the success of home-grown duo OutKast; however, it was not until the 2000s that Atlanta moved "from the margins to becoming hip-hop's center of gravity, part of a larger shift in hip-hop innovation to the South". Also in the 2000s, Atlanta was recognized by the Brooklyn-based "Vice" magazine for its impressive yet under-appreciated Indie rock scene, which revolves around the various live music venues found on the city's alternative eastside.
The city is also home to many Christian Artists such as Passion Worship Band, Chris Tomlin, Kristian Stanfill, Christy Nockels, Watermark, Jamie Grace, Lecrae, Andy Mineo, David Crowder, Kari Jobe
Christian Record Label Six Steps Records has its headquarters located in Atlanta at Passion City Church 515 Garson Drive.
Tourism.
As of 2010, Atlanta is the seventh-most visited city in the United States, with over 35 million visitors per year. Although the most popular attraction among visitors to Atlanta is the Georgia Aquarium, the world's largest indoor aquarium, Atlanta's tourism industry mostly driven by the city's history museums and outdoor attractions. Atlanta contains a notable amount of historical museums and sites, including the Martin Luther King, Jr. National Historic Site, which includes the preserved boyhood home of Dr. Martin Luther King, Jr., as well as his final resting place; the Atlanta Cyclorama & Civil War Museum, which houses a massive painting and diorama in-the-round, with a rotating central audience platform, depicting the Battle of Atlanta in the Civil War; the World of Coca-Cola, featuring the history of the world famous soft drink brand and its well-known advertising; the Carter Center and Presidential Library, housing U.S. President Jimmy Carter's papers and other material relating to the Carter administration and the Carter family's life; and the Margaret Mitchell House and Museum, site of the writing of the best-selling novel Gone With the Wind.
Atlanta also contains various outdoor attractions. The Atlanta Botanical Garden, adjacent to Piedmont Park, is home to the Kendeda Canopy Walk, a skywalk that allows visitors to tour one of the city's last remaining urban forests from . The Canopy Walk is considered the only canopy-level pathway of its kind in the United States. Zoo Atlanta, located in Grant Park, accommodates over 1,300 animals representing more than 220 species. Home to the nation's largest collections of gorillas and orangutans, the Zoo is also one of only four zoos in the U.S. to house giant pandas. Festivals showcasing arts and crafts, film, and music, including the Atlanta Dogwood Festival, the Atlanta Film Festival, and Music Midtown, respectively, are also popular with tourists.
Tourists are also drawn to the city's culinary scene, which comprises a mix of urban establishments garnering national attention, ethnic restaurants serving cuisine from every corner of the world, and traditional eateries specializing in Southern dining. Since the turn of the 21st century, Atlanta has emerged as a sophisticated restaurant town. Many restaurants opened in the city's gentrifying neighborhoods have received praise at the national level, including Bocado, Bacchanalia, and Miller Union in West Midtown, Empire State South in Midtown, and Two Urban Licks and Rathbun's on the east side. In 2011, the "New York Times" characterized Empire State South and Miller Union as reflecting "a new kind of sophisticated Southern sensibility centered on the farm but experienced in the city." Visitors seeking to sample international Atlanta are directed to Buford Highway, the city's international corridor. There, the million-plus immigrants that make Atlanta home have established various authentic ethnic restaurants representing virtually every nationality on the globe. For traditional Southern fare, one of the city's most famous establishments is The Varsity, a long-lived fast food chain and the world's largest drive-in restaurant. Mary Mac's Tea Room and Paschal's are more formal destinations for Southern food.
Sports.
Atlanta is home to professional franchises for three major team sports: the Atlanta Braves of Major League Baseball, the Atlanta Hawks of the National Basketball Association, and the Atlanta Falcons of the National Football League. The Braves, who moved to Atlanta in 1966, were established as the Boston Red Stockings in 1871 and are the oldest continually operating professional sports franchise in the United States. The Braves won the World Series in 1995, and had an unprecedented run of 14 straight divisional championships from 1991 to 2005. 
The Atlanta Falcons have played in Atlanta since 1966. The Falcons have won the division title five times (1980, 1998, 2004, 2010, 2012) and the conference championship once, when they finished as the runner-up to the Denver Broncos in Super Bowl XXXIII in 1999. The Atlanta Hawks began in 1946 as the Tri-Cities Blackhawks, playing in Moline, Illinois. The team moved to Atlanta in 1968, and they currently play their games in Philips Arena. The Atlanta Dream is the city's Women's National Basketball Association franchise.
Atlanta has also had its own professional ice hockey and soccer franchises. The National Hockey League (NHL) has had two Atlanta franchises: the Atlanta Flames began play in 1972 before moving to Calgary in 1980, while the Atlanta Thrashers began play in 1999 before moving to Winnipeg in 2011. The Atlanta Chiefs was the city's professional soccer team from 1967 to 1972, and the team won a national championship in 1968.
Atlanta has been the host city for various international, professional and collegiate sporting events. Most famously, Atlanta hosted the Centennial 1996 Summer Olympics. Atlanta has also hosted Super Bowl XXVIII in 1994 and Super Bowl XXXIV in 2000. In professional golf, The Tour Championship, the final PGA Tour event of the season, is played annually at East Lake Golf Club. In 2001 and 2011, Atlanta hosted the PGA Championship, one of the four major championships in men's professional golf, at the Atlanta Athletic Club. In professional ice hockey, the city hosted the 56th NHL All-Star Game in 2008, three years before the Thrashers moved. In 2011, Atlanta hosted professional wrestling's annual WrestleMania. The city has hosted the NCAA Final Four Men's Basketball Championship four times, most recently in 2013. In college football, Atlanta hosts the Chick-fil-A Kickoff Game, the SEC Championship Game, and the Chick-fil-A Peach Bowl.
Parks and recreation.
Atlanta's 343 parks, nature preserves, and gardens cover , which amounts to only 5.6% of the city's total acreage, compared to the national average of just over 10%. However, 64% of Atlantans live within a 10-minute walk of a park, a percentage equal to the national average. Furthermore, in its 2013 ParkScore ranking, the The Trust for Public Land, a national land conservation organization, reported that among the park systems of the 50 most populous U.S. cities, Atlanta's park system received a ranking of 31. Piedmont Park, located in Midtown is Atlanta's most iconic green space. The park, which has undergone a major renovation and expansion in recent years, attracts visitors from across the region and hosts cultural events throughout the year. Other notable city parks include Centennial Olympic Park, a legacy of the 1996 Summer Olympics that forms the centerpiece of the city's tourist district; Woodruff Park, which anchors the campus of Georgia State University; Grant Park, home to both Zoo Atlanta and the Atlanta Cyclorama & Civil War Museum; and Chastain Park, which houses an amphitheater used for live music concerts. The Chattahoochee River National Recreation Area, located in the northwestern corner of the city, preserves a stretch of the river for public recreation opportunities. The Atlanta Botanical Garden, adjacent to Piedmont Park, contains formal gardens, including a Japanese garden and a rose garden, woodland areas, and a conservatory that includes indoor exhibits of plants from tropical rainforests and deserts. The BeltLine, a former rail corridor that forms a loop around Atlanta's core, will eventually be transformed into a series of parks, connected by a multi-use trail, increasing Atlanta's park space by 40%.
Atlanta offers resources and opportunities for amateur and participatory sports and recreation. Jogging is a particularly popular local sport. The Peachtree Road Race, the world's largest race, is held annually on Independence Day. The Georgia Marathon, which begins and ends at Centennial Olympic Park, routes through the city's historic east side neighborhoods. Golf and tennis are also popular in Atlanta, and the city contains six public golf courses and 182 tennis courts. Facilities located along the Chattahoochee River cater to watersports enthusiasts, providing the opportunity for kayaking, canoeing, fishing, boating, or tubing. The city's only skate park, a facility that offers bowls, curbs, and smooth-rolling concrete mounds, is located at Historic Fourth Ward Park.
Government and politics.
Atlanta is governed by a mayor and the Atlanta City Council. The city council consists of 15 representatives—one from each of the city's 12 districts and three at-large positions. The mayor may veto a bill passed by the council, but the council can override the veto with a two-thirds majority. The mayor of Atlanta is Kasim Reed, a Democrat elected on a nonpartisan ballot whose first term in office will expire at the end of 2013. Every mayor elected since 1973 has been black. In 2001, Shirley Franklin became the first woman to be elected Mayor of Atlanta, and the first African-American woman to serve as mayor of a major southern city. Atlanta city politics suffered from a notorious reputation for corruption during the 1990s administration of Bill Campbell, who was convicted by a federal jury in 2006 on three counts of tax evasion in connection with gambling income he received while Mayor during trips he took with city contractors.
As the state capital, Atlanta is the site of most of Georgia's state government. The Georgia State Capitol building, located downtown, houses the offices of the governor, lieutenant governor and secretary of state, as well as the General Assembly. The Governor's Mansion is located in a residential section of Buckhead. Atlanta serves as the regional hub for many arms of the federal bureaucracy, including the Federal Reserve Bank of Atlanta and the Centers for Disease Control and Prevention. Atlanta also plays an important role in federal judiciary system, containing the United States Court of Appeals for the Eleventh Circuit and of the United States District Court for the Northern District of Georgia.
Historically, Atlanta has been a stronghold for the Democratic Party. Although municipal elections are officially nonpartisan, nearly all of the city's elected officials are registered Democrats. The city is split between 14 state house districts and four state senate districts, all held by Democrats. At the federal level, Atlanta is split between two congressional districts. The northern three-fourths of the city is located in the 5th district, represented by Democrat John Lewis. The southern fourth is in the 13th district, represented by Democrat David Scott.
The city is served by the Atlanta Police Department, which numbers 2,000 officers and oversaw a 40% decrease in the city's crime rate between 2001 and 2009. Specifically, homicide decreased by 57%, rape by 72%, and violent crime overall by 55%. Crime is down across the country, but Atlanta's improvement has occurred at more than twice the national rate. Nevertheless, Forbes ranked Atlanta as the sixth most dangerous city in the United States in 2012.
Education.
Due to the more than 30 colleges and universities located in the city, Atlanta is considered a center for higher education.
Atlanta Public Schools enrolls 55,000 students in 106 schools, some of which are operated as charter schools. The district has been plagued by a widely publicized cheating scandal exposed in 2009. Atlanta is also served by various private schools, as well as parochial Roman Catholic schools operated by the Archdiocese of Atlanta.
Media.
The primary network-affiliated television stations in Atlanta are WXIA-TV (NBC), WGCL-TV (CBS), WSB-TV (ABC), and WAGA-TV (Fox). The Atlanta metropolitan area is served by two public television stations and one public radio station. WGTV is the flagship station of the statewide Georgia Public Television network and is a PBS member station, while WPBA is owned by Atlanta Public Schools. Georgia Public Radio is listener-funded and comprises one NPR member station, WABE, a classical music station operated by Atlanta Public Schools.
Atlanta is served by the "Atlanta Journal-Constitution", its only major daily newspaper with wide distribution. The "Atlanta Journal-Constitution" is the result of a 1950 merger between "The Atlanta Journal" and "The Atlanta Constitution", with staff consolidation occurring in 1982 and separate publication of the morning "Constitution" and afternoon "Journal" ceasing in 2001. Alternative weekly newspapers include "Creative Loafing", which has a weekly print circulation of 80,000. "Atlanta" magazine is an award-winning, monthly general-interest magazine based in and covering Atlanta.
Transportation.
Atlanta's transportation infrastructure comprises a complex network that includes a heavy rail subway system, multiple interstate highways, the world's busiest airport, and over of bike paths.
The Metropolitan Atlanta Rapid Transit Authority (MARTA) provides public transportation in the form of buses and heavy rail. Notwithstanding heavy automotive usage in Atlanta, the city's subway system is the eighth busiest in the country. MARTA rail lines connect many key destinations, such as the airport, Downtown, Midtown, Buckhead, and Perimeter Center. However, significant destinations, such as Emory University, Cumberland and Turner Field, remain unserved. As a result, a 2012 Brookings Institution study placed Atlanta 87th of 100 metro areas for transit accessibility. Emory University operates its Cliff shuttle buses with 200,000 boardings per month, while private minibuses ply Buford Highway. Amtrak, the national rail passenger system, provides service to Atlanta via the "Crescent train" (New York–New Orleans), which stops at Peachtree Station.
With a comprehensive network of freeways that radiate out from the city, automobiles are the dominant mode of transportation in the region. Three major interstate highways converge in Atlanta: I-20 (east-west), I-75 (northwest-southeast), and I-85 (northeast-southwest). The latter two combine in the middle of the city to form the Downtown Connector (I-75/85), which carries more than 340,000 vehicles per day and is one of the ten most congested segments of interstate highway in the United States. Atlanta is mostly encircled by Interstate 285, a beltway locally known as "the Perimeter" that has come to mark the boundary between "Inside the Perimeter" (ITP), the city and close-in suburbs, and "Outside the Perimeter" (OTP), the outer suburbs and exurbs. The heavy reliance on automobiles for transportation in Atlanta has resulted in traffic, commute, and air pollution rates that rank among the worst in the country.
Hartsfield-Jackson Atlanta International Airport, the world's busiest airport as measured by passenger traffic and aircraft traffic, offers air service to over 150 U.S. destinations and more than 80 international destinations in 52 countries, with over 2,700 arrivals and departures daily. Delta Air Lines maintains its largest hubs at the airport. Situated () south of downtown, the airport covers most of the land inside a wedge formed by Interstate 75, Interstate 85, and Interstate 285.
Cycling is a growing mode of transportation in Atlanta, more than doubling since 2009, when it comprised 1.1% of all commutes (up from 0.3% in 2000). Although Atlanta's lack of bike lanes and hilly topography may deter many residents from cycling, the city's transportation plan calls for the construction of of bike lanes by 2020, with the BeltLine helping to achieve this goal.
Tree canopy.
Atlanta has a reputation as a "city in a forest" due to an abundance of trees that is rare among major cities. The city's main street is named after a tree, and beyond the Downtown, Midtown, and Buckhead business districts, the skyline gives way to a dense canopy of woods that spreads into the suburbs. The city is home to the Atlanta Dogwood Festival, an annual arts and crafts festival held one weekend during early April, when the native dogwoods are in bloom. However, the nickname is also factually accurate, as the city's tree coverage percentage is at 36%, the highest out of all major American cities, and above the national average of 27%. Atlanta's tree coverage does not go unnoticed—it was the main reason cited by "National Geographic" in naming Atlanta a "Place of a Lifetime".
The city's lush tree canopy, which filters out pollutants and cools sidewalks and buildings, has increasingly been under assault from man and nature due to heavy rains, drought, aged forests, new pests, and urban construction. A 2001 study found that Atlanta's heavy tree cover declined from 48% in 1974 to 38% in 1996. However, the problem is being addressed by community organizations and city government: Trees Atlanta, a non-profit organization founded in 1985, has planted and distributed over 75,000 shade trees in the city, while Atlanta's government has awarded $130,000 in grants to neighborhood groups to plant trees.
Sister cities.
Atlanta has 22 sister cities, as designated by Sister Cities International, Inc. (SCI):

</doc>
<doc id="3143" url="http://en.wikipedia.org/wiki?curid=3143" title="Axiology">
Axiology

Axiology (from Greek , "axiā", "value, worth"; and , "-logos") is the philosophical study of value. It is either the collective term for ethics and aesthetics—philosophical fields that depend crucially on notions of value—or the foundation for these fields, and thus similar to value theory and meta-ethics. The term was first used by Paul Lapie, in 1902, and Eduard von Hartmann, in 1908.
Axiology studies mainly two kinds of values: ethics and aesthetics. Ethics investigates the concepts of "right" and "good" in individual and social conduct. Aesthetics studies the concepts of "beauty" and "harmony." Formal axiology, the attempt to lay out principles regarding value with mathematical rigor, is exemplified by Robert S. Hartman's Science of Value. Studies of both kinds are found in Cultura: International Journal of Philosophy of Culture and Axiology.
History.
Between the 5th and 6th century B.C., it was important in Greece to be knowledgeable if you were to be successful. Philosophers began to recognize that differences existed between the laws and morality of society. Socrates held the belief that knowledge had a vital connection to virtue, making morality and democracy closely intertwined. Socrates' student, Plato furthered the belief by establishing virtues which should be followed by all. With the fall of the government, values became individual, causing skeptic schools of thought to flourish, ultimately shaping a pagan philosophy that is thought to have influenced and shaped Christianity. During these medieval times, Aquinas argued for a separation between natural and religious virtues. This concept led philosophers to distinguish between judgments based on fact and judgments based on values, creating division between science and philosophy.
Axiological issues in communication studies.
Communication theorists seek to contribute to mutual intelligence about the anatomy and operation of human communication. The axiological issues that are significant for the evolution of communication theory are whether research can be truly free of value and whether the end for the administered research should be designed to expand knowledge or to change society. For communication theorists, a primary interest is with the philosophical establishment of the research approach. A continuing value debate occurs between scholars who comply with a conventional scientific approach and those who take an interpretivist approach to communication development.
Those who take a conventional scientific approach believe that research must be free of values in order to be valid. Therefore, it is necessary for the scientist to approach their research in a neutral and objective manner. In contrast, the interpretivists argue that it is impossible for research to be completely free of personal values, as research is always biased towards the values of the researcher. According to interpretivists, these biases are sometimes so entrenched in the researcher's culture that they will most likely go unnoticed during research. Since no one can truly be unbiased, some groups are more knowledgeable about certain things than other groups due to their positions in society, and they can be considered more qualified to perform research on certain topics as a result.

</doc>
<doc id="3144" url="http://en.wikipedia.org/wiki?curid=3144" title="A Doll's House">
A Doll's House

"A Doll's House" (; also translated as "A Doll House") is a three-act play in prose by Henrik Ibsen. It premiered at the Royal Theatre in Copenhagen, Denmark, on 21 December 1879, having been published earlier that month.
The play is significant for its critical attitude toward 19th century marriage norms. It aroused great controversy at the time, as it concludes with the protagonist, Nora, leaving her husband and children because she wants to discover herself. Ibsen was inspired by the belief that "a woman cannot be herself in modern society," since it is "an exclusively male society, with laws made by men and with prosecutors and judges who assess feminine conduct from a masculine standpoint." Its ideas can also be seen as having a wider application: Michael Meyer argued that the play's theme is not women's rights, but rather "the need of every individual to find out the kind of person he or she really is and to strive to become that person." In a speech given to the Norwegian Association for Women's Rights in 1898, Ibsen insisted that he "must disclaim the honor of having consciously worked for the women's rights movement," since he wrote "without any conscious thought of making propaganda," his task having been "the "description of humanity"."
In 2006, the centennial of Ibsen's death, "A Doll's House" held the distinction of being the world's most performed play for that year. UNESCO has inscribed Ibsen's autographed manuscripts of "A Doll's House" on the Memory of the World Register in 2001, in recognition of their historical value.
Title.
The title of the play is most commonly translated as "A Doll's House", though some scholars use "A Doll House". John Simon argues that the only significance in the alternative translation is the difference in the way the toy is named in Britain and the United States. Egil Törnqvist argues that the alternative "simply sounds more idiomatic to Americans." See Simon (1991, 55), Törnqvist (1995, 54), and Worthen (2004, 666–692).
Synopsis.
Act one.
The play opens at Christmas time as Nora Helmer enters her home carrying a number of packages. Nora's husband Torvald is working in his study when she arrives. He playfully rebukes her for spending so much money on Christmas gifts, calling her his "little squirrel". He teases her about how she spent weeks making gifts and ornaments by hand last year because money was scarce. This year Torvald is due a promotion at the bank where he works, so Nora feels that they can let themselves go a little. The maid announces two visitors: Mrs. Kristine Linde, an old friend of Nora's, who has come seeking employment, and Dr. Rank, a close friend of the family, who is let into the study. Kristine has had a difficult few years, ever since her husband died, leaving her with no money or children. Nora explains that things have not been easy for them either: Torvald became sick and they had to travel to Italy so he could recover. Kristine further explains that when her mother was ill, she had to take care of her brothers, but now that they are grown she feels her life is "unspeakably empty". Nora promises to talk to Torvald about finding her a job. Kristine gently tells Nora that she is like a child. Nora is offended, so she reveals that she borrowed money so they could travel to Italy in order to improve Torvald's health. She told Torvald that her father gave her the money, but in fact she managed to illegally borrow it without his knowledge. Over the years she has been secretly working and saving up to pay it off.
Krogstad, a lower-level employee at Torvald's bank arrives and goes into the study. Nora is clearly uneasy when she sees him. Dr. Rank leaves the study and mentions that he feels wretched, though, like everyone, he wants to go on living. In contrast to his physical illness, he says that the man in the study, Krogstad, is "morally diseased".
After the meeting with Krogstad, Torvald comes out of the study. Nora asks him if he can give Kristine a position at the bank and Torvald is very positive, saying that this is a fortunate moment, as a position has just become available. Torvald, Linde, and Dr. Rank leave the house, leaving Nora alone. The nanny returns with the children and Nora plays with them for a while until Krogstad creeps into the living room and surprises her. Krogstad tells Nora that Torvald intends to fire him at the bank and asks her to intercede with Torvald to allow him to keep his job. She refuses and Krogstad threatens to blackmail her about the loan she took out for the trip to Italy; he knows that she obtained this loan by forging her father's signature. Krogstad leaves and when Torvald returns she tries to convince him not to fire Krogstad. Torvald refuses to hear her pleas, explaining that Krogstad is a liar and a hypocrite and that he committed a terrible crime: he forged someone's name. Torvald feels physically ill in his presence of a man "poisoning his own children with lies and dissimulation".
Act two.
Kristine arrives to help Nora repair a dress for a costume function that she and Torvald plan to attend the next day. Torvald returns from the bank, and Nora pleads with him to reinstate Krogstad, claiming she is worried Krogstad will publish libelous articles about Torvald and ruin his career. Torvald dismisses her fears and explains that, although Krogstad is a good worker and seems to have turned his life around, he must be fired because he is not deferential enough to Torvald in front of other bank personnel. Torvald then retires to study his work.
Dr. Rank, a family friend, arrives. Nora asks him for a favour, but Rank responds by revealing that he has entered the terminal stage of tuberculosis of the spine and that he has always been secretly in love with her. Nora tries to deny the first revelation and make light of it but is more disturbed by his declaration of love. She tries clumsily to tell him that she is not in love with him but that she loves him dearly as a friend.
Desperate after being fired by Torvald, Krogstad arrives at the house. Nora convinces Dr. Rank to go into Torvald's study so he will not see Krogstad. When Krogstad confronts Nora, he declares that he no longer cares about the remaining balance of Nora's loan, but that he will instead preserve the associated bond in order to blackmail Torvald into not only keeping him employed but also promoting him as well. Nora explains that she has done her best to persuade her husband, but he refuses to change his mind. Krogstad informs Nora that he has written a letter detailing her crime (forging her father's signature of surety on the bond) and put it in Torvald's mailbox, which is locked.
Nora tells Kristine of her difficult situation. Kristine says that she and Krogstad are still in love and promises to try to convince him to relent.
Torvald enters and tries to retrieve his mail, but Nora distracts him by begging him to help her with the dance she has been rehearsing for the costume party, feigning anxiety about performing. She dances so badly and acts so childishly that Torvald agrees to spend the whole evening coaching her. When the others go to dinner, Nora stays behind for a few minutes and contemplates killing herself to save her husband from the shame of the revelation of her crime and (more importantly) to pre-empt any gallant gesture on his part to save her reputation.
Act three.
Kristine tells Krogstad that she only married her husband because she had no other means to support her sick mother and young siblings and that she has returned to offer him her love again. She believes that he would not have stooped to unethical behavior if he had not been devastated by her abandonment and been in dire financial straits. Krogstad is moved and offers to take back his letter to Torvald. However, Kristine decides that Torvald should know the truth for the sake of his and Nora's marriage.
After literally dragging Nora home from the party, Torvald goes to check his mail, but is interrupted by Dr. Rank, who has followed them. Dr. Rank chats for a while, conveying obliquely to Nora that this is a final goodbye, as he has determined that his death is near. Dr. Rank leaves, and Torvald retrieves his letters. As he reads them, Nora steels herself to take her life. Torvald confronts her with Krogstad's letter. Enraged, he declares that he is now completely in Krogstad's power – he must yield to Krogstad's demands and keep quiet about the whole affair. He berates Nora, calling her a dishonest and immoral woman and telling her that she is unfit to raise their children. He says that from now on their marriage will be only a matter of appearances.
A maid enters, delivering a letter to Nora. The letter is from Krogstad, yet Torvald demands to read the letter, taking it from Nora. Torvald exults that he is saved, as Krogstad has returned the incriminating bond, which Torvald immediately burns along with Krogstad's letters. He takes back his harsh words to his wife and tells her that he forgives her. Nora realizes that her husband is not the strong and gallant man she thought he was and that he truly loves himself more than he does her.
Torvald explains that, when a man has forgiven his wife, it makes him love her all the more since it reminds him that she is totally dependent on him, like a child. He dismisses the fact that Nora had to make the agonizing choice between her conscience and his health, and ignores her years of secret efforts to free them from the ensuing obligations and danger of loss of reputation. He preserves his peace of mind by thinking of the incident as a mere mistake that she made owing to her dumbness, one of her most endearing feminine traits.
Nora tells Torvald that she is leaving him to live alone so that she can find out who she is and what she believes and decide what to do with her life. She says that she has been treated like a doll to play with for her whole life, first by her father and then by him. Concerned for the family reputation, Torvald insists that she fulfill her duty as a wife and mother, but Nora says that her first duties are to herself and that she cannot be a good mother or wife without learning to be more than a plaything. She reveals that she had expected that he would want to sacrifice his reputation for hers and that she had planned to kill herself to prevent him from doing so. She now realizes that Torvald is not at all the kind of person she had believed him to be and that their marriage has been based on mutual fantasies and misunderstanding.
Torvald is unable to comprehend Nora's point of view, since it contradicts all that he has been taught about the female mind throughout his life. Furthermore, he is so narcissistic that it is impossible for him to understand how he appears to her, as selfish, hypocritical and more concerned with public reputation than with actual morality. Nora leaves her keys and wedding ring and, as Torvald breaks down and begins to cry, baffled by what has happened, Nora leaves the house, slamming the door behind herself. She never comes back again.
Alternative ending.
Ibsen's German agent felt that the original ending would not play well in German theatres; therefore, for it to be considered acceptable, Ibsen was forced to write an alternative ending for the German premiere. In this ending, Nora is led to her children after having argued with Torvald. Seeing them, she collapses, and the curtain is brought down. Ibsen later called the ending a disgrace to the original play and referred to it as a 'barbaric outrage'.
Composition and publication.
Real-life inspiration.
"A Doll's House" was based on the life of Laura Kieler (maiden name Laura Smith Petersen), a good friend of Ibsen. Much that happened between Nora and Torvald happened to Laura and her husband, Victor. Much like the play, Laura signs the illegal loan in order to save her husband. She wants the money to find a cure for her husband's tuberculosis. She wrote to Ibsen, asking for his recommendation of her work to his publisher, thinking that the sales of her book would repay her debt. At his refusal, she forged a check for the money. At this point she was found out. In real life, when Victor discovered about Laura's secret loan, he divorced her and had her committed to an asylum. Two years later, she returned to her husband and children at his urging, and she went on to become a well-known Danish author, living to the age of 83.
Ibsen wrote "A Doll's House" at the point when Laura Kieler had been committed to the asylum, and the fate of this friend of the family shook him deeply, perhaps also because Laura had asked him to intervene at a crucial point in the scandal, which he did not feel able or willing to do. Instead, he turned this life situation into an aesthetically shaped, successful drama. In the play, Nora leaves Torvald with head held high, though facing an uncertain future given the limitations single women faced in the society of the time.
Kieler eventually rebounded from the shame of the scandal and had her own successful writing career while remaining discontented with sole recognition as "Ibsen's Nora" years afterwards.
Composition.
Ibsen started thinking about the play around May 1878, although he did not begin its first draft until a year later, having reflected on the themes and characters in the intervening period (he visualised its protagonist, Nora, for instance, as having approached him one day wearing "a blue woolen dress"). He outlined his conception of the play as a "modern tragedy" in a note written in Rome on 19 October 1878. "A woman cannot be herself in modern society," he argues, since it is "an exclusively male society, with laws made by men and with prosecutors and judges who assess feminine conduct from a masculine standpoint."
Publication.
Ibsen sent a fair copy of the completed play to his publisher on 15 September 1879. It was first published in Copenhagen on 4 December 1879, in an edition of 8,000 copies that sold out within a month; a second edition of 3,000 copies followed on 4 January 1880 and a third edition of 2,500 was issued on 8 March.
Production history.
"A Doll's House" received its world premiere on 21 December 1879 at the Royal Theatre in Copenhagen, Denmark, with Betty Hennings as Nora and Emil Poulsen as Torvald. Writing for the Norwegian newspaper "Folkets Avis", the critic Erik Bøgh admired Ibsen's originality and technical mastery: "Not a single declamatory phrase, no high dramatics, no drop of blood, not even a tear." Every performance of its run was sold out. Another production opened at the Royal Theatre in Stockholm, Sweden, on 8 January 1880, while productions in Christiania (with Johanne Juell as Nora and Arnoldus Reimers as Torvald) and Bergen followed shortly after.
In Germany, the actress Hedwig Niemann-Raabe refused to perform the play as written, declaring that ""I" would never leave "my" children!" Since the playwright's wishes were not protected by copyright, Ibsen decided to avoid the danger of being re-written by a lesser dramatist by committing what he called a "barbaric outrage" on his play himself and giving it an alternative ending in which Nora did not leave. A production of this version opened in Flensburg in February 1880. This version was also played in Hamburg, Dresden, Hanover, and Berlin, although, in the wake of protests and a lack of success, Niemann-Raabe eventually restored the original ending. Another production of the original version, some rehearsals of which Ibsen attended, opened on 3 March 1880 at the Residenz Theatre in Munich.
In Great Britain, the only way in which the play was initially allowed to be given in London was in an adaptation by Henry Arthur Jones and Henry Herman called "Breaking a Butterfly". This adaptation was produced at the Princess Theatre, 3 March 1884. The first British production of the play in its regular form opened on 7 June 1889 at the Novelty Theatre, starring Janet Achurch as Nora and Charles Carrington as Torvald. Achurch played Nora again for a 7-day run in 1897. Soon after its London premiere, Achurch brought the play to Australia in 1889.
The play was first seen in America when, during 1883, in Louisville, Kentucky, Helena Modjeska acted Nora. The play made its Broadway premiere at the Palmer's Theatre on 21 December 1889, starring Beatrice Cameron as Nora Helmer.
It was first performed in France in 1894.
Other productions in the United States include one in 1902 starring Minnie Maddern Fiske and a 1971 production starring Claire Bloom. A 1973 production starred Liv Ullmann and a 1997 production starred Janet McTeer at the Belasco Theater, which received three Tony Awards and the Drama Desk Award for Outstanding Revival of a Play.
A new translation by Zinnie Harris at the Donmar Warehouse, starring Gillian Anderson, Toby Stephens, Anton Lesser, Tara FitzGerald and Christopher Eccleston opened in May 2009.
Criticism.
"A Doll's House" criticises the traditional roles of men and women in 19th-century marriage. To many 19th-century Europeans, this was scandalous. The Swedish playwright August Strindberg attacked the play in his volume of short stories "Getting Married" (1884). Nothing was considered more holy than the covenant of marriage, and to portray it in such a way was completely unacceptable; however, a few more open-minded critics such as the Irish playwright George Bernard Shaw found Ibsen's willingness to examine society without prejudice, exhilarating. In Germany, the production's lead actress refused to play the part of Nora unless Ibsen changed the ending, which, under pressure, he eventually did. In the alternative ending, Nora gives her husband another chance after he reminds her of her responsibility to their children. This ending proved unpopular and Ibsen later regretted his decision on the matter. Virtually all productions today, however, use the original ending, as do nearly all of the film versions of this play, including Dariush Mehrjui's "Sara" (the Argentine version, made in 1943 and starring Delia Garcés, does not; it also modernizes the story, setting it in the early 1940s).
Because of the radical departure from traditional behavior and theatrical convention involved in Nora's leaving home, her act of slamming the door as she leaves has come to represent the play itself. One critic noted, "That slammed door reverberated across the roof of the world."
Screen adaptations.
"A Doll's House" has been adapted for the cinema on many occasions. A 1923 German silent film "Nora" was directed by Berthold Viertel. Two film versions were released in 1973: one was directed by Joseph Losey, starring Jane Fonda, David Warner and Trevor Howard; and the other by Patrick Garland with Claire Bloom, Anthony Hopkins, and Ralph Richardson. Dariush Mehrjui's film "Sara" (1993) is based on "A Doll's House", with the plot transferred to Iran. "Sara", played by Niki Karimi, is the "Nora" of Ibsen's play. In Calcutta, India, a Bengali version "Putul Khela" was made in the 1950s based on Ibsen's play by Sombhu Mitra, a theatre personality. The Young Vic theatre in London has recently released a short film called Nora Hattie Morahan portraying what a modern day Nora might look like.
There have been several television versions. A 'live' version for American TV was transmitted in 1959 which was directed by George Schaefer. This version featured Julie Harris, Christopher Plummer, Hume Cronyn, Eileen Heckart and Jason Robards. In 1992, David Thacker directed a British television adaptation with Juliet Stevenson, Trevor Eve and David Calder. A 1974 West German television adaptation, titled "Nora Helmer" was directed by Rainer Werner Fassbinder and starred Margit Carstensen in the title role. A 1938 US radio production starred Joan Crawford as Nora and Basil Rathbone as Torvald. A later US radio version by the Theatre Guild in 1947 featured Rathbone with Wendy Hiller and Catherine Rowan, his co-star from a contemporary Broadway production.

</doc>
<doc id="3146" url="http://en.wikipedia.org/wiki?curid=3146" title="AIM-7 Sparrow">
AIM-7 Sparrow

The AIM-7 Sparrow is an American, medium-range semi-active radar homing air-to-air missile operated by the United States Air Force, United States Navy and United States Marine Corps, as well as other various air forces and navies. Sparrow and its derivatives were the West's principal beyond visual range (BVR) air-to-air missile from the late 1950s until the 1990s. It remains in service, although it is being phased out in aviation applications in favor of the more advanced AIM-120 AMRAAM. The Self-Defence Forces of Japan also employ the Sparrow missile, though it is being phased out and replaced by the Mitsubishi AAM-4. NATO pilots use the brevity code Fox One in radio communication to signal launch of a Semi-Active Radar Homing Missile such as the Sparrow.
The Sparrow was used as the basis for a surface-to-air missile, the RIM-7 Sea Sparrow, which is used by a number of navies for air defense of its ships.
Development.
Sparrow I.
After a protracted development cycle the initial AAM-N-2 "Sparrow" entered limited operational service in 1954 with specially modified Skyknights all weather carrier night fighters. And in 1956, they were carried by the F3H-2M Demon and F7U Cutlass fighter aircraft. Compared to the modern versions, the Sparrow I was more streamlined and featured a bullet-shaped airframe with a long pointed nose.
Sparrow I was a limited and rather primitive weapon. The limitations of beam-riding guidance (which was slaved to an optical sight on single seater fighters and a radar with night fighters) restricted the missile to attacks against targets flying a straight course and made it essentially useless against a maneuvering target. Only about 2,000 rounds were produced to this standard.
Sparrow II.
As early as 1950 Douglas examined equipping the Sparrow with an active radar seeker, initially known as XAAM-N-2a "Sparrow II, the original retroactively becoming Sparrow I". In 1952 it was given the new code AAM-N-3. The active radar made the Sparrow II a "fire and forget" weapon, allowing several to be fired at separate targets at the same time.
By 1955 Douglas proposed going ahead with development, intending it to be the primary weapon for the F5D Skylancer interceptor. It was later selected, with some controversy, to be the primary weapon for the Canadian Avro Arrow supersonic interceptor, along with the new Astra fire-control system. For Canadian use and as a second source for US missiles, Canadair was selected to build the missiles in Quebec.
The small size of the missile forebody and the K-band AN/APQ-64-radar limited performance, and it was never able to work in testing. After considerable development and test firings in the U.S. and Canada, Douglas abandoned development in 1956. Canadair continued development until the Arrow was cancelled in 1958.
Sparrow X.
A subvariant of the Sparrow I armed with the same nuclear warhead as the MB-1 Genie was proposed in 1958, but was cancelled shortly thereafter.
Sparrow III.
Concurrently with the development of the Sparrow I, in 1951, Raytheon began work on the semi-active radar homing version of Sparrow family of missiles, the AAM-N-6 "Sparrow III". The first of these weapons entered United States Navy service in 1958.
The AAM-N-6a was similar to the -6, but used a new Thiokol liquid-fuel rocket engine for improved performance. It also included changes to the guidance electronics to make it effective at higher closing speeds. The -6a was also selected to arm the Air Force's "F-110A Spectre" (F-4 Phantom) fighters in 1962, known to them as the AIM-101. It entered production in 1959, with 7500 being built.
Another upgrade reverted to a Rocketdyne solid-fuel motor for the AAM-N-6b, which started production in 1963. The new motor significantly increased maximum range to for head-on attacks.
During this year the Navy and Air Force agreed on standardized naming conventions for their missiles. The Sparrows became the AIM-7 series. The original Sparrow I and aborted Sparrow II became the AIM-7A and AIM-7B, despite both being out of service. The -6, -6a and -6B became the AIM-7C, AIM-7D and AIM-7E respectively.
25,000 AIM-7Es were produced, and saw extensive use during the Vietnam War, where its performance was generally considered disappointing. The mixed results were a combination of reliability problems (exacerbated by the tropical climate), limited pilot training in fighter-to-fighter combat, and restrictive rules of engagement that generally prohibited BVR (beyond visual range) engagements. The Pk (kill probability) of the AIM-7E was less than 10%; US fighter pilots shot down 59 aircraft out of the 612 Sparrows fired. Of the 612 AIM-7D/E/E-2 missiles fired, 97 (or 15.8%) hit their targets, resulting in 56 (or 9.2%) kills. Two kills were obtained beyond visual range.
In 1969 an improved version, the E-2, was introduced with clipped wings and various changes to the fuzing. Considered a "dogfight Sparrow", the AIM-7E-2 was intended to be used at shorter ranges where the missile was still travelling at high speeds, and in the head-on aspect, making it much more useful in the visual limitations imposed on the engagements. Even so, its kill rate was only 13% in combat, leading to a practice of ripple-firing all four at once in hopes of increasing kill probability. Its worst tendency was that of detonating prematurely, approximately a thousand feet in front of the launching aircraft, but it also had many motor failures, erratic flights, and fuzing problems. An E-3 version included additional changes to the fuzing, and an E-4 featured a modified seeker for use with the F-14 Tomcat.
U.S. AIM-7 Sparrow Aerial Combat Victories in the Vietnam War 1965-1973.
Improved versions of the AIM-7 were developed in the 1970s in an attempt to address the weapon's limitations. The AIM-7F, which entered service in 1976, had a dual-stage rocket motor for longer range, solid-state electronics for greatly improved reliability, and a larger warhead. Even this version had room for improvement, leading British Aerospace and the Italian firm Alenia to develop advanced versions of Sparrow with better performance and improved electronics as the BAe Skyflash and Alenia Aspide, respectively.
The most common version of the Sparrow today, the AIM-7M, entered service in 1982 and featured a new inverse monopulse seeker (matching the capabilities of Skyflash), active radar fuse, digital controls, improved ECM resistance, and better low-altitude performance. It was used to good advantage in the 1991 Gulf War, where it scored many USAF air-to-air kills. Of 44 missiles fired, 30 (68.2%) hit their intended targets resulting in 24/26 (54.5%/59.1%) kills. 19 kills were obtained beyond visual range.
The AIM-7P is similar in most ways to the M versions, and was primarily an upgrade program for existing M-series missiles. The main changes were to the software, improving low-level performance. A follow-on Block II upgrade added a new rear receiver allowing the missile to receive mid-course correction from the launching aircraft. Plans initially called for all M versions to be upgraded, but currently P's are being issued as required to replace M's lost or removed from the inventory.
The final version of the missile was to have been the AIM-7R, which added an infrared seeker to an otherwise unchanged AIM-7P Block II. A general wind-down of the budget led to it being cancelled in 1997. 
Sparrow is now being phased out with the availability of the active-radar AIM-120 AMRAAM, but is likely to remain in service for several years.
Foreign versions.
Canada.
As part of the Avro Arrow program, Canadair partnered with Douglas in the development of the Sparrow II (AIM-7B). After Douglas dropped out of this program, Canadair continued on with it until the termination of the Arrow.
Italy.
The Italian company Finmeccanica, Alenia Difesa licensed the AIM-7E Sparrow technology from the US, and produced its own improved version called Aspide.
UK.
British Aerospace (BAe) licensed the AIM-7E2 technology in the 1970s, producing the Skyflash missile. Skyflash used a Marconi XJ521 monopulse Semi-Active seeker together with improvements to the electronics. It was powered by the Aerojet Mk52 mod 2 rocket engine (later by the Rocketdyne Mk38 mod 4). Skyflash entered service with the Royal Air Force (RAF) on their Phantom FG.1/FGR.2 in 1976, and later on the Tornado F3. Skyflash was also exported to Sweden for use on their Viggen fighters.
An upgraded version with active radar seeker, called Active Sky Flash was proposed by BAe and Thomson-CSF, but did not receive funding because the RAF opted for other missiles.
People's Republic of China.
The LY-60/FD-60/PL-10 is a family of PRC missiles developed by the Shanghai Academy of Science and Technology, largely based on the Italian Aspide missile - a version of the Sparrow. There are four versions of the basic design, three of which are surface-to-air and one air-to-air.
Design.
The Sparrow has four major sections: guidance section, warhead, control, and rocket motor (currently the Hercules MK-58 solid-propellant rocket motor). It has a cylindrical body with four wings at mid-body and four tail fins. Although the external dimensions of the Sparrow remained relatively unchanged from model to model, the internal components of newer missiles represent major improvements, with vastly increased capabilities. The warhead is of the continuous-rod type.
As with other semi-active radar guided missiles, the missile does not generate radar signals, but instead homes in on reflected continuous-wave signals from the launch platform's radar. The receiver also senses the guidance radar to enable comparisons that enhance the missile's resistance to passive jamming.
Principle of guidance (semi-active version).
The launching aircraft will illuminate the target with its radar. In radars of the 1950s these were single target tracking devices using a nutating horn as part of the antenna. This caused the beam to be swept in a small cone. Signal processing would be applied to determine the direction of maximum illumination and so develop a signal to steer the antenna toward the target. The missile detects the reflected signal from the target with a high gain antenna in a similar fashion and steers the entire missile toward closure with the target. The missile guidance also samples a portion of the illuminating signal via rearward pointing waveguides. The comparison of these two signals enabled logic circuits to determine the true target reflection signal, even if the target were to eject radar-reflecting chaff.

</doc>
<doc id="3147" url="http://en.wikipedia.org/wiki?curid=3147" title="AIM-120 AMRAAM">
AIM-120 AMRAAM

The AIM-120 Advanced Medium-Range Air-to-Air Missile, or AMRAAM (pronounced "am-ram"), is a modern beyond-visual-range air-to-air missile (BVRAAM) capable of all-weather day-and-night operations. Designed with the same form-and-fit factors as the previous generation of semiactive guided Sparrow missiles, it is a fire-and-forget missile with active guidance. When an AMRAAM missile is being launched, NATO pilots use the brevity code – Fox Three.
Origins.
AIM-7 Sparrow MRM.
The AIM-7 Sparrow medium range missile (MRM) was purchased by the US Navy from original developer Howard Hughes in the 1950s as its first operational air-to-air missile with "beyond visual range" (BVR) capability. With an effective range of about , it was introduced as a radar beam riding missile and then it was improved to a semiactive radar guided missile which would home in on reflections from a target illuminated by the radar of the launching aircraft. It was effective at visual to beyond visual range. The early beam riding versions of the Sparrow missiles were integrated onto the F3H Demon and F7U Cutlass, but the definitive AIM-7 Sparrow was the primary weapon for the all-weather F-4 Phantom II fighter/interceptor, which lacked an internal gun in its U.S. Navy, U.S. Marine Corps, and early U.S. Air Force versions. The F-4 carried up to four AIM-7s in built-in recesses under its belly.
Although designed for use against non-maneuvering targets such as bombers, due to poor performance against fighters over North Vietnam, these missiles were progressively improved until they proved highly effective in dogfights. Together with the short range infrared guided AIM-9 Sidewinder, they replaced the AIM-4 Falcon IR and radar guided series for use in air combat by the USAF as well. A disadvantage to semiactive homing was that only one target could be illuminated by the launching fighter plane at a time. Also, the launching aircraft had to remain pointed in the direction of the target (within the azimuth and elevation of its own radar set) which could be difficult or dangerous in air-to-air combat.
AIM-54 Phoenix LRM.
The US Navy later developed the AIM-54 Phoenix long range missile (LRM) for the fleet air defense mission. It was a large Mach 4 missile designed to counter cruise missiles and the bombers that launched them. Originally intended for the straight-wing Douglas F6D Missileer and then the navalized version of the F-111B, it finally saw service with the Grumman F-14 Tomcat, the only fighter capable of carrying such a heavy missile. Phoenix was the first US fire-and-forget multiple launch radar-guided missile: one which used its own active guidance system to guide itself without help from the launch aircraft when it closed on its target. This in theory gave a Tomcat with a six-Phoenix load the unprecedented capability of tracking and destroying up to six targets beyond visual range, as far as away – the only US fighter with such capability.
A full load of six Phoenix missiles and its dedicated launcher exceeded a typical Vietnam-era bomb load. Its service in the US Navy was primarily as a deterrent, as its use was hampered by restrictive Rules of engagement in conflicts such as Operations Desert Storm, Southern Watch and Iraqi Freedom. The US Navy retired the Phoenix in 2004 in light of availability of the AIM-120 AMRAAM on the F/A-18 Hornet and the pending retirement of the F-14 Tomcat from active service in late 2006.
ACEVAL/AIMVAL.
The Department of Defense conducted an extensive evaluation of air combat tactics and missile technology from 1974–78 at Nellis AFB using the F-14 Tomcat and F-15 Eagle equipped with Sparrow and Sidewinder missiles as blue force and Aggressor F-5E aircraft equipped with AIM-9L all-aspect Sidewinders as the Red force. This Joint Test and Evaluaton (JT&E) was designated Air Combat Evaluation/Air Intercept Missile Evaluation (ACEVAL/AIMVAL). A principal finding was the necessity to produce illumination for the Sparrow until impact resulted in the Red Force being able to launch their all-aspect Sidewinders before impact thereby resulting in mutual kills. What was needed was Phoenix type multiple launch and terminal active capability in a Sparrow size airframe. This led to a Memorandum of Agreement (MOA) with European allies (principally the UK and Germany for development) for the US to develop an Advanced Medium Range Air-to-Air Missile (AMRAAM) with the USAF as lead service. The MOA also assigned responsibility for development of an Advanced Short Range Air-to-Air Missile to the European team – this would become the British ASRAAM.
Requirements.
By the 1990s, the reliability of the Sparrow had improved so much from the dismal days of Vietnam that it accounted for the largest number of aerial targets destroyed in Desert Storm. But while the USAF had passed on the Phoenix and their own similar AIM-47/YF-12 to optimize dogfight performance, they still needed a multiple-launch fire-and-forget capability for the F-15 and F-16. AMRAAM would need to be fitted on fighters as small as the F-16, and fit in the same spaces that were designed to fit the Sparrow on the F-4 Phantom. The European partners needed AMRAAM to be integrated on aircraft as small as the Sea Harrier. The US Navy needed AMRAAM to be carried on the F/A-18 Hornet and wanted capability for two to be carried on a launcher that normally carried one Sparrow to allow for more air-to-ground weapons. 
The AMRAAM became one of the primary air-to-air weapons of the new F-22 Raptor fighter, which needed to place all of its weapons into internal weapons bays in order to help achieve an extremely low radar cross-section.
Development.
AMRAAM was developed as the result of an agreement (the Family of Weapons MOA, no longer in effect by 1990), among the United States and several other NATO nations to develop air-to-air missiles and to share production technology. Under this agreement the U.S. was to develop the next generation medium range missile (AMRAAM) and Europe would develop the next generation short range missile (ASRAAM). Although Europe initially adopted the AMRAAM, an effort to develop the MBDA Meteor, a competitor to AMRAAM, was begun in Great Britain. Eventually the ASRAAM was developed solely by the British, but using another source for its infrared seeker. After protracted development, the deployment of AMRAAM (AIM-120A) began in September 1991 in US Air Force F-15 Eagle fighter squadrons. The US Navy soon followed (in 1993) in its the F/A-18 Hornet squadrons.
The eastern counterpart of AMRAAM is the somewhat similar Russian Air Force AA-12 "Adder", sometimes referred to in the West as the "AMRAAMski." Likewise, France began its own air-to-air missile development with the MICA concept that used a common airframe for separate radar-guided and infrared-guided versions.
Operational features summary.
AMRAAM has an all-weather, beyond-visual-range (BVR) capability. It improves the aerial combat capabilities of US and allied aircraft to meet the current threat of enemy air-to-air weapons as they existed in 1991. AMRAAM serves as a follow-on to the AIM-7 Sparrow missile series. The new missile is faster, smaller, and lighter, and has improved capabilities against low-altitude targets. It also incorporates a datalink to guide the missile to a point where its active radar turns on and makes terminal intercept of the target. An inertial reference unit and micro-computer system makes the missile less dependent upon the fire-control system of the aircraft.
Once the missile closes in on the target, its active radar guides it to intercept. This feature, known as "fire-and-forget", frees the aircrew from the need to further provide guidance, enabling the aircrew to aim and fire several missiles simultaneously at multiple targets and perform evasive maneuvers while the missiles guide themselves to the targets.
The missile also features the ability to "Home on Jamming," giving it the ability to switch over from active radar homing to passive homing – homing on jamming signals from the target aircraft. Software on board the missile allows it to detect if it is being jammed, and guide on its target using the proper guidance system.
Guidance system overview.
Interception course stage.
AMRAAM uses two-stage guidance when fired at long range. The aircraft passes data to the missile just before launch, giving it information about the location of the target aircraft from the launch point and its direction and speed. The missile uses this information to fly on an interception course to the target using its built in inertial navigation system (INS). This information is generally obtained using the launching aircraft's radar, although it could come from an infrared search and tracking system (IRST), from a data link from another fighter aircraft, or from an AWACS aircraft.
After launch, if the firing aircraft or surrogate continues to track the target, periodic updates—such as changes in the target's direction and speed—are sent from the launch aircraft to the missile, allowing the missile to adjust its course so that it is able to close to a self-homing distance where it will be close enough to "catch" the target aircraft in the "basket" (the missile's radar field of view in which it will be able to lock onto the target aircraft, unassisted by the launch aircraft).
Not all armed services using the AMRAAM have elected to purchase the mid-course update option, which limits AMRAAM's effectiveness in some scenarios. The RAF initially opted not to use mid-course update for its Tornado F3 force, only to discover that without it, testing proved the AMRAAM was less effective in BVR engagements than the older semiactive radar homing BAE Skyflash weapon—the AIM-120's own radar is necessarily of limited range and power compared to that of the launch aircraft.
Terminal stage and impact.
Once the missile closes to self-homing distance, it turns on its active radar seeker and searches for the target aircraft. If the target is in or near the expected location, the missile will find it and guide itself to the target from this point. If the missile is fired at short range (typically visual range), it can use its active seeker just after launch, making the missile truly "fire and forget". However, this tactic is considerably risky – the now-active AMRAAM will acquire and home in on the first target it sees, regardless of friend or foe.
Boresight mode.
Apart from the slave mode, there is a free guidance mode, called boresight. This mode is radar guidance-free, the missile just fires and locks the first thing it sees. This mode can be used for defensive shot, i.e. when the enemy has numerical superiority.
Kill probability and tactics.
General considerations.
The kill probability (Pk) is determined by several factors, including aspect (head-on interception, side-on or tail-chase), altitude, the speed of the missile and the target, and how hard the target can turn. Typically, if the missile has sufficient energy during the terminal phase, which comes from being launched at close range to the target from an aircraft with an altitude and speed advantage, it will have a good chance of success. This chance drops as the missile is fired at longer ranges as it runs out of overtake speed at long ranges, and if the target can force the missile to turn it might bleed off enough speed that it can no longer chase the target. Operationally, the missile, which was designed for beyond visual range combat, has a Pk of 46% when fired at targets beyond visual range (13 missiles for 6 kills). In addition, the targets lacked missile warning systems, were not maneuvering, and were not attempting to engage the fighter that fired the AMRAAM. One of the targets was a US Army Blackhawk helicopter.
Lower-capability targets.
This leads to two main engagement scenarios. If the target is not armed with any medium or long-range fire-and-forget weapons, the attacking aircraft need only get close enough to the target and launch the AMRAAM. In these scenarios, the AMRAAM has a high chance of hitting, especially against low-maneuverability targets. The launch distance depends upon whether the target is heading towards or away from the firing aircraft. In a head-on engagement, the missile can be launched at longer range, since the range will be closing fast. In this situation, even if the target turns around, it is unlikely it can speed up and fly away fast enough to avoid being overtaken and hit by the missile (as long as the missile is not released too early). It is also unlikely the enemy can outmaneuver the missile since the closure rate will be so great. In a tail-on engagement, the firing aircraft might have to close to between one-half and one-quarter maximum range (or maybe even closer for a very fast target) in order to give the missile sufficient energy to overtake the targets.
If the targets are armed with missiles, the fire-and-forget nature of the AMRAAM is valuable, enabling the launching aircraft to fire missiles at the target and subsequently take defensive actions. Even if the targets have longer-range semiactive radar homing (SARH) missiles, they will have to chase the launching aircraft in order for the missiles to track them, effectively flying right into the AMRAAM. If the target aircraft fires missiles and then turns and runs away, those missiles will not be able to hit. Of course, if the target aircraft have long range missiles, even if they are not fire-and-forget, the fact that they force the launching aircraft to turn and run reduces the kill probability, since it is possible that without the mid-course updates the missiles will not find the target aircraft. However the chance of success is still good and compared to the relative impunity the launching aircraft enjoy, this gives the AMRAAM-equipped aircraft a decisive edge. If one or more missiles fail to hit, the AMRAAM-equipped aircraft can turn and re-engage, although they will be at a disadvantage compared to the chasing aircraft due to the speed they lose in the turn, and would have to be careful that they are not being tracked with SARH missiles.
Similarly armed targets.
The other main engagement scenario is against other aircraft with fire-and-forget missiles like the Vympel R-77 (NATO AA-12 "Adder") – perhaps MiG-29s, Su-27s or similar. In this case engagement is very much down to teamwork and could be described as "a game of chicken." Both flights of aircraft can fire their missiles at each other beyond visual range (BVR), but then face the problem that if they continue to track the target aircraft in order to provide mid-course updates for the missile's flight, they are also flying into their opponents' missiles. This assumes of course that all aircraft will detect each other.
Variants and upgrades.
Air-to-air missile versions.
There are currently four main variants of AMRAAM, all in service with the United States Air Force, United States Navy, and the United States Marine Corps. The AIM-120A is no longer in production and shares the enlarged wings and fins with the successor AIM-120B. The AIM-120C has smaller "clipped" aerosurfaces to enable internal carriage on the USAF F-22 Raptor. AIM-120B deliveries began in 1994.
The AIM-120C deliveries began in 1996. The C-variant has been steadily upgraded since it was introduced. The AIM-120C-6 contained an improved fuse (Target Detection Device) compared to its predecessor. The AIM-120C-7 development began in 1998 and included improvements in homing and greater range (actual amount of improvement unspecified). It was successfully tested in 2003 and is currently being produced for both domestic and foreign customers. It helped the U.S. Navy replace the F-14 Tomcats with F/A-18E/F Super Hornets – the loss of the F-14's long-range AIM-54 Phoenix missiles (already retired) is offset with a longer-range AMRAAM-D. The lighter weight of the advanced AMRAAM enables a hornet pilot greater bring-back weight upon carrier landings.
The AIM-120D is an upgraded version of the AMRAAM with improvements in almost all areas, including 50% greater range (than the already-extended range AIM-120C-7) and better guidance over its entire flight envelope yielding an improved kill probability (Pk). Raytheon began testing the D model on August 5, 2008, the company reported that an AIM-120D launched from an F/A-18F Super Hornet passed within lethal distance of a QF-4 target drone at the White Sands Missile Range.
The AIM-120D (P3I Phase 4, formerly known as AIM-120C-8) is a development of the AIM-120C with a two-way data link, more accurate navigation using a GPS-enhanced IMU, an expanded no-escape envelope, improved HOBS (High-Angle Off-Boresight) capability, and a 50% increase in range. The AIM-120D is a joint USAF/USN project, and is currently in the testing phase. The USN will field it from 2014, and AIM-120D will be carried by all Pacific carrier groups by 2020, although the 2013 sequestration cuts could push back this later date to 2022.
There are also plans for Raytheon to develop a ramjet-powered derivative of the AMRAAM, the Future Medium Range Air-Air Missile (FMRAAM). It is not known whether the FMRAAM will be produced since the target market, the British Ministry of Defence, has chosen the Meteor missile over the FMRAAM for a BVR missile for the Eurofighter Typhoon aircraft.
Raytheon is also working with the Missile Defense Agency to develop the Network Centric Airborne Defense Element (NCADE), an anti-ballistic missile derived from the AIM-120. This weapon will be equipped with a Ramjet engine and an IR seeker derived from the Sidewinder missile. In place of a proximity-fused warhead, the NCADE will use a kinetic energy hit-to-kill vehicle based on the one used in the Navy's RIM-161 Standard Missile 3.
The −120A and −120B models are currently nearing the end of their service life while the −120D variant has just entered full production. AMRAAM was due to be replaced by the USAF, the U.S. Navy, and the U.S. Marine Corps after 2020 by the Joint Dual Role Air Dominance Missile (JDRADM). This was unexpectedly terminated in the 2013 budget plan, and so the future replacement is uncertain.
Ground-launched systems.
Raytheon successfully tested launching AMRAAM missiles from a five-missile carrier on a M1097 Humvee. This system will be known as the SLAMRAAM (Surface Launched (SL) and AMRAAM). They receive their initial guidance information from a radar not mounted on the vehicle. Since the missile is launched without the benefit of an aircraft's speed or high altitude, its range is considerably shorter. Raytheon is currently marketing an SL-AMRAAM EX, purported to be an extended range AMRAAM and bearing a resemblance to the ESSM (Evolved Sea Sparrow Missile).
The Norwegian Advanced Surface-to-Air Missile System (NASAMS), developed by Kongsberg Defence & Aerospace, consists of a number of vehicle-pulled launch batteries (containing six AMRAAMs each) along with separate radar trucks and control station vehicles.
While still under evaluation for replacement of current US Army assets, the SL-AMRAAM has been deployed in several nations' military forces. The United Arab Emirates (UAE) has requested the purchasing of SL-AMRAAM as part of a larger 7 billion dollar foreign military sales package. The sale would include 288 AMRAAM C-7 missiles.
The US Army has test fired the SL-AMRAAM from a HIMARS artillery rocket launcher as a common launcher, as part of a move to switch to a larger and more survivable launch platform.
The National Guard Association of the United States has sent a letter asking for the United States Senate to stop the Army's plan to drop the SLAMRAAM program because without it there would be no path to modernize the Guard's AN/TWQ-1 Avenger Battalions.
On January 6, 2011, Secretary of Defense Robert M. Gates announced that the U.S. Army has decided to terminate acquisition of the SLAMRAAM as part of a budget-cutting effort.
Operational history.
The AMRAAM was used for the first time on December 27, 1992, when a USAF F-16D shot down an Iraqi MiG-25 that violated the southern no-fly-zone. Interestingly enough, this missile was returned from the flight line as defective a day earlier. AMRAAM gained a second victory in January 1993 when an Iraqi MiG-23 was shot down by a USAF F-16C.
The third combat use of the AMRAAM was in 1994, when a Republika Srpska Air Force J-21 Jastreb aircraft was shot down by a USAF F-16C that was patrolling the UN-imposed no-fly-zone over Bosnia. In that engagement at least 3 other Serbian aircraft were shot down by USAF F-16C fighters using AIM-9 missiles (see Banja Luka incident for more details). At that point three launches in combat resulted in three kills, resulting in the AMRAAM being informally named "slammer" in the second half of the 1990s.
In 1998 and 1999 AMRAAMs were again fired by USAF F-15 fighters at Iraqi aircraft violating the No-Fly-Zone, but this time they failed to hit their targets. During the spring of 1999, AMRAAMs saw their main combat action during Operation Allied Force, the Kosovo bombing campaign. Six Serbian MiG-29 were shot down by NATO (4 USAF F-15C, 1 USAF F-16C, 1 Dutch F-16A MLU), all of them using AIM-120 missiles (the kill by the F-16C may have happened due to friendly fire, from SA-7 MANPAD fired by Serbian infantry).
As of mid 2008, the AIM-120 AMRAAM has shot down nine aircraft (six MiG-29s, one MiG-25, one MiG-23, and one Soko J-21 Jastreb). An AMRAAM was also involved in a friendly-fire incident in 1994 when F-15 fighters patrolling the Northern No-Fly Zone inadvertently shot down a pair of U.S. Army Black Hawk helicopters.
Since 2007 Raytheon has continued to slip on AMRAAM deliveries, leading the USAF to withhold $621 million in 2012 on account of 193 missiles not delivered.
Foreign sales.
In 2006 Poland received AIM-120C-5 missiles to arm its new F-16C/D Block 52+ fighters.
In early 2006, the Pakistan Air Force (PAF) ordered 500 AIM-120C-5 AMRAAM missiles as part of a $650 million F-16 ammunition deal to equip its F-16C/D Block 50/52+ and F-16A/B Block 15 MLU fighters. The PAF got the first three F-16C/D Block 50/52+ aircraft on July 3, 2010 and first batch of AMRAAMs on July 26, 2010.
In 2007, the United States government agreed to sell 218 AIM-120C-7 missiles to Taiwan as part of a large arms sales package that also included 235 AGM-65G-2 Maverick missiles. Total value of the package, including launchers, maintenance, spare parts, support and training rounds, was estimated at around US$421 million. This supplemented an earlier Taiwanese purchase of 120 AIM-120C-5 missiles a few years ago.
2008 has brought announcements of new or additional sales to Singapore, Finland, Morocco and South Korea; in December 2010 the Swiss government requested 150 AIM-120C-7 missiles. Sales to Finland have stalled, because the manufacturer has not been able to fix a mysterious bug that causes the rocket motors of the missile to fail in cold tests.
Cold weather malfunctions.
Finnish Defence Forces reported on September 3, 2012 that the United States had not delivered any of the AMRAAM anti-aircraft missiles they had ordered due to a mysterious engine malfunction in cold weather that the manufacturer, Raytheon, has not been able to determine the fault of. Colonel Kari Renko, an engineer at the Finnish Air Force, was quoted by Helsingin Sanomat as saying about this failure, ""The problem involves the rocket engines which have been in use for decades"" and that Finland first was told of the problems by the Americans about two years ago. The reason for the malfunction has been determined to be a change in the chemical formula of the rocket propellant to comply with new environmental regulations. The change caused the supplier of AMRAAM rocket motors, Alliant Techsystems, to produce motors that were unreliable, especially in cold conditions where aircraft carrying them would fly. ATK has been unable to find a solution, and no new AMRAAM missiles had been delivered to the USAF since 2010 as a result. In late 2012, the Air Force solved the problem by selecting Norwegian ammunition manufacturer Nammo Raufoss to be their new supplier of AMRAAM rocket motors. (Nammo is a 50/50 joint venture of the state of Norway and the Finnish partly state-owned Patria corporation.)

</doc>
<doc id="3149" url="http://en.wikipedia.org/wiki?curid=3149" title="AGM-88 HARM">
AGM-88 HARM

The AGM-88 High-speed Anti-Radiation Missile (HARM) is a tactical, air-to-surface missile designed to home in on electronic transmissions coming from surface-to-air radar systems. It was originally developed by Texas Instruments as a replacement for the AGM-45 Shrike and AGM-78 Standard ARM system. Production was later taken over by Raytheon Corporation when it purchased the defense production business of Texas Instruments.
Description.
The AGM-88 can detect, attack and destroy a radar antenna or transmitter with minimal aircrew input. The proportional guidance system that homes in on enemy radar emissions has a fixed antenna and seeker head in the missile's nose. A smokeless, solid-propellant, booster-sustainer rocket motor propels the missile at speeds over Mach 2. HARM, a Navy-led program, was initially integrated onto the A-6E, A-7 and F/A-18 and later onto the EA-6B. RDT&E for use on the F-14 was begun, but not completed. The American Air Force introduced HARM on the F-4G Wild Weasel and later on specialized F-16s equipped with the HARM Targeting System (HTS).
History.
Deployment.
The HARM missile was approved for full production in March 1983, and then deployed in late 1985 with VA-72 and VA-46 aboard the aircraft carrier USS "America". It was soon used in combat—in March 1986 against a Libyan SA-5 site in the Gulf of Sidra, and then Operation Eldorado Canyon in April. HARM was used extensively by the United States Navy and the United States Air Force for Operation Desert Storm during the Gulf War of 1991.
During the Gulf War, the HARM was involved in a friendly fire incident when the pilot of an F-4G Wild Weasel escorting a B-52 bomber mistook the latter's tail gun radar for an Iraqi AAA site. (This was after the tail gunner of the B-52 had targeted the F-4G, mistaking it for an Iraqi MiG.) The F-4 pilot launched the missile and then saw that the target was the B-52, which was hit. It survived with shrapnel damage to the tail and no casualties. The B-52 was subsequently renamed "In HARM's Way".
"Magnum" is spoken over the radio to announce the launch of an AGM-88. During the Gulf War, if an aircraft was illuminated by enemy radar a bogus "Magnum" call on the radio was often enough to convince the operators to power down.
This technique would also be employed in Serbia during air operations in 1999.
In 2013 President Obama offered the AGM-88 to Israel for the first time.
AGM-88E AARGM.
The newest upgrade is a joint venture by the Italian Ministry of Defense and the US Department of Defense: the "AGM-88E Advanced Anti-Radiation Guided Missile" ("AARGM"), produced by Alliant Techsystems.
In November 2005, the Italian Ministry of Defense and the US Department of Defense signed a Memorandum of Agreement on the joint development of the AGM-88E AARGM missile. Italy was providing $20 million of developmental funding as well as several millions worth material, equipment and related services. The Italian Air Force was expected to procure up to 250 missiles for its Tornado ECR aircraft. Thus flight test program was set to integrate the AARGM onto Tornado ECR's weapon system.
The AARGM features the latest software, enhanced capabilities intended to counter radar shutdown and passive radar using an additional active millimeter wave seeker. It was released in November 2010.
The Navy demonstrated the AARGM's capability during Initial Operational Test and Evaluation (IOT&E) in spring 2012 with live firing of 12 missiles. Aircrew and maintenance training with live missiles was completed in June. The Navy authorized Full-Rate Production (FRP) of the AARGM in August 2012, with 72 missiles for the Navy and nine for the Italian Air Force to be delivered in 2013. A U.S. Marine Corps F/A-18 Hornet squadron will be the first forward-deployed unit with the AGM-88E.
It will be initially integrated onto the FA-18C/D, FA-18E/F, EA-18G, and Tornado ECR aircraft and later on the F-35.
In September 2013, ATK delivered the 100th AARGM to the U.S. Navy. The AGM-88E program is on schedule and on budget, with Full Operational Capability (FOC) planned for September 2014.

</doc>
<doc id="3151" url="http://en.wikipedia.org/wiki?curid=3151" title="AGM-65 Maverick">
AGM-65 Maverick

The AGM-65 Maverick is an air-to-surface tactical missile (AGM) designed for close air support. The most widely produced precision-guided missile in the Western world, it is effective against a wide range of tactical targets, including armor, air defenses, ships, ground transportation and fuel storage facilities. Originally designed and built by Hughes Missile Systems, development of the AGM-65 spanned from 1966 to 1972, after which it entered service with the United States Air Force in August 1972. Since then, it has been exported to more than 30 countries and is certified on 25 aircraft. The Maverick served during the Vietnam, Yom Kippur, Iran–Iraq and Gulf Wars, along with other smaller conflicts, destroying enemy forces and installations with varying degrees of success.
Since its introduction into service, numerous Maverick versions had been designed and produced, using electro-optical, laser, charge-coupled device and infra-red guidance systems. The AGM-65 has two types of warhead: one has a contact fuze in the nose, the other has a heavyweight warhead fitted with a delayed-action fuze, which penetrates the target with its kinetic energy before detonating. The Maverick shares the same configuration as Hughes's AIM-4 Falcon and AIM-54 Phoenix, and measures more than in length and in diameter.
Development.
The Maverick's development history began in 1965, when the United States Air Force (USAF) began a program to develop a replacement to the AGM-12 Bullpup. With a range of , the radio-guided Bullpup was introduced in 1959 and was considered a "silver bullet" by operators. However, the launch aircraft was required to fly straight towards the target during the missile's flight instead of performing evasive manoeuvres, thus risking the crew.
From 1966 to 1968, Hughes Missile Systems and Rockwell competed for the contract to build the new missile. Each were allocated $3 million for preliminary design and engineering work of the Maverick in 1966. In 1968, Hughes emerged with the $95 million contract for further development and testing of the missile; at the same time, contract options called for 17,000 missiles to be procured. Hughes conducted a smooth development of the AGM-65 Maverick, culminating in the first, and successful, firing of the AGM-65 on a tank at Air Force Missile Development Center at Holloman Air Force Base, New Mexico, on 18 December 1969. In July 1971, the USAF and Hughes signed a $69.9 million contract for 2,000 missiles, the first of which was delivered in 1972.
Although early operational results were favorable, military planners predicted that the Maverick would fare less successfully in the hazy conditions of Central Europe, where it would have been used against Warsaw Pact forces. As such, development of the AGM-65B began in 1975 before it was delivered during the late 1970s. When production of the AGM-65A/B was ended in 1978, more than 35,000 missiles had been built.
More versions of the Maverick appeared, among which was the laser-guided AGM-65C/E. Development of the AGM-65C started in 1978 by Rockwell, who built a number of development missiles for the USAF. Due to high cost, the version was not procured by the USAF, and instead entered service with the United States Marine Corps (USMC) as the AGM-65E. Another major development was the AGM-65D, which employed an imaging infrared (IIR) seeker and thus is all-weather operable. The five-year development period of the AGM-65D started in 1977 and ended with the first delivery to the USAF in October 1983. The version received initial operating capability in February 1986.
The AGM-65F is a hybrid Maverick combining the AGM-65D's IIR seeker and warhead and propulsion components of the AGM-65E. Deployed by the United States Navy (USN), the AGM-65F is optimized for maritime strike roles. The first AGM-65F launch from the P-3C took place in 1989, and in 1994, the USN awarded Unisys a contract to integrate the version with the P-3C. Meanwhile, Hughes produced the AGM-65G, which essentially has the same guidance system as the D, with some software modifications that track larger targets, coupled with a shaped-charge warhead.
In the mid-1990s to early 2000s, there were several ideas of enhancing the Maverick's potential. Among them was the stillborn plan to incorporate the Mavericks active millimeter wave radars, which can determine the exact shape of a target. Another study called "Longhorn Project" was conducted by Hughes, and later Raytheon following the absorption of Hughes into Raytheon, looked a Maverick version equipped with turbojet engines instead of rocket motors. The "Maverick ER", as it was dubbed, would have a "significant increase in range" compared to the Maverick's current range of . The proposal was abandoned, but if the Maverick ER had entered production, it would have replaced the AGM-119B Penguin carried on the MH-60R.
The most modern versions of the Maverick are the AGM-65H/K, which were in production as of 2007. The AGM-65H was developed by coupling the AGM-65B with a charge-coupled device (CCD) seeker optimized for desert operations and which has three times the range of the original TV-sensor; a parallel USN program aimed at rebuilding AGM-65Fs with newer CCD seekers resulted in the AGM-65J. The AGM-65K, meanwhile, was developed by replacing the AGM-65G's IR guidance system with an electro-optical television guidance system.
Design.
The Maverick has a modular design construction, allowing a different combination of the guidance package and warhead to be attached to the rocket motor section to produce a different weapon. It has long-chord delta wings and a cylindrical body, reminiscent of the AIM-4 Falcon and the AIM-54 Phoenix.
Different models of the AGM-65 have used electro-optical, laser, and infra-red guidance systems. The AGM-65 has two types of warheads: one has a contact fuze in the nose, the other has a heavyweight warhead fitted with a delayed-action fuze, which penetrates the target with its kinetic energy before detonating. The latter is most effective against large, hard targets. The propulsion system for both types is a solid-fuel rocket motor behind the warhead.
The Maverick missile is unable to lock onto targets on its own; it has to be given input by the pilot or Weapon Systems Officer (WSO) after which it follows the path to the target autonomously, allowing the WSO to fire and forget. In an A-10 Thunderbolt, for example, the video fed from the seeker head is relayed to a screen in the cockpit, where the pilot can check the locked target of the missile before launch. A crosshair on the head-up display is shifted by the pilot to set the approximate target while the missile will then automatically recognize and lock on to the target. Once the missile is launched, it requires no further assistance from the launch vehicle and tracks its target automatically. This fire-and-forget property is not shared by the E version that uses semi-active laser homing.
Deployment.
The Maverick was declared operational on 30 August 1972 with the F-4D/Es and A-7s initially cleared for the type; the missile made its combat debut four months later with the USAF in the Vietnam War. During the Yom Kippur War in October 1973, the Israelis used Mavericks to destroy and disable enemy vehicles. Deployment of early versions of the Mavericks in these two wars were successful due to the favorable atmospheric conditions that suited the electro-optical TV seeker. Ninety-nine missiles were fired during the two wars, eighty-four of which were successful.
In June 1975, during a border confrontation, Iranian troops fired twelve Mavericks, all successful, at Iraqi tanks. Five years later, during Operation "Morvarid" as part of the Iran–Iraq War, Iranian F-4s used Mavericks to sink three OSA II missile boats and four P-6 combat ships.
In August 1990, Iraq invaded Kuwait. In early 1991, the US-led Coalition executed Operation "Desert Storm" during which Mavericks played a crucial role in the ousting of Iraqi forces from Kuwait. Employed by F-15E Strike Eagles, F-18 Hornets, AV-8B Harriers, F-16 Fighting Falcons and A-10 Thunderbolts, but used mainly by the last two, more than 5,000 Mavericks were deployed to attack armored targets. The most-used variant by the USAF was the IIR-guided AGM-65D. The reported hit rate by USAF Mavericks was 80–90%, while for the USMC it was 60%. The Maverick was used again in Iraq during the 2003 Iraq War, during which 918 were fired.
The first time the Maverick were fired from a Lockheed P-3 Orion at a hostile vessel was when the USN and coalition units came to the aid of Libyan rebels to engage the Libyan Coast Guard vessel "Vittoria" in the port of Misrata, Libya, during the late evening of 28 March 2011. "Vittoria" was engaged and fired upon by a USN P-3C Maritime Patrol aircraft with AGM-65 Maverick missiles.
Iranian AGM-65 Maverick missiles have been used in various operations such as Fatholmobin whereas Iranian AH-1J's fired 11 Mavericks.
Launch platforms.
United States.
LAU-117 Maverick launchers have been used on USN, USAF, and USMC aircraft:
Export.
The Maverick has been exported to at least 30 countries:

</doc>
<doc id="3152" url="http://en.wikipedia.org/wiki?curid=3152" title="AIM-54 Phoenix">
AIM-54 Phoenix

The AIM-54 Phoenix is a radar-guided, long-range air-to-air missile (AAM), carried in clusters of up to six missiles on the Grumman F-14 Tomcat, its only launch platform. The Phoenix was the United States' only long-range air-to-air missile. The combination of Phoenix missile and the AN/AWG-9 guidance radar was the first aerial weapons system that could simultaneously engage multiple targets. Both the missile and the aircraft were used by the United States Navy and are now retired, the AIM-54 Phoenix in 2004 and the F-14 in 2006. They were replaced by the shorter-range AIM-120 AMRAAM, employed on the F/A-18 Hornet and F/A-18E/F Super Hornet. Following the retirement of the F-14 by the U.S. Navy, the weapon's only current operator is the Islamic Republic of Iran Air Force. Brevity code "Fox Three" was used when firing the AIM-54.
Background.
Since 1951, the Navy faced the initial threat from the Tupolev Tu-4K 'Bull' carrying anti-ship missiles. Eventually, during the height of the Cold War, the threat would have actually expanded into regimental-size raids of Tu-16 Badger and Tu-22M Backfire bombers equipped with low-flying, long-range, high-speed, nuclear-armed cruise missiles and considerable Electronic Counter Measures (ECM) of various types.
The Navy would require a long-range, long-endurance interceptor aircraft to defend carrier battle groups against this threat. The projected F6D Missileer was intended to fulfill this mission and oppose the attack far from the fleet it was defending. The weapon needed for interceptor aircraft, the Bendix AAM-N-10 Eagle, would be an air-to-air missile of unprecedented range when compared to contemporary AIM-7 Sparrow missiles. It would work together with Westinghouse AN/APQ-81 radar.
Development.
The Missileer project was cancelled in December 1960, but in the early 1960s Navy made the next interceptor attempt with the F-111B, and they needed a new missile design.
At the same time, the USAF canceled the projects for their land-based high-speed interceptor aircraft, the North American XF-108 Rapier and the Lockheed YF-12, and left the capable AIM-47 Falcon missile at a quite advanced stage of development, but with no effective launch platform.
The AIM-54 Phoenix, developed for the F-111B fleet air defense fighter, had an airframe with 4 cruciform fins that was a scaled-up version of the AIM-47.
One characteristic of the Missileer ancestry was that the radar sent it mid-course corrections, which allowed the fire control system to "loft" the missile up over the target into thinner air where it had better range.
The F-111B was canceled in 1968. Its weapons system, the AIM-54 working with the AWG-9 radar, migrated to the new U.S. Navy fighter project, the VFX, which would later become the F-14 Tomcat.
In 1977, development of a significantly improved Phoenix version, the AIM-54C, was developed to better counter projected threats from tactical anti-naval aircraft and cruise missiles, and its final upgrade included a re-programmable memory capability to keep pace with emerging ECM.
Usage in comparison to other weapon systems.
The AIM-54/AWG-9 combination was the first to have multiple track capability (up to 24 targets) and launch (up to 6 Phoenixes can be launched nearly simultaneously); the large missile is equipped with a conventional warhead. The AWG-9 radar system carried by the F-111B and F-14 Tomcat was one of largest and most powerful ever fitted to a fighter.
On the F-14, 4 missiles can be carried under the fuselage tunnel attached to special aerodynamic pallets, plus 2 under glove stations. A full load of 6 Phoenix missiles and the unique launch rails weigh in at over , about twice the weight of Sparrows, so it was more common to carry a mixed load of 4 Phoenix, 2 Sparrow and 2 Sidewinder missiles.
Before the introduction of the Phoenix missile, most other US aircraft relied on the smaller, less-expensive AIM-7 Sparrow; classified as a Medium Range Missile (MRM). Guidance for the Sparrow required that the launching aircraft use its radar to continuously illuminate a single target for the missile's "passive" seeker to track, or guidance would be lost. This method meant the aircraft no longer had a search capability while supporting the launched Sparrow, effectively reducing situational awareness.
The Tomcat's AWG-9 radar was capable of tracking up to 24 targets in Track-While-Scan mode, with the AWG-9 selecting up to six priority targets for potential launch by the AIM-54. The pilot or Radar Intercept Officer (RIO) could then launch the AIM-54 Phoenix missiles when launch parameters were met. The large Tactical Information Display (TID) in the RIO's cockpit gave an unprecedented amount of information to the aircrew (the pilot had the ability to monitor the RIO's display) and, importantly, the AWG-9 could continually search and track multiple targets after Phoenix missiles were launched, thereby maintaining situational awareness of the battlespace.
Link-4 datalink capability allowed US Navy Tomcats to share information with the E-2C Hawkeye AEW aircraft, and during Desert Shield in 1990, the Link-4A was introduced and allowed the Tomcats to have a fighter-to-fighter datalink capability, further enhancing overall situational awareness. The F-14D entered service with the JTIDS that brought the even better Link-16 datalink "picture" to the cockpit.
Active guidance.
The Phoenix has several guidance modes and achieves its longest range by using mid-course updates from the F-14A/B AWG-9 radar (APG-71 radar in the F-14D) as it climbs to cruise between and at close to Mach 5. Phoenix uses its high altitude to gain gravitational potential energy, which is later converted into kinetic energy as the missile dives at high velocity towards its target. At around from the target, the missile activates its own radar to provide terminal guidance. Minimum engagement range for the Phoenix is around ; active homing would initiate upon launch at this distance.
Legacy.
The AIM-54 Phoenix was retired from USN service on September 30, 2004. F-14 Tomcats were retired on September 22, 2006. They were replaced by shorter-range AIM-120 AMRAAMs, employed on the F/A-18E/F Super Hornet. Both the F-14 Tomcat and AIM-54 Phoenix missile continue in the service of the Islamic Republic of Iran Air Force, although the operational abilities of these aircraft and the missiles are questionable, since the US refused to supply spare parts and maintenance after the 1979 revolution, except for a brief period during the Iran-Contra Affair.
Despite the much-vaunted capabilities, the Phoenix was rarely used in combat, with only two confirmed launches and no confirmed targets destroyed in US Navy service, though a large number of kills were claimed by Iranian F-14s during the Iran–Iraq War. The USAF F-15 Eagle had responsibility for overland Combat Air Patrol (CAP) duties in Desert Storm in 1991, primarily because of the onboard F-15 IFF capabilities. The Tomcat did not have the requisite IFF capability mandated by the JFACC to satisfy the Rules of Engagement (ROE) to utilize the Phoenix capability at Beyond Visual Range (BVR). From an engineering and service standpoint, the Phoenix could be said to be a notable success. As the only surviving member of the Falcon missile family, it was not adopted by any other nation besides Iran, any other US armed service, or used on any other aircraft. It was heavy, large, expensive and not practical in close combat compared to the Sparrow or AMRAAM.
Variants.
There were also test, evaluation, ground training, and captive air training versions of the missile; designated ATM-54, AEM-54, DATM-54A, and CATM-54. The flight versions had A and C versions. The DATM-54 was not made in a C version as there was no change in the ground handling characteristics.
Iranian combat experiences with the AIM-54 Phoenix.
There is very little information available regarding Iran's use of its 79 F-14A Tomcats (delivered prior to 1979) in most western outlets; the exception being a book released by Osprey Publishing titled ""Iranian F-14 Tomcats in Combat"" by Tom Cooper and Farzad Bishop. Most of the research contained in the book was based on pilot interviews.
Reports vary on the use of the 285 missiles supplied to Iran, during the Iran–Iraq War, 1980–88. Unverified rumors that US technical personnel sabotaged the aircraft and weapons before they left the country following the 1979 Iranian Revolution imply that the Iranians might have found it impossible to fire the missiles. However, the IRIAF was able to repair the sabotage and the damage only affected a limited number of planes, not the entire fleet.
Some claim that it is unlikely that the Phoenix was used operationally. First, as difficult as the missile and fire control systems were to operate, Iran had hired many American technicians. Upon leaving, they took most of the knowledge about how to operate and maintain these complex weapon systems with them. Also, without a steady supply of engineering support from Hughes Aircraft Missile Systems Group and corresponding spares and upgrades, even a technically competent operator would have extreme difficulty fielding operational weapons.
Others claim that the primary use of the F-14 was as an airborne early warning aircraft, guarded by other fighters. However, Cooper claims that the IRIAF used the F-14 actively as a fighter-interceptor, and at times as an escort fighter, with the AIM-54 scoring 60–70 kills. F-14s were often used to protect IRIAF tankers supporting strike packages into Iraq, and scanned over the border with their radars, often engaging detected Iraqi flights. Also, some F-14s were modified into specialized airborne early warning aircraft.
Supporters of these claims point to the fact that, in the 1991 Gulf War, Iraqi fighter pilots consistently turned and fled as soon as American F-14 pilots turned on their fighters' very distinctive AN/AWG-9 radars, which suggests that Iraqi pilots had learned to avoid the F-14.
According to Cooper, the Islamic Republic of Iran Air Force was able to keep its F-14 fighters and AIM-54 missiles in regular use during the entire Iran–Iraq War, though periodic lack of spares grounded at times large parts of the fleet. At worst, during late 1987, the stock of AIM-54 missiles was at its lowest, with less than 50 operational missiles available. The missiles needed fresh thermal batteries that could only be purchased from the US. Iran found a clandestine buyer that supplied it with batteries — though those did cost up to $10,000 USD each. Iran did receive spares and parts for both the F-14s and AIM-54s from various sources during the Iran–Iraq War, and has received more spares after the conflict. Iran started a heavy industrial program to build spares for the planes and missiles, and although there are claims that it no longer relies on outside sources to keep its F-14s and AIM-54s operational, there is evidence that Iran continues to procure parts clandestinely.
Iran claims to be working on building an equivalent missile.
Characteristics.
The following is a list AIM-54 Phoenix specifications:
-*Actual Range Classified

</doc>
<doc id="3155" url="http://en.wikipedia.org/wiki?curid=3155" title="Lockheed AC-130">
Lockheed AC-130

The Lockheed AC-130 gunship is a heavily armed ground-attack aircraft variant of the C-130 Hercules transport plane. The basic airframe is manufactured by Lockheed, while Boeing is responsible for the conversion into a gunship and for aircraft support. The AC-130A Gunship II superseded the AC-47 Gunship I during the Vietnam War.
The gunship's sole user is the United States Air Force, which uses AC-130H Spectre, AC-130U Spooky, AC-130J Ghostrider, and AC-130W Stinger II variants for close air support, air interdiction and force protection. Close air support roles include supporting ground troops, escorting convoys, and flying urban operations. Air interdiction missions are conducted against planned targets and targets of opportunity. Force protection missions include defending air bases and other facilities. AC-130Us are based at Hurlburt Field, Florida, while AC-130Hs and AC-130Ws are based at Cannon AFB, New Mexico. The AC-130s deploy to bases worldwide in support of operations. The gunship squadrons are part of the Air Force Special Operations Command (AFSOC), a component of the United States Special Operations Command (SOCOM).
All of the weaponry aboard is mounted to fire from the left (port) side of the non-pressurised aircraft. During an attack the gunship performs a pylon turn, flying in a large circle around the target, allowing it to fire at it far longer than a conventional attack aircraft. The AC-130H Spectre was armed with two 20 mm M61 Vulcan cannons, one Bofors 40mm autocannon, and one 105 mm M102 cannon, although on most missions after 1994 the 20 mm cannons were removed due to their incompatibility with precision targeting and to carry more 40 mm and 105 mm ammunition. Another reason the 20 mm cannons were removed was due to insufficient slant range to target to operate outside of the shoulder launched missile threat envelope. The upgraded AC-130U "Spooky" has a single 25 mm GAU-12 Equalizer in place of the Spectre's twin 20 mm cannons, an improved fire control system, and increased ammunition capacity. New AC-130J gunships based on MC-130J Combat Shadow II special operations tankers were planned . The AC-130W is armed with one 30 mm Bushmaster cannon and can release the AGM-176 Griffin missile.
Development.
Origins.
During the Vietnam War, the C-130 Hercules was selected to replace the Douglas AC-47 Spooky gunship (Project Gunship I) in order to improve mission endurance and increase capacity to carry munitions. Capable of flying faster than helicopters and at high altitudes with excellent loiter time, the use of the pylon turn allowed the AC-47 to deliver continuous accurate fire to a single point on the ground.
In 1967, JC-130A USAF 54-1626 was selected for conversion into the prototype AC-130A gunship (Project Gunship II). The modifications were done at Wright-Patterson Air Force Base by the Aeronautical Systems Division. A direct view night vision telescope was installed in the forward door, an early forward looking infrared (FLIR) in the forward part of the left wheel well, and Gatling guns fixed facing down and aft along the left side. The analog fire control computer prototype was handcrafted by RAF Wing Commander Tom Pinkerton at the USAF Avionics Laboratory at Wright-Patterson AFB. Flight testing of the prototype was performed primarily at Eglin Air Force Base, followed by further testing and modifications. By September 1967, the aircraft was certified ready for combat testing and was flown to Nha Trang Air Base, South Vietnam for a 90-day test program. The AC-130 was later supplemented by the AC-119 Shadow (Project Gunship III), which later proved to be underpowered.
Seven more warplanes were converted to the "Plain Jane" configuration like the AC-130 prototype in 1968, and one aircraft received the "Surprise Package" equipment in 1969. Surprise Package included the latest 20 mm rotary cannons and 40 mm Bofors cannon but no 7.62 mm close support armament. Surprise Package served as a test bed for the avionic systems and armament for the AC-130E.
In 1970, ten more AC-130As were acquired under the "Pave Pronto" project. In the summer of 1971, Surprise Package equipped AC-130s were converted to the Pave Pronto configuration and assumed their new nickname 'Thor'. Conversion of C-130Es into AC-130Es for the "PAVE Spectre" project followed.
Regardless of their project names the aircraft were more commonly referred to by the squadron's call sign 'Spectre'.
Recent and planned upgrades.
In 2007, Air Force Special Operations Command (AFSOC) initiated a program to upgrade the armament of AC-130s. The test program planned for the 25 mm GAU-12/U and 40 mm Bofors cannon on the AC-130U gunships to be replaced with two 30 mm Mk 44 Bushmaster II cannon. In 2007, the Air Force modified four AC-130U gunships as test platforms for the Bushmasters. These were referred to as AC-130U Plus 4 or AC-130U+4. AFSOC, however, canceled its plans to install the new cannons on its fleet of AC-130Us. It has since removed the guns and re-installed the original 40 mm and 25mm cannons and returned the planes to combat duty. Brigadier General Bradley A. Heithold, AFSOC's director of plans, programs, requirements, and assessments, said on 11 August 2008 that the effort was canceled because of problems with the Bushmaster's accuracy in tests "at the altitude we were employing it". There were also schedule considerations that drove the decision, he said.
There were also plans to possibly replace the 105 mm cannon with a breech-loading 120 mm M120 mortar, and to give the AC-130 a standoff capability using either the AGM-114 Hellfire missile, the Advanced Precision Kill Weapon System (based on the Hydra 70 rocket), or the Viper Strike glide bomb.
The Air Force awarded L-3 Communications a $61 million contract to add weapons packages to eight MC-130W Combat Spear special-mission aircraft to give them a gunship-like attack capability. L-3 will provide weapons kits, named "precision strike packages", for installation on the aircraft at Warner Robins Air Logistics Center, Georgia. MC-130Ws fitted with the weapons will be known as "Dragon Spears". Air Force Special Operations Command is arming these aircraft to relieve the high operational demands on its regular AC-130 gunships until new AC-130Js enter the fleet. The MC-130W Dragon Spear was renamed the AC-130W Stinger II in 2011.
The Air Force launched an initiative in 2011 to acquire 16 new gunships based on new-built MC-130J Combat Shadow II special operations tankers outfitted with a "precision strike package" to give them an attack capability. The Air Force is requesting $1.6 billion from Fiscal 2011 through 2015 for this recapitalization. These aircraft would increase the size of the Air Force's gunship fleet to 33 aircraft, a net increase of eight after the planned retirement of eight aging AC-130Hs. The first aircraft would be bought in Fiscal 2012, followed by two in Fiscal 2013, five in Fiscal 2014, and the final eight in Fiscal 2015. The decision to remain with the C-130s to fill the need came after funding to acquire 16 C-27Js was removed from the fiscal 2010 budget. The AC-130J will follow the path of the Dragon Spear program, along lines generally similar to the USMC Harvest HAWK program. On January 9, 2013, the Air Force began converting the first MC-130J Combat Shadow II into the new AC-130J Ghostrider. It is expected to be completed in November 2013 with flight testing by December 2013. The first AC-130J is to enter service in 2017.
Design.
Overview.
The AC-130 is a heavily armed long-endurance aircraft carrying an array of anti-ground oriented weapons that are integrated with sophisticated sensors, navigation, and fire control systems. It is capable of delivering precision firepower or area-saturation fire over a target area over a long period of time, at night or in adverse weather. The sensor suite consists of a television sensor, infrared sensor, and radar. These sensors allow the gunship to visually or electronically identify friendly ground forces and targets in most weather conditions.
The AC-130U is equipped with the AN/APQ-180, a synthetic aperture radar for long-range target detection and identification. The gunship's navigational devices include inertial navigation systems and a Global Positioning System. The AC-130U employs technologies developed in the 1990s which allow it to attack two targets simultaneously. It has twice the munitions capacity of the AC-130H. Although the AC-130U conducts some operations in daylight, most of its combat missions are conducted at night. The AC-130H's unit cost is US$132.4 million, and the AC-130U's cost is US$190 million (fiscal 2001 dollars).
Upgrades.
During the Vietnam era, the various AC-130 versions following the Pave Pronto modifications were equipped with a magnetic anomaly detector (MAD) system called the Black Crow (AN/ASD-5), a highly sensitive passive device with a phased-array antenna located in the left-front nose radome that could pick up localized deviations in earth's magnetic field that is normally used to detect submerged submarines. The Black Crow system was slaved into the targeting computers of the AC-130A/E/H, enabling the detection of the unshielded ignition coils of North Vietnamese trucks hidden under dense jungle foliage, typical along the Ho Chi Minh trail. It could also detect hand-held transmitter signals of air controllers on the ground to identify and locate targets.
The PGM-38/U Enhanced 25 mm High Explosive Incendiary (HEI) round was created to expand the AC-130U gunships' mission in standoff range and survivability for its 25 mm GAU-12/U gun system. This round is a combination of the existing PGU-25 HEI and a M758 fuze designated as FMU-151/B to meet the MIL-STD-1316. The FMU-151 has an improved arming delay with multi-sensitive range.
Operational history.
Vietnam War.
The AC-130 gunship first arrived in South Vietnam on 21 September 1967 under the Gunship II program and began combat operations over Laos and South Vietnam that year. In June 1968, AC-130s were deployed to Tan Son Nhut AB near Saigon for support against the Tet Offensive. By 30 October 1968 enough AC-130 Gunship IIs arrived to form a squadron, the 16th Special Operations Squadron (SOS) of the 8th Tactical Fighter Wing (TFW), at Ubon Royal Thai Air Force Base, Thailand. It was at this time that the C-130A gunship was designated the AC-130A.
On 18 August 1968, an AC-130 gunship flying an armed reconnaissance mission in Vietnam's III Corps was diverted to support a Special Forces base at Katum. The ground commander quickly assessed the accurate fire and capabilities of this weapon system and called for fire on his own perimeter when the Viet Cong attempted to bridge the wire on the west side of his position.
By December 1968 most AC-130s flew under F-4 Phantom II escort (to protect the gunship against heavy and concentrated AAA fire) from the 497th Tactical Fighter Squadron, normally three Phantoms per Gunship. In late 1969, under the code name of "Surprise Package", 56-0490 arrived with solid-state laser-illuminated low-light-level-TV with a companion YAG laser designator, an improved forward looking infrared (FLIR) sensor, video recording for TV and FLIR, an inertial navigation system, and a prototype digital fire control computer. The remaining AC-130s were refitted with upgraded similar equipment in the summer of 1970, and then redeployed to Ubon RTAFB. On 25 October 1971, the first "Cadillac" gunship, the AC-130E arrived in Vietnam. On 17 February 1972, the first 105 mm cannon arrived for service with Spectre and was installed on Gunship 570. It was used from mid-February until the aircraft received battle damage to its right flap. The 105 was switched to Gunship 571 and was used until 30 March when the aircraft was shot down.
On 28 January 1973, the Vietnam peace accord went into effect, marking the end of Spectre operations in Vietnam. Spectre was still needed and active in the region, supporting operations in Laos and Cambodia. On 22 February 1973, American offensive operations in Laos ended and the gunships became totally committed to operations in the Cambodian conflict.
On 12 April 1975 Khmer Rouge were threatening the capital of Phnom Penh and AC-130s were called upon to help in Operation Eagle pull, the final evacuation of American and allied officials from Phnom Penh before it fell to the communists. The AC-130 was also over Saigon 30 April 1975 to protect the final evacuation in Operation Frequent Wind. When the SS Mayaguez was seized by Khmer Rouge soldiers and sailors on 15 May 1975, on the open sea, Spectres were called upon.
AC-130s destroyed more than 10,000 trucks and participated in many crucial close air support missions in Vietnam.
Six Spectres and 52 aircrew members were lost to enemy fire. On 24 May 1969, Spectre lost its first gunship.
Cold War and later action.
With the conclusion of hostilities in Southeast Asia in the mid-1970s, the AC-130H became the sole gunship in the regular Air Force, home based at Hurlburt Field, Florida, while the AC-130A fleet was transferred to the Air Force Reserve's 919th Tactical Airlift Group (919 TAG) at Eglin AFB Auxiliary Field #3/Duke Field, Florida. With the transition to the AC-130A, the 919 TAG was then redesignated as the 919th Special Operations Group (919 SOG).
In the late 1970s when the AC-130H fleet was first being modified for in-flight refueling capability, a demonstration mission was planned and flown from Hurlburt Field, Florida, non-stop, to conduct a 2-hour live-fire mission over Empire Firing Range in the Republic of Panama, then return home. This 13-hour mission with two in-flight refuelings from KC-135 tankers proved the validity of flying long-range missions outside the contiguous United States to attack targets then return to home base without intermediate stops.
AC-130s from both the 4th and 16th Special Operations Squadrons have been deployed in nearly every conflict the United States has been involved in, officially and unofficially, since the end of the Vietnam War.
In July 1979, AC-130H crews deployed to Howard Air Force Base, Panama, as a precaution against possible hostile actions against American personnel during the Nicaraguan Revolution. New time aloft and non-stop distance records were subsequently set by a 16th SOS 2-ship AC-130H formation flight that departed Hurlburt Field on 13 November 1979 and landed on 15 November at Andersen Air Force Base, Guam, a distance of and 29 hours 43 minutes non-stop, refueling four times in-flight. Refueling support for the Guam deployment was provided by KC-135 crews from the 305th Air Refueling Wing from Grissom AFB, Indiana.
In November 1979, four AC-130H gunships flew nonstop from Hurlburt Field to Anderson AFB, Guam, because of the hostage situation at the Embassy in Iran. At Guam, AC-130H crews developed communications-out/lights-out refueling procedures for later employment by trial-and-error. This deployment with the 1 SOW/CC as Task Force commander was directed from the office of the CJCS for fear that Iranian militants could begin executing American Embassy personnel who had been taken hostage on 4 November. One early option considered AC-130H retaliatory punitive strikes deep within Iran. Later gunship flights exceeded the 1979 Hurlburt-to-Guam flight. Upon return in March 1980, the four planes soon found themselves in Egypt to support the ill-fated hostage rescue attempt.
During Operation Urgent Fury in Grenada in 1983, AC-130s suppressed enemy air defense systems and attacked ground forces enabling the assault of the Point Salines Airfield via airdrop and air-land of friendly forces. The AC-130 aircrew earned the Lieutenant General William H. Tunner Award for the mission.
The AC-130Hs of the 16th Special Operations Squadron unit maintained an ongoing rotation to Howard AB, Panama, monitoring activities in El Salvador and other Central American points of interest, with rules of engagement eventually permitting attacks on FMLN targets. This commitment of Maintainers and crews started in 1983 and lasted until 1990. The AC-130 is considered to have hastened the end of the Salvadoran Civil War in the 1980s. Crews flew undercover missions from Honduras and attacked guerrilla camps and concentrations.
AC-130s also had a primary role during the United States invasion of Panama (named Operation Just Cause) 1989 when they destroyed Panama Defense Force headquarters and numerous command-and-control facilities, and provided close air support for US ground troops. Aircrews earned the Mackay Trophy for the most meritorious flight of the year, and the Tunner Award.
Persian Gulf War and the 1990s.
During the Gulf War of 1990–91 (Operations Desert Shield and Desert Storm), Regular Air Force and Air Force Reserve AC-130s provided close air support and force protection (air base defense) for ground forces, and battlefield interdiction. The primary interdiction targets were early warning/ground control intercept (EW/GCI) sites along the southern border of Iraq. The first gunship to enter the Battle of Khafji helped stop a southbound Iraqi armored column on 29 January 1991. One day later three more gunships provided further aid to Marines participating in the operation. The gunships attacked Iraqi positions and columns moving south to reinforce their positions north of the city.
Despite the threat of surface-to-air missiles (SAMs) and increasing visibility during the early morning hours of 31 January 1991, one AC-130H, AF Serial No. 69-6567, call-sign Spirit 03, opted to stay to continue to protect the Marines. A lone Iraqi with a Strela-2 MANPAD shot Spirit 03 down, and all 14 crew members died.
The military has used AC-130 gunships during the humanitarian operations in Somalia (Operation Restore Hope and Operation United Shield) in 1992–93, Operation Uphold Democracy in Haiti in 1994. AC-130s took part in Operation Assured Response in Liberia in 1996 and in Operation Silver Wake in 1997, the evacuation of American non-combatants from Albania.
AC-130s took part in the NATO missions in Bosnia and Herzegovina and Kosovo during the 1990s.
The AC-130U gunship set a new record for the longest sustained flight by any C-130 on 22 and 23 October 1997, when two AC-130U gunships flew 36 hours nonstop from Hurlburt Field, Florida to Taegu Air Base (Daegu), South Korea, being refueled seven times in the air by KC-135 tankers. The two gunships took on 410,000 lb (186,000 kg) of fuel. Gunships also were part of the buildup of U.S. forces in 1998 to compel Iraq to allow UNSCOM weapons inspections.
Operations since 2001.
The United States used gunships during War in Afghanistan (Operation Enduring Freedom) in Afghanistan (2001–), and Iraq War (Operation Iraqi Freedom) in Iraq (2003–10). In 2007, US Special Operations forces also used the AC-130 in attacks on suspected al-Qaeda militants in Somalia.
Close air support was the main mission of the AC–130 in Iraq. Night after night, at least one AC–130 was in the air to fulfill one or more air support requests (ASRs). A typical mission had the AC–130 supporting a single brigade’s ASRs followed by aerial refueling and another 2 hours with another brigade or SOF team. The use of AC-130s in places like Fallujah, urban settings where insurgents were among crowded populations of non combatants, was criticized by human rights groups. AC-130s were also used for intelligence gathering with their sophisticated long-range video, infrared and radar sensors.
AC-130 strikes were directed by special forces on known Taliban locations during the early days of the war in Afghanistan. U.S. Special Operations Forces are using the AC-130 to support its operations. The day after arriving in Afghanistan, the AC-130s attacked Taliban and Al-Qaeda forces near the city of Konduz and were directly responsible for the city's surrender the next day. On 26 November 2001, AC-130 Spectres were called in to put down a rebellion at the prison fort of Qala-i-Janghi. The 16 SOS flew missions over Mazar-i-Sharif, Konduz, Kandahar, Shkin, Asadabad, Bagram, Baghran, Tora Bora, and virtually every other part of Afghanistan. Spectre participated in countless operations within Afghanistan, performing on-call close air support and armed reconnaissance.
In March 2002, three AC-130 Spectres provided 39 crucial combat missions in support of Operation Anaconda in Afghanistan. During the intense fighting, the planes expended more than 1,300 40 mm and 1,200 105 mm rounds.
There are eight AC-130H and seventeen AC-130U aircraft in active-duty service as of July 2010.
In March 2011, the U.S. Air Force deployed two AC-130U gunships to take part in Operation Odyssey Dawn, the U.S. military intervention in Libya, which eventually came under NATO as Operation Unified Protector.
As of September 2013, 14 MC-130W Dragon Spear aircraft have been converted to AC-130W Stinger II gunships. The Stinger gunships have been deployed to Afghanistan to replace the aging AC-130H and provide an example for the new AC-130J Ghostrider. Modifications began with crews cutting holes in the plane to make room for weapons, and adding kits and bomb bases for laser-guided munitions. Crews added a 105 mm cannon, 20 in infrared and electro-optical sensors, and the ability to carry 250-pound bombs on the wings.
Operators.
United States Air Force
Air Force Reserve
Air Force Systems Command
Air Force Materiel Command
Aircraft on display.
One of the first seven AC-130A aircraft deployed to Vietnam was AF Serial No. 53-3129, named "First Lady" in November 1970. This aircraft was a conversion of the first production C-130. On 25 March 1971, it took an anti-aircraft artillery hit in the belly just aft of the nose gear wheel well over the Ho Chi Minh trail in Laos. The 37 mm shell destroyed everything below the crew deck and barely missed striking two crew members. In 1975, after the conclusion of US involvement in the Vietnam war, it was transferred to the Air Force Reserve, where it served with the 711th Special Operations Squadron of the 919th Special Operations Wing. In 1980, the aircraft was upgraded from the original three-bladed propellers to the quieter four-bladed propellers and was eventually retired in late 1995. The retirement also marked an end to the Air Force Reserve Command flying the AC-130A. The aircraft now sits on display in the final Air Force Reserve Command configuration with grey paint, black markings, and the four-bladed Hamilton Sunstrand 54H60-91 props at the Air Force Armament Museum at Eglin Air Force Base, Florida, USA.
A second AC-130A, AF Serial No. 56-0509, named the "Ultimate End", was accepted by the Air Force on 28 February 1957, and modified to the AC-130A configuration on 27 July 1970. The aircraft participated in the Vietnam War and the rescue of the SS Mayaguez. "Ultimate End" demonstrated the durability of the C-130 after surviving hits in five places by 37 mm anti-aircraft artillery on 12 December 1970, extensive left wing leading edge damage on 12 April 1971 and a 57 mm round damaging the belly and injuring one crewman on 4 March 1972. "Ultimate End" was reassigned to the Air Force Reserve's 919th Special Operations Wing at Eglin AFB Auxiliary Field No.3 / Duke Field on 17 June 1975, where it continued in service until retired in the fall 1994 and transferred to Air Force Special Operations Command's "Heritage Air Park" at Hurlburt Field, Florida. While assigned to the 711th Special Operations Squadron, "Ultimate End" served in Operations JUST CAUSE in Panama, DESERT STORM in Kuwait and Iraq, and UPHOLD DEMOCRACY in Haiti. After 36 years and seven months of service, 24 years as a gunship, "Ultimate End" retired from active service on 1 October 1994. It made its last flight from Duke Field to Hurlburt Field on 20 October 1994. The Spectre Association dedicated "Ultimate End" (which served with the 16 SOS in Vietnam) on 4 May 1995. Lt Col Michael Byers, then 16 SOS commander, represented the active-duty gunship force and Clyde Gowdy of the Spectre Association represented all Spectre personnel past and present for the unveiling of a monument at the aircraft and the dedication as a whole.
A third AC-130A, AF Serial No. 54-1630, is on display in the Cold War Gallery at the National Museum of the United States Air Force at Wright-Patterson AFB, Ohio. Named "Azrael" for the angel of death in Islam who severs the soul from the body. This aircraft figured prominently in the closing hours of Operation Desert Storm. On 26 February 1991, Coalition ground forces were driving the Iraqi Army out of Kuwait. With an Air Force Reserve crew called to active duty, Azrael was sent to the Al Jahra highway (Highway 80) between Kuwait City and Basra, Iraq, to intercept the convoys of tanks, trucks, buses, and cars fleeing the battle. Facing SA-6 and SA-8 surface-to-air missiles and 37 mm and 57 mm radar-guided anti-aircraft artillery the crew attacked and destroyed or disabled most of the convoys. "Azrael" was also assigned to the 919th Special Operations Wing and retired to the museum in October 1995.
Another AC-130A, AF Serial No. 54-1626, the original prototype AC-130 named "Gunship II" is on display at the outdoor Air Park at the National Museum of the United States Air Force at Wright-Patterson AFB, Ohio. This aircraft served in Southeast Asia from 1967 to 1972, then served in JC-130A test configuration. It was transferred to the National Museum of the United States Air Force in 1976, and converted back to AC-130A configuration in the late 1990s.
AC-130A Serial No. 54-1623, c/n 3010, named "Ghost Rider" served in Southeast Asia and later conflicts until being retired in 1997 to Dobbins AFB, Georgia. Ghost Rider eventually was transferred and displayed at the Lockheed Museum at Marietta, Georgia.

</doc>
<doc id="3158" url="http://en.wikipedia.org/wiki?curid=3158" title="Alternative">
Alternative

Alternative may refer to:

</doc>
<doc id="3160" url="http://en.wikipedia.org/wiki?curid=3160" title="Alternative algebra">
Alternative algebra

In abstract algebra, an alternative algebra is an algebra in which multiplication need not be associative, only alternative. That is, one must have
for all "x" and "y" in the algebra. Every associative algebra is obviously alternative, but so too are some strictly non-associative algebras such as the octonions. The sedenions, on the other hand, are not alternative.
The associator.
Alternative algebras are so named because they are precisely the algebras for which the associator is alternating. The associator is a trilinear map given by
By definition a multilinear map is alternating if it vanishes whenever two of its arguments are equal. The left and right alternative identities for an algebra are equivalent to
Both of these identities together imply that the associator is totally skew-symmetric. That is,
for any permutation σ. It follows that
for all "x" and "y". This is equivalent to the "flexible identity"
The associator of an alternative algebra is therefore alternating. Conversely, any algebra whose associator is alternating is clearly alternative. By symmetry, any algebra which satisfies any two of:
is alternative and therefore satisfies all three identities.
An alternating associator is always totally skew-symmetric. The converse holds so long as the characteristic of the base field is not 2.
Properties.
Artin's theorem states that in an alternative algebra the subalgebra generated by any two elements is associative. Conversely, any algebra for which this is true is clearly alternative. It follows that expressions involving only two variables can be written without parenthesis unambiguously in an alternative algebra. A generalization of Artin's theorem states that whenever three elements formula_12 in an alternative algebra associate (i.e. formula_13) the subalgebra generated by those elements is associative.
A corollary of Artin's theorem is that alternative algebras are power-associative, that is, the subalgebra generated by a single element is associative. The converse need not hold: the sedenions are power-associative but not alternative.
The Moufang identities
hold in any alternative algebra.
In a unital alternative algebra, multiplicative inverses are unique whenever they exist. Moreover, for any invertible element formula_17 and all formula_18 one has
This is equivalent to saying the associator formula_20 vanishes for all such formula_17 and formula_18. If formula_17 and formula_18 are invertible then formula_25 is also invertible with inverse formula_26. The set of all invertible elements is therefore closed under multiplication and forms a Moufang loop. This "loop of units" in an alternative ring or algebra is analogous to the group of units in an associative ring or algebra.
Applications.
The projective plane over any alternative division ring is a Moufang plane.
The close relationship of alternative algebras and composition algebras was given by Guy Roos in 2008: He shows (page 162) the relation for an algebra "A" with unit element "e" and an involutive anti-automorphism formula_27 such that "a" + "a"* and "aa"* are on the line spanned by "e" for all "a" in "A". Use the notation "n"("a") = "aa"*. Then if "n" is a non-singular mapping into the field of "A", and "A" is alternative, then ("A,n") is a composition algebra.

</doc>
<doc id="3162" url="http://en.wikipedia.org/wiki?curid=3162" title="Arbitrage">
Arbitrage

In economics and finance, arbitrage () is the practice of taking advantage of a price difference between two or more markets: striking a combination of matching deals that capitalize upon the imbalance, the profit being the difference between the market prices. When used by academics, an arbitrage is a transaction that involves no negative cash flow at any probabilistic or temporal state and a positive cash flow in at least one state; in simple terms, it is the possibility of a risk-free profit after transaction costs. For instance, an arbitrage is present when there is the opportunity to instantaneously buy low and sell high.
In principle and in academic use, an arbitrage is risk-free; in common use, as in statistical arbitrage, it may refer to "expected" profit, though losses may occur, and in practice, there are always risks in arbitrage, some minor (such as fluctuation of prices decreasing profit margins), some major (such as devaluation of a currency or derivative). In academic use, an arbitrage involves taking advantage of differences in price of a "single" asset or "identical" cash-flows; in common use, it is also used to refer to differences between "similar" assets (relative value or convergence trades), as in merger arbitrage.
People who engage in arbitrage are called arbitrageurs ()—such as a bank or brokerage firm. The term is mainly applied to trading in financial instruments, such as bonds, stocks, derivatives, commodities and currencies.
Arbitrage-free.
If the market prices do not allow for profitable arbitrage, the prices are said to constitute an arbitrage equilibrium or arbitrage-free market. An arbitrage equilibrium is a precondition for a general economic equilibrium. The "no arbitrage" assumption is used in quantitative finance to calculate a unique risk neutral price for derivatives.
Conditions for arbitrage.
Arbitrage is possible when one of three conditions is met:
Arbitrage is not simply the act of buying a product in one market and selling it in another for a higher price at some later time. The transactions must occur "simultaneously" to avoid exposure to market risk, or the risk that prices may change on one market before both transactions are complete. In practical terms, this is generally possible only with securities and financial products that can be traded electronically, and even then, when each leg of the trade is executed the prices in the market may have moved. Missing one of the legs of the trade (and subsequently having to trade it soon after at a worse price) is called 'execution risk' or more specifically 'leg risk'.
In the simplest example, any good sold in one market should sell for the same price in another. Traders may, for example, find that the price of wheat is lower in agricultural regions than in cities, purchase the good, and transport it to another region to sell at a higher price. This type of price arbitrage is the most common, but this simple example ignores the cost of transport, storage, risk, and other factors. "True" arbitrage requires that there be no market risk involved. Where securities are traded on more than one exchange, arbitrage occurs by simultaneously buying in one and selling on the other.
See rational pricing, particularly arbitrage mechanics, for further discussion.
Mathematically it is defined as follows:
where formula_2 and formula_3 denotes the portfolio value at time "t".
Price convergence.
Arbitrage has the effect of causing prices in different markets to converge. As a result of arbitrage, the currency exchange rates, the price of commodities, and the price of securities in different markets tend to converge. The speed at which they do so is a measure of market efficiency. Arbitrage tends to reduce price discrimination by encouraging people to buy an item where the price is low and resell it where the price is high (as long as the buyers are not prohibited from reselling and the transaction costs of buying, holding and reselling are small relative to the difference in prices in the different markets).
Arbitrage moves different currencies toward purchasing power parity. As an example, assume that a car purchased in the United States is cheaper than the same car in Canada. Canadians would buy their cars across the border to exploit the arbitrage condition. At the same time, Americans would buy US cars, transport them across the border, then sell them in Canada. Canadians would have to buy American dollars to buy the cars and Americans would have to sell the Canadian dollars they received in exchange. Both actions would increase demand for US dollars and supply of Canadian dollars. As a result, there would be an appreciation of the US currency. This would make US cars more expensive and Canadian cars less so until their prices were similar. On a larger scale, international arbitrage opportunities in commodities, goods, securities and currencies tend to change exchange rates until the purchasing power is equal.
In reality, most assets exhibit some difference between countries. These, transaction costs, taxes, and other costs provide an impediment to this kind of arbitrage. Similarly, arbitrage affects the difference in interest rates paid on government bonds issued by the various countries, given the expected depreciations in the currencies relative to each other (see interest rate parity).
Risks.
Arbitrage transactions in modern securities markets involve fairly low day-to-day risks, but can face extremely high risk in rare situations, particularly financial crises, and can lead to bankruptcy. Formally, arbitrage transactions have negative skew – prices can get a small amount closer (but often no closer than 0), while they can get very far apart. The day-to-day risks are generally small because the transactions involve small differences in price, so an execution failure will generally cause a small loss (unless the trade is very big or the price moves rapidly). The rare case risks are extremely high because these small price differences are converted to large profits via leverage (borrowed money), and in the rare event of a large price move, this may yield a large loss.
The main day-to-day risk is that part of the transaction fails – execution risk. The main rare risks are counterparty risk and liquidity risk – that a counterparty to a large transaction or many transactions fails to pay, or that one is required to post margin and does not have the money to do so.
In the academic literature, the idea that seemingly very low risk arbitrage trades might not be fully exploited because of these risk factors and other considerations is often referred to as limits to arbitrage.
Execution risk.
Generally it is impossible to close two or three transactions at the same instant; therefore, there is the possibility that when one part of the deal is closed, a quick shift in prices makes it impossible to close the other at a profitable price. However, this is not necessarily the case. Many exchanges and inter-dealer brokers allow multi legged trades (e.g. basis block trades on LIFFE).
Competition in the marketplace can also create risks during arbitrage transactions. As an example, if one was trying to profit from a price discrepancy between IBM on the NYSE and IBM on the London Stock Exchange, they may purchase a large number of shares on the NYSE and find that they cannot simultaneously sell on the LSE. This leaves the arbitrageur in an unhedged risk position.
In the 1980s, risk arbitrage was common. In this form of speculation, one trades a security that is clearly undervalued or overvalued, when it is seen that the wrong valuation is about to be corrected by events. The standard example is the stock of a company, undervalued in the stock market, which is about to be the object of a takeover bid; the price of the takeover will more truly reflect the value of the company, giving a large profit to those who bought at the current price—if the merger goes through as predicted. Traditionally, arbitrage transactions in the securities markets involve high speed, high volume and low risk. At some moment a price difference exists, and the problem is to execute two or three balancing transactions while the difference persists (that is, before the other arbitrageurs act). When the transaction involves a delay of weeks or months, as above, it may entail considerable risk if borrowed money is used to magnify the reward through leverage. One way of reducing the risk is through the illegal use of inside information, and in fact risk arbitrage with regard to leveraged buyouts was associated with some of the famous financial scandals of the 1980s such as those involving Michael Milken and Ivan Boesky.
Mismatch.
Another risk occurs if the items being bought and sold are not identical and the arbitrage is conducted under the assumption that the prices of the items are correlated or predictable; this is more narrowly referred to as a convergence trade. In the extreme case this is merger arbitrage, described below. In comparison to the classical quick arbitrage transaction, such an operation can produce disastrous losses.
Counterparty risk.
As arbitrages generally involve "future" movements of cash, they are subject to counterparty risk: if a counterparty fails to fulfill their side of a transaction. This is a serious problem if one has either a single trade or many related trades with a single counterparty, whose failure thus poses a threat, or in the event of a financial crisis when many counterparties fail. This hazard is serious because of the large quantities one must trade in order to make a profit on small price differences.
For example, if one purchases many risky bonds, then hedges them with CDSes, profiting from the difference between the bond spread and the CDS premium, in a financial crisis the bonds may default "and" the CDS writer/seller may itself fail, due to the stress of the crisis, causing the arbitrageur to face steep losses.
Liquidity risk.
Arbitrage trades are necessarily synthetic, "leveraged" trades, as they involve a short position. If the assets used are not identical (so a price divergence makes the trade temporarily lose money), or the margin treatment is not identical, and the trader is accordingly required to post margin (faces a margin call), the trader may run out of capital (if they run out of cash and cannot borrow more) and be forced to sell these assets at a loss even though the trades may be expected to ultimately make money. In effect, arbitrage traders synthesize a put option on their ability to finance themselves.
Prices may diverge during a financial crisis, often termed a "flight to quality"; these are precisely the times when it is hardest for leveraged investors to raise capital (due to overall capital constraints), and thus they will lack capital precisely when they need it most.
Types of arbitrage.
Spatial arbitrage.
Also known as Geographical arbitrage is the simplest form of arbitrage. In case of spatial arbitrage, an arbs (arbitrageurs) looks for pricing discrepancies across geographically separate markets. For example, there may be a bond dealer in Virginia offering a bond at 100-12/23 and a dealer in Washington is bidding 100-15/23 for the same bond. For whatever reason, the two dealers have not spotted the aberration in the prices, but the arbs does. The arb immediately buys the bond from the Virginia dealer and sells it to the Washington dealer.
Merger arbitrage.
Also called risk arbitrage, merger arbitrage generally consists of buying/holding the stock of a company that is the target of a takeover while shorting the stock of the acquiring company.
Usually the market price of the target company is less than the price offered by the acquiring company.
The spread between these two prices depends mainly on the probability and the timing of the takeover being completed as well as the prevailing level of interest rates.
The bet in a merger arbitrage is that such a spread will eventually be zero, if and when the takeover is completed. The risk is that the deal "breaks" and the spread massively widens.
Municipal bond arbitrage.
Also called "municipal bond relative value arbitrage", "municipal arbitrage", or just "muni arb", this hedge fund strategy involves one of two approaches.
Generally, managers seek relative value opportunities by being both long and short municipal bonds with a duration-neutral book. The relative value trades may be between different issuers, different bonds issued by the same entity, or capital structure trades referencing the same asset (in the case of revenue bonds). Managers aim to capture the inefficiencies arising from the heavy participation of non-economic investors (i.e., high income "buy and hold" investors seeking tax-exempt income) as well as the "crossover buying" arising from corporations' or individuals' changing income tax situations (i.e., insurers switching their munis for corporates after a large loss as they can capture a higher after-tax yield by offsetting the taxable corporate income with underwriting losses). There are additional inefficiencies arising from the highly fragmented nature of the municipal bond market which has two million outstanding issues and 50,000 issuers in contrast to the Treasury market which has 400 issues and a single issuer.
Second, managers construct leveraged portfolios of AAA- or AA-rated tax-exempt municipal bonds with the duration risk hedged by shorting the appropriate ratio of taxable corporate bonds. These corporate equivalents are typically interest rate swaps referencing Libor or SIFMA[http://www.bondmarkets.com/story.asp?id=1157. The arbitrage manifests itself in the form of a relatively cheap longer maturity municipal bond, which is a municipal bond that yields significantly more than 65% of a corresponding taxable corporate bond. The steeper slope of the municipal yield curve allows participants to collect more after-tax income from the municipal bond portfolio than is spent on the interest rate swap; the carry is greater than the hedge expense. Positive, tax-free carry from muni arb can reach into the double digits. The bet in this municipal bond arbitrage is that, over a longer period of time, two similar instruments—municipal bonds and interest rate swaps—will correlate with each other; they are both very high quality credits, have the same maturity and are denominated in U.S. dollars. Credit risk and duration risk are largely eliminated in this strategy. However, basis risk arises from use of an imperfect hedge, which results in significant, but range-bound principal volatility. The end goal is to limit this principal volatility, eliminating its relevance over time as the high, consistent, tax-free cash flow accumulates. Since the inefficiency is related to government tax policy, and hence is structural in nature, it has not been arbitraged away.
Note, however, that many municipal bonds are callable, and that this imposes substantial additional risks to the strategy.
Convertible bond arbitrage.
A convertible bond is a bond that an investor can return to the issuing company in exchange for a predetermined number of shares in the company.
A convertible bond can be thought of as a corporate bond with a stock call option attached to it.
The price of a convertible bond is sensitive to three major factors:
Given the complexity of the calculations involved and the convoluted structure that a convertible bond can have, an arbitrageur often relies on sophisticated quantitative models in order to identify bonds that are trading cheap versus their theoretical value.
Convertible arbitrage consists of buying a convertible bond and hedging two of the three factors in order to gain exposure to the third factor at a very attractive price.
For instance an arbitrageur would first buy a convertible bond, then sell fixed income securities or interest rate futures (to hedge the interest rate exposure) and buy some credit protection (to hedge the risk of credit deterioration).
Eventually what he'd be left with is something similar to a call option on the underlying stock, acquired at a very low price.
He could then make money either selling some of the more expensive options that are openly traded in the market or delta hedging his exposure to the underlying shares.
Depository receipts.
A depositary receipt is a security that is offered as a "tracking stock" on another foreign market. For instance a Chinese company wishing to raise more money may issue a depository receipt on the New York Stock Exchange, as the amount of capital on the local exchanges is limited. These securities, known as ADRs (American depositary receipt) or GDRs (global depository receipt) depending on where they are issued, are typically considered "foreign" and therefore trade at a lower value when first released. Many ADR's are exchangeable into the original security (known as fungibility) and actually have the same value. In this case there is a spread between the perceived value and real value, which can be extracted. Other ADR's that are not exchangeable often have much larger spreads. Since the ADR is trading at a value lower than what it is worth, one can purchase the ADR and expect to make money as its value converges on the original. However there is a chance that the original stock will fall in value too, so by shorting it one can hedge that risk.
Dual-listed companies.
A dual-listed company (DLC) structure involves two companies incorporated in different countries contractually agreeing to operate their businesses as if they were a single enterprise, while retaining their separate legal identity and existing stock exchange listings. In integrated and efficient financial markets, stock prices of the twin pair should move in lockstep. In practice, DLC share prices exhibit large deviations from theoretical parity. Arbitrage positions in DLCs can be set up by obtaining a long position in the relatively underpriced part of the DLC and a short position in the relatively overpriced part. Such arbitrage strategies start paying off as soon as the relative prices of the two DLC stocks converge toward theoretical parity. However, since there is no identifiable date at which DLC prices will converge, arbitrage positions sometimes have to be kept open for considerable periods of time. In the meantime, the price gap might widen. In these situations, arbitrageurs may receive margin calls, after which they would most likely be forced to liquidate part of the position at a highly unfavorable moment and suffer a loss. Arbitrage in DLCs may be profitable, but is also very risky.
A good illustration of the risk of DLC arbitrage is the position in Royal Dutch Shell—which had a DLC structure until 2005—by the hedge fund Long-Term Capital Management (LTCM, see also the discussion below). Lowenstein (2000) describes that LTCM established an arbitrage position in Royal Dutch Shell in the summer of 1997, when Royal Dutch traded at an 8 to 10 percent premium. In total $2.3 billion was invested, half of which long in Shell and the other half short in Royal Dutch (Lowenstein, p. 99). In the autumn of 1998 large defaults on Russian debt created significant losses for the hedge fund and LTCM had to unwind several positions. Lowenstein reports that the premium of Royal Dutch had increased to about 22 percent and LTCM had to close the position and incur a loss. According to Lowenstein (p. 234), LTCM lost $286 million in equity pairs trading and more than half of this loss is accounted for by the Royal Dutch Shell trade.
Private to public equities.
The market prices for privately held companies are typically viewed from a return on investment perspective (such as 25%), whilst publicly held and or exchange listed companies trade on a Price to earnings ratio (P/E) (such as a P/E of 10, which equates to a 10% ROI). Thus, if a publicly traded company specialises in the acquisition of privately held companies, from a per-share perspective there is a gain with every acquisition that falls within these guidelines. Exempli gratia, Berkshire Hathaway. A hedge fund that is an example of this type of arbitrage is Greenridge Capital, which acts as an angel investor retaining equity in private companies which are in the process of becoming publicly traded, buying in the private market and later selling in the public market. Private to public equities arbitrage is a term which can arguably be applied to investment banking in general. Private markets to public markets differences may also help explain the overnight windfall gains enjoyed by principals of companies that just did an initial public offering (IPO).
Regulatory arbitrage.
Regulatory arbitrage is where a regulated institution takes advantage of the difference between its real (or economic) risk and the regulatory position. For example, if a bank, operating under the Basel I accord, has to hold 8% capital against default risk, but the real risk of default is lower, it is profitable to securitise the loan, removing the low risk loan from its portfolio. On the other hand, if the real risk is higher than the regulatory risk then it is profitable to make that loan and hold on to it, provided it is priced appropriately. Regulatory arbitrage can result in parts of entire businesses being unregulated as a result of the arbitrage.
This process can increase the overall riskiness of institutions under a risk insensitive regulatory regime, as described by Alan Greenspan in his October 1998 speech on The Role of Capital in Optimal Banking Supervision and Regulation.
Regulatory Arbitrage was used for the first time in 2005 when it was applied by Scott V. Simpson, a partner at law firm Skadden, Arps, to refer to a new defence tactic in hostile mergers and acquisitions where differing takeover regimes in deals involving multi-jurisdictions are exploited to the advantage of a target company under threat.
In economics, regulatory arbitrage (sometimes, tax arbitrage) may be used to refer to situations when a company can choose a nominal place of business with a regulatory, legal or tax regime with lower costs. For example, an insurance company may choose to locate in Bermuda due to preferential tax rates and policies for insurance companies. This can occur particularly where the business transaction has no obvious physical location: in the case of many financial products, it may be unclear "where" the transaction occurs.
Regulatory arbitrage can include restructuring a bank by outsourcing services such as IT. The outsourcing company takes over the installations, buying out the bank's assets and charges a periodic service fee back to the bank. This frees up cashflow usable for new lending by the bank. The bank will have higher IT costs, but counts on the multiplier effect of money creation and the interest rate spread to make it a profitable exercise.
Example:
Suppose the bank sells its IT installations for 40 million USD. With a reserve ratio of 10%, the bank can create 400 million USD in additional loans (there is a time lag, and the bank has to expect to recover the loaned money back into its books). The bank can often lend (and securitize the loan) to the IT services company to cover the acquisition cost of the IT installations. This can be at preferential rates, as the sole client using the IT installation is the bank. If the bank can generate 5% interest margin on the 400 million of new loans, the bank will increase interest revenues by 20 million. The IT services company is free to leverage their balance sheet as aggressively as they and their banker agree to. This is the reason behind the trend towards outsourcing in the financial sector. Without this money creation benefit, it is actually more expensive to outsource the IT operations as the outsourcing adds a layer of management and increases overhead.
According to PBS Frontline's 2012 four-part documentary, "Money, Power, and Wall Street," regulatory arbitrage, along with asymmetric bank lobbying in Washington and abroad, allowed investment banks in the pre- and post-2008 period to continue to skirt laws and engage in the risky proprietary trading of opaque derivatives, swaps, and other credit-based instruments invented to circumvent legal restrictions at the expense of clients, government, and publics.
Due to the Affordable Care Act’s expansion of Medicaid coverage, one form of Regulatory Arbitrage can now be found when businesses engage in “Medicaid Migration”, a maneuver by which qualifying employees who would typically be enrolled in company health plans elect to enroll in Medicaid instead. These programs that have similar characteristics as insurance products to the employee, but have radically different cost structures, resulting in significant expense reductions for employers.
Telecom arbitrage.
Telecom arbitrage companies allow phone users to make international calls for free through certain access numbers. Such services are offered in the United Kingdom; the telecommunication arbitrage companies get paid an interconnect charge by the UK mobile networks and then buy international routes at a lower cost. The calls are seen as free by the UK contract mobile phone customers since they are using up their allocated monthly minutes rather than paying for additional calls.
Such services were previously offered in the United States by companies such as FuturePhone.com. These services would operate in rural telephone exchanges, primarily in small towns in the state of Iowa. In these areas, the local telephone carriers are allowed to charge a high "termination fee" to the caller's carrier in order to fund the cost of providing service to the small and sparsely populated areas that they serve. However, FuturePhone (as well as other similar services) ceased operations upon legal challenges from AT&T and other service providers.
Statistical arbitrage.
Statistical arbitrage is an imbalance in expected nominal values. A casino has a statistical arbitrage in every game of chance that it offers—referred to as the house advantage, house edge, vigorish or house vigorish.
The fall of Long-Term Capital Management.
Long-Term Capital Management (LTCM) lost 4.6 billion U.S. dollars in fixed income arbitrage in September 1998. LTCM had attempted to make money on the price difference between different bonds. For example, it would sell U.S. Treasury securities and buy Italian bond futures. The concept was that because Italian bond futures had a less liquid market, in the short term Italian bond futures would have a higher return than U.S. bonds, but in the long term, the prices would converge. Because the difference was small, a large amount of money had to be borrowed to make the buying and selling profitable.
The downfall in this system began on August 17, 1998, when Russia defaulted on its ruble debt and domestic dollar debt. Because the markets were already nervous due to the Asian financial crisis, investors began selling non-U.S. treasury debt and buying U.S. treasuries, which were considered a safe investment. As a result the price on US treasuries began to increase and the return began decreasing because there were many buyers, and the return (yield) on other bonds began to increase because there were many sellers (i.e. the price of those bonds fell). This caused the difference between the prices of U.S. treasuries and other bonds to increase, rather than to decrease as LTCM was expecting. Eventually this caused LTCM to fold, and their creditors had to arrange a bail-out. More controversially, officials of the Federal Reserve assisted in the negotiations that led to this bail-out, on the grounds that so many companies and deals were intertwined with LTCM that if LTCM actually failed, they would as well, causing a collapse in confidence in the economic system. Thus LTCM failed as a fixed income arbitrage fund, although it is unclear what sort of profit was realized by the banks that bailed LTCM out.
Etymology.
"Arbitrage" is a French word and denotes a decision by an arbitrator or arbitration tribunal. (In modern French, """" usually means referee or umpire.) In the sense used here it is first defined in 1704 by Mathieu de la Porte in his treatise "" as a consideration of different exchange rates to recognize the most profitable places of issuance and settlement for a bill of exchange ("".)

</doc>
<doc id="3165" url="http://en.wikipedia.org/wiki?curid=3165" title="ACF Fiorentina">
ACF Fiorentina

ACF Fiorentina, commonly referred to as simply Fiorentina, is a professional Italian football club from Florence, Tuscany. Founded by a merger in 1926 (refounded in 2002 following bankruptcy), Fiorentina have played at the top level of Italian football for the majority of their existence; only four clubs have played in more Serie A seasons.
Fiorentina have won two Italian Championships, in 1955–56 and again in 1968–69, as well as winning six Coppa Italia trophies and one Italian Super Cup. On the European stage, Fiorentina won the UEFA Cup Winners' Cup in 1960–61 and lost the final one year later. They finished runners-up in the 1956–57 European Cup, losing against Real Madrid, and also came close to winning the UEFA Cup, finishing as runners-up in the 1989–90 season.
Since 1931, the club have played at the Stadio Artemio Franchi, which currently has a capacity of 47,282. The stadium has used several names over the years and has undergone several renovations. Fiorentina are known widely by the nickname "Viola", a reference to their distinctive purple colours.
History.
Foundation to World War II.
Associazione Calcio Fiorentina was founded in the autumn of 1926 by local noble and National Fascist Party member Luigi Ridolfi, who initiated the merger of two older Florentine clubs, CS Firenze and PG Libertas. The aim of the merger was to give Florence a strong club to rival those of the more dominant Italian Football Championship sides of the time from Northwest Italy. Also influential was the cultural revival and rediscovery of "Calcio Fiorentino", an ancestor of modern football that was played by members of the Medici family.
After a rough start and three seasons in lower leagues, Fiorentina reached the Serie A in 1931. That same year saw the opening of the new stadium, originally named Giovanni Berta, after a prominent fascist, but now known as "Stadio Artemio Franchi". At the time, the stadium was a masterpiece of engineering, and its inauguration was monumental. To be able to compete with the best teams in Italy, Fiorentina strengthened their team with some new players, notably the Uruguayan Pedro Petrone, nicknamed "el Artillero". Despite enjoying a good season and finishing in fourth place, Fiorentina were relegated the following year, although they would return quickly to Serie A. In 1941 they won their first Coppa Italia, but the team were unable to build on their success during the 1940s because of World War II and other troubles.
First "scudetto" and '50–'60s.
In 1950, Fiorentina started to achieve consistent top-five finishes in the domestic league. The team consisted of great players such as well-known goalkeeper Giuliano Sarti, Sergio Cervato, Francesco Rosella, Guido Gratton, Giuseppe Chiappella and Aldo Scaramucci but above all, the attacking duo of Brazilian Julinho and Argentinian Miguel Montuori. This team won Fiorentina's first "scudetto" (Italian championship) in 1955–56, 12 points ahead of second-place Milan. Milan beat Fiorentina to top spot the following year, but more significantly Fiorentina became the first Italian team to play in a European Cup final, when a disputed penalty led to a 2–0 defeat at the hands of Alfredo Di Stéfano's Real Madrid.
Fiorentina were runners-up again in the three subsequent seasons. In the 1960–61 season the club won the Coppa Italia again and was also successful in Europe, winning the first Cup Winners' Cup against Rangers.
After several years of runner-up finishes, Fiorentina dropped away slightly in the 1960s, bouncing from fourth to sixth place, although the club won the Coppa Italia and the Mitropa Cup in 1966.
Second "scudetto" and '70s.
While the 1960s did result in some trophies and good Serie A finishes for Fiorentina, nobody believed that the club could challenge for the title. The 1968–69 season started with Milan as frontrunners, but on match day 7, they lost to Bologna and were overtaken by Gigi Riva's Cagliari. Fiorentina, after an unimpressive start, then moved to the top of the Serie A, but the first half of their season finished with a 2–2 draw against Varese, leaving Cagliari as outright league leader. The second half of the season was a three-way battle between the three contending teams, Milan, Cagliari, and Fiorentina. Milan fell away, instead focusing their efforts on the European Cup, and it seemed that Cagliari would retain top spot. After Cagliari lost against Juventus, however, Fiorentina took over at the top. The team then won all of their remaining matches, beating rivals Juve in Turin on the penultimate matchday to seal their second, and last, national title. In the European Cup competition the following year, Fiorentina had some good results, including a win in the Soviet Union against Dynamo Kyiv, but they were eventually knocked out in the quarter-finals after a 3–0 defeat in Glasgow to Celtic.
"Viola" players began the 1970s decade with "Scudetto" sewed on their breast, but the period was not especially fruitful for the team. After a fifth place finish in 1971, they finished in mid-table almost every year, even flirting with relegation in 1972 and 1978. The "Viola" did win the Anglo-Italian League Cup in 1974 and won the Coppa Italia again in 1975. The team consisted of young talents like Vincenzo Guerini and Moreno Roggi, who had the misfortune to suffer bad injuries, and above all Giancarlo Antognoni, who would later become an idol to Fiorentina's fans. The young average age of the players led to the team being called "Fiorentina Ye-Ye".
Pontello era.
In 1980, Fiorentina was bought by Flavio Pontello, who came from a rich house-building family. He quickly changed the team's anthem and logo, leading to some complaints by the fans, but he started to bring in high-quality players such as Francesco Graziani and Eraldo Pecci from Torino; Daniel Bertoni from Sevilla FC; Daniele Massaro from Monza; and a young Pietro Vierchowod from Sampdoria. The team was built around Giancarlo Antognoni, and in 1982, Fiorentina were involved in an exciting duel with rivals Juventus. After a bad injury to Antognoni, the league title was decided on the final day of the season when Fiorentina were denied a goal against Cagliari and were unable to win. Juventus won the title with a disputed penalty and the rivalry between the two teams erupted.
The following years were strange for Fiorentina, who vacillated between high finishes and relegation battles. Fiorentina also bought two interesting players, "El Puntero" Ramón Díaz and, most significantly, the young Roberto Baggio.
In 1990, Fiorentina fought to avoid relegation right up until the final day of the season, but did reach the UEFA Cup final, where they again faced Juventus. The Turin team won the trophy, but Fiorentina's "tifosi" once again had real cause for complaint: the second leg of the final was played in Avellino (Fiorentina's home ground was suspended), a city with a lot of Juventus' fans, and emerging star Roberto Baggio was sold to the rival team on the day of the final. Pontello, suffering from economic difficulties, was selling all the players and was forced to leave the club after serious riots in Florence's streets. The club was then acquired by the famous filmmaker Mario Cecchi Gori.
Cecchi Gori era: from Champions League to bankruptcy.
[[File:Gabriel batistuta.jpg|thumb|175x175px|
Gabriel Batistuta, the most prominent Fiorentina player of the 1990s]]
The first season under Cecchi Gori's ownership was one of stabilisation, after which the new chairman started to sign some good players like Brian Laudrup, Stefan Effenberg, Francesco Baiano and, most importantly, Gabriel Batistuta, who became an iconic player for the team during the 1990s. In 1993, however, Cecchi Gori died and was succeeded as chairman by his son, Vittorio. Despite a good start to the season, Cecchi Gori fired the coach, Luigi Radice, after a defeat against Atalanta, and replaced him with Aldo Agroppi. The results were dreadful: Fiorentina fell into the bottom half of the standings and were relegated on the last day of the season.
Claudio Ranieri was brought in as coach for the 1993–94 season, and that year, Fiorentina dominated Serie B, Italy's second division. Upon their return to Serie A, Ranieri put together a good team centred around new top scorer Batistuta, signing the young talent Rui Costa from Benfica and the new world champion Brazilian defender Márcio Santos. The former became an idol to Fiorentina fans, while the second disappointed and was sold after only a season. The "Viola" finished the season in 10th place.
The following season, Cecchi Gori bought other important players, namely Stefan Schwarz. The club again proved its mettle in cup competitions, winning the Coppa Italia against Atalanta and finishing joint-third in Serie A. In the summer, Fiorentina became the first non-national champions to win the Supercoppa Italiana, defeating Milan 2–1 at the San Siro.
Fiorentina's 1995–96 season was disappointing in the league, but they did reach the Cup Winners' Cup semi-final by beating Gloria Bistrita, Sparta Prague, and Benfica. The team lost the semi-final to the eventual winner of the competition, FC Barcelona (Away 1–1, Home 0–2). The season's main signings were Luís Oliveira and Andrei Kanchelskis, the latter of whom suffered a lot of injuries.
At the end of the season, Ranieri left Fiorentina for Valencia CF in Spain and Cecchi Gori appointed Alberto Malesani. Fiorentina played well but struggled against smaller teams, although they did manage to qualify for the UEFA Cup. Malesani left Fiorentina after only a season and was succeeded by Giovanni Trapattoni. With Trapattoni's expert guidance and Batistuta's goals, Fiorentina challenged for the title in 1998–99 but finished the season in third, earning them qualification for the Champions League. The following year was disappointing in Serie A, but "Viola" played some historical matches in the Champions League, beating Arsenal 1–0 at the old Wembley Stadium and Manchester United 2–0 in Florence. They were ultimately eliminated in the second group stage.
At the end of the season, Trapattoni left the club and was replaced by Turkish coach Fatih Terim. More significantly, however, Batistuta was sold to Roma, who eventually won the title the following year. Fiorentina played well in 2000–01 and stayed in the top half of Serie A, despite the resignation of Terim and the arrival of Roberto Mancini. They also won the Coppa Italia for the sixth and last time.
The year 2001 heralded major changes for Fiorentina, as the terrible state of the club's finances was revealed: they were unable to pay wages and had debts of around USD 50 million. The club's owner, Vittorio Cecchi Gori, was able to raise some more money, but even this soon proved to be insufficient resources to sustain the club. Fiorentina were relegated at the end of the 2001–02 season and went into judicially controlled administration in June 2002. This form of bankruptcy (sports companies cannot exactly fail in this way in Italy, but they can suffer a similar procedure) meant that the club was refused a place in Serie B for the 2002–03 season, and as a result effectively ceased to exist.
The 2000s: Della Valle era.
The club was promptly re-established in August 2002 as Associazione Calcio Fiorentina e Florentia Viola with shoe and leather entrepreneur Diego Della Valle as new owner and the club was admitted into Serie C2, the fourth tier of Italian football. The only player to remain at the club in its new incarnation was Angelo Di Livio, whose commitment to club's cause further endeared him to the fans. Helped by Di Livio and 30-goal striker Christian Riganò, the club won its Serie C2 group with considerable ease, which would normally have led to a promotion to Serie C1. Due to the bizarre "Caso Catania" (Catania Case), however, the club skipped Serie C1 and was admitted into Serie B, something that was only made possible by the Italian Football Federation's decision to resolve the Catania situation by increasing the number of teams in Serie B from 20 to 24 and promoting Fiorentina for "sports merits." In the 2003 off-season, the club also bought back the right to use the Fiorentina name and the famous shirt design, and re-incorporated itself as ACF Fiorentina. The club finished the 2003–04 season in sixth place and won the playoff against Perugia to return to top-flight football.
In their first season back in Serie A, however, the club struggled to avoid relegation, only securing survival on the last day of the season on head-to-head record against Bologna and Parma. In 2005, Della Valle decided to appoint Pantaleo Corvino as new sports director, followed by the appointment of Cesare Prandelli as head coach in the following season. The club made several signings during the summer transfer market, most notably Luca Toni and Sébastien Frey. This drastic move earned them a fourth place finish with 74 points and a Champions League qualifying round ticket. Toni scored 31 goals in 38 appearances, the first player to pass the 30-goal mark since Antonio Valentin Angelillo in the 1958–59 season, for which he was awarded the European Golden Boot. On 14 July 2006, however, Fiorentina were relegated to Serie B due to their involvement in the 2006 Serie A match fixing scandal and given a 12-point penalty. The team was reinstated to the Serie A on appeal, but with a 19-point penalty for the 2006–07 season. The team's UEFA Champions League place was also rescinded. After the start of the season, Fiorentina's penalisation was reduced from 19 points to 15 on appeal to the Italian courts. In spite of this penalty, they managed to secure a place in the UEFA Cup.
Despite Toni's departure to Bayern Munich, Fiorentina had a strong start to the 2007–08 season and were tipped by Italian national team coach Marcello Lippi, among others, as a surprise challenger for the "Scudetto", and although this form tailed off towards the middle of the season, the "Viola" managed to qualify for the Champions League. In Europe, the club reached the semi-final of the UEFA Cup, where they were ultimately defeated by Rangers on penalties. The 2008–09 season continued this success, a fourth place finish assuring Fiorentina's spot in 2010's Champions League playoffs. Their European campaign was also similar to that of the previous run, relegated to the 2008–09 UEFA Cup and were eliminated by AFC Ajax in the end.
In the 2009–10 season, Fiorentina started their domestic campaign strongly before steadily losing momentum and slipped to mid-table positions at the latter half of the season. In Europe, the team proved to be a surprise dark horse: after losing their first away fixture against Olympique Lyonnais, they staged a comeback with a five-match streak by winning all their remaining matches (including defeating Liverpool home and away). The "Viola" qualified as group champions, but eventually succumbed to Bayern Munich due to the away goals rule. This was controversial due to a mistaken refereeing decision by Tom Henning Øvrebø, who allowed a clearly offside goal for Bayern in the first leg. Bayern eventually finished the tournament as runners-up, making a deep run all the way to the final. The incident called into attention the possible implementation of video replays in football. Despite a good European run and reaching the semi-finals in the Coppa Italia, Fiorentina failed to qualify for Europe.
During this period, on 24 September 2009, Andrea Della Valle resigned from his position as Chairman of Fiorentina, and announced all duties would be temporarily transferred to Mario Cognini, Fiorentina's Vice-President until a permanent position could be filled.
The 2010s: Post-Prandelli Era.
In June 2010, the "Viola" bid farewell to long-time manager Prandelli (by then the longest-serving coach in the team's history), who was departing to coach the Italian national team. Catania's young Siniša Mihajlović was appointed to replace him. Come the 2010–11 Serie A, despite not having the distraction of a European competition, Fiorentina spent the early weeks of the season in last place. This season was plagued with a widespread injury problems that saw most of the starting XI sidelined for significant portions of the year, particularly first-choice goalkeeper Sébastien Frey, who ended his season early, and star man Stevan Jovetić, who missed the season entirely. The team's form later improved and the "Viola" finished ninth.
In spite of pressures, Mihajlović remained at the helm for the 2011–12 season. At this point pillars from the Prandelli era were allowed to leave, including Adrian Mutu and Sébastien Frey. With these apparent signs of decline, coupled with mediocre performances in the league, pressures were again mounting, as the club looked no closer of achieving the immediate post-Prandelli objective of returning to Europe. Following a 1–0 defeat to Chievo in November, Mihajlović was sacked and replaced by Delio Rossi.
Supporters warmed to Rossi, but after a brief period of improvements, the "Viola" was again dragged down to battle the relegation dogfights. By early 2012, the team had also lost striker Alberto Gilardino and sporting director Pantaleo Corvino—the latter was sacked following a 0–5 home defeat to Juventus. With top player Riccardo Montolivo set to leave on a Bosman in the summer, Fiorentina steadily moved away from their 2009 side. Despite poor records, their bid for survival was kept alive by a number of upset victories away from home, particularly at Roma and Milan—the latter was significant as it allowed Juventus to leapfrog Milan to the top of the table, where they remained for the remainder of the season. The lowest point of the campaign was the home game against Novara: trailing 0–2 within half an hour, manager Rossi decided to substitute midfielder Adem Ljajić early. The latter sarcastically applauded him in frustration, but Rossi retaliated by physical assault. Montolivo's second-half brace salvaged a point by making it 2–2, and the team was faced with needing one point to avoid the drop with only two matches to go. Despite the obvious crisis that would have resulted in firing a manager at this delicate stage, the club was forced to sack Rossi following the Ljajić incident. In the next match away at Lecce (who were also battling relegation), Fiorentina secured survival under caretaker manager Vincenzo Guerini by winning it 1–0 courtesy of a lone Alessio Cerci goal. They finished thirteenth.
2012–13 season.
To engineer a resurrection, the Della Valle family invested heavily in the summer of 2012. Sporting director Daniele Prade, appointed earlier as Corvino's successor, staged a major overhaul of the team's roster. Arrivals of particular note were new coach Vincenzo Montella, goalkeeper Emiliano Viviano, and a quartet of playmakers: Matías Fernández, Borja Valero, Alberto Aquilani, and David Pizarro. The scale of the turnaround was so large that 17 out of the 26 senior players were new players, 9 of which are among the starting XI. By the end of the transfer window, only two players, Jovetić and Manuel Pasqual, remained from the Champions League class of 2009.
The effects were immediate, as the Florentine side raced out of the gates, finishing the calendar year in joint third place. The January transfer window saw them secure the services of Villarreal CF striker Giuseppe Rossi, the third Villarreal player signed during the season. They finished the season in fourth place, agonizingly close to the final Champions League spot, which was won on the final day by Milan. La Viola will however compete in the 2013–14 UEFA Europa League, and have high hopes for improving on their impressive maiden season under Vincenzo Montella.
2013–14 season.
So far during the summer of 2013 Fiorentina have continued strengthening the squad, adding former Spanish international Joaquín from Málaga, as well as Marcos Alonso, another Spaniard, from English Championship club Bolton Wanderers. La Viola also added Oleksandr Yakovenko from Belgian club Anderlecht, Marko Bakić from Torino and Gustavo Munúa from Levante. Returning from loans are Mattia Cassani, Juan Manuel Vargas, and Rubén Olivera. Fiorentina also signed star striker Mario Gómez from Bayern Munich for €20 million, while losing Stevan Jovetić to Premier League club Manchester City. Luca Toni also left the club, for Serie A newcomers Hellas Verona. Haris Seferović was sold to Real Sociedad in La Liga.
Fiorentina will start their Europa League campaign in the play-off round, having finished 4th in Serie A the previous season.
Managerial history.
Fiorentina have had many managers and head coaches throughout their history. Below is a chronological list from the club's foundation in 1926 to the present day.
Club strip.
Badge.
The official emblem of the city of Florence, a red fleur-de-lis on a white field, has been pivotal in the all-round symbolism of the club.
Over the course of the club's history, they have had several badge changes, all of which incorporated Florence's fleur-de-lis in some way. The first one was nothing more than the city's coat of arms, a white shield with the red fleur-de-lis inside. It was soon changed to a very stylised fleur-de-lis, always red, and sometimes even without the white field. The most common symbol, adopted for about twenty years, had been a white lozenge with the flower inside. During the season they were Italian champions, the lozenge disappeared and the flower was overlapped with the "scudetto".
The logo introduced by owner Flavio Pontello in 1980 was particularly distinct, consisting of one-half of the city of Florence's emblem and one-half of the letter "F", for Fiorentina. People disliked it when it was introduced, believing it was a commercial decision and, above all, because the symbol bore more of a resemblance to a halberd than a fleur-de-lis.
Today's logo is a kite shaped double lozenge bordered in gold. The outer lozenge has a purple background with the letters "AC" in white and the letter "F" in red, standing for the club's name. The inner lozenge is white with a gold border and the red fleur-de-lis of Florence. This logo had been in use from 1992 to 2002, but after the financial crisis and resurrection of the club the new one couldn't use the same logo. Florence's "comune" instead granted Florentia Viola use of the stylised coat of arms used in other city documents. Diego Della Valle acquired the current logo the following year in a judicial auction for a fee of €2.5 million, making it the most expensive logo in Italian football.
Kit and colours.
When Fiorentina was founded in 1926, the players wore red and white halved shirts derived from the colour of the city emblem. The more well-known and highly distinctive purple kit was adopted in 1928 and has been used ever since, giving rise to the nickname "La Viola" ("The Purple (team)"). Tradition has it that Fiorentina got their purple kit by mistake after an accident washing the old red and white coloured kits in the river.
The away kit has always been predominantly white, sometimes with purple and red elements, sometimes all-white. The shorts had been purple when the home kit was with white shorts. Fiorentina's third kit was first one in the 1995–96 season and it was all-red with purple borders and two lilies on the shoulders. The red shirt has been the most worn 3rd shirt by Fiorentina, although they also wore rare yellow shirts ('97–'98, '99–'00 and '10–'11) and a sterling version, mostly in the Coppa Italia, in 2000–01.
Honours.
National titles.
Serie A:
Coppa Italia:
Supercoppa Italiana:
Europeans titles.
European Cup:
UEFA Cup Winners' Cup:
UEFA Cup:
Minor titles.
Mitropa Cup
Anglo-Italian League Cup
Serie B
Serie C2 (as "Florentia Viola")
UEFA rankings.
Club coefficients.
This is the UEFA club's coefficient as of 25 July 2013:
ACF Fiorentina as a company.
Since re-established in 2002, ACF Fiorentina S.p.A. yet to self-sustain to keep the team in top division as well as in European competitions. In the 2005 financial year, the club made a net loss of €9,159,356, followed by a net loss of €19,519,789. In 2006 (2005–06 Serie A and 2006–07 Serie A), Fiorentina heavily invested on players, made the amortisation of intangible asset (the player contract) had increased from €17.7 million to €24 million. However the club suffered from 2006 Italian football scandal, meant the club did not qualify for Europe. In 2007 Fiorentina almost break-even, with a net loss of just €3,704,953. In 2007 financial year the TV revenue increased after qualified to 2007–08 UEFA Cup. Despite qualified to 2008–09 UEFA Champions League, Fiorentina made a net loss of €9,179,484 in 2008 financial year after the increase in TV revenue was outweighed by the increase in wage. In the 2009 financial year, Fiorentina made a net profit of €4,442,803, largely due to the profit on selling players (€33,631,489 from players such as Felipe Melo, Giampaolo Pazzini and Zdravko Kuzmanovic; increased from about €3.5 million in 2008). However it also offset by the write-down of selling players (€6,062,545, from players such as Manuel da Costa, Arturo Lupoli and Davide Carcuro).
After the club failed to qualify to Europe at the end of 2009–10 Serie A, as well as lack of player profit, Fiorentina turnover was decreased from €140,040,713 in 2009 to just €79,854,928, despite wage bill also fell, "la Viola" still made a net loss of €9,604,353. In the 2011 financial year, the turnover slipped to €67,076,953, as the club's lack of capital gains from selling players and 2010 financial year still included the instalments from UEFA for participating 2009–10 UEFA Europa League. Furthermore, the gate income had dropped from €11,070,385 to €7,541,260. The wage bill did not fall much and in reverse the amortisation of transfer fee had sightly increased due to new signing. "La Viola" had saving in other cost but counter-weighted by huge €11,747,668 write-down for departed players, due to D'Agostino, Frey and Mutu, but the former would counter-weight by co-ownership financial income, which all made the operating cost remained high as worse as last year. Moreover in 2010 the result was boosted by acquiring the asset from subsidiary (related to AC Fiorentina) and the re-valuation of its value in separate balance sheet. If deducting that income (€14,737,855), 2010 financial year was net loss 24,342,208 and 2011 result was worsen €8,131,876 only in separate balance sheet.
ACF Fiorentina re-capitalized in 2006, for €34.7 million. In the next year "la Viola" re-capitalized €20 million and €20M again in 2008. In 2009 Fiorentina re-capitalized for €10 million only and did not had a re-capitalization in 2010 and 2011 financial year.

</doc>
<doc id="3168" url="http://en.wikipedia.org/wiki?curid=3168" title="Afrobeat">
Afrobeat

Afrobeat is a combination of traditional Nigerian music, jazz, highlife, funk, and chanted vocals, fused with percussion and vocal styles, popularised in Africa in the 1970s. Its main creator was the Nigerian multi-instrumentalist and bandleader Fela Kuti, who gave it its name, who used it to revolutionize musical structure as well as the political context in his native Nigeria. It was Kuti who coined the term "afrobeat" upon his return from a U.S. tour with his group Nigeria '70 (formerly Koola Lobitos). Afrobeat features chants, call-and-response vocals, and complex, interacting rhythms.
The new sound hailed from a club that he established called the Afro-Shrine. Upon arriving in Nigeria, Kuti also changed the name of his group to Africa '70. The band maintained a five-year residency in the Afro-Shrine from 1970 to 1975 while afrobeat thrived among Nigerian youth. Afrobeat is now one of the most recognizable music genres in the world and has influenced as many Western musicians as it has African ones with its exuberant style and polyrhythms.
Origins.
Afrobeat originated from Ghana's highlife and heavy African drumbeats and was later exported to the southern part of Nigeria in the 1970s where Kuti, on his return from Ghana where he learnt the genre, experimented with many different forms of contemporary music of the time. Prevalent in his and Lagbaja's music are native African harmonies and rhythms, taking different elements and combining, modernizing, and improvising upon them. Politics are essential to Afrobeat, since founder Kuti used social criticism to pave the way for social change. His message can be described as confrontational and controversial, which can be related to the political climate of most of the African countries in the 1970s, many of which were dealing with political injustice and military corruption while recovering from the transition from colonial governments to self-determination. As the genre spread throughout the African continent many bands took up the style. The recordings of these bands and their songs were rarely heard or exported outside the originating countries but many can now be found on compilation albums and CDs from specialist record shops.
Instrumentation.
Big band (15 to 30 pieces: Fela-era afrobeat) and energetic performances
Fela Kuti included the traditional Gbedu drum in his ensemble, with a percussionist pounding out a thunderous rhythm from an eight foot drum lying on its side.
Influence.
Many jazz musicians have been attracted to afrobeat. From Roy Ayers in the 1970s to Randy Weston in the 1990s, there have been collaborations which have resulted in albums such as "Africa: Centre of the World" by Roy Ayers, released on the Polydore label in 1981. In 1994 Branford Marsalis, the American jazz saxophonist, included samples of Fela's "Beast of No Nation" on his "Buckshot LeFonque" album. The new generation of DJs and musicians of the 2000s who have fallen in love with both Kuti's material and other rare releases have made compilations and remixes of these recordings, thus re-introducing the genre to new generations of listeners and fans of afropop and groove (see Afrobeats section below).
Afrobeat has profoundly influenced important contemporary producers and musicians like Brian Eno and David Byrne, who credit Fela Kuti as an essential influence. Both worked on Talking Heads' highly acclaimed 1980 album "Remain In Light" which brought polyrhythmic afrobeat influences to Western music. More recently, the horn section of Antibalas have been guest musicians on TV On The Radio's highly acclaimed 2008 album "Dear Science", as well as on British band Foals' 2008 album, "Antidotes". Some Afrobeat influence can also be found in the music of Vampire Weekend and Paul Simon.
Afrobeats.
Recent years have seen the creation of the moniker "Afrobeats" (a term also sometimes used to denote Afropop or popular Nigerian music), in which Afrobeat is often blended with highlife, fuji music, Jùjú, hiphop, rnB and traditional Yoruba music.
Notable afrobeat musicians.
Today.
There are several active afrobeat bands worldwide today. Afrobeat today is often mixed with other genres, such as hip hop, makossa, gospel, dancehall and galala.
Modern afrobeat bands/artists include:

</doc>
<doc id="3170" url="http://en.wikipedia.org/wiki?curid=3170" title="Arithmetic function">
Arithmetic function

In number theory, an arithmetic, arithmetical, or number-theoretic function is a real or complex valued function "ƒ"("n") defined on the set of natural numbers (i.e. positive integers) that "expresses some arithmetical property of "n"."
An example of an arithmetic function is the non-principal character (mod 4) defined by
To emphasize that they are being thought of as functions rather than sequences, values of an arithmetic function are usually denoted by "a"("n") rather than "a""n".
There is a larger class of number-theoretic functions that do not fit the above definition, e.g. the prime-counting functions. This article provides links to functions of both classes.
Notation.
formula_3   and   formula_4   mean that the sum or product is over all prime numbers:
Similarly,   formula_7   and   formula_8   mean that the sum or product is over all prime powers with strictly positive exponent (so 1 is not included):
formula_10   and   formula_11   mean that the sum or product is over all positive divisors of "n", including 1 and "n". E.g., if "n" = 12,
The notations can be combined:   formula_13   and   formula_14   mean that the sum or product is over all prime divisors of "n". E.g., if "n" = 18,
and similarly   formula_16   and   formula_17   mean that the sum or product is over all prime powers dividing "n". E.g., if "n" = 24,
Multiplicative and additive functions.
An arithmetic function "a" is
Two whole numbers "m" and "n" are called coprime if their greatest common divisor is 1; i.e., if there is no prime number that divides both of them.
Then an arithmetic function "a" is
Ω("n"), ω("n"), ν"p"("n") – prime power decomposition.
The fundamental theorem of arithmetic states that any positive integer "n" can be represented uniquely as a product of powers of primes:   formula_19   where "p"1 < "p"2 < ... < "p""k" are primes and the "aj" are positive integers. (1 is given by the empty product.)
It is often convenient to write this as an infinite product over all the primes, where all but a finite number have a zero exponent. Define ν"p"("n") as the exponent of the highest power of the prime "p" that divides "n". I.e. if "p" is one of the "p""i" then ν"p"("n") = "a""i", otherwise it is zero. Then
In terms of the above the functions ω and Ω are defined by
To avoid repetition, whenever possible formulas for the functions listed in this article are given in terms of "n" and the corresponding "p""i", "a""i", ω, and Ω.
Multiplicative functions.
σ"k"("n"), τ("n"), "d"("n") – divisor sums.
σ"k"("n") is the sum of the "k"th powers of the positive divisors of "n", including 1 and "n", where "k" is a complex number.
σ1("n"), the sum of the (positive) divisors of "n", is usually denoted by σ("n").
Since a positive number to the zero power is one, σ0("n") is therefore the number of (positive) divisors of "n"; it is usually denoted by "d"("n") or τ("n") (for the German "Teiler" = divisors).
Setting "k" = 0 in the second product gives
φ("n") – Euler totient function.
φ("n"), the Euler totient function, is the number of positive integers not greater than "n" that are coprime to "n".
J"k"("n") – Jordan totient function.
J"k"("n"), the Jordan totient function, is the number of "k"-tuples of positive integers all less than or equal to "n" that form a coprime ("k" + 1)-tuple together with "n". It is a generalization of Euler's totient, .
μ("n") - Möbius function.
μ("n"), the Möbius function, is important because of the Möbius inversion formula. See Dirichlet convolution, below.
This implies that μ(1) = 1. (Because Ω(1) = ω(1) = 0.)
τ("n") – Ramanujan tau function.
τ("n"), the Ramanujan tau function, is defined by its generating function identity:
Although it is hard to say exactly what "arithmetical property of "n"" it "expresses", (τ("n") is (2π)−12 times the "n"th Fourier coefficient in the q-expansion of the modular discriminant function) it is included among the arithmetical functions because it is multiplicative and it occurs in identities involving certain σ"k"("n") and "r""k"("n") functions (because these are also coefficients in the expansion of modular forms).
"c""q"("n") – Ramanujan's sum.
"c""q"("n"), Ramanujan's sum, is the sum of the "n"th powers of the primitive "q"th roots of unity:
Even though it is defined as a sum of complex numbers (irrational for most values of "q"), it is an integer. For a fixed value of "n" it is multiplicative in "q":
Many of the functions mentioned in this article have expansions as series involving these sums; see the article Ramanujan's sum for examples.
Completely multiplicative functions.
λ("n") – Liouville function.
λ("n"), the Liouville function, is defined by
χ("n") – characters.
All Dirichlet characters χ("n") are completely multiplicative. An example is the non-principal character (mod 4) defined in the introduction. Two characters have special notations:
The principal character (mod "n") is denoted by χ0("a") (or χ1("a")). It is defined as
The quadratic character (mod "n") is denoted by the Jacobi symbol for odd "n" (it is not defined for even "n".):
In this formula formula_32 is the Legendre symbol, defined for all integers "a" and all odd primes "p" by
Following the normal convention for the empty product, formula_34
Additive functions.
ω("n") – distinct prime divisors.
ω("n"), defined above as the number of distinct primes dividing "n", is additive.
Completely additive functions.
Ω("n") – prime divisors.
Ω("n"), defined above as the number of prime factors of "n" counted with multiplicities, is completely additive.
ν"p"("n") – prime power dividing "n".
For a fixed prime "p", ν"p"("n"), defined above as the exponent of the largest power of "p" dividing "n", is completely additive.
Neither multiplicative nor additive.
π("x"), Π("x"), θ("x"), ψ("x") – prime count functions.
Unlike the other functions listed in this article, these are defined for non-negative real (not just integer) arguments. They are used in the statement and proof of the prime number theorem.
π("x"), the prime counting function, is the number of primes not exceeding "x".
A related function counts prime powers with weight 1 for primes, 1/2 for their squares, 1/3 for cubes, ...
θ(x) and ψ(x), the Chebyshev functions
are defined as sums of the natural logarithms of the primes not exceeding "x":
Λ("n") – von Mangoldt function.
Λ("n"), the von Mangoldt function, is 0 unless the argument is a prime power, in which case it is the natural log of the prime:
"p"("n") – partition function.
"p"("n"), the partition function, is the number of ways of representing "n" as a sum of positive integers, where two representations with the same summands in a different order are not counted as being different:
λ("n") – Carmichael function.
λ("n"), the Carmichael function, is the smallest positive number such that formula_41   for all "a" coprime to "n". Equivalently, it is the least common multiple of the orders of the elements of the multiplicative group of integers modulo "n".
For powers of odd primes and for 2 and 4, λ("n") is equal to the Euler totient function of "n"; for powers of 2 greater than 4 it is equal to one half of the Euler totient function of "n":
and for general "n" it is the least common multiple of λ of each of the prime power factors of "n":
"h"("n") – Class number.
"h"("n"), the class number function, is the order of the ideal class group of an algebraic extension of the rationals with discriminant "n". The notation is ambiguous, as there are in general many extensions with the same discriminant. See quadratic field and cyclotomic field for classical examples.
"r""k"("n") – Sum of "k" squares.
"r""k"("n") is the number of ways "n" can be represented as the sum of "k" squares, where representations that differ only in the order of the summands or in the signs of the square roots are counted as different.
Summation functions.
Given an arithmetic function "a(n)", its summation function "A(x)" is defined by
"A" can be regarded as a function of a real variable. Given a positive integer "m", "A" is constant along open intervals "m" < "x" < "m" + 1, and has a jump discontinuity at each integer for which "a(m)" ≠ 0.
Since such functions are often represented by series and integrals, to achieve pointwise convergence it is usual to define the value at the discontinuities as the average of the values to the left and right:
Individual values of arithmetic functions may fluctuate wildly – as in most of the above examples. Summation functions "smooth out" these fluctuations. In some cases it may be possible to find asymptotic behaviour for the summation function for large "x".
A classical example of this phenomenon is given by the divisor summatory function, the summation function of "d"("n"), the number of divisors of "n":
The average order of an arithmetic function is some simpler or better-understood function which has the same summation function asmyptotically, and hence takes the same values "on average". We say that the "average order" of "f" is "g" if
as "x" tends to infinity. The example above shows that "d"("n") has the average order log("n").
Dirichlet convolution.
Given an arithmetic function "a(n)", let "Fa(s)", for complex "s", be the function defined by the corresponding Dirichlet series (where it converges):
"Fa(s)" is called a generating function of "a(n)". The simplest such series, corresponding to the constant function "a"("n") = 1 for all "n", is ς("s") the Riemann zeta function.
The generating function of the Möbius function is the inverse of the zeta function:
Consider two arithmetic functions "a" and "b" and their respective generating functions "F""a"("s") and "F""b"("s"). The product "F""a"("s")"F""b"("s") can be computed as follows:
It is a straightforward exercise to show that if "c"("n") is defined by
then
This function "c" is called the Dirichlet convolution of "a" and "b", and is denoted by formula_56.
A particularly important case is convolution with the constant function "a"("n") = 1 for all "n", corresponding to multiplying the generating function by the zeta function:
Multiplying by the inverse of the zeta function gives the Möbius inversion formula:
If "f" is multiplicative, then so is "g". If "f" is completely multiplicative, then "g" is multiplicative, but may or may not be completely multiplicative. The article multiplicative function has a short proof.
Relations among the functions.
There are a great many formulas connecting arithmetical functions with each other and with the functions of analysis, especially powers, roots, and the exponential and log functions.
Here are a few examples:
Sums of squares.
There is a formula for r3 in the section on class numbers below.
Define the function σ"k"*("n") as
That is, if "n" is odd, σ"k"*("n") is the sum of the "k"th powers of the divisors of "n", i.e. σ"k"("n"), and if "n" is even it is the sum of the "k"th powers of the even divisors of "n" minus the sum of the "k"th powers of the odd divisors of "n".
Adopt the convention that Ramanujan's τ("x") = 0 if "x" is not an integer.
Divisor sum convolutions.
Here "convolution" does not mean "Dirichlet convolution" but instead refers to the formula for the coefficients of the product of two power series:
The sequence formula_84 is called the convolution or the Cauchy product of the sequences "a""n" and "b""n".
<br>See Eisenstein series for a discussion of the series and functional identities involved in these formulas.
Since σk("n") (for natural number "k") and τ("n") are integers, the above formulas can be used to prove congruences for the functions. See Tau-function for some examples.
Extend the domain of the partition function by setting "p"(0) = 1.
Class number related.
Peter Gustav Lejeune Dirichlet discovered formulas that relate the class number "h" of quadratic number fields to the Jacobi symbol.
An integer "D" is called a fundamental discriminant if it is the discriminant of a quadratic number field. This is equivalent to "D" ≠ 1 and either a) "D" is squarefree and "D" ≡ 1 (mod 4) or b) "D" ≡ 0 (mod 4), "D"/4 is squarefree, and "D"/4 ≡ 2 or 3 (mod 4).
Extend the Jacobi symbol to accept even numbers in the "denominator" by defining the Kronecker symbol:
Then if "D" < −4 is a fundamental discriminant
There is also a formula relating "r"3 and "h". Again, let "D" be a fundamental discriminant, "D" < −4. Then
Prime-count related.
Let formula_94   be the "n"th harmonic number.   Then
The Riemann hypothesis is also equivalent to the statement that, for all "n" > 5040,
Menon's identity.
In 1965 P. Kesava Menon proved
This has been generalized by a number of mathematicians, e.g.:
B. Sury
N. Rao
where "a"1, "a"2, ..., "a""s" are integers, gcd("a"1, "a"2, ..., "a""s", "n") = 1.
L. Tóth
where "m"1 and "m"2 are odd, "m" = lcm("m"1, "m"2).
In fact, if "f" is any arithmetical function
where * stands for Dirichlet convolution.
Miscellaneous.
Let "m" and "n" be distinct, odd, and positive. Then the Jacobi symbol satisfies the Law of Quadratic Reciprocity:

</doc>
<doc id="3172" url="http://en.wikipedia.org/wiki?curid=3172" title="ANSI C">
ANSI C

ANSI C, also known as C89 and C90 depending on the year of ratification, refers to the family of successive standards published by the American National Standards Institute (ANSI) for the C programming language. Software developers writing in C are encouraged to conform to the standards, as doing so aids portability between compilers.
History and outlook.
The first standard for C was published by ANSI. Although this document was subsequently adopted by International Organization for Standardization (ISO) and subsequent revisions published by ISO have been adopted by ANSI, the name ANSI C (rather than ISO C) is still more widely used. While some software developers use the term ISO C, others are standards body–neutral and use Standard C.
C89.
In 1983, the American National Standards Institute formed a committee, X3J11, to establish a standard specification of C. After a long and arduous process, the standard was completed in 1989 and ratified as ANSI X3.159-1989 "Programming Language C." This version of the language is often referred to as "ANSI C". Later on sometimes the label "C89" is used to distinguish it from C99 but using the same labelling method.
C90.
The same standard as C89 was ratified by the International Organization for Standardization as ISO/IEC 9899:1990, with only formatting changes, which is sometimes referred to as C90. Therefore, the terms "C89" and "C90" refer to essentially the same language.
This standard has been withdrawn by both INCITS and ISO/IEC.
AMD1.
ISO also published an amendment in 1994, referred to as AMD1, (ISO/IEC 9899/Amd.1:1995) introducing minor changes to the language and the library.
C95.
1995 the ISO published an extension, called Amendment 1, for the ANSI-C standard. It full name finally was "ISO/IEC 9899/AMD1:1995" or nicknamed "C95". Aside to error correction there were further changes to the language capabilities.
C99.
In March 2000, ANSI adopted the ISO/IEC 9899:1999 standard. This standard is commonly referred to as C99.
This standard has been withdrawn by ISO/IEC in favour of C11, but is still approved by INCITS.
C11.
"C11" is the new standard for the C programming language.
Support from major compilers.
ANSI C is now supported by almost all the widely used compilers. Most of the C code being written nowadays is based on ANSI C. Any program written "only" in standard C and without any hardware dependent assumptions is virtually guaranteed to compile correctly on any platform with a conforming C implementation. Without such precautions, most programs may compile only on a certain platform or with a particular compiler, due, for example, to the use of non-standard libraries, such as GUI libraries, or to the reliance on compiler- or platform-specific attributes such as the exact size of certain data types and byte endianness.
Compliance detectability.
To mitigate the differences between K&R C and the ANSI C standard, the codice_4 ("standard c") macro can be used to split code into ANSI and K&R sections.
<syntaxhighlight lang="c">
</syntaxhighlight>
It's better to use "codice_5" as above rather than "codice_6" because some implementation may set codice_4 to zero to indicate non-ANSI compliance. "codice_8" will treat any identifiers that couldn't be replaced by a macro as zero (codice_9). Thus even if the macro "codice_4" is not defined to signify non-ANSI compliance, "codice_8" will work as shown.
In the above example, a prototype is used in a function declaration for ANSI compliant implementations, while an obsolescent non-prototype declaration is used otherwise. Those are still ANSI-compliant as of C99

</doc>
<doc id="3173" url="http://en.wikipedia.org/wiki?curid=3173" title="Alien and Sedition Acts">
Alien and Sedition Acts

The Alien and Sedition Acts were four bills that were passed by the Federalists in the 5th United States Congress and signed into law by President John Adams in 1798 in the aftermath of the French Revolution and during an undeclared naval war with France, later known as the Quasi-War. The Naturalization Act increased the residency requirement for American citizenship from 5 to 14 years, and allowed the president to imprison or deport aliens who were considered "dangerous to the peace and safety of the United States". They also restricted speech which was critical of the federal government. Authored by the Federalists, the laws were purported to strengthen national security, but most historians have concluded they were primarily an attempt to suppress voters who disagreed with the Federalist party. At the time, most immigrants (namely Irish and French) supported Thomas Jefferson and the Democratic-Republicans, the political opponents of the Federalists. This act was repealed in 1802 by the Naturalization Law of 1802.
The acts were denounced by Democratic-Republicans and ultimately helped them to victory in the 1800 election, when Thomas Jefferson became President. The Sedition Act and the Alien Friends Act were allowed to expire in 1800 and 1801, respectively. The Alien Enemies Act remains in effect as 50 USC Sections 21–24.
History.
Opposition to Federalists, spurred on by Democratic-Republicans, reached new heights at this time with the Democratic-Republicans supporting France still in the midst of the French Revolution. Some appeared to desire an event similar to the French Revolution to come to the United States to overthrow the government. When Democratic-Republicans in some states refused to enforce federal laws, such as the 1791 whiskey tax, the first tax levied by the national government, and threatened to rebel, Federalists threatened to send in the army to force them to capitulate. As the unrest sweeping Europe was bleeding over into the United States, calls for secession reached unparalleled heights, and the fledgling nation seemed ready to rip itself apart. Some of this was seen by Federalists as having been caused by French and French-sympathizing immigrants. The Alien Act and the Sedition Act were meant to guard against this perceived threat of anarchy.
Democratic-Republicans denounced them, though they did use them after the 1800 election against Federalists. They were a major political issue in the elections of 1798 and 1800. They were very controversial in their own day, as they remain to the present day. Opposition to them resulted in the highly controversial Virginia and Kentucky Resolutions, authored by James Madison and Thomas Jefferson. Prominent prosecutions under the Sedition Act include:
Effect of the acts.
The Democratic-Republicans made the Alien and Sedition Acts an important issue in the 1800 election. Thomas Jefferson, upon assuming the Presidency, pardoned those still serving sentences under the Sedition Act, though he also used the acts to prosecute several of his own critics before the acts expired. It has been said that the Alien Acts were aimed at Albert Gallatin; and the Sedition Act aimed at Benjamin Bache's "Aurora". While government authorities prepared lists of aliens for deportation, many aliens fled the country during the debate over the Alien and Sedition Acts, and Adams never signed a deportation order.
The Alien and Sedition Acts were never appealed to the Supreme Court, whose right of judicial review was not established until "Marbury v. Madison" in 1803. Subsequent mentions in Supreme Court opinions beginning in the mid-20th century have assumed that the Sedition Act would today be found unconstitutional.
Jefferson and James Madison also secretly drafted the Kentucky and Virginia Resolutions denouncing the federal legislation, though many other state legislatures strongly opposed these resolutions. Though the resolutions followed Madison's "interposition" approach, Jefferson advocated nullification and at one point drafted a threat for Kentucky to secede. Jefferson's biographer Dumas Malone argued that this might have gotten Jefferson impeached for treason, had his actions become known at the time. In writing the Kentucky Resolutions, Jefferson warned that, "unless arrested at the threshold," the Alien and Sedition Acts would "necessarily drive these states into revolution and blood." Historian Ron Chernow says of this "he wasn't calling for peaceful protests or civil disobedience: he was calling for outright rebellion, if needed, against the federal government of which he was vice president." Jefferson "thus set forth a radical doctrine of states' rights that effectively undermined the constitution." Chernow argues that neither Jefferson nor Madison sensed that they had sponsored measures as inimical as the Alien and Sedition Acts themselves. Historian Garry Wills argued "Their nullification effort, if others had picked it up, would have been a greater threat to freedom than the misguided and sedition laws, which were soon rendered feckless by ridicule and electoral pressure" The theoretical damage of the Kentucky and Virginia resolutions was "deep and lasting, and was a recipe for disunion". George Washington was so appalled by them that he told Patrick Henry that if "systematically and pertinaciously pursued", they would "dissolve the union or produce coercion". The influence of Jefferson's doctrine of states' rights reverberated right up to the Civil War and beyond. Future president James Garfield, at the close of the Civil War, said that Jefferson's Kentucky Resolution "contained the germ of nullification and secession, and we are today reaping the fruits".

</doc>
<doc id="3175" url="http://en.wikipedia.org/wiki?curid=3175" title="Antinomy">
Antinomy

Antinomy (Greek ἀντί, "against, in opposition to," and νόμος, "law") literally means the mutual incompatibility, real or apparent, of two laws. It is a term used in logic and epistemology, particularly in the philosophy of Kant.
Kant's use.
The term acquired a special significance in the philosophy of Immanuel Kant (1724–1804), who used it to describe the equally rational but contradictory results of applying to the universe of pure thought the categories or criteria of reason that are proper to the universe of sensible perception or experience (phenomena). Empirical reason cannot here play the role of establishing rational truths because it goes beyond possible experience and is applied to the sphere of that which transcends it.
For Kant there are four antinomies, connected with:
In each antinomy, a thesis is contradicted by an antithesis. For example: in the First Antinomy, Kant proves the thesis that time must have a beginning by showing that if time had no beginning, then an infinity would have elapsed up until the present moment. This is a manifest contradiction because infinity cannot, by definition, be completed by "successive synthesis"—yet just such a finalizing synthesis would be required by the view that time is infinite; so the thesis is proven. Then he proves the antithesis, that time has no beginning, by showing that if time had a beginning, then there must have been "empty time" out of which time arose. This is incoherent (for Kant) for the following reason: Since, necessarily, no time elapses in this pretemporal void, then there could be no alteration, and therefore nothing (including time) would ever come to be: so the antithesis is proven. Reason makes equal claim to each proof, since they are both correct, so the question of the limits of time must be regarded as meaningless.
This was part of Kant's critical program of determining limits to science and philosophical inquiry. These contradictions are inherent in reason when it is applied to the world as it is in itself, independently of our perceptions of it (this has to do with the distinction between phenomena and noumena). Kant's goal in his critical philosophy was to identify what claims we are and are not justified in making, and the antinomies are a particularly illustrative example of his larger project.
However, there are many other examples of antinomy besides these four. Contradictory phrases, such as "There is no absolute truth" can be considered an antinomy because this statement is suggesting in itself to be an absolute truth, and therefore denies itself any truth in its statement.

</doc>
<doc id="3189" url="http://en.wikipedia.org/wiki?curid=3189" title="Ascending chain condition">
Ascending chain condition

In mathematics, the ascending chain condition (ACC) and descending chain condition (DCC) are finiteness properties satisfied by some algebraic structures, most importantly, ideals in certain commutative rings. These conditions played an important role in the development of the structure theory of commutative rings in the works of David Hilbert, Emmy Noether, and Emil Artin.
The conditions themselves can be stated in an abstract form, so that they make sense for any partially ordered set. This point of view is useful in abstract algebraic dimension theory due to Gabriel and Rentschler.
Definition.
A partially ordered set (poset) "P" is said to satisfy the ascending chain condition (ACC) if every strictly ascending sequence of elements eventually terminates. Equivalently, given any sequence
there exists a positive integer "n" such that
Similarly, "P" is said to satisfy the descending chain condition (DCC) if every strictly descending sequence of elements eventually terminates, that is, there is no infinite descending chain. Equivalently every descending sequence
of elements of "P", eventually stabilizes.

</doc>
<doc id="3192" url="http://en.wikipedia.org/wiki?curid=3192" title="Adin Steinsaltz">
Adin Steinsaltz

Rabbi Adin Steinsaltz (Hebrew: עדין שטיינזלץ) or Adin Even Yisrael (Hebrew: עדין אבן ישראל) (born 1937) is a teacher, philosopher, social critic, and spiritual mentor, who has been hailed by "Time" magazine as a "once-in-a-millennium scholar". He has devoted his life to making the Talmud accessible to all Jews. Originally published in modern Hebrew, with a running commentary to facilitate learning, his "Steinsaltz edition of the Talmud" has also been translated into English, French, Russian and Spanish. Beginning in 1989, Steinsaltz published several tractates in Hebrew and English of the Babylonian (Bavli) Talmud in an English-Hebrew edition. The first volume of a new English-Hebrew edition, the Koren Talmud Bavli, was released in May, 2012, with thirteenth tractates in print by July 2014. New volumes are being released following the Daf Yomi cycle.
Biography.
Born in Jerusalem in 1937 to secular parents, Steinsaltz studied mathematics, physics, and chemistry at the Hebrew University, in addition to rabbinical studies. Following graduation, he established several experimental schools after an unsuccessful attempt to start a neo-Hassidic community in the Negev desert, and, at the age of 23, became Israel’s youngest school principal.
In 1965, he founded the Israel Institute for Talmudic Publications and began his monumental work on the Talmud, including translation into Hebrew, English, Russian, and various other languages. The Steinsaltz editions of the Talmud include translation from the original Aramaic and a comprehensive commentary. Steinsaltz completed his Hebrew edition of the entire Babylonian Talmud in November 2010, at which time Koren Publishers Jerusalem became the publisher of all of his works, including the Talmud. While not without criticism (e.g. by Neusner, 1998), the Steinsaltz edition is widely used throughout Israel, the United States and the world. Over 2 million volumes of the Steinsaltz Talmud have been distributed to date. The out-of-print Random House publication of "" is widely regarded as the most accurate and least redacted of any English language edition and is sought after on that basis by scholars and collectors. Controversial Talmud passages previously obscured, omitted entirely or confined to footnotes in English translations like the Soncino Talmud, receive full exposition in the Steinsaltz Talmud. Random House halted publication of the Steinsaltz Talmud after less than one-third of the English translation had been published. 
The Steinsaltz editions of the Talmud have opened up the world of Talmud study to thousands of people outside the walls of the traditional yeshiva, including women, who traditionally were not taught Talmud. Regarding the access that his work provides, Rabbi Steinsaltz says:
Rabbi Steinsaltz's classic work of Kabbalah, "The Thirteen Petalled Rose", was first published in 1980 and now appears in eight languages. In all, Steinsaltz has authored some 60 books and hundreds of articles on subjects including Talmud, Jewish mysticism, Jewish philosophy, sociology, historical biography, and philosophy. Many of these works have been translated into English by his close personal friend, now deceased, Yehuda Hanegbi. His latest book is a memoir-biography on the Lubavitcher Rebbe, Rabbi Menachem Mendel Schneersohn, published by Maggid Books (2014).
In the summer of 1989, a group of rabbis including Elazar Shach placed a ban on three of Steinsaltz's books. 
Continuing his work as a teacher and spiritual mentor, Steinsaltz established a network of schools and educational institutions in Israel and the former Soviet Union. He has served as scholar in residence at the Woodrow Wilson International Center for Scholars in Washington, D.C. and the Institute for Advanced Study in Princeton. His honorary degrees include doctorates from Yeshiva University, Ben Gurion University of the Negev, Bar Ilan University, Brandeis University, and Florida International University. Steinsaltz is also Rosh Yeshiva of Yeshivat Hesder Tekoa. 
Being a follower of Rabbi Menachem Mendel Schneerson of Chabad-Lubavitch, he went to help Jews in the Soviet Union assisting Chabad's "shluchim" (propagators) network. Deeply involved in the future of the Jews in the former Soviet Union, Steinsaltz serves as the region's "Duchovny Ravin" (Spiritual Rabbi), a historic Russian title which indicates that he is the spiritual mentor of Russian Jewry. In this capacity, Steinsaltz travelled to Russia and the Republics once each month from his home in Jerusalem. During his time in the former Soviet Union he founded the Jewish University, both in Moscow and Saint Petersburg. The Jewish University is the first degree-granting institution of Jewish studies in the former Soviet Union. 
Steinsaltz has taken a cautious approach to interfaith dialogues. During a visit of a delegation of Roman Catholic cardinals in Manhattan in January 2004, he said that “you do not have to raise over-expectations of a meeting as it doesn't signify in itself a breakthrough, however, the opportunity for cardinals and rabbis to speak face to face is valuable. It's part of a process in which we can talk to each other in a friendly way," Rabbi Steinsaltz said.”, and called for “a theological dialogue that asks the tough questions, such as whether Catholicism allows for Jews to enter eternal paradise.”
Steinsaltz and his wife live in Jerusalem, and have three children and more than ten grandchildren. His son, Rabbi Menachem Even-Israel, is the Director of Educational Programs at the Steinsaltz Center in the Nachlaot neighborhood of Jerusalem.
Head of the new Sanhedrin.
Rabbi Steinsaltz accepted the position as Nasi (President) of the 2004 attempt to revive the Sanhedrin. In 2008 he resigned from this position due to differences of opinion.
As a speaker.
Steinsaltz is a popular University and radio commentator. He has been invited to speak at the Aspen Institute for Humanistic Studies at Yale University in 1979. In Jerusalem, he gives evening seminars, which according to Newsweek usually last till 2 in the morning, and have attracted prominent politicians as the former Prime Minister Levi Eshkol and former Finance Minister Pinhas Sapir.
Awards and critical reception.
Rabbi Steinsaltz has received many awards and prizes, among them the Israel Prize for Jewish studies in 1988.
On 9 February 2012, Steinsaltz was honored by Israeli President Shimon Peres with Israel's first President's Prize for his scholarship in Talmud.
The Jewish Book Council named the Koren Talmud Bavli with commentary, translation and notes by Rabbi Adin Steinsaltz, a 2012 National Jewish Book Award winner in the category of Modern Jewish Thought and Experience. 
However not all reception of Steinsaltz' studies have been wholly positive. In the relatively calm academic world Jacob Neusner's combatively titled "How Adin Steinsaltz Misrepresents the Talmud. Four False Propositions from his "Reference Guide."" (1998) displays strong disagreement.

</doc>
<doc id="3198" url="http://en.wikipedia.org/wiki?curid=3198" title="A. E. Housman">
A. E. Housman

Alfred Edward Housman (; 26 March 1859 – 30 April 1936), usually known as A. E. Housman, was an English classical scholar and poet, best known to the general public for his cycle of poems "A Shropshire Lad". Lyrical and almost epigrammatic in form, the poems wistfully evoke the dooms and disappointments of youth in the English countryside. Their beauty, simplicity and distinctive imagery appealed strongly to late Victorian and Edwardian taste, and to many early 20th-century English composers (beginning with Arthur Somervell) both before and after the First World War. Through their song-settings, the poems became closely associated with that era, and with Shropshire itself.
Housman was one of the foremost classicists of his age and has been ranked as one of the greatest scholars who ever lived. He established his reputation publishing as a private scholar and, on the strength and quality of his work, was appointed Professor of Latin at University College London and then at Cambridge. His editions of Juvenal, Manilius and Lucan are still considered authoritative.
Life.
The eldest of seven children, Housman was born at Valley House in Fockbury, a hamlet on the outskirts of Bromsgrove in Worcestershire, to Sarah Jane (née Williams, married 17 Jun 1858 in Woodchester, Gloucester) and Edward Housman (whose family came from Lancaster), and was baptized on 24 Apr 1859 at Christ Church, in Catshill. His mother died on his twelfth birthday, and his father, a country solicitor, later remarried, to an elder cousin, Lucy, in 1873. Housman's brother Laurence Housman and sister Clemence Housman also became writers.
Housman was educated at Bromsgrove School, where he laid a strong academic grounding and won prizes for his poetry. In 1877, he won an open scholarship to St John's College, Oxford, where he studied classics. Although by nature rather withdrawn, Housman formed strong friendships with two roommates, Moses Jackson and A. W. Pollard. Jackson became the great love of Housman's life, but he was heterosexual and did not reciprocate Housman's feelings. Housman obtained a first in classical Moderations in 1879, but his dedication to textual analysis, particularly with Propertius, led him to neglect ancient history and philosophy, which formed part of the Greats curriculum. Accordingly, he failed to obtain a degree. Though some attribute Housman's unexpected failure in his final exams directly to his rejection by Jackson, most biographers suggest that there are more obvious reasons. Housman was indifferent to philosophy, overconfident in his preternatural gifts, felt contempt for inexact learning, and enjoyed idling away his time with Jackson. He may also have been distracted by news of his father's desperate illness. The failure left him with a deep sense of humiliation, and a determination to vindicate his genius.
After Oxford, Jackson got a job as a clerk in the Patent Office in London and arranged a job there for Housman as well. They shared a flat with Jackson's brother Adalbert until 1885 when Housman moved to lodgings of his own. Moses Jackson moved to India in 1887. When Jackson returned briefly to England in 1889 to marry, Housman was not invited to the wedding and knew nothing about it until the couple had left the country. Adalbert Jackson died in 1892. Housman continued pursuing classical studies independently and published scholarly articles on such authors as Horace, Propertius, Ovid, Aeschylus, Euripides and Sophocles. He gradually acquired such a high reputation that in 1892 he was offered the professorship of Latin at University College London, which he accepted. Many years later, the UCL academic staff common room was dedicated to his memory as the Housman Room.
The pleasures Housman enjoyed included gastronomy, flying in aeroplanes, and frequent visits to France, where he read "books which were banned in Britain as pornographic". A fellow don described him as being "descended from a long line of maiden aunts".
Although Housman's early work and his sphere of responsibilities as professor included both Latin and Greek, he began to focus his energy on Latin poetry. When asked later why he had stopped writing about Greek poetry, he responded, "I found that I could not attain to excellence in both." In 1911, he took the Kennedy Professorship of Latin at Trinity College, Cambridge, where he remained for the rest of his life. Classics Professor G. P. Goold at University College, wrote of Housman's scholarly accomplishments: "The legacy of Housman's scholarship is a thing of permanent value; and that value consists less in obvious results, the establishment of general propositions about Latin and the removal of scribal mistakes, than in the shining example he provides of a wonderful mind at work... He was and may remain the last great textual critic." During 1903–1930, he published his critical edition of Manilius's "Astronomicon" in five volumes. He also edited works of Juvenal (1905) and Lucan (1926). Many colleagues were unnerved by his scathing critical attacks on those whom he found guilty of shoddy scholarship. In his paper "The Application of Thought to Textual Criticism," (1921) Housman stated: "A textual critic engaged upon his business is not at all like Newton investigating the motion of the planets: he is much more like a dog hunting for fleas." He declared many of his contemporary scholars to be stupid, lazy, vain, or all three, proclaiming: "Knowledge is good, method is good, but one thing beyond all others is necessary; and that is to have a head, not a pumpkin, on your shoulders, and brains, not pudding, in your head." His younger colleague A. S. F. Gow quotes examples of these attacks, noting that they "were often savage in the extreme." Gow also relates how Housman intimidated his students, sometimes reducing them to tears. According to Gow, Housman could never remember his students' names, maintaining that "had he burdened his memory by the distinction between Miss Jones and Miss Robinson, he might have forgotten that between the second and fourth declension." One notable pupil was Enoch Powell. Housman found his true vocation in classical studies and treated poetry as a secondary activity. He did not speak about his poetry in public until 1933 when he gave a lecture, "The Name and Nature of Poetry", in which he argued that poetry should appeal to emotions rather than to the intellect.
Housman died aged 77, in Cambridge. His ashes are buried near St Laurence's Church, Ludlow, Shropshire. The University of Worcester has acknowledged Housman's local connection by naming a new building after him.
Poetry.
"A Shropshire Lad".
During his years in London, A. E. Housman completed "A Shropshire Lad", a cycle of 63 poems. After several publishers had turned it down, he published it at his own expense in 1896. The emotion and vulnerability revealed in the book surprised both his colleagues and his students. At first selling slowly, it rapidly became a lasting success. Its appeal to English musicians had helped to make it widely known before World War I, when its themes struck a powerful chord with English readers. "A Shropshire Lad" has been in print continuously since May 1896.
The poems are pervaded by deep pessimism and preoccupation with death, without religious consolation. Housman wrote most of them while living in Highgate, London, before ever visiting that part of Shropshire (about thirty miles from his boyhood home), which he presented in an idealised pastoral light, as his 'land of lost content'. Housman himself acknowledged the influence of the songs of William Shakespeare, the Scottish Border ballads and Heinrich Heine, but specifically denied any influence of Greek and Latin classics in his poetry.
Later collections.
In the early 1920s, when Moses Jackson was dying in Canada, Housman wanted to assemble his best unpublished poems so that Jackson could read them before his death. These later poems, mostly written before 1910, show a greater variety of subject and form than those in "A Shropshire Lad" but lack the consistency of his previously published work. He published them as "Last Poems" (1922) because he felt his inspiration was exhausted and that he should not publish more in his lifetime. This proved true. After his death Housman's brother, Laurence, published further poems which appeared in "More Poems" (1936) and "Collected Poems" (1939). Housman also wrote a parodic "Fragment of a Greek Tragedy", in English, and humorous poems published posthumously under the title "Unkind to Unicorns".
John Sparrow found statements in a letter written late in Housman's life which describe how his poems came into existence:
Poetry was for him ...'a morbid secretion', as the pearl is for the oyster. The desire, or the need, did not come upon him often, and it came usually when he was feeling ill or depressed; then whole lines and stanzas would present themselves to him without any effort, or any consciousness of composition on his part. Sometimes they wanted a little alteration, sometimes none; sometimes the lines needed in order to make a complete poem would come later, spontaneously or with 'a little coaxing'; sometimes he had to sit down and finish the poem with his head. That ... was a long and laborious process.
Sparrow himself adds, "How difficult it is to achieve a satisfactory analysis may be judged by considering the last poem in "A Shropshire Lad". Of its four stanzas, Housman tells us that two were 'given' him ready made; one was coaxed forth from his subconsciousness an hour or two later; the remaining one took months of conscious composition. No one can tell for certain which was which."
"De Amicitia" (Of Friendship).
In 1942 Laurence Housman also deposited an essay entitled "A. E. Housman's 'De Amicitia'" in the British Library, with the proviso that it was not to be published for 25 years. The essay discussed A. E. Housman's homosexuality and his love for Jackson. Despite the conservative nature of the times, Housman, as distinct from the prudence of his public life, was quite open in his poetry, and especially his "A Shropshire Lad", about his deeper sympathies. Poem 30 of that sequence, for instance, speaks of how "Fear contended with desire": "Others, I am not the first / have willed more mischief than they durst". In "More Poems", he buries his love for Moses Jackson in the very act of commemorating it, as his feelings of love break his friendship, and must be carried silently to the grave:.
<poem>
Because I liked you better
Than suits a man to say
It irked you, and I promised
To throw the thought away.
To put the world between us
We parted, stiff and dry;
Goodbye, said you, forget me.
I will, no fear, said I
If here, where clover whitens
The dead man's knoll, you pass,
And no tall flower to meet you
Starts in the trefoiled grass,
Halt by the headstone naming
The heart no longer stirred,
And say the lad that loved you
Was one that kept his word.
</poem>
His poem, "Oh who is that young sinner with the handcuffs on his wrists?", written after the trial of Oscar Wilde, addressed more general social injustice towards homosexuality. In the poem the prisoner is suffering "for the colour of his hair", a natural, given attribute which, in a clearly coded reference to homosexuality, is reviled as "nameless and abominable" (recalling the legal phrase "peccatum illud horribile, inter christianos non nominandum", "the sin so horrible, not to be named amongst Christians").
Housman in other art forms.
Music and art song.
Housman's poetry, especially "A Shropshire Lad", provided texts for a significant number of British, and in particular English, composers in the first half of the 20th century. The national, pastoral and traditional elements of his style resonated with similar trends in English music. The first was probably the cycle "A Shropshire Lad" set by Arthur Somervell in 1904, who had begun to develop the concept of the English song-cycle in his version of Tennyson's "Maud" a little previously. Ralph Vaughan Williams produced his most famous settings of six songs, the cycle "On Wenlock Edge", for string quartet, tenor and piano (dedicated to Gervase Elwes) in 1909, and it became very popular after Elwes recorded it with the London String Quartet and Frederick B. Kiddle in 1917. Between 1909 and 1911 George Butterworth produced settings in two collections or cycles, as "Six Songs from A Shropshire Lad", and "Bredon Hill and other songs". He also wrote an orchestral tone poem on "A Shropshire Lad" (first performed at Leeds Festival under Arthur Nikisch in 1912).
Butterworth's death on the Somme in 1916 was considered a great loss to English music; Ivor Gurney, another most important setter of Housman ("Ludlow and Teme", a work for voice and string quartet, and a song-cycle on Housman works, both of which won the Carnegie Award) experienced emotional breakdowns which were popularly (but wrongly) believed to have originated from shell-shock. Hence the fatalistic strain of the poems, and the earlier settings, foreshadowed responses to the universal bereavement of the First World War and became assimilated into them. This was reinforced when their foremost interpreter and performer, Gervase Elwes (who had initiated the music festivals at Brigg in Lincolnshire at which Percy Grainger and others had developed their collections of country music) died in a horrific accident in 1921. Elwes had been closely identified with English wartime morale, having given six benefit performances of "The Dream of Gerontius" on consecutive nights in 1916, and many concerts in France in 1917 for British soldiers.
Among other composers who set Housman songs were John Ireland (song cycle, "Land of Lost Content"), Michael Head (e.g. 'Ludlow Fair'), Graham Peel (a famous version of 'In Summertime on Bredon'), Ian Venables (Songs of Eternity and Sorrow), and the American Samuel Barber (e.g. 'With rue my heart is laden'). Gerald Finzi repeatedly began settings, though never finished any. Even composers not directly associated with the 'pastoral' tradition, such as Arnold Bax, Lennox Berkeley and Arthur Bliss, were attracted to Housman's poetry. A 1976 catalogue listed 400 musical settings of Housman's poems. Housman's poetry influenced British music in a way comparable to that of Walt Whitman in the music of Delius, Vaughan Williams and others: Housman's works provided song texts, Whitman's the texts for larger choral works. The contemporary New Zealand composer David Downes includes a setting of "March" on his CD "The Rusted Wheel of Things".
Works titled after Housman.
Housman is the main character in the 1997 Tom Stoppard play "The Invention of Love". Many titles for novels and films have been drawn from Housman's poetry. The line "There's this to say for blood and breath,/ they give a man a taste for death" supplies the title for Peter O'Donnell's 1969 Modesty Blaise thriller, "A Taste for Death", also the inspiration for P. D. James' 1986 crime novel, "A Taste for Death", the seventh in her Adam Dalgliesh series. The title of Nicholas Blake's 1949 detective novel "Head of a Traveller" is a quotation from Housman's parody "Fragment of a Greek Tragedy". The last words of the poem "On Wenlock Edge" are used by Audrey R. Langer for the title of the 1989 novel "Ashes Under Uricon". The Nobel Prize winning novelist Patrick White named his 1955 novel "The Tree of Man" also after a line in "On Wenlock Edge" and Arthur C. Clarke's first novel, "Against the Fall of Night", is taken from a work in Housman's "More Poems". The 2009 novel "Blood's a Rover" by James Ellroy takes its title from Housman's poem "Reveille", and a line from Housman's poem XVI "How Clear, How Lovely Bright", was used for the title of the last "Inspector Morse" book "The Remorseful Day" by Colin Dexter. "Blue Remembered Hills", a television play by Dennis Potter, takes its title from "Into My Heart an Air That Kills" from "A Shropshire Lad", the cycle also providing the name for the James Bond film "Die Another Day": "But since the man that runs away / Lives to die another day".
In the 1985 film "Out of Africa" Karen "Tanja" Blixen, played by Meryl Streep, cites poems by A.E. Housman twice. In one key scene, when she is finally invited by the male members of the country club, she gives a toast citing from “With rue my heart is laden”. Secondly, when she gives the eulogy at Denys Finch Hatton's funeral, she recites an abbreviated version of “To an athlete dying young”.
Works.
Published lectures.
These lectures are listed by date of delivery, with date of first publication given separately if different.
Prose collections.
Selected Prose, edited by John Carter, Cambridge University Press, 1961

</doc>
<doc id="3201" url="http://en.wikipedia.org/wiki?curid=3201" title="Attribution of recent climate change">
Attribution of recent climate change

Attribution of recent climate change is the effort to scientifically ascertain mechanisms responsible for recent changes observed in the Earth's climate. The effort has focused on changes observed during the period of instrumental temperature record, when records are most reliable; particularly on the last 50 years, when human activity has grown fastest and observations of the troposphere have become available. The dominant mechanisms (to which recent climate change has been attributed) are anthropogenic, i.e., the result of human activity. They are:
There are also natural mechanisms for variation including climate oscillations, changes in solar activity, and volcanic activity.
According to the Intergovernmental Panel on Climate Change (IPCC), it is "extremely likely" that human influence was the dominant cause of global warming between 1951 and 2010. The IPCC defines "extremely likely" as indicating a probability of 95 to 100%, based on an expert assessment of all the available evidence.
Multiple lines of evidence support attribution of recent climate change to human activities:
The IPCC's attribution of recent global warming to human activities is a view shared by most scientists, and is also supported by 196 other scientific organizations worldwide (see also: scientific opinion on climate change).
Background.
[[Image:3 examples of internal climate variability (1950-2012), the El Niño – Southern Oscillation, the Arctic Oscillation, and the North Atlantic Oscillation (NOAA).png|thumb |right |alt=Refer to caption |This image shows three examples of internal climate variability measured between 1950 and 2012: the El Niño–Southern oscillation, the Arctic oscillation, and the North Atlantic oscillation.]]
This section introduces some concepts in climate science that are used in the following sections:
Factors affecting Earth's climate can be broken down into feedbacks and forcings.
A forcing is something that is imposed externally on the climate system. External forcings include natural phenomena such as volcanic eruptions and variations in the sun's output. Human activities can also impose forcings, for example, through changing the composition of the atmosphere.
Radiative forcing is a measure of how various factors alter the energy balance of the Earth's atmosphere. A positive radiative forcing will tend to increase the energy of the Earth-atmosphere system, leading to a warming of the system. Between the start of the Industrial Revolution in 1750, and the year 2005, the increase in the atmospheric concentration of carbon dioxide (chemical formula: CO2) led to a positive radiative forcing, averaged over the Earth's surface area, of about 1.66 watts per square metre (abbreviated W m−2).
Climate feedbacks can either amplify or dampen the response of the climate to a given forcing.
There are many feedback mechanisms in the climate system that can either amplify (a positive feedback) or diminish (a negative feedback) the effects of a change in climate forcing.
Aspects of the climate system will show variation in response to changes in forcings.
In the absence of forcings imposed on it, the climate system will still show internal variability (see images opposite). This internal variability is a result of complex interactions between components of the climate system, such as the coupling between the atmosphere and ocean (see also the later section on Internal climate variability and global warming). An example of internal variability is the El Niño-Southern Oscillation.
Detection vs. attribution.
[[File:Detection and attribution of climate change (NOAA NCDC).png|thumb |right |425px|alt=Refer to caption and adjacent text|In detection and attribution, the natural factors considered usually include changes in the Sun's output and volcanic eruptions, as well as natural modes of variability such as El Niño and La Niña. Human factors include the emissions of heat-trapping "greenhouse" gases and particulates as well as clearing of forests and other land-use changes. Figure source: NOAA NCDC.]]
Detection and attribution of climate signals, as well as its common-sense meaning, has a more precise definition within the climate change literature, as expressed by the IPCC.
"Detection" of a signal requires demonstrating that an observed change is statistically significantly different from that which can be explained by natural internal variability.
"Attribution" requires demonstrating that a signal is:
Detection does not imply attribution, and is easier to show than attribution. Unequivocal attribution would require controlled experiments with multiple copies of the climate system, which is not possible. Therefore, attribution, as described above, can only be done within some margin of error. For example, the IPCC's Fourth Assessment Report says "it is "extremely likely" that human activities have exerted a substantial net warming influence on climate since 1750," where "extremely likely" indicates a probability greater than 95%.
Key attributions.
Greenhouse gases.
Carbon dioxide is the primary greenhouse gas that is contributing to recent climate change. is absorbed and emitted naturally as part of the carbon cycle, through animal and plant respiration, volcanic eruptions, and ocean-atmosphere exchange. Human activities, such as the burning of fossil fuels and changes in land use (see below), release large amounts of carbon to the atmosphere, causing concentrations in the atmosphere to rise.
The high-accuracy measurements of atmospheric CO2 concentration, initiated by Charles David Keeling in 1958, constitute the master time series documenting the changing composition of the atmosphere. These data have iconic status in climate change science as evidence of the effect of human activities on the chemical composition of the global atmosphere.
Along with CO2, methane and nitrous oxide are also major forcing contributors to the greenhouse effect. The Kyoto Protocol lists these together with hydrofluorocarbons (HFCs), perfluorocarbons (PFCs), and sulphur hexafluoride (SF6), which are entirely artificial (i.e. anthropogenic) gases, which also contribute to radiative forcing in the atmosphere. The chart at right attributes anthropogenic greenhouse gas emissions to eight main economic sectors, of which the largest contributors are power stations (many of which burn coal or other fossil fuels), industrial processes, transportation fuels (generally fossil fuels), and agricultural by-products (mainly methane from enteric fermentation and nitrous oxide from fertilizer use).
Water vapor.
Water vapor is the most abundant greenhouse gas and also the most important in terms of its contribution to the natural greenhouse effect, despite having a short atmospheric lifetime (about 10 days). Some human activities can influence local water vapor levels. However, on a global scale, the concentration of water vapor is controlled by temperature, which influences overall rates of evaporation and precipitation. Therefore, the global concentration of water vapor is not substantially affected by direct human emissions.
Land use.
Climate change is attributed to land use for two main reasons. Between 1750 and 2007, about two-thirds of anthropogenic carbon dioxide emissions were produced from burning fossil fuels, and about one-third of emissions from changes in land use, primarily deforestation. Deforestation both reduces the amount of carbon dioxide absorbed by deforested regions and releases greenhouse gases directly, together with aerosols, through biomass burning that frequently accompanies it.
A second reason that climate change has been attributed to land use is that the terrestrial albedo is often altered by use, which leads to radiative forcing. This effect is more significant locally than globally.
Livestock and land use.
Worldwide, livestock production occupies 70% of all land used for agriculture, or 30% of the ice-free land surface of the Earth.
More than 18% of anthropogenic greenhouse gas emissions are attributed to livestock and livestock-related activities such as deforestation and increasingly fuel-intensive farming practices. Specific attributions to the livestock sector include:
Aerosols.
With virtual certainty, scientific consensus has attributed various forms of climate change, chiefly cooling effects, to aerosols, which are small particles or droplets suspended in the atmosphere.
Key sources to which anthropogenic aerosols are attributed include:
Attribution of 20th century climate change.
Over the past 150 years human activities have released increasing quantities of greenhouse gases into the atmosphere. This has led to increases in mean global temperature, or global warming. Other human effects are relevant—for example, sulphate aerosols are believed to have a cooling effect. Natural factors also contribute. According to the historical temperature record of the last century, the Earth's near-surface air temperature has risen around 0.74 ± 0.18 °Celsius (1.3 ± 0.32 °Fahrenheit).
A historically important question in climate change research has regarded the relative importance of human activity and non-anthropogenic causes during the period of instrumental record. In the 1995 Second Assessment Report (SAR), the IPCC made the widely quoted statement that "The balance of evidence suggests a discernible human influence on global climate". The phrase "balance of evidence" suggested the (English) common-law standard of proof required in civil as opposed to criminal courts: not as high as "beyond reasonable doubt". In 2001 the Third Assessment Report (TAR) refined this, saying "There is new and stronger evidence that most of the warming observed over the last 50 years is attributable to human activities". The 2007 Fourth Assessment Report (AR4) strengthened this finding:
Other findings of the IPCC Fourth Assessment Report include:
Over the past five decades there has been a global warming of approximately 0.65 °C (1.17 °F) at the Earth's surface (see historical temperature record). Among the possible factors that could produce changes in global mean temperature are internal variability of the climate system, external forcing, an increase in concentration of greenhouse gases, or any combination of these. Current studies indicate that the increase in greenhouse gases, most notably , is mostly responsible for the observed warming. Evidence for this conclusion includes:
Details on attribution.
Recent scientific assessments find that most of the warming of the Earth's surface over the past 50 years has been caused by human activities (see also the section on scientific literature and opinion). This conclusion rests on multiple lines of evidence. Like the warming "signal" that has gradually emerged from the "noise" of natural climate variability, the scientific evidence for a human influence on global climate has accumulated over the past several decades, from many hundreds of studies. No single study is a "smoking gun." Nor has any single study or combination of studies undermined the large body of evidence supporting the conclusion that human activity is the primary driver of recent warming.
The first line of evidence is based on a physical understanding of how greenhouse gases trap heat, how the climate system responds to increases in greenhouse gases, and how other human and natural factors influence climate. The second line of evidence is from indirect estimates of climate changes over the last 1,000 to 2,000 years. These records are obtained from living things and their remains (like tree rings and corals) and from physical quantities (like the ratio between lighter and heavier isotopes of oxygen in ice cores), which change in measurable ways as climate changes. The lesson from these data is that global surface temperatures over the last several decades are clearly unusual, in that they were higher than at any time during at least the past 400 years. For the Northern Hemisphere, the recent temperature rise is clearly unusual in at least the last 1,000 years (see graph opposite).
The third line of evidence is based on the broad, qualitative consistency between observed changes in climate and the computer model simulations of how climate would be expected to change in response to human activities. For example, when climate models are run with historical increases in greenhouse gases, they show gradual warming of the Earth and ocean surface, increases in ocean heat content and the temperature of the lower atmosphere, a rise in global sea level, retreat of sea ice and snow cover, cooling of the stratosphere, an increase in the amount of atmospheric water vapor, and changes in large-scale precipitation and pressure patterns. These and other aspects of modelled climate change are in agreement with observations.
"Fingerprint" studies.
Finally, there is extensive statistical evidence from so-called "fingerprint" studies. Each factor that affects climate produces a unique pattern of climate response, much as each person has a unique fingerprint. Fingerprint studies exploit these unique signatures, and allow detailed comparisons of modelled and observed climate change patterns. Scientists rely on such studies to attribute observed changes in climate to a particular cause or set of causes. In the real world, the climate changes that have occurred since the start of the Industrial Revolution are due to a complex mixture of human and natural causes. The importance of each individual influence in this mixture changes over time. Of course, there are not multiple Earths, which would allow an experimenter to change one factor at a time on each Earth, thus helping to isolate different fingerprints. Therefore, climate models are used to study how individual factors affect climate. For example, a single factor (like greenhouse gases) or a set of factors can be varied, and the response of the modelled climate system to these individual or combined changes can thus be studied.
For example, when climate model simulations of the last century include all of the major influences on climate, both human-induced and natural, they can reproduce many important features of observed climate change patterns. When human influences are removed from the model experiments, results suggest that the surface of the Earth would actually have cooled slightly over the last 50 years (see graph, opposite). The clear message from fingerprint studies is that the observed warming over the last half-century cannot be explained by natural factors, and is instead caused primarily by human factors.
Another fingerprint of human effects on climate has been identified by looking at a slice through the layers of the atmosphere, and studying the pattern of temperature changes from the surface up through the stratosphere (see the section on solar activity). The earliest fingerprint work focused on changes in surface and atmospheric temperature. Scientists then applied fingerprint methods to a whole range of climate variables, identifying human-caused climate signals in the heat content of the oceans, the height of the tropopause (the boundary between the troposphere and stratosphere, which has shifted upward by hundreds of feet in recent decades), the geographical patterns of precipitation, drought, surface pressure, and the runoff from major river basins.
Studies published after the appearance of the IPCC Fourth Assessment Report in 2007 have also found human fingerprints in the increased levels of atmospheric moisture (both close to the surface and over the full extent of the atmosphere), in the decline of Arctic sea ice extent, and in the patterns of changes in Arctic and Antarctic surface temperatures.
The message from this entire body of work is that the climate system is telling a consistent story of increasingly dominant human influence – the changes in temperature, ice extent, moisture, and circulation patterns fit together in a physically consistent way, like pieces in a complex puzzle.
Increasingly, this type of fingerprint work is shifting its emphasis. As noted, clear and compelling scientific evidence supports the case for a pronounced human influence on global climate. Much of the recent attention is now on climate changes at continental and regional scales, and on variables that can have large impacts on societies. For example, scientists have established causal links between human activities and the changes in snowpack, maximum and minimum (diurnal) temperature, and the seasonal timing of runoff over mountainous regions of the western United States. Human activity is likely to have made a substantial contribution to ocean surface temperature changes in hurricane formation regions. Researchers are also looking beyond the physical climate system, and are beginning to tie changes in the distribution and seasonal behaviour of plant and animal species to human-caused changes in temperature and precipitation.
For over a decade, one aspect of the climate change story seemed to show a significant difference between models and observations. In the tropics, all models predicted that with a rise in greenhouse gases, the troposphere would be expected to warm more rapidly than the surface. Observations from weather balloons, satellites, and surface thermometers seemed to show the opposite behaviour (more rapid warming of the surface than the troposphere). This issue was a stumbling block in understanding the causes of climate change. It is now largely resolved. Research showed that there were large uncertainties in the satellite and weather balloon data. When uncertainties in models and observations are properly accounted for, newer observational data sets (with better treatment of known problems) are in agreement with climate model results.
[[File:Effect of various natural and human factors on global mean temperature between 1889-2006 (NASA).png|thumb|right|alt=Refer to caption|This set of graphs shows the estimated contribution of various natural and human factors to changes in global mean temperature between 1889–2006. Estimated contributions are based on multivariate analysis rather than model simulations. The graphs show that human influence on climate has eclipsed the magnitude of natural temperature changes over the past 120 years. Natural influences on temperature—El Niño, solar variability, and volcanic aerosols—have varied approximately plus and minus 0.2 °C (0.4 °F), (averaging to about zero), while human influences have contributed roughly 0.8 °C (1 °F) of warming since 1889.]]
This does not mean, however, that all remaining differences between models and observations have been resolved. The observed changes in some climate variables, such as Arctic sea ice, some aspects of precipitation, and patterns of surface pressure, appear to be proceeding much more rapidly than models have projected. The reasons for these differences are not well understood. Nevertheless, the bottom-line conclusion from climate fingerprinting is that most of the observed changes studied to date are consistent with each other, and are also consistent with our scientific understanding of how the climate system would be expected to respond to the increase in heat-trapping gases resulting from human activities.
Extreme weather events.
One of the subjects discussed in the literature is whether or not extreme weather events can be attributed to human activities. Seneviratne "et al." (2012) stated that attributing individual extreme weather events to human activities was challenging. They were, however, more confident over attributing changes in long-term trends of extreme weather. For example, Seneviratne "et al." (2012) concluded that human activities had likely led to a warming of extreme daily minimum and maximum temperatures at the global scale.
[[File:Shifting Distribution of Summer Temperature Anomalies2.png|thumb|left|alt=refer to caption|Frequency of occurrence (vertical axis) of local June–July–August temperature anomalies (relative to 1951–1980 mean) for Northern Hemisphere land in units of local standard deviation (horizontal axis). According to Hansen "et al." (2012), the distribution of anomalies has shifted to the right as a consequence of global warming, meaning that unusually hot summers have become more common. This is analogous to the rolling of a dice: cool summers now cover only half of one side of a six-sided die, white covers one side, red covers four sides, and an extremely hot (red-brown) anomaly covers half of one side.]]
Another way of viewing the problem is to consider the effects of human-induced climate change on the probability of future extreme weather events. Stott "et al." (2003), for example, considered whether or not human activities had increased the risk of severe heat waves in Europe, like the one experienced in 2003. Their conclusion was that human activities had very likely more than doubled the risk of heat waves of this magnitude.
An analogy can be made between an athlete on steroids and human-induced climate change. In the same way that an athlete's performance may increase from using steroids, human-induced climate change increases the risk of some extreme weather events.
Hansen "et al." (2012) suggested that human activities have greatly increased the risk of summertime heat waves. According to their analysis, the land area of the Earth affected by very hot summer temperature anomalies has greatly increased over time (refer to graphs on the left). In the base period 1951-1980, these anomalies covered a few tenths of 1% of the global land area. In recent years, this has increased to around 10% of the global land area. With high confidence, Hansen "et al." (2012) attributed the 2010 Moscow and 2011 Texas heat waves to human-induced global warming.
An earlier study by Dole "et al." (2011) concluded that the 2010 Moscow heatwave was mostly due to natural weather variability. While not directly citing Dole "et al." (2011), Hansen "et al." (2012) rejected this type of explanation. Hansen "et al." (2012) stated that a combination of natural weather variability and human-induced global warming was responsible for the Moscow and Texas heat waves.
Scientific literature and opinion.
There are a number of examples of published and informal support for the consensus view. As mentioned earlier, the IPCC has concluded that most of the observed increase in globally averaged temperatures since the mid-20th century is "very likely" due to human activities. The IPCC's conclusions are consistent with those of several reports produced by the US National Research Council.
A report published in 2009 by the U.S. Global Change Research Program concluded that "warming is unequivocal and primarily human-induced."
A number of scientific organizations have issued statements that support the consensus view. Two examples include:
Reviews of scientific opinion.
As described above, a small minority of scientists do disagree with the consensus: see list of scientists opposing global warming consensus. For example Willie Soon and Richard Lindzen say that there is insufficient proof for anthropogenic attribution. Generally this position requires new physical mechanisms to explain the observed warming.
Difficulties in attribution.
At the time of the IPCC Fourth Assessment Report, attribution was possible for a number of observed changes in the climate (see effects of global warming). However, attribution was found to be more difficult when assessing changes over smaller regions (less than continental scale) and over short time periods (less than 50 years).
Over larger regions, averaging reduces natural variability of the climate, making detection and attribution easier.
Solar activity.
Solar sunspot maximum occurs when the magnetic field of the sun collapses and reverse as part of its average 11 year solar cycle (22 years for complete North to North restoration).
The role of the sun in recent climate change has been looked at by climate scientists. Since 1978, output from the Sun has been measured by satellites significantly more accurately than was previously possible from the surface. These measurements indicate that the Sun's total solar irradiance has not increased since 1978, so the warming during the past 30 years cannot be directly attributed to an increase in total solar energy reaching the Earth (see graph above, left). In the three decades since 1978, the combination of solar and volcanic activity probably had a slight cooling influence on the climate.
Climate models have been used to examine the role of the sun in recent climate change.
Models are unable to reproduce the rapid warming observed in recent decades when they only take into account variations in total solar irradiance and volcanic activity. Models are, however, able to simulate the observed 20th century changes in temperature when they include all of the most important external forcings, including human influences and natural forcings. As has already been stated, Hegerl "et al." (2007) concluded that greenhouse gas forcing had "very likely" caused most of the observed global warming since the mid-20th century. In making this conclusion, Hegerl "et al." (2007) allowed for the possibility that climate models had been underestimated the effect of solar forcing.
The role of solar activity in climate change has also been calculated over longer time periods using "proxy" datasets, such as tree rings.
Models indicate that solar and volcanic forcings can explain periods of relative warmth and cold between A.D. 1000 and 1900, but human-induced forcings are needed to reproduce the late-20th century warming.
Another line of evidence against the sun having caused recent climate change comes from looking at how temperatures at different levels in the Earth's atmosphere have changed.
Models and observations (see figure above, middle) show that greenhouse gas results in warming of the lower atmosphere at the surface (called the troposphere) but cooling of the upper atmosphere (called the stratosphere). Depletion of the ozone layer by chemical refrigerants has also resulted in a cooling effect in the stratosphere. If the sun was responsible for observed warming, warming of the troposphere at the surface and warming at the top of the stratosphere would be expected as increase solar activity would replenish ozone and oxides of nitrogen. The stratosphere has a reverse temperature gradient than the troposphere so as the temperature of the troposphere cools with altitude, the stratosphere rises with altitude. Hadley cells are the mechanism by which equatorial generated ozone in the tropics (highest area of UV irradiance in the stratosphere) is moved poleward. Global climate models suggest that climate change may widen the Hadley cells and push the jetstream northward thereby expanding the tropics region and resulting in warmer, dryer conditions in those areas overall.
Non-consensus views.
Habibullo Abdussamatov (2004), head of space research at St. Petersburg's Pulkovo Astronomical Observatory in Russia, has argued that the sun is responsible for recently observed climate change. Journalists for news sources canada.com (Solomon, 2007b), National Geographic News (Ravillious, 2007), and LiveScience (Than, 2007) reported on the story of warming on Mars. In these articles, Abdussamatov was quoted. He stated that warming on Mars was evidence that global warming on Earth was being caused by changes in the sun.
Ravillious (2007) quoted two scientists who disagreed with Abdussamatov: Amato Evan, a climate scientist at the University of Wisconsin-Madison, in the US, and Colin Wilson, a planetary physicist at Oxford University in the UK. According to Wilson, "Wobbles in the orbit of Mars are the main cause of its climate change in the current era" (see also orbital forcing). Than (2007) quoted Charles Long, a climate physicist at Pacific Northwest National Laboratories in the US, who disagreed with Abdussamatov.
Than (2007) pointed to the view of Benny Peiser, a social anthropologist at Liverpool John Moores University in the UK. In his newsletter, Peiser had cited a blog that had commented on warming observed on several planetary bodies in the Solar system. These included Neptune's moon Triton, Jupiter, Pluto and Mars. In an e-mail interview with Than (2007), Peiser stated that:"I think it is an intriguing coincidence that warming trends have been observed on a number of very diverse planetary bodies in our solar system, (...) Perhaps this is just a fluke."Than (2007) provided alternative explanations of why warming had occurred on Triton, Pluto, Jupiter and Mars.
The US Environmental Protection Agency (US EPA, 2009) responded to public comments on climate change attribution. A number of commenters had argued that recent climate change could be attributed to changes in solar irradiance. According to the US EPA (2009), this attribution was not supported by the bulk of the scientific literature. Citing the work of the IPCC (2007), the US EPA pointed to the low contribution of solar irradiance to radiative forcing since the start of the Industrial Revolution in 1750. Over this time period (1750 to 2005), the estimated contribution of solar irradiance to radiative forcing was 5% the value of the combined radiative forcing due to increases in the atmospheric concentrations of carbon dioxide, methane and nitrous oxide (see graph opposite).
Effect of cosmic rays.
Henrik Svensmark has suggested that the magnetic activity of the sun deflects cosmic rays, and that this may influence the generation of cloud condensation nuclei, and thereby have an effect on the climate. The website ScienceDaily reported on a 2009 study that looked at how past changes in climate have been affected by the Earth's magnetic field. Geophysicist Mads Faurschou Knudsen, who co-authored the study, stated that the study's results supported Svensmark's theory. The authors of the study also acknowledged that plays an important role in climate change.
Consensus view on cosmic rays.
The view that cosmic rays could provide the mechanism by which changes in solar activity affect climate is not supported by the literature. Solomon "et al." (2007) state:[..] the cosmic ray time series does not appear to correspond to global total cloud cover after 1991 or to global low-level cloud cover after 1994. Together with the lack of a proven physical mechanism and the plausibility of other causal factors affecting changes in cloud cover, this makes the association between galactic cosmic ray-induced changes in aerosol and cloud formation controversial
Studies by Lockwood and Fröhlich (2007) and Sloan and Wolfendale (2008) found no relation between warming in recent decades and cosmic rays. Pierce and Adams (2009) used a model to simulate the effect of cosmic rays on cloud properties. They concluded that the hypothesized effect of cosmic rays was too small to explain recent climate change. Pierce and Adams (2009) noted that their findings did not rule out a possible connection between cosmic rays and climate change, and recommended further research.
Erlykin "et al." (2009) found that the evidence showed that connections between solar variation and climate were more likely to be mediated by direct variation of insolation rather than cosmic rays, and concluded: "Hence within our assumptions, the effect of varying solar activity, either by direct solar irradiance or by varying cosmic ray rates, must be less than 0.07 °C since 1956, i.e. less than 14% of the observed global warming." Carslaw (2009) and Pittock (2009) review the recent and historical literature in this field and continue to find that the link between cosmic rays and climate is tenuous, though they encourage continued research. US EPA (2009) commented on research by Duplissy "et al." (2009):The CLOUD experiments at CERN are interesting research but do not provide conclusive evidence that cosmic rays can serve as a major source of cloud seeding. Preliminary results from the experiment (Duplissy et al., 2009) suggest that though there was some evidence of ion mediated nucleation, for most of the nucleation events observed the contribution of ion processes appeared to be minor. These experiments also showed the difficulty in maintaining sufficiently clean conditions and stable temperatures to prevent spurious aerosol bursts. There is no indication that the earlier Svensmark experiments could even have matched the controlled conditions of the CERN experiment. We find that the Svensmark results on cloud seeding have not yet been shown to be robust or sufficient to materially alter the conclusions of the assessment literature, especially given the abundance of recent literature that is skeptical of the cosmic ray-climate linkage
Earlier climate changes.
Factors other than increased concentrations can initiate warming or cooling episodes (see, "e.g.", orbital forcing). The ice core record shows that on some occasions temperature starts rising hundreds of years before increases. Such results confirm that the relationship between and climate can go in both directions: changes in concentrations can affect climate, while changes in climate can affect concentrations. One proposed mechanism for this effect is increased release of sequestered from oceans as circulation patterns shift, perhaps abruptly, in response to climate change.
A more speculative and polemical inference sometimes drawn is that the causal relationship between temperature rises and global concentrations is only one-way, so that historical increases in have been nothing more than the product of independently rising temperatures. However, a strictly "one-way" view of the relationship between and temperature contradicts basic results in physics, specifically the fact that the absorption and emission of infrared radiation by increases as its atmospheric concentration increases.
First principles as well as empirical observation suggest that positive feedbacks from concentrations amplify warming initially caused by other factors:
Close analysis of the relationship between the two curves temperature and shows that, within the uncertainties of matching their timescales, the temperature led by a few centuries. This is expected, since it was changes in the Earth's orbital parameters (including the shape of its orbit around the Sun, and the tilt of Earth's axis) that caused the small initial temperature rise. This then raised atmospheric levels, in part by outgassing from the oceans, causing the temperature to rise further. By amplifying each other's response, this "positive feedback" can turn a small initial perturbation into a large climate change. There is therefore no surprise that the temperature and rose in parallel, with the temperature initially in advance. In the current case, the situation is different, because human actions are raising the level, and we are starting to observe the temperature response. 
Present levels greatly exceed the range found in the ice core data. Isotopic analysis of atmospheric confirms that fossil fuel burning is the source of most of the increase, unlike during prior interglacial periods. As noted above, models that include increased levels when simulating recent climate match the observed data far better than those that do not.
Internal climate variability and global warming.
One of the issues that has been raised in the media is the view that global warming "stopped in 1998".
This view ignores the presence of internal climate variability, an example of which is the El Niño-Southern Oscillation (ENSO). The El Niño in 1998 was particularly strong, possibly one of the strongest of the 20th century.
Cooling between 2006 and 2008, for instance, has likely been driven by La Niña, the opposite of El Niño conditions. The area of cooler-than-average sea surface temperatures that defines La Niña conditions can push global temperatures downward, if the phenomenon is strong enough. Even accounting for the presence of internal climate variability, recent years rank among the warmest on record. For example, every year of the 2000s was warmer than the 1990 average.
Another example of internal climate variability is the Atlantic Multi-decadal Oscillation (AMO), which is an alternating pattern of heat-distributing ocean circulation that brings warmed waters from the tropics to high latitudes. One cycle of the oscillation takes about 65 to 70 years. Over that time, the amount of heat moved northward along the western side of the North Atlantic Ocean varies, increasing and decreasing temperatures in the North Atlantic Ocean and the surrounding continental margins.
Although not directly dictated by this decades-long phenomenon, global temperatures are influenced by the oscillation, just as average temperatures can be driven up or down by El Niño and La Niña events. The transport of such a massive quantity of heat helps explain some counter-intuitive temperature trends during the twentieth century. Despite the increase in greenhouse gas concentrations that began in the mid-1800s, between 1850 and 1900, global temperatures showed little significant change. Between 1900 and 1940, temperatures rose. After 1940, temperatures declined for 35 years. According to Michael Schlesinger, of the University of Illinois at Urbana-Champaign, the Atlantic Multi-decadal Oscillation helps to explain the lack of a direct correlation between concentrations and global temperatures over this time period.
David Easterling (Chief of the Scientific Services Division at NOAA's National Climatic Data Center) and co-author Michael Wehner have examined observed temperatures and model simulations of future temperatures. For observed temperatures, Easterling relied on a data set generated by NOAA's National Climatic Data Center, incorporating globally averaged surface air temperatures from 1901 to 2008. In that data set, he found two periods—1977 to 1985 and 1981 to 1989—that showed slight cooling, similar to what appears in the Hadley Centre's data set from 1998 to 2008.
For model simulations, Easterling used a database of predicted temperatures, validated by its ability to retroactively "predict" temperatures for years past. He assumed a "business-as-usual" scenario in terms of greenhouse gas emissions, with little future reduction. When projecting temperatures for the twenty-first century, he found two more periods—2001 to 2010 and 2016 to 2031—that showed no trend, again similar to 1998 to 2008. Every one of these no-trend periods occurred against a backdrop of rising temperatures.
Projected impacts of internal climate variability.
Easterling's finding that natural variability can cause short-term cooling, or no-trend periods within longer-term warming, correlates with other predictions about future temperatures. In May 2008, a group of climate modelers in Germany published projections that incorporated current understanding of natural cycles to make a climate forecast for the next decade. The group forecast that global surface temperatures might not increase much over the next decade (2010–2020), as cooling driven by natural variability offsets human-caused warming. After a decade or so of stability, however, the model indicates that temperatures would begin to rise.
In contrast, the UK Met Office predicted warming beginning in a few years from 2009. Both groups, however, agreed that after a short period of negative or no trend in the early 2000s, global temperatures would begin to rise, perhaps quickly. In other words, the no-trend period in the Hadley Centre's data set is consistent with what climatologists have predicted.
List of scientific organizations that support the consensus.
The following scientific organizations support the view that human activities have contributed to observed climate change:

</doc>
<doc id="3203" url="http://en.wikipedia.org/wiki?curid=3203" title="Achduart">
Achduart

Achduart (Gaelic: Achadh Dhubhaird) is a small hamlet in Coigach, in Wester Ross in northwestern Scotland, now within the Highland council area. It is situated about 4 km southeast of the village of Achiltibuie, at the end of a minor road. A footpath continues on to the hamlet of Culnacraig, then along the coast past Ben More Coigach to Strathcanaird. 
Achduart has accommodation facilities for tourists, who come for its proximity to the ocean as well as its seclusion and remoteness. There is a Scottish Youth Hostels Association hostel in Acheninver, a short distance to the north.
The name of Achduart comes from the Gaelic for "the field at the black headland". Achduart was part of the Estate of Coigach, Lochbroom, belonging to the Countess of Cromartie.
The dominant geographical feature in the area is Cairn Conmheall, which rises to 541 metres.

</doc>
<doc id="3204" url="http://en.wikipedia.org/wiki?curid=3204" title="Achiltibuie">
Achiltibuie

Achiltibuie ( or "Field of the yellow-haired boy") is a long linear village in Ross and Cromarty, Highland, on the Coigach coast of northwestern Scotland, overlooking Badentarbet Bay to the west. Loch Broom and the Summer Isles lie to the south. Located 10 miles (16 km) northwest of Ullapool as the crow flies, to the architecturally acclaimed Brochs of Coigach. 
History.
The first post office in the village opened on 28 July 1884.
The Eagle Of The Ninth.
The Roman epic The Eagle, based on the 1954 novel "The Eagle of the Ninth" by Rosemary Sutcliff, was filmed on location in Achiltibuie for a week in October 2009. The main location was Fox Point, Old Dornie. The Pictish village which was constructed at Fox Point was used on most days of the filming. Other sites included Achnahaird beach where a horse chase was filmed and Loch Lurgainn. 
Notable residents.
Achiltibuie is the main small village on the Coigach peninsula (total pop. Approx 300)
Notable recent achievements.
'Coigach Community Rowing' the crew members of which coastal rowing club are all local, won the World St Ayles Skiff Rowing Championships in July 2013 and a mixed crew from the club won the Alan Spong Trophy for 1st Mixed crew 4-oar rowing at the Thames Great River Race in September 2013. Coigach Community Rowing hand-built their two St Ayles rowing skiffs, the 'Coigach Lass' and the 'Lily~Rose' and race under the auspices of the Scottish Coastal Rowing Association, which is the governing body of St Ayles class coastal rowing around the world. The St Ayles skiff, a beautiful 22 foot long, beam 5'8" traditional style double end type rowing boat was designed by Iain Oughtered for the Anstruther Fisheries Museum in Fife to promote and re-introduce the sport of coastal rowing and associated boat building skills around Scotland and the rest of the world.

</doc>
<doc id="3205" url="http://en.wikipedia.org/wiki?curid=3205" title="Adaptive expectations">
Adaptive expectations

In economics, adaptive expectations is a hypothesized process by which people form their expectations about what will happen in the future based on what has happened in the past. For example, if inflation has been higher than expected in the past, people would revise expectations for the future.
One simple version of adaptive expectations is stated in the following equation, where formula_1 is the next year's rate of inflation that is currently expected; formula_2is this year's rate of inflation that was expected last year; and formula_3 is this year's actual rate of inflation:
where formula_5 is between 0 and 1. This says that current expectations of future inflation reflect past expectations and an "error-adjustment" term, in which current expectations are raised (or lowered) according to the gap between actual inflation and previous expectations. This error-adjustment is also called ""partial adjustment"."
The theory of adaptive expectations can be applied to all previous periods so that current inflationary expectations equal:
where formula_7 equals actual inflation formula_8 years in the past. Thus, current expected inflation reflects a weighted average of all past inflation, where the weights get smaller and smaller as we move further in the past.
Once a forecasting error is made by agents, due to a stochastic shock, they will be unable to correctly forecast the price level again even if the price level experiences no further shocks since they only ever incorporate part of their errors. The backward nature of expectation formulation and the resultant systematic errors made by agents (see Cobweb model) was unsatisfactory to economists such as John Muth, who was pivotal in the development of an alternative model of how expectations are formed, called rational expectations. This has largely replaced adaptive expectations in macroeconomic theory since its assumption of optimality of expectations is consistent with economic theory.

</doc>
<doc id="3209" url="http://en.wikipedia.org/wiki?curid=3209" title="Mexican tetra">
Mexican tetra

The Mexican tetra or blind cave fish ("Astyanax mexicanus") is a freshwater fish
of the family Characidae of the order Characiformes.
The type species of its genus, it is native to the Nearctic ecozone, originating in the lower Rio Grande and the Neueces and Pecos Rivers in Texas, as well as the central and eastern parts of Mexico.
Growing to a maximum overall length of 12 cm (4.7 in), the Mexican tetra is of typical characin shape, with unremarkable, drab coloration. Its blind cave form, however, is notable for having no eyes and being albino, that is, completely devoid of pigmentation; it has a pinkish-white color to its body.
This fish, especially the blind variant, is reasonably popular among aquarists.
"A. mexicanus" is a peaceful species that spends most of its time in midlevel water above the rocky and sandy bottoms of pools and backwaters of creeks and rivers of its native environment. Coming from a subtropical climate, it prefers water with 6.0–7.8 pH, a hardness of up to 30 dGH, and a temperature range of 20 to 25°C (68 to 77°F). In the winter, it migrates to warmer waters. Its natural diet consists of crustaceans, insects, and annelids, although in captivity it is omnivorous.
The Mexican tetra has been treated as a subspecies of "A. fasciatus," the banded tetra, but this is not widely accepted.
Blind cave form.
"A. mexicanus" is famous for its blind cave form, which is known by such names as blind cave tetra, blind tetra, and blind cavefish. These forms have lost their sight and even their eyes. These fish can still, however, find their way around by means of their lateral lines, which are highly sensitive to fluctuating water pressure. Currently, 29 cave populations are known, dispersed over three geographically distinct areas in a karst region of northeastern Mexico. Recent studies suggest at least two distinct genetic lineages occur among the blind populations, and the current distribution of populations arose by at least five independent invasions.
The eyed and eyeless forms of "A. mexicanus," being members of the same species, are closely related and can interbreed making this species an excellent model organism for examining convergent and parallel evolution, regressive evolution in cave animals, and the genetic basis of regressive traits.
"Astyanax jordani", another blind cave fish, is sometimes confused with the cave form of "A. mexicanus."
Evolution research.
The surface and cave forms of the Mexican tetra have proven powerful subjects for scientists studying evolution. When the surface-dwelling ancestors of current cave populations entered the subterranean environment, the change in ecological conditions rendered their phenotype—which included many biological functions dependent on the presence of light—subject to natural selection and genetic drift. One of the most striking changes to evolve was the loss of eyes. This is referred to as a "regressive trait" because the surface fish that originally colonized caves possessed eyes. In addition to regressive traits, cave forms evolved "constructive traits". In contrast to regressive traits, the purpose or benefit of constructive traits is generally accepted. Active research focuses on the mechanisms driving the evolution of regressive traits, such as the loss of eyes, in "A. mexicanus". Recent studies have produced evidence that the mechanism may be direct selection, or indirect selection through antagonistic pleiotropy, rather than genetic drift and neutral mutation, the traditionally favored hypothesis for regressive evolution.
The blind form of the Mexican tetra is different from the surface-dwelling form in a number of ways, including having unpigmented skin, having a better olfactory sense by having taste buds all over its head, and by being able to store four times more energy as fat, allowing it to deal with irregular food supplies more effectively.
Darwin said of sightless fish:
Modern genetics has made clear that the lack of use does not, in itself, necessitate a feature's disappearance. [http://www.findarticles.com/p/articles/mi_m1134/is_5_114/ai_n13811128] In this context, the positive genetic benefits have to be considered, i.e., what advantages are obtained by cave-dwelling tetras by losing their eyes? Possible explanations include:
Another likely explanation for the loss of its eyes is that of selective neutrality and genetic drift; in the dark environment of the cave, the eyes are neither advantageous nor disadvantageous and thus any genetic factors that might impair the eyes (or their development) can take hold with no consequence on the individual or species. Because there is no selection pressure for sight in this environment, any number of genetic abnormalities that give rise to the damage or loss of eyes could proliferate among the population with no effect on the fitness of the population.
Among some creationists, the cave tetra is seen as evidence 'against' evolution. One argument claims this is an instance of "devolution"—showing an evolutionary trend of decreasing complexity. But evolution is a nondirectional process, and while increased complexity is a common effect, there is no reason why evolution cannot tend towards simplicity if that makes an organism better suited to its environment.
Research by MIT biology professor Susan Lindquist shows that inhibition of the HSP90 protein has a dramatic effect in the development of the blind tetra. This research is seen by creationists as evidence of "built-in adaptability, not slow and gradual evolution."
In the aquarium.
The blind cave tetra is a fairly hardy species. Their lack of sight does not hinder their ability to get food. They prefer subdued lighting with a rocky substrate, like gravel, mimicking their natural environment. They become semi-aggressive as they age, and are by nature schooling fish.

</doc>
<doc id="3211" url="http://en.wikipedia.org/wiki?curid=3211" title="Atom probe">
Atom probe

The atom probe is a microscope used in material science that was invented in 1967 by Erwin Wilhelm Müller, J. A. Panitz, and S. Brooks McLane. The atom probe is closely related to the field ion microscope, the first microscopic instrument capable of atomic resolution, developed in 1951 by Erwin Wilhelm Müller.
Atom probes are unlike conventional optical or electron microscopes, in that the magnification effect comes from the magnification provided by a highly curved electric field, rather than by the manipulation of radiation paths. Technically, the method is destructive in nature removing ions from a sample surface in order to image and identify them, generating magnifications sufficient to observe individual atoms as they are removed from the sample surface. Through coupling of this magnification method with time of flight mass spectrometry, ions evaporated by application of electric pulses can have their mass-to-charge ratio computed.
Through successive evaporation of material, layers of atoms are removed from a specimen, allowing for probing not only of the surface, but also through the material itself. Computer methods are utilised to rebuild a three-dimensional view of the sample, prior to it being evaporated, providing atomic scale information on the structure of a sample, as well as providing the type atomic species information. The instrument allows the three-dimensional reconstruction of up to hundreds of millions of atoms from a sharp tip (corresponding to specimen volumes of 10,000-1,000,000 nm3).
Overview.
Atom probe samples are shaped to implicitly provide a highly curved electric potential to induce the resultant magnification, as opposed to direct use of lensing, such as via magnetic lenses. Furthermore, in normal operation (as opposed to a field ionization modes) the atom probe does not utilize a secondary source to probe the sample. Rather, the sample is evaporated in a controlled manner (field evaporation) and the evaporated ions are impacted onto a detector, which may be up to several meters from the specimen.
The samples used in atom probe are usually a metallic or semi-conducting material, with the needle geometry produced by electropolishing, or focused ion beam methods. Since 2006, commercial systems with laser pulsing have become available and this has expanded applications into insulating materials such as ceramics, and even geological materials. Preparation is done, often by hand, to manufacture a tip radius sufficient to induce a high electric field, with radii on the order of 100 nm.
To conduct an atom probe experiment, such a needle is placed in an ultra high vacuum chamber. After introduction into the vacuum system, the sample is reduced to cryogenic temperatures (typically 20-100 K) and manipulated such that the needle's point is aimed towards an ion detector. A pulsed high voltage source (typically 1-2 kV) is generated and applied to the specimen, with pulse repetition rates in the hundreds of kilohertz range. The application of the pulsed voltage to the sample allows for individual ions at the sample surface to have their electric field, and hence atomic bonding, temporarily disrupted. This results in ejection of an ionised atom from the sample surface at a known time. The delay between application of the pulse and detection of the ion allows for the computation of a mass-to-charge ratio.
Whilst the uncertainty in the atomic mass computed by time-of-flight methods in atom probe is sufficiently small to allow for detection of individual isotopes within a material this uncertainty may still, in some cases, confound definitive identification of atomic species. Effects such as superposition of differing ions with multiple electrons removed, or through the presence of complex species formation during evaporation may cause two or more species to have sufficiently close time-of-flights to make definitive identification impossible.
History.
Field ion microscopy.
Field ion microscopy techniques were initially construed as a modification of field emission, a technique which allows for a stream of electrons to be emitted from a sharp needle when subjected to a sufficiently high electric field (~3-6 V/nm). The needle is oriented towards a phosphor screen to create a projected image of the tip's work function, near the specimen surface. Whilst providing a projected image, the technique has limited resolution (2-2.5 nm), due to both quantum mechanical effects and lateral variation in electron velocity.
In field ion and atom probe methods, the polarity of the electric field is reversed, with a high positive field applied to the sample. For field ion microscopy a gas is introduced, known as the imaging gas, into the chamber at very low pressures. By application of a voltage during the presence of this gas, gas ions in the vicinity of the tip undergo ionisation, with greater numbers of ions generated immediately above atoms occupying edge or planar sites.
Imaging atom probe (IAP).
The imaging atom probe (IAP), invented in 1974 by J. A. Panitz, decreased the need to move the tip. In the IAP, ions emitted from the surface are recorded and mass analyzed at a detector placed within 12 cm of the tip (to provide a reasonably large field of view). By "time-gating" the detector for the arrival of a particular species of interest its crystallographic distribution on the surface, and as a function of depth, can be determined. Without time-gating all of the species reaching the detector are analyzed.
Atom-probe tomography (APT).
Atom-probe tomography (APT) uses a position-sensitive detector to deduce the lateral location of atoms. This allows 3-D reconstructions to be generated. The idea of the APT, inspired by J. A. Panitz's patent, was developed by Mike Miller starting in 1983 and culminated with the first prototype in 1986. Various refinements were made to the instrument, including the use of a so-called position-sensitive (PoS) detector by Alfred Cerezo, Terence Godfrey, and George D. W. Smith at Oxford University in 1988. The Tomographic Atom Probe (TAP), developed by researchers at the University of Rouen in France in 1993, introduced a multichannel timing system and multianode array. Both instruments (PoSAP and TAP) were commercialized by Oxford Nanoscience and CAMECA respectively. Since then, there have been many refinements to increase the field of view, mass and position resolution, and data acquisition rate of the instrument. The Local Electrode Atom Probe was first introduced in 2003 by Imago Scientific Instruments. In 2005, the commercialization of the pulsed laser atom probe (PLAP) expanded the avenues of research from highly conductive materials (metals) to poor conductors (semiconductors like silicon) and even insulating materials. AMETEK acquired CAMECA in 2007 and Imago Scientific Instruments (Madison, WI) in 2010, making the company the sole commercial developer of APTs with more than 60 instruments installed around the world in 2014.
The first few decades of work with APT focused on metals. However, more recent work has been done on semiconductors, ceramic and geologic materials, and organic biomaterials. The most advanced study of biological material to date using APT involved analyzing the chemical structure of teeth of the radula of chiton "Chaetopleura apiculata". In this study, the use of APT showed chemical maps of organic fibers in the surrounding nano-crystalline magnetite in the chiton teeth, fibers which were often co-located with sodium or magnesium. This method could be used for further study of enamel and dentin of human teeth and bones.
Theory.
Field evaporation.
Field evaporation is an effect that can occur when an atom bonded at the surface of a material is in the presence of a sufficiently high and appropriately directed electric field, where the electric field is the differential of electric potential (voltage) with respect to distance. Once this condition is met, it is sufficient that local bonding at the specimen surface is capable of being overcome by the field, allowing for evaporation of an atom from the surface to which it is otherwise bonded.
Ion flight.
Whether evaporated from the material itself, or ionised from the gas, the ions that are evaporated are accelerated by electrostatic force, acquiring most of their energy within a few tip-radii of the sample.
Subsequently, the accelerative force on any given ion is controlled by the electrostatic equation, where "n" is the ionisation state of the ion, and "e" is the fundamental electric charge.
This can be equated with the mass of the ion, "m", via Newton's law (F=ma):
Relativistic effects in the ion flight are usually ignored, as realisable ion speeds are only a very small fraction of the speed of light.
Assuming that the ion is accelerated during a very short interval, the ion can be assumed to be travelling at constant velocity. As the ion will travel from the tip at voltage V1 to some nominal ground potential, the speed at which the ion is travelling can be estimated by the energy transferred into the ion during (or near) ionisation. Therefore the ion speed can be computed with the following equation, which relates kinetic energy to energy gain due to the electric field.
Where "U" is the ion velocity. Solving for "U", the following relation is found:
Let's say that for at a certain ionization voltage, a singly charged hydrogen ion acquires a resulting velocity of X ms−1. A singly charged deuterium ion under the sample conditions would have acquired roughly X/1.41 ms−1. If a detector was placed at a distance of 1 m, the ion flight times would be 1/X and 1.41/X s. Thus, the time of the ion arrival can be used to infer the ion type itself, if the evaporation time is known.
From the above equation, it can be re-arranged to show that
given a known flight distance. F, for the ion, and a known flight time, t,
and thus one can substitute these values to obtain the mass-to-charge for the ion.
Thus for an ion which traverses a 1 m flight path, across a time of 2000 ns, given an initial accelerating voltage of 5000 V and noting that one amu is 1×10−27 kg, the mass-to-charge ratio (more correctly the mass-to-ionisation value ratio) becomes Z amu/charge.
Magnification.
The magnification in an atom is due to the projection of ions radially away from the small, sharp tip. Subsequently, in the far field, the ions will be greatly magnified. This magnification is sufficient to observe field variations due to individual atoms, thus allowing in field ion and field evaporation modes for the imaging of single atoms.
The standard projection model for the atom probe is an emitter geometry that is based upon a revolution of a conic section, such as a sphere, hyperboloid or paraboloid. For these tip models, solutions to the field may be approximated or obtained analytically. The magnification for a spherical emitter is inversely proportional to the radius of the tip, given a projection directly onto a spherical screen, the following equation can be obtained geometrically.
Where rscreen is the radius of the detection screen from the tip centre, and rtip the tip radius. Practical tip to screen distances may range from several centimeters to several meters, with increased detector area required at larger to subtend the same field of view.
Practically speaking, the usable magnification will be limited by several effects, such as lateral vibration of the atoms prior to evaporation.
Whilst the magnification of both the field ion and atom probe microscopes is extremely high, the exact magnification is dependent upon conditions specific to the examined specimen, so unlike for conventional electron microscopes, there is often little direct control on magnification, and furthermore, obtained images may have strongly variable magnifications due to fluctuations in the shape of the electric field at the surface.
Reconstruction.
The computational conversion of the ion sequence data, as obtained from a position sensitive detector, to a three-dimensional visualisation of atomic types, is termed "reconstruction". Reconstruction algorithms are typically geometrically based, and have several literature formulations. Most models for reconstruction assume that the tip is a spherical object, and utilise empirical corrections to stereographic projection to convert detector positions back to a 2D surface embedded in R3. By sweeping this surface through R3 as a function of the ion sequence input data, such as via ion-ordering, a volume is generated onto which positions the 2D detector positions can be computed and placed three-dimensional space.
Typically the sweep takes the simple form of an advancement of the surface, such that the surface is expanded in a symmetric manner about its advancement axis, with the advancement rate set by some nominal volume attributed to each event, representative of the atomic volume of the atom prior to evaporation. This causes the final reconstructed volume to assume a rounded-conical shape, similar to a badminton shuttlecock. The detected events thus become a point cloud data with attributed experimentally measured values, such as ion time of flight or experimentally derived quantities, e.g. time of flight or detector data.
This form of data manipulation allows for rapid computer visualisation and analysis, with data presented as point cloud data with additional information, such as each ion's mass to charge (as computed from the velocity equation above), voltage or other auxiliary measured quantity or computation therefrom.
Data features.
The canonical feature of atom probe data its high spatial resolution in the direction through the material, which has been attributed to an ordered evaporation sequence. This data can therefore image near atomically sharp interfaces with the associated chemical information.
The data obtained from the evaporative process is however not without artefacts that form the physical evaporation or ionisation process. A key feature of the evaporation or field ion images is that the data density is highly inhomogeneous, due to the corrugation of the specimen surface at the atomic scale. This corrugation gives rise to strong electric field gradients in the near-tip zone (on the order of an atomic radii or less from the tip), which during ionisation deflects ions away from the electric field normal.
The resultant deflection means that in these regions of high curvature, atomic terraces are belied by a strong anisotropy in the detection density. Where this occurs due to a few atoms on a surface is usually referred to as a "pole", as these are coincident with the crystallographic axes of the specimen (FCC, BCC, HCP) etc. Where the edges of an atomic terrace causes deflection, a low density line is formed and is termed a "zone line".
These poles and zone-lines, whilst inducing fluctuations in data density in the reconstructed datasets, which can prove problematic during post-analysis, are critical for determining information such as angular magnification, as the crystallographic relationships between features are typically well known.
When reconstructing the data, owing to the evaporation of successive layers of material from the sample, the lateral and in-depth reconstruction values are highly anisotropic. Determination of the exact resolution of the instrument is of limited use, as the resolution of the device is set by the physical properties of the material under analysis.
Systems.
Atom probe devices have only relatively recently been produced commercially, and many designs have been constructed since the method's inception. Initial field ion microscopes, precursors to modern atom probes, were usually glass blown devices developed by individual research laboratories.
System layout.
At a minimum, an atom probe will consist of several key pieces of equipment.
Optionally, an atom probe may also include laser-optical systems for laser beam preparation, targeting and pulsing, if using laser-evaporation methods. Staged vacuum systems are regularly employed to ensure that the system vacuum conditions remain stable. In-situ reaction systems may also be employed for some studies.
Performance.
Collectable ion volumes were previously limited to several thousand, or tens of thousands of ionic events. Subsequent electronics and instrumentation development has increased the rate of data accumulation, with datasets of several tens of million atoms (dataset volumes of 105 nm3)
Applications.
Metallurgy.
Atom probe has typically been employed in the chemical analysis of alloy systems at the atomic level. This has arisen as a result of voltage pulsed atom probes providing good chemical and sufficient spatial information in these materials. Metal samples from large grained alloys may be simple to fabricate, particularly from wire samples, with hand-electropolishing techniques giving good results.
Subsequently, atom probe has been used in the analysis of the chemical composition of a wide range of alloys.
Such data is critical in determining the effeto of alloy constituents in a bulk material, identification of solid-state reaction features, such as solid phase precipitates. Such information may not be amenable to analysis by other means (e.g. TEM) owing to the difficulty in generating a three-dimensional dataset with composition.
Semiconductors.
Semi-conductor materials are often analysable in atom probe, however sample preparation may be more difficult, and interpretation of results may be more complex, particularly if the semi-conductor contains phases which evaporate at differing electric field strengths.
Applications such as ion implantation may be used to identify the distribution of dopants inside a semi-conducting material, which is increasingly critical in the correct design of modern nanometre scale electronics.
External links.
Research groups and facilities.
Americas.
National Institute of Standards and Technology, Boulder, Colorado

</doc>
<doc id="3212" url="http://en.wikipedia.org/wiki?curid=3212" title="Al Capone">
Al Capone

Alphonse Gabriel "Al" Capone (; January 17, 1899 – January 25, 1947) was a Chicago gangster who attained national fame during the Prohibition era. His seven year reign as crime boss ended when when he was 33 years old.
Born in the borough of Brooklyn in New York City to Italian immigrants, Capone was a Five Points Gang member who became a bouncer in organised crime premises such as brothels. In his early twenties, he moved to Chicago becoming bodyguard and trusted factotum for Johnny Torrio, head of a criminal syndicate illegally supplying alcohol, and politically protected through the Unione Siciliane. A conflict with the West Side gang was instrumental in Capone's rise and fall, Torrio had been precipitated into retirement after West Side gunmen almost killed him, thereby bringing about Capone's succession. He expanded the bootlegging business through increasingly violent means, but his mutually profitable relationships with mayor William Hale Thompson and the city's police, meant Capone seemed safe from law enforcement. Apparently revelling in the attention, such as the cheers when he appeared at ball games, Capone made donations to various charities and was viewed by many to be a "modern-day Robin Hood". However, the Saint Valentine's Day Massacre of West Siders damaged Chicago's image, leading influential citizens to demand action from central government.
The federal authorities became intent on jailing Capone and prosecuted him for tax evasion in 1931. The case was highly politicised and both prosecutors and judge later received preferment. During prior and ultimately abortive negotiations to pay the government any back tax he owed, Capone had made admissions of his income; the judge deemed these statements could be used as evidence at the trial, and also refused to let Capone plead guilty for a lighter sentence. The effect of such decisions by the judge was added to by the incompetence of Capone's defence attorneys. Capone was convicted and sentenced to a then record breaking 11 years in federal prison. Replacing his old defence team with lawyers expert in tax law, his appeal grounds were strengthened by a Supreme court ruling, but Capone again found his status as a symbol of criminality meant that judges decided in his disfavor. Already showing signs of syphilitic dementia by early in his sentence, he became increasingly debilitated before being released after 8 years. On January 25, 1947, he died from cardiac arrest after suffering a stroke. Capone's conviction had negligible effect on the prevalence of organised crime in Chicago.
Early life.
Alphonse (or Alfonse, which one is unknown) Gabriel Capone was born in the borough of Brooklyn in New York on January 17, 1899. His parents, Gabriele Capone (December 12, 1864 – November 14, 1920) and Teresina Raiola (December 28, 1867 – November 29, 1952), were immigrants from Italy. His father was a barber from Castellammare di Stabia, a town about south of Naples, and his mother was a seamstress and the daughter of Angelo Raiola from Angri, a town in the Province of Salerno.
Gabriele and Teresa had nine children: Alphonse "Scarface Al" Capone, James Capone (who later changed his name to Richard Hart and became, ironically, a Prohibition agent in Homer, Nebraska), Raffaele Capone (also known as Ralph "Bottles" Capone, who took charge of his brother's beverage industry), Salvatore "Frank" Capone, John Capone, Albert Capone, Matthew Capone, Rose Capone, and Mafalda Capone (who married John J. Maritote). His two brothers, Ralph Capone and Frank Capone worked with him in his empire. Frank did so until his death on April 1, 1924 and Ralph ran the bottling companies (both legal and illegal) early on, and was also the front man for the Chicago Outfit for some time until he was imprisoned for tax evasion in 1932. The Capone family immigrated to the United States, first immigrating from Italy to Fiume, Austria-Hungary (now Rijeka, Croatia) in 1893, traveling on a ship to the U.S. and finally settled at 95 Navy Street, in the Navy Yard section of downtown Brooklyn. Gabriele Capone worked at a nearby barber shop at 29 Park Avenue. When Al was 11, the Capone family moved to 38 Garfield Place in Park Slope, Brooklyn.
Capone showed promise as a student, but had trouble with the rules at his strict parochial Catholic school. He dropped out of school at the age of 14, after being expelled for hitting a female teacher in the face. He worked at odd jobs around Brooklyn, including a candy store and a bowling alley. During this time, Capone was influenced by gangster Johnny Torrio, whom he came to regard as a mentor.
Career.
After his initial stint with small-time gangs that included the Junior Forty Thieves and the Bowery Boys, Capone joined the Brooklyn Rippers and then the powerful Five Points Gang based in Lower Manhattan. During this time, he was employed and mentored by fellow racketeer Frankie Yale, a bartender in a Coney Island dance hall and saloon called the Harvard Inn. After he inadvertently insulted a woman while working the door at a Brooklyn night club, Capone was slashed by her brother, Frank Gallucio. The wounds led to the nickname that Capone loathed: "Scarface". Yale insisted that Capone apologize to Gallucio, and later Capone hired him as a bodyguard. When photographed, Capone hid the scarred left side of his face saying the injuries were war wounds. Capone was called "Snorky", a term for a sharp dresser, by his closest friends.
Marriage and family.
On December 30, 1918, at age 19, Capone married Mae Josephine Coughlin, who was Irish Catholic and who, earlier that month, had given birth to their first son, Albert Francis ("Sonny") Capone. As Capone was under the age of 21, his parents had to consent to the marriage in writing.
Chicago.
At about twenty years of age, Capone left New York for Chicago at the invitation of Johnny Torrio, who been imported by bootlegger James "Big Jim" Colosimo as an enforcer. Capone began in Chicago as bouncer in a brothel Torrio opened. It may have been around this time Capone contracted syphilis, which timely use of Salvarsan probably could have cured, but he apparently never sought treatment. In 1923, he purchased a small house at 7244 South Prairie Avenue in the Park Manor neighborhood on the city's south side for . In the early years of the decade, Capone's name began appearing in newspaper sports pages, where he was described as a boxing promoter.
Chicago's location on Lake Michigan gave access to a vast inland territory, and it was well-served by railroads. Torrio took over the crime empire of "Big Jim" Colosimo after his murder, which Capone was suspected of.
With Capone as his right hand man, Torrio headed an essentially Italian organized crime group that was the biggest in the city. Wary of being drawn into gang wars, he tried to proceed by negotiating agreements between rival crime groups over territory. The smaller, mixed ethnicity, North Side Gang of Dean O'Banion came under pressure from the Genna brothers, who were allied to Torrio. O'Banion found that for all Torrio's pretensions to be a settler of disputes, he was unhelpful with the encroachment of the Gennas into the North Side. In a fateful step, Torrio had, or acquiesced to the Gennas having, the relatively business-minded O'Banion killed at his flower shop in October 1924. This placed Hymie Weiss at the head of the the gang, backed by Vincent Drucci and Bugs Moran. Under Weiss, who had been a close friend of O'Banion, the North Siders treated revenge on his killers as a priority.
Boss.
In January 1925 Capone was ambushed, leaving him shaken but unhurt. Twelve days later, Torrio was returning from a shopping trip when he was shot several times. After recovering Torrio effectively resigned and handed over to Capone, who at 26 years old became the new boss of an organisation that took in illegal breweries and a transportation network that reached to Canada, with political and law-enforcement protection, In turn he was able to use more violence to increase revenue; refusal to purchase often resulted in the premises being blown up, as many as a hundred people were killed in liquor bombings during the twenties. Rivals saw Capone as responsible for the proliferation of brothels in the city.
Capone indulged in custom suits, cigars, gourmet food and drink (his preferred liquor was Templeton Rye from Iowa) and female companionship. He was particularly known for his flamboyant and costly jewelry. His favorite responses were "I am just a businessman, giving the people what they want", and "All I do is satisfy a public demand". Capone had become a national celebrity and talking point. 
After using bribery and widespread intimidation to take over during elections for the town council, Capone based himself in Cicero, this made it difficult for the North Siders to target him. Capone's driver was found tortured and murdered, there was then an attempt on Weiss's life in the Chicago Loop. On September 20, 1926, the North Side gang used a ploy outside the Capone headquarters the Hawthorne Inn, aimed at drawing him to the windows. Gunmen in several cars then opened fire with Thompson submachine guns and shotguns at the windows of first floor restaurant, where Capone was often found. He was unhurt, but called for a truce; the negotiations fell through. Three weeks later Weiss was killed outside the former O'Banion flower shop North Side headquarters. In January 1927, the Hawthorne's restaurant owner, a friend of Capone, was kidnapped and killed by Moran and Drucci. 
Capone became increasingly security-minded and desirous of getting away from Chicago. As a precaution, he and his entourage would often show up suddenly at one of Chicago's train depots and buy up an entire Pullman sleeper car on night trains to places such as Cleveland, Omaha, Kansas City, Little Rock or Hot Springs, where they would spend a week in luxury hotel suites under assumed names. In 1928, Capone bought a 14-room retreat on Palm Island, Florida, close to Miami Beach.
Political alliances.
The protagonists of Chicago's politics, and even newspaper circulation 'wars', had long been associated with questionable methods, but the need for bootleggers to have protection in city hall introduced a far more serious level of violence and graft. Capone is generally seen as having had an appreciable effect in bringing about the victories of Republican William Hale Thompson especially in the 1927 mayoral campaign when Thompson campaigned for a wide open town, at one time hinting that he'd reopen illegal saloons. Such a proclamation helped Thompson's campaign gain the support of Capone and Thompson's campaign allegedly accepted a contribution of $250,000 from the gangster. In the 1927 mayoral race, Thompson beat William Emmett Dever by a relatively slim margin. Thompson's powerful Cook County machine had drawn on the often-parochial Italian community, but this was in tension with his highly successful courting of African Americans.
Capone continued to back Thompson and on the polling day of April 10, 1928 in the so-called Pineapple Primary, voting booths in the wards where Thompson's opponents were thought to have support were targeted by Capone's bomber, James Belcastro, causing the deaths of at least 15 people. Belcastro also was accused of the murder of lawyer Octavius Granady, an African American who dared to challenged Thompson's candidate for the African American vote, and was chased through the streets on polling day by cars of gunmen before being shot dead. Belcastro's co-charged included four policemen; all charges were dropped after key witnesses recanted their statements. An indication of the attitude of local law enforcement to Capone's organisation came in 1931 when Belcastro was wounded in a shooting; police suggested to skeptical journalists that Belcastro was an independent operator. The 1929 Saint Valentine's Day massacre led to public disquiet about Thompson's alliance with Capone; a factor in Anton J. Cermak winning the mayoral election on April 6, 1931.
Saint Valentine's Day Massacre.
Capone was widely assumed to have been responsible for ordering the 1929 Saint Valentine's Day Massacre in an attempt to kill the head of the much-attenuated 'North Side' gang, Bugs Moran. Moran was the last surviver of the main north side gunmen, his succession had come because his similarly aggressive predecessors Vincent Drucci and Hymie Weiss had been killed in the violence that followed the murder of original leader, Dean O'Banion.
To monitor their targets' habits and movements, Capone's men rented an apartment across from the trucking warehouse that served as a Moran headquarters. On the morning of Thursday February 14, 1929, Capone's lookouts signaled gunmen disguised as police to start a "raid". The "faux" police lined the seven victims along a wall without a struggle then signaled for accomplices with machine guns. The seven victims were machine-gunned and shot-gunned. Photos of the massacre victims shocked the public and damaged Capone's reputation. Within days Capone received a summons to testify before a Chicago grand jury on violations of federal Prohibition law, but he claimed to be too unwell to attend at that time.
Trials.
In March 27, 1929, as he left a Chicago courtroom after testifying to a grand jury investigating violations of federal prohibition laws, Capone was arrested by FBI agents on charges of having committed contempt of court by feigning illness to avoid an earlier appearance. In May 1929, Capone was sentenced to a prison term in Philadelphia, having been convicted within 16 hours of an arrest for carrying a gun during a trip there. A week after he was released, in March 1930, Capone was listed as the number one 'Public Enemy' on the unofficial Chicago Crime Commission's widely publicized list.
In April 1930, Capone was arrested on vagrancy charges when visiting his Miami Beach property, the governor having ordered sheriffs to run him out of the state. For having claimed Miami police had refused him food and water and threatened to arrest his family, Capone was charged with perjury, but acquitted after a three day trial in July. In September a Chicago judge issued a warrant for Capone on charges of vagrancy, and then used the publicity to run against Thompson in the Republican primary. In Feb 1931 Capone was tried on the contempt of court charge. In court Judge James Herbert Wilkerson intervened to reinforce questioning of Capone's doctor by the prosecutor (who Wilkerson later went into private practice with). Wilkerson sentenced Capone to six months, but while on appeal of the contempt conviction he remained free.
The key to Capone's conviction on tax charges originated in his offer to pay tax. Ralph, his brother and a gangster in his own right, was tried for tax evasion in 1930. After being convicted in a two week trial Wilkerson presided over, Ralph spent the next three years in prison. Following Ralph and others' convictions on tax charges Capone had ordered his lawyer to regularise his tax position. Crucially, during the ultimately abortive negotiations that followed, his lawyer stated the income Capone was willing to pay tax on for various years, for instance income of $100,000 for 1928 and 1929 was admitted. Hence without any investigation the government had been given a letter from a lawyer acting for Capone conceding his large taxable income for certain years. In 1931 Capone was indicted for income tax evasion, as well as and various violations of the Volstead Act (Prohibition) at the Chicago Federal Building in the courtroom of Judge James Herbert Wilkerson. U. S. Attorney George E. Q. Johnson agreed to a deal that that he hoped might result in the judge giving Capone a couple of years, but Judge Wilkerson (who had been aware of the deal all along) refused to allow Capone to plead guilty for a reduced sentence. On the second day of the trial Judge Wilkerson overruled objections that a lawyer could not confess for his client. Saying anyone making a statement to the government did so at his own risk, Wilkerson deemed the 1930 letter to federal authorities from a lawyer acting for Capone could be admitted into evidence.
Much was later made of other evidence such as witnesses and ledgers but these strongly implied rather than stated Capone's control. The ledgers were inadmissible on statute of limitations grounds, but Capone's lawyers incompetently failed to make the necessary timely objection, they also ran a basically irrelevant defence of gambling losses. Judge Wilkerson allowed Capone's spending to be presented at very great length. Although there was no doubt that Capone "spent" vast sums, legally speaking the case against him centred on the size of his income. Capone was convicted and in November 1931 was sentenced to eleven years in federal prison, fined $50,000 plus $7,692 for court costs, and in addition liable for $215,000 plus interest due on his back taxes. The contempt of court sentence was concurrently served. New lawyers hired to represent Capone were Washington-based tax experts. They filed a writ of habeas corpus based on a Supreme court ruling that tax evasion was not fraud, which apparently meant Capone had been convicted on charges relating to years that were actually outside the time limit for prosecution. However, a judge creatively interpreted the law so that the time Capone had spent in Miami was subtracted from the age of the offences, thereby denying the appeal of both Capone's conviction and sentence.
Imprisonment.
In May 1932, aged 33, Capone was sent to Atlanta U.S. Penitentiary. Upon his arrival at Atlanta, the 250 lb Capone was officially diagnosed with syphilis and gonorrhea. He was also suffering from withdrawal symptoms from cocaine addiction, use of which had perforated his septum. Capone was competent at his prison job of stitching soles on shoes for eight hours a day, but his letters were barely coherent. He was seen as a weak personality and so out of his depth dealing with bullying fellow inmates that his cellmate, seasoned convict, Red Rudinsky, feared Capone would have a breakdown. Rudinsky, formerly a small time criminal associated with the Capone gang, found himself becoming a protector for Capone. The conspicuous protection of Rudinsky and other friendly prisoners, as well as accusations from less friendly inmates, fueled suspicion that Capone was receiving special treatment in Atlanta. While no solid evidence ever emerged, it formed part of the rationale for moving Capone to the recently opened Alcatraz Federal Penitentiary
At Alcatraz, Capone's decline became increasingly evident as neurosyphilis progressively eroded his mental faculties. He spent the last year of his sentence in the prison hospital, confused and disoriented. Capone completed his term in Alcatraz on January 6, 1939, and was transferred to the Federal Correctional Institution at Terminal Island in California, to serve out his sentence for contempt of court. He was paroled on November 16, 1939, and, after having spent a short time in a hospital, returned to his home in Palm Island, Florida.
Later years and death.
Capone's mental health had continued to deteriorate due to neurosyphilis. In 1946, his physician and a Baltimore psychiatrist performed examinations and concluded Capone had the mental capability of a 12-year-old child.
Capone spent the last years of his life at his mansion in Florida. On January 21, 1947, Capone had a stroke. He regained consciousness and started to improve but contracted pneumonia. He suffered a fatal cardiac arrest the next day. On January 25, 1947, Al Capone died in his home, surrounded by his family, and wаs buried аt Mount Carmel Cemetery in Hillside, Illinois.
Chicago aftermath.
Although the main effect of Capone's conviction was he ceased to be boss immediately on his imprisonment, those involved in the jailing of Capone portrayed it as having dealt a fatal blow to the city's organized crime syndicate. But, far from being smashed the Chicago Outfit continued, without being troubled by the Chicago police. Once prohibition was repealed, organised crime in the city, already chary of attention after the example of Capone's notoriety having brought him down, found it had a concomitantly lower profile and managed to move on and monopolize labour unions and gambling in the city without incurring serious investigation. In the late 50's, FBI agents discovered an organisation led by Capone's former lieutenants reigning supreme over the Chicago underworld.
In popular culture.
One of the most notorious American gangsters of the 20th century, Capone has been the subject of numerous articles, books, and films. Capone's personality and character have been used in fiction as a model for crime lords and criminal masterminds ever since his death. The stereotypical image of a mobster wearing a blue pinstriped suit and tilted fedora is based on photos of Capone. His accent, mannerisms, facial construction, physical stature, and parodies of his name have been used for numerous gangsters in comics, movies, music, and literature.
Film and television.
Capone has been portrayed on screen by:
Actors playing characters based on Capone include:

</doc>
<doc id="3214" url="http://en.wikipedia.org/wiki?curid=3214" title="Amplifier figures of merit">
Amplifier figures of merit

In electronics, the figures of merit of an amplifier are numerical measures that characterize its properties and performance. Figures of merit can be given as a list of specifications that include properties such as gain, bandwidth, noise and linearity, among others listed in this article. Figures of merit are important for determining the suitability of a particular amplifier for an intended use.
Gain.
The gain of an amplifier is the ratio of output to input power or amplitude, and is usually measured in decibels. (When measured in decibels it is logarithmically related to the power ratio: "G"(dB)=10 log("Pout" /("Pin")). RF amplifiers are often specified in terms of the maximum power gain obtainable, while the voltage gain of audio amplifiers and instrumentation amplifiers will be more often specified (since the amplifier's input impedance will often be much higher than the source impedance, and the load impedance higher than the amplifier's output impedance). For example, an audio amplifier with a gain given as 20 dB will have a "voltage gain" of ten (but a power gain of 100 would only occur in the event the input and output impedances were identical).
If two equivalent amplifiers are being compared, the amplifier with higher gain settings would be more sensitive as it would take less input signal to produce a given amount of power.
Bandwidth.
The bandwidth of an amplifier is the range of frequencies for which the amplifier gives "satisfactory performance". The definition of "satisfactory performance" may be different for different applications. However, a common and well-accepted metric is the half power points (i.e. frequency where the power goes down by half its peak value) on the output vs. frequency curve. Therefore bandwidth can be defined as the difference between the lower and upper half power points. This is therefore also known as the bandwidth. Bandwidths (otherwise called "frequency responses") for other response tolerances are sometimes quoted (, etc.) or "plus or minus 1dB" (roughly the sound level difference people usually can detect).
The gain of a good quality full-range audio amplifier will be essentially flat between 20 Hz to about 20 kHz (the range of normal human hearing). In ultra high fidelity amplifier design, the amplifier's frequency response should extend considerably beyond this (one or more octaves either side) and might have points < 10 Hz and > . Professional touring amplifiers often have input and/or output filtering to sharply limit frequency response beyond ; too much of the amplifier's potential output power would otherwise be wasted on infrasonic and ultrasonic frequencies, and the danger of AM radio interference would increase. Modern switching amplifiers need steep low pass filtering at the output to get rid of high frequency switching noise and harmonics.
The range of frequency over which the gain is equal to or greater than 70.7% of its maximum gain is termed as bandwidth.
Efficiency.
Efficiency is a measure of how much of the power source is usefully applied to the amplifier's output. Class A amplifiers are very inefficient, in the range of 10–20% with a max efficiency of 25% for direct coupling of the output. Inductive coupling of the output can raise their efficiency to a maximum of 50%.
Drain efficiency is the ratio of output RF power to input DC power when primary input DC power has been fed to the drain of a field-effect transistor. Based on this definition, the drain efficiency cannot exceed 25% for a class A amplifier that is supplied drain bias current through resistors (because RF signal has its zero level at about 50% of the input DC). Manufacturers specify much higher drain efficiencies, and designers are able to obtain higher efficiencies by providing current to the drain of the transistor through an inductor or a transformer winding. In this case the RF zero level is near the DC rail and will swing both above and below the rail during operation. While the voltage level is above the DC rail current is supplied by the inductor.
Class B amplifiers have a very high efficiency but are impractical for audio work because of high levels of distortion (See: Crossover distortion). In practical design, the result of a tradeoff is the class AB design. Modern Class AB amplifiers commonly have peak efficiencies between 30–55% in audio systems and 50-70% in radio frequency systems with a theoretical maximum of 78.5%.
Commercially available Class D switching amplifiers have reported efficiencies as high as 90%. Amplifiers of Class C-F are usually known to be very high efficiency amplifiers. RCA manufactured an AM broadcast transmitter employing a single class-C low mu triode with an RF efficiency in the 90% range.
More efficient amplifiers run cooler, and often do not need any cooling fans even in multi-kilowatt designs. The reason for this is that the loss of efficiency produces heat as a by-product of the energy lost during the conversion of power. In more efficient amplifiers there is less loss of energy so in turn less heat.
In RF linear Power Amplifiers, such as cellular base stations and broadcast transmitters, special design techniques can be used to improve efficiency. Doherty designs, which use a second output stage as a "peak" amplifier, can lift efficiency from the typical 15% up to 30-35% in a narrow bandwidth. Envelope Tracking designs are able to achieve efficiencies of up to 60%, by modulating the supply voltage to the amplifier in line with the envelope of the signal.
Linearity.
An ideal amplifier would be a totally linear device, but real amplifiers are only linear within limits. 
When the signal drive to the amplifier is increased, the output also increases until a point is reached where some part of the amplifier becomes saturated and cannot produce any more output; this is called clipping, and results in distortion.
In most amplifiers a reduction in gain takes place before hard clipping occurs; the result is a "compression" effect, which (if the amplifier is an audio amplifier) sounds much less unpleasant to the ear. For these amplifiers, the 1 dB compression point is defined as the input power (or output power) where the gain is 1 dB less than the small signal gain. Sometimes this non linearity is deliberately designed in to reduce the audible unpleasantness of hard clipping under overload.
Ill effects of non linearity can be reduced with negative feedback.
Linearization is an emergent field, and there are many techniques, such as feed forward, predistortion, postdistortion, in order to avoid the undesired effects of the non-linearities.
Noise.
This is a measure of how much noise is introduced in the amplification process. Noise is an undesirable but inevitable product of the electronic devices and components; also, much noise results from intentional economies of manufacture and design time. The metric for noise performance of a circuit is noise figure or noise factor. Noise figure is a comparison between the output signal to noise ratio and the thermal noise of the input signal.
Output dynamic range.
Output dynamic range is the range, usually given in dB, between the smallest and largest useful output levels. The lowest useful level is limited by output noise, while the largest is limited most often by distortion. The ratio of these two is quoted as the amplifier dynamic range. More precisely, if "S" = maximal allowed signal power and "N" = noise power, the dynamic range "DR" is "DR = (S + N ) /N".
In many switched mode amplifiers, dynamic range is limited by the minimum output step size.
Slew rate.
Slew rate is the maximum rate of change of the output, usually quoted in volts per second (or microsecond). Many amplifiers are ultimately slew rate limited (typically by the impedance of a drive current having to overcome capacitive effects at some point in the circuit), which sometimes limits the full power bandwidth to frequencies well below the amplifier's small-signal frequency response.
Rise time.
The rise time, tr, of an amplifier is the time taken for the output to change from 10% to 90% of its final level when driven by a step input.
For a Gaussian response system (or a simple RC roll off), the rise time is approximated by: 
tr * BW = 0.35, where tr is rise time in seconds and BW is bandwidth in Hz.
Settling time and ringing.
The time taken for the output to settle to within a certain percentage of the final value (for instance 0.1%) is called the settling time, and is usually specified for oscilloscope vertical amplifiers and high accuracy measurement systems. Ringing refers to an output variation that cycles above and below an amplifier's final value and leads to a delay in reaching a stable output. Ringing is the result of overshoot caused by an underdamped circuit.
Overshoot.
In response to a step input, the overshoot is the amount the output exceeds its final, steady-state value.
Stability.
Stability is an issue in all amplifiers with feedback, whether that feedback is added intentionally or results unintentionally. It is especially an issue when applied over multiple amplifying stages.
Stability is a major concern in RF and microwave amplifiers. The degree of an amplifier's stability can be quantified by a so-called stability factor. There are several different stability factors, such as the Stern stability factor and the Linvil stability factor, which specify a condition that must be met for the absolute stability of an amplifier in terms of its two-port parameters.

</doc>
<doc id="3217" url="http://en.wikipedia.org/wiki?curid=3217" title="Army of Darkness">
Army of Darkness

Army of Darkness (also known as "Evil Dead 3: Army of Darkness" and stylized onscreen as "Bruce Campbell vs. Army of Darkness") is a 1992 American comedy-dark fantasy film directed by Sam Raimi. It is the third installment of "The Evil Dead" franchise. The film was written by Sam Raimi and his brother Ivan, produced by Robert Tapert, and stars Bruce Campbell (also acting as co-producer) and Embeth Davidtz. Continuing from "Evil Dead II", Ash Williams (Campbell) is trapped in the Middle Ages and battles the undead in his quest to return to the present.
The film was produced as part of a production deal with Universal Studios after the financial success of "Darkman". Filming took place in California in 1991. "Army of Darkness" premiered on October 9, 1992 at the Sitges Film Festival, and was released in the United States on February 19, 1993. It grossed $11.503 million domestically and another $10 million outside the USA for a total worldwide gross of $21.5 million. Critical response was positive. Since its video release it has acquired a massive cult following, along with the other two films in the trilogy. The film was dedicated to Irvin Shapiro, who died during the film's production in 1989 on New Year's Day.
On March 9, 2013, shortly before the release of "Evil Dead", a reboot and loose continuation of the franchise, Raimi confirmed that the fifth installment of the series would be "Army of Darkness 2". Campbell confirmed that he would star as an older, but not necessarily wiser, Ash. In addition, it will be followed by a crossover with the "Evil Dead" reboot and its upcoming sequel.
Plot.
After being pulled through a time portal, Ash Williams lands in 1300 AD, where he is almost immediately captured by Lord Arthur's men, who suspect him to be an agent for Duke Henry, with whom Arthur is at war. He is enslaved along with the captured Henry, his gun and chainsaw confiscated, and is taken to a castle. Ash is thrown in a pit where he fights off a Deadite and regains his weapons from Arthur's Wise Man. After demanding Henry and his men be set free (as he knew Henry was innocent, and his persecution was simply a witch hunt) and killing a deadite in full view of everyone, Ash is celebrated as a hero. He also grows attracted to Sheila, the sister of one of Arthur's fallen knights.
According to the Wise Man, the only way Ash can return to his time is to retrieve the "Necronomicon Ex-Mortis", a book with magical powers. After bidding goodbye to Sheila, Ash starts his search for the "Necronomicon". As he enters a haunted forest, an unseen force pursues Ash through the woods. Fleeing, he ducks into a windmill where he crashes into a mirror. The small reflections of Ash climb out from the shattered mirror and torment him. One of the reflections dives down Ash's throat and uses his body to become a life-sized clone of Ash and attack him, after which Ash kills and buries the clone.
When he arrives at the "Necronomicon"s location, he finds three books instead of one. Ash eventually finds the real one and attempts to say the magic phrase that will allow him to remove the book safely – "Klaatu barada nikto". However, forgetting the last word, he tries to trick the book by mumbling and coughing the missing word. He then grabs the book from the cradle, and rushes back to the castle, while the dead rise from graves all around. During Ash's panicked ride back, his evil copy rises from his grave and unites the Deadites into the Army of Darkness.
Despite causing the predicament faced by the medieval soldiers, Ash initially demands to be returned to his own time. However, Sheila is captured by a Flying Deadite, and later transformed into a Deadite. Ash becomes determined to lead the humans against the army of the dead. Reluctantly, the people agree to join Ash. Using scientific knowledge from textbooks in the trunk of his 1973 Oldsmobile Delta 88, and enlisting the help of Duke Henry, Ash successfully leads the medieval soldiers to victory over the Deadites and Evil Ash, saving Sheila and bringing peace between Arthur and Henry in the process.
Original Ending.
The original ending, preferred by Raimi and Campbell themselves, in which Ash oversleeps in the cave and wakes up in a post-apocalyptic future, was restored to the film for the UK VHS release, which also had the S-Mart ending put in as a post-credit extra. This scene has been restored on the "Army of Darkness: Director's Cut" Region 3 DVD released by MGM, the "director's cut bootleg edition" DVD and the double-disc DVD, which also featured the S-Mart ending of the film. The S-Mart ending was shot for the American release; the studio wanted to end the film on a high note for the character of Ash. Raimi believed Ash to be more of a fool, which is why he liked to torture him so much in his films; Ash being a goof and drinking too much potion was in his character.
Production.
Plans to make a third "Evil Dead" film had been circulating for a number of years, even prior to the production of "Darkman". "Evil Dead II" made enough money internationally that Dino De Laurentiis was willing to finance a sequel. Director and script writer Sam Raimi drew from a variety of sources, including literature with "A Connecticut Yankee in King Arthur's Court" and Jonathan Swift's "Gulliver's Travels" and films like "The Seventh Voyage of Sinbad", "Jason and the Argonauts", and The Three Stooges. "Evil Dead II", according to Bruce Campbell, "was originally designed to go back into the past to 1300, but we couldn't muster it at the time, so we decided to make an interim version, not knowing if the 1300 story would ever get made". Promotional drawings were created and published in "Variety" during the casting process before the budget was deemed too little for the plot. The working title for the project was "Evil Dead III: Army of Darkness". The title "Army of Darkness" came from an idea by Irvin Shapiro, during the production of "Evil Dead II". This was used after Sam Raimi was unable to use his original title "The Medieval Dead." ("The Medieval Dead" would later be used as the film's subtitle for its UK release as "Army of Darkness: The Medieval Dead").
Screenplay and pre-production.
Initially, Raimi invited Scott Spiegel to co-write "Army of Darkness" because he had done a good job on "Evil Dead II", but he was busy on rewrites for the Clint Eastwood film "The Rookie". After the good experience of writing the screenplay for a film called "Easy Wheels", Sam and his brother Ivan Raimi decided to co-write the film together. They worked on the script throughout the pre-production and production of "Darkman". After filming "Darkman", they took the script out and worked on it in more detail. Raimi says that Ivan "has a good sense of character" and that he brought more comedy into the script. Campbell remembers, "We all decided, 'Get him out of the cabin.' There were earlier drafts where part three still took place there, but we thought, 'Well, we all know that cabin, it's time to move on.' The three of us decided to keep it in 1300, because it's more interesting". Campbell and Tapert would read the script drafts, give Raimi their notes and he would decide which suggestions to keep and which ones to discard.
The initial budget was $8 million but during pre-production, it became obvious that this was not going to be enough. "Darkman" was also a financial success and De Laurentiis had multi-picture deal with Universal and so "Army of Darkness" became one of the films. The studio decided to contribute half of the film's $12 million budget. However, the film's ambitious scope and its extensive effects work forced Campbell, Raimi and producer Robert Tapert to put up $1 million of their collective salaries to shoot a new ending and not film a scene where a possessed woman pushes down some giant pillars. Visual effects supervisor William Mesa showed Raimi storyboards he had from Victor Fleming's film "Joan of Arc" that depicted huge battle scenes and he picked out 25 shots to use in "Army of Darkness". A storyboard artist worked closely with the director in order to blend the shots from the "Joan of Arc" storyboards with the battle scenes in his film.
Traci Lords was among the actresses auditioning for the film, saying in 2001, "I didn't get the part but I clicked with Bruce ," with whom she would later work as a guest star in the TV series "".
Principal photography.
Principal photography took place between soundstage and on-location work. "Army of Darkness" was filmed in Bronson Canyon and Vasquez Rocks Natural Area Park. The interior shots were filmed on an Introvision stage in Hollywood. Raimi's use of the Introvision process was a tribute to the stop-motion animation work of Ray Harryhausen. Introvision uses front-projected images with live actors instead of the traditional rear projection that Harryhausen and others used. Introvision blended components with more realistic-looking results. To achieve this effect, Raimi used 60-foot-tall Scotchlite front-projection screens, miniatures and background plates. According to the director, the advantage of using this technique was "the incredible amount of interaction between the background, which doesn't exist, and the foreground, which is usually your character".
Shooting began in mid-1991, and it lasted for about 100 days. It was a mid-summer shoot and while on location on a huge castle set that was built near Acton, California on the edge of the Mojave Desert, the cast and crew endured very hot conditions during the day and very cold temperatures at night. Most of the film took place at night and the filmmakers shot most of the film during the summer when the days were longest and the nights were the shortest. It would take an hour and a half to light an area leaving the filmmakers only six hours left to shoot a scene. Money problems forced cinematographer Bill Pope to shoot only for certain hours Monday through Friday because he could not be paid his standard fee. Mesa shot many of the action sequences on the weekend.
It was a difficult shoot for Campbell who had to learn elaborate choreography for the battle scenes, which involved him remembering a number system because the actor was often fighting opponents that were not really there. Mesa remembers, "Bruce was cussing and swearing some of the time because you had to work on the number system. Sam would tell us to make it as complicated and hard for Bruce as possible. 'Make him go through torture!' So we'd come up with these shots that were really, really difficult, and sometimes they would take thirty-seven takes". Some scenes, like Evil Ash walking along the graveyard while his skeleton minions come to life, blended stop-motion animation with live skeletons that were mechanically rigged, with prosthetics and visual effects.
Post-production.
While Dino De Laurentiis gave Raimi and his crew freedom to shoot the film the way they wanted, Universal took over during post-production. Universal was not happy with Raimi's cut because it did not like his original ending, feeling it was negative. A more upbeat ending was shot a month later in a lumber store in Malibu, California. Then, two months after principal filming was finished, a round of re-shoots began in Santa Monica and involved Ash in the windmill and the scenes with Bridget Fonda. Raimi recalls, "Actually, I kind of like the fact that there are two endings, that in one alternate universe Bruce is screwed, and in another universe he's some cheesy hero".
Raimi needed $3 million to finish his film, but Universal was not willing to give him the money and delayed its release due to a dispute with De Laurentiis over the rights to the Hannibal Lecter character which Universal needed so that they could film a sequel to "The Silence of the Lambs". The matter was finally resolved, but the release date for "Army of Darkness"' was pushed back from summer of 1992 to February 1993.
For the film's poster, Universal brought Campbell in to take several reference head shots and asked him to strike a sly look on his face. They showed him a rough of the Frank Frazetta-like painting. The actor had a day to approve it or, as he was told, there would be no ad campaign for the film. Raimi ran into further troubles when the MPAA gave it an NC-17 rating for a shot of a female Deadite being decapitated early on in the film. Universal wanted a PG-13 rating, so Raimi made a few cuts and was still stuck with an R rating. In response, Universal turned the film over to outside film editors who cut the film to 81 minutes and another version running 87 minutes that was eventually released in theaters, still with an R rating.
Music.
Danny Elfman, who composed the score for "Darkman", wrote the "March of the Dead" theme for "Army of Darkness". After the re-shoots were completed, Joseph LoDuca, who composed the music for "The Evil Dead" and "Evil Dead II", returned to score the film. The composer used his knowledge of synthesizers and was able to present many cues in a mock-up form before he took them in front of an orchestra.
Reception.
Box office.
"Army of Darkness" was released by Universal on February 19, 1993 in 1,387 theaters in the United States, grossing $4.4 million (38.5% of total gross) on its first weekend. In total, the film earned $11.5 million in the US.
Critical response.
The film currently holds a 70% "Fresh" rating on the review aggregate website Rotten Tomatoes, based on 43 reviews, which made its critical reception above average but is much lower than "The Evil Dead" and "Evil Dead II", which both received 98% critical approval. On Metacritic, the film holds a score of 57 out of 100, indicating "mixed or average reviews". Roger Ebert gave the film two out of four stars and wrote, "The movie isn't as funny or entertaining as "Evil Dead II", however, maybe because the comic approach seems recycled." In her review for "The New York Times", Janet Maslin praised, "Mr. Campbell's manly, mock-heroic posturing is perfectly in keeping with the director's droll outlook." Desson Howe, in this review for "The Washington Post" praised the film's style: "Bill Pope's cinematography is gymnastic and appropriately frenetic. The visual and make-up effects (from artist-technicians William Mesa, Tony Gardner and others) are incredibly imaginative." However, "Entertainment Weekly" gave the film a "C+" rating and wrote, "This spoofy cast of thousands looks a little too much like a crew of bland Hollywood extras. By the time "Army of Darkness" turns into a retread of "Jason and the Argonauts", featuring an army of fighting skeletons, the film has fallen into a ditch between parody and spectacle."
Accolades.
"Army of Darkness" won the Saturn Award for Best Horror Film (1994). It was also nominated for Best Make-Up. "Army of Darkness" was nominated for the Grand Prize at Avoriaz Fantastic Film Festival, and won the Golden Raven at the Brussels International Festival of Fantasy Film in 1993. The film also won the Critics' Award at Fantasporto, and was nominated for the International Fantasy Film Award in the category of Best Film in 1993. It was also nominated for Best Film at Sitges, the Spanish International Film Festival.
Sequel.
In March 2013, shortly before the release of "Evil Dead", a reboot and loose continuation of the franchise, Raimi confirmed that the fourth "Evil Dead" film will be "Army of Darkness 2". Campbell confirmed that he would star as an older, but not necessarily wiser, Ash. At a WonderCon panel in March 2013, Campbell and Fede Alvarez, director of the reboot, stated that their ultimate plan was for Alvarez's "Evil Dead 2" and Raimi's "Army of Darkness 2" to be followed by a seventh film which would merge the narratives of Ash and Mia. On October 18, 2013, Campbell once again confirmed in an interview with ComicBook.com that he will be reprising his role as Ash in the sequel. Fede Alvarez posted a status update on his Twitter account that Raimi will direct the sequel. Campbell later commented that the rumor about him returning is false.
Comics.
"Army of Darkness" had a comic book adaptation and several comic book sequels.

</doc>
<doc id="3218" url="http://en.wikipedia.org/wiki?curid=3218" title="RUR-5 ASROC">
RUR-5 ASROC

The RUR-5 ASROC (for Anti-Submarine ROCket) is an all-weather, all sea-conditions anti-submarine missile system. Developed by the United States Navy in the 1950s, it was deployed in the 1960s, updated in the 1990s, and eventually installed on over 200 USN surface ships, specifically cruisers, destroyers, and frigates. The ASROC has been deployed on scores of warships of many other navies, including Canada, Germany, Italy, Japan, the Republic of China, Greece, Pakistan and others.
History.
ASROC started development as the Rocket Assisted Torpedo (RAT) program by the Naval Ordnance Test Station at China Lake in the early 1950s to develop a surface warship ASW weapon counter to the new post-World War II submarines which ran quieter, at much higher speed and could attack from much longer range with high speed homing torpedoes. In addition, the goal was to take advantage of modern sonars with a much larger detection range. An extended range torpedo delivered by parachute from the air would allow warships the stand-off capability to attack hostile submarines with very little advance notice to the hostile submarine. The RAT program came in three phases: RAT-A, RAT-B and RAT-C. RAT-A (and its follow-on, RAT-B) were efforts to develop a compact and economical stand-off ASW for smaller warships, but were found to be either unreliable or had too short a range. RAT-C was a program to develop a stand-off ASW weapon that used a nuclear depth charge. This would require a range of at least 8,000 yards to escape potential damage from the underwater blast. Unlike the original RAT program rockets, the RAT-C was considerably larger to accomplish the extended range needed and was to be fitted to larger warships. With the failure of both the RAT-A and RAT-B programs, RAT-C was redesigned from a stand-off nuclear ASW weapon to one that could use not only a nuclear depth charge but also a homing ASW torpedo. To obtain the accuracy needed, the RAT-C rocket launcher had to be redesigned with larger side fins. This program finally combined reliability and accuracy, along with the necessary stand-off range. However, before RAT-C reached initial operational status in 1960 aboard the large US Navy destroyer-leader Norfolk, its name was changed to the present ASROC.
Description.
After a surface ship, patrol plane or anti-submarine helicopter detects an enemy submarine by using sonar or other sensors, it could relay the sub's position to an ASROC-equipped ship for attack. The attacking ship would then fire an ASROC missile carrying an acoustic homing torpedo or a Nuclear Depth Bomb (NDB) onto an unguided ballistic trajectory toward the target. At a pre-determined point on the missile's trajectory, the payload separates from the missile and deploys a parachute to permit splashdown and water entry at a low speed and with minimum detectable noise. The water entry activates the torpedo, which is guided by its own sonar system, and homes in on the target using either active sonar or passive sonar. 
In cases where the ASROC missile carried an NDB, the unguided bomb would sink quickly to a predetermined depth where it would detonate. The nuclear-armed ASROC was never used beyond one or two tests in 1961-62. Eventually the Limited Nuclear Test Ban Treaty banning underwater nuclear tests went into effect. The nuclear weapon was never used in combat. An ASROC missile could hypothetically carry a 10 kiloton W44 nuclear warhead, although the W44-armed nuclear weapons were retired by 1989, and all types of nuclear depth bombs were removed from deployment. 
The first ASROC system using the MK-112 "Matchbox" launcher, was developed in the 1950s and installed in the 1960s. This system was phased out in the 1990s and replaced with the RUM-139 Vertical Launch ASROC, or "VLA".
Specific installations.
The 31 U.S. Navy "Spruance"-class destroyers were all built with the Mark 16 Mod 7 ASROC Launching Group and MK 4 ASROC Weapons Handling System (AWHS) reload system. These had one standard Mark 112 octuple ASROC launcher, located immediately above a reload system holding an additional 16 assembled rounds (two complete reloads of eight missiles apiece). Thus, each "Spruance"-class destroyer originally carried a maximum total of 24 ASROC. 
Most other US Navy and allied navy destroyers, destroyer escorts, frigates, and several different classes of cruisers only carried the one ASROC 'matchbox' MK 112 launcher with eight ASROC missiles (although later in service, some of those missiles could be replaced by the "Harpoon" anti-ship missile). The "matchbox" Mk 112 launchers were capable of carrying a mixture of the two types. Reloads were carried in many classes, either on first level of the superstructure immediately abaft the launcher, or in a separate deckhouse just forward or abaft the Mk 112. 
The MK 16 Launching Group also had configurations that supported HARPOON RGM-84 (Onboard Knox Class Destroyer Escorts (Frigates)) or a variation of the Tartar missile in limited distribution. 
Ships with the Mk 26 GMLS, and late marks of the Mk 10 GMLS aboard the "Belknap"-class, could accommodate ASROC in these power-loaded launchers (the Mk 13 GMLS was not able to fire the weapon, as the launcher rail was too short).
Most "Spruance"-class destroyers were later modified to include the Mk 41 VLS, these launchers are capable of carrying a mixture of the RUM-139 VL-ASROC, the "Tomahawk" TLAM, and other missiles. All of the "Spruance" destroyers carried two separate quad "Harpoon" launchers. Other US ships with the Mk 41 can also accommodate VL-ASROC.

</doc>
<doc id="3221" url="http://en.wikipedia.org/wiki?curid=3221" title="Ahmed al-Nami">
Ahmed al-Nami

Ahmed bin Abdullah al-Nami (Arabic: أحمد بن عبد الله النعمي, ; also transliterated as Alnami; August 17, 1977 – September 11, 2001) was one of four hijackers of United Airlines Flight 93 as part of the September 11 attacks. 
Born in Saudi Arabia, Nami had served as a muezzin and was a college student. He left his family in 2000 to complete the Hajj, but later went to Afghanistan bound for an al-Qaeda training camp where he befriended other future hijackers and would soon be chosen to participate in the attacks.
He arrived in the United States in May 2001, on a tourist visa, where he would settle in Florida up until the attacks. On September 11, 2001, Nami boarded United 93 and assisted in the hijacking of the plane, which crashed into a field in rural Shanksville, Pennsylvania, after a passenger uprising, due to the passengers receiving information from their families of the 3 other hijacked planes that hit the World Trade Center and the Pentagon.
History.
Nami, much like Abdulaziz al-Omari, Wail al-Shehri, Waleed al-Shehri, and Mohand al-Shehri was born in the 'Asir Province in Saudi Arabia. Born to the Quraish tribe of Saudi Arabia, Nami served as a muezzin at the Seqeley mosque after having reportedly become very religious sometime in early 1999. That autumn he enrolled in the King Khaled University at Abha to study Sharia, he left his family home in Khamis Mushayt in the summer of 2000 to complete the Hajj, but never returned – instead travelling to the Al Farouq training camp in Afghanistan where he met and befriended Waleed and Wail al-Shehri, two brothers from Khamis Mushayt, and Saeed al-Ghamdi. The four reportedly pledged themselves to Jihad in the Spring of 2000, in a ceremony presided over by Wail – who had dubbed himself "Abu Mossaeb al-Janubi" after one of Muhammad's companions. Dubbed "Abu Hashim", Nami was considered "gentle in manner" by his colleagues, and reported that he had a dream in which he rode a mare along with Muhammad, and that the prophet told him to dismount and fight his enemies to liberate his land.
During his time at al-Farooq, there is a curious mention under Mushabib al-Hamlan's details that Nami had recently had laser eye surgery, an uncited fact that does not reappear. 
By October he had taken a prospective hijacker Mushabib al-Hamlan from Afghanistan to Saudi Arabia where they both procured B-1/B-2 tourist/business visas on October 28 – but Hamlan then decided not to proceed and is thought to have returned to his family. Nami's Visa application has since been reviewed, and while he mentioned that Mushabib will be travelling with him, he listed his occupation as "student" but failed to provide an address for his school, and listed his intended address in the United States merely as Los Angeles – in the end he never used this Visa to enter the United States, and reported his passport (C115007, which showed evidence of travel to Afghanistan) as "lost", and procured a new one from Jeddah (C505363). He used the new passport to acquire a new B-1/B-2 visa in Jeddah on April 23, again recopying his answers from previously although crossing out the lines regarding Mushabib and previous attempts to acquire a visa. He was interviewed by a consular officer, who again approved his application. Records at the time only recorded past failures to procure a visa, so the officer had no way of realising that Nami had successfully received an earlier visa.
In mid-November 2000, the 9/11 Commission believes that Nami, Wail and Waleed al-Shehri, all of whom had obtained their U.S. visas in late October, traveled in a group from Saudi Arabia to Beirut and then onward to Iran where they could travel through to Afghanistan without getting their passports stamped. This probably followed their return to Saudi Arabia to get "clean" passports. An associate of a senior Hezbollah operative is thought to have been on the same flight, although this may have been a coincidence. 
While in the United Arab Emirates, Nami purchased traveler's cheques presumed to have been paid for by Mustafa al-Hawsawi. Five other hijackers also passed through the UAE and purchased travellers cheques, including Majed Moqed, Saeed al-Ghamdi, Hamza al-Ghamdi, Ahmed al-Haznawi and Wail al-Shehri.
2001.
In March 2001, Ahmed al-Nami appeared in an al-Qaeda farewell video showing 13 of the "muscle hijackers" before they left their training centre in Kandahar; while he does not speak, he is seen studying maps and flight manuals.
On April 23, Nami was recorded obtaining a new US Visa.
On May 28, Nami arrived in the United States from Dubai with fellow-hijackers Mohand al-Shehri and Hamza al-Ghamdi. By early June, Nami was living in apartment 1504 at the Delray Racquet Club condominiums with Saeed al-Ghamdi in Delray Beach, Florida. He telephoned his family in 'Asir shortly after arriving in the country.
In June, he phoned his family for the last time.
He was one of 9 hijackers to open a SunTrust bank account with a cash deposit around June 2001, and on June 29 received either a Florida State Identification Card or Drivers License. 
He may have been one of three hijackers that listed the Naval Air Station in Pensacola, Florida as their permanent address on drivers' licenses, though other sources claim he listed the Delray condominium.
On August 28, Nami and Ahmed al-Haznawi reportedly bothered a Delray Beach resident, Maria Siscar Simpson, to let them through her apartment to retrieve a towel that had fallen off their balcony onto hers.
On September 5, Nami and Saeed al-Ghamdi purchased tickets for a September 7 flight to Newark at Mile High Travel on Commercial Boulevard—paying cash for their tickets. Ziad Jarrah and Ahmed al-Haznawi also purchased tickets for the same flight from Passage Tours.
On September 7, all four Flight 93 hijackers flew from Fort Lauderdale to Newark International Airport aboard Spirit Airlines.
Attacks.
On September 11, 2001, Nami arrived in Newark to board United Airlines Flight 93 along with Saeed al-Ghamdi, Ahmed al-Haznawi and Ziad Jarrah. Some reports suggest Haznawi was pulled aside for screening while others claim there is no record of whether any of the four were screened; the lack of CCTV cameras at the time has compounded the problem. Nami boarded the plane between 7:39 am and 7:48 am; seated in First Class 3C, next to Saeed al-Ghamdi.
Due to the flight's routine delay, the pilot and crew were notified of the previous hijackings and were told to be on the alert, though within two minutes Jarrah had stormed the cockpit leaving the pilots dead or injured.
At least two of the cellphone calls made by passengers indicate that all the hijackers they saw were wearing red bandanas, which some have questioned may have signified an allegiance to the Egyptian Islamic Jihad. The calls also indicated that one of the men had tied a box around his torso, and claimed there was a bomb inside; it is not known which hijacker this was.
Passengers on the plane heard through phone calls the fates of the other hijacked planes, and organized a brief assault to retake the cockpit. The hijackers crashed the plane into the Pennsylvanian countryside rather than cede control of the plane. All aboard died.
Aftermath.
After the attacks, an employee of Saudi Arabian Airlines named Ahmed al-Nami came forward to say that he feared his identity had been stolen, although he had never lost his passport.
In popular culture.
He has been portrayed by British actor Jamie Harding in the 2006 film "United 93" and Asim Wali in the film Flight 93.

</doc>
<doc id="3222" url="http://en.wikipedia.org/wiki?curid=3222" title="Ahmed al-Haznawi">
Ahmed al-Haznawi

Ahmed Ibrahim al-Haznawi (, ) (October 11, 1980 – September 11, 2001) was one of four hijackers of United Airlines Flight 93 as part of the September 11 attacks.
A Saudi, Haznawi had trained in Afghanistan after leaving his family to fight in Chechnya in 2000. He arrived in the United States in June 2001 under the direction of Al-Qaeda for terrorist attacks, on a tourist visa. Once he was in the U.S., he settled in Florida and helped plan out how the attacks would take place.
On September 11, 2001, Haznawi boarded United Airlines Flight 93 and assisted in the hijacking of the plane, which crashed into a field in Shanksville, Pennsylvania, after a passenger uprising.
Early life.
Ahmed al-Haznawi was the son of a Saudi imam from the Al-Bahah province, a province in the south west of Saudi Arabia. It is the capital of Al Bahah Province nestled between the resorts of Mecca and Abha, Al Bahah is one of the Kingdom’s prime tourist attractions. Haznawi grew up in the village of Hazna, where his father was a cleric at the mosque in the central marketplace section of the village. Haznawi belonged to a family that was part of the larger, al-Ghamdi tribe, sharing the same tribal affiliation with fellow hijackers Saeed al-Ghamdi, Hamza al-Ghamdi, and Ahmed al-Ghamdi. He memorised the Quran, giving him the title Hafiz.
This group is noted as being some of the more religiously observant of the hijackers, and they are thought to have met each other some time in 1999.
History.
1999–2000.
Haznawi announced he was leaving his family in 1999 to help fight in Chechnya, although his father forbade him. His father and brother, Abdul Rahman al-Haznawi, reportedly last heard from him in late 2000, after he made references to training in Afghanistan.
On November 12, 2000, Haznawi applied for and received a two-year US B-1/B-2
(tourist/business) visa in Jeddah, Saudi Arabia.
From November 27, 2000 through December 27, 2000, Haznawi was in Saudi Arabia for Ramadan. It is theorized that during this trip, he may have initially told Saeed and Hamza al-Ghamdi about the operation.
Some time late in 2000, Haznawi traveled to the United Arab Emirates, where he purchased traveler's cheques presumed to have been paid for by Mustafa al-Hawsawi. Five other hijackers also passed through the UAE and purchased travellers cheques, including Majed Moqed, Saeed al-Ghamdi, Hamza al-Ghamdi, Wail al-Shehri and Ahmed al-Nami.
2001.
He was one of four hijackers believed to be staying at a Kandahar guest house in March 2001, where they were seen by Mohammed Jabarah. Jabarah remembered Haznawi specifically, saying that he was "very devout and could recite the entire Koran from memory".
On June 8 he arrived in Miami, Florida with fellow hijacker Wail al-Shehri. He was one of 9 hijackers to open a SunTrust bank account with a cash deposit around June 2001. He is believed to have moved in with Ziad Jarrah, who got a new apartment on Bougainvilla Dr. in Lauderdale-by-the-Sea, after both men gave the landlord photocopies of their German passports, which he later turned over to the FBI.
On June 25, Jarrah took Haznawi to Holy Cross Hospital in Fort Lauderdale on advice of his landlord Charles Lisa. Haznawi was treated by Dr. Christos Tsonas, who gave him antibiotics for a small gash on his left calf. While he told staff that he had bumped into a suitcase, the media briefly reported it as a sign of cutaneous anthrax and a possible link to the 2001 anthrax attacks, although FBI later addressed the rumors stating that "Exhaustive testing did not support that anthrax was present anywhere the hijackers had been."
On July 10 Haznawi obtained a Florida driver's license, later obtaining another copy on September 7, 2001 by filling out a change-of-address form. Five other suspected hijackers also received duplicate Florida licenses in 2001, and others had licenses from different states. Some have speculated that this was to allow multiple persons to use the same identity.
Jarrah and Haznawi both received their one-way tickets for United Airlines Flight 93, on September 5. On September 7, all four Flight 93 hijackers flew from Fort Lauderdale to Newark International Airport aboard Spirit Airlines.
Attacks.
On September 11, 2001, Haznawi arrived at Newark International Airport to board Flight 93. Although he was selected for additional security by CAPPS and screened, he was able to board the flight without incident, with only his checked bags requiring extra screening for explosives.
Due to the flight's delay, the pilot and crew were notified of the previous hijackings that day and were told to be on the alert. Within minutes, Flight 93 was hijacked as well.
At least two of the cellphone calls made by passengers indicate that all the hijackers they saw were wearing red bandanas, which some believe may have signified an allegiance to the Egyptian Islamic Jihad. The calls also indicated that one of the men had tied a box around his torso, and claimed there was a bomb inside - it is not known which hijacker this was. Some passengers expressed doubt that the bomb was real.
Passengers on the plane heard through phone calls the fates of the other hijacked planes. A passenger uprising soon took place. Hijacker-pilot, Ziad Jarrah, crashed the plane into an empty field near Shanksville, Pennsylvania in order to prevent the passengers from gaining control of the plane. The crash killed everyone on board.
Aftermath.
After the attacks, before the release of the FBI pictures of the hijackers, Arab News reported that Haznawi's brother Abdul Rahman had told al-Madinah newspaper that a photograph published by local newspapers bore no resemblance to his brother.
A videotape released with The Wills of the New York and Washington Battle Martyrs, was aired on Al Jazeera on April 16, 2002. While the name beneath the speaker read "al-Ghamdi", the image is of Haznawi speaking. Officials suggested that the name was merely a reference to his tribal affiliation. The film was thought to have been made in March 2001. In it, he talked about his plans to bring the "bloodied message" to America. In September 2002, a similar tape made by Abdulaziz al-Omari appeared.
He has been portrayed by actors Omar Berdouni in United 93, and Zak Santiago in Flight 93.

</doc>
<doc id="3225" url="http://en.wikipedia.org/wiki?curid=3225" title="Athanasius of Alexandria">
Athanasius of Alexandria

Saint Athanasius of Alexandria (; "c". 296–298 – 2 May 373), also called Athanasius the Great, Athanasius the Confessor or, primarily in the Coptic Orthodox Church, Athanasius the Apostolic, was the twentieth bishop of Alexandria (as Athanasius I). His episcopate (because of the importance of the see, considered an archbishopric by Rome, the Coptic papacy, or an Orthodox patriarchate) lasted 45 years (c. 8 June 328 – 2 May 373), of which over 17 were spent in five exiles ordered by four different Roman emperors. Athanasius is a renowned Christian theologian, a Church Father, the chief defender of Trinitarianism against Arianism, and a noted Egyptian leader of the fourth century.
Conflict with Arius and Arianism as well as successive Roman emperors shaped Athanasius' career. In 325, at the age of 27, Athanasius began his leading role against the Arians as his bishop's assistant during the First Council of Nicaea. Roman emperor Constantine the Great had convened the council in May–August 325 to address the Arian position that the Son of God, Jesus of Nazareth, is of a distinct substance from the Father. Three years after Nicæa, Athanasius succeeded his mentor as archbishop of Alexandria. In addition to the conflict with the Arians (including powerful and influential Arian churchmen led by Eusebius of Nicomedia), he struggled against the Emperors Constantine, Constantius II, Julian the Apostate and Valens. He was known as "Athanasius Contra Mundum".
Nonetheless, within a few years of his departure, St. Gregory of Nazianzus called him the ""Pillar of the Church"". His writings were well regarded by all Church fathers who followed, in both the West and the East, who noted their rich devotion the Word-become-man, great pastoral concern, and profound interest in monasticism. Athanasius is counted as one of the four great Eastern Doctors of the Church in the Roman Catholic Church. In Eastern Orthodoxy, he is labeled the "Father of Orthodoxy". Some Protestants label him "Father of The Canon". Athanasius is venerated as a Christian saint, whose feast day is 2 May in Western Christianity, 15 May in the Coptic Orthodox Church, and 18 January in the other Eastern Orthodox Churches. He is venerated by the Roman Catholic Church, Oriental and Eastern Orthodox churches, the Lutherans, and the Anglican Communion.
Biography.
Athanasius came from a Christian family, as in his writings he tells more than once of an aunt who taught him some principles of the Christian faith, and a father who did the same, as well as mentioning (once) his mother doing the same. His parents were wealthy enough to afford giving him an esteemed secular education, as described below. He had a Christian brother, and later in his life, during one of his many exiles, hid in his father's tomb in a Christian cemetery. ["It was during this period, the final exile, that he is said to have spent four months in hiding in his father's tomb (Soz., "Hist. Eccl.", VI, xii; Soc., "Hist. Eccl.", IV, xii)"].
National origin controversy.
Athanasius was an Egyptian born in the city of Alexandria or possibly the nearby Nile Delta town of Damanhur around 293–298 (see birth year controversy below). Some Western scholars consider his command of Greek, in which he wrote most of his surviving works, evidence that he was a Greek born in Alexandria. However, in Coptic literature, Athanasius is the first patriarch of Alexandria to use Coptic as well as Greek in his writings. That Athanasius assumed the Episcopal see of Alexandria at a time of increased local discord against Rome and become a noted Egyptian leader lends additional support to his Egyptian ancestry.
However, Mother Frances Alice Monica Forbes gives witness in her historical account; Saint Athanasius, that when the Patriarch Alexander was about to die on his death-bed called Athanasius, who fled fearing he would be constrained to be made Bishop. "When the Bishops of the Church assembled to elect their new Patriarch, the whole Catholic population surrounded the church, holding up their hands to Heaven and crying; "Give us Athanasius!" The Bishops had nothing better. Athanasius was thus elected, as St. Gregory tells..." 
Education.
Alexandria was the most important trade center in the whole empire during Athanasius' boyhood. Intellectually, morally, and politically—it epitomized the ethnically diverse Graeco-Roman world, even more than Rome or Constantinople, Antioch or Marseilles. Its famous catechetical school, while sacrificing none of its famous passion for orthodoxy since the days of Pantaenus, Clement of Alexandria, Origen of Alexandria, Dionysius and Theognostus, had begun to take on an almost secular character in the comprehensiveness of its interests, and had counted influential pagans among its serious auditors.
Athanasius recounts being a student, as well as being educated by the Martyrs of the Great (tenth) and last persecution of Christianity by pagan Rome. This persecution was most severe in the East, particularly in Egypt and Palestine. Peter of Alexandria, the 17th archbishop of Alexandria, was martyred in 311 in the closing days of that persecution, and may have been one of those teachers. His successor as bishop of Alexandria, Alexander of Alexandria (312–328) was an Origenist as well as a documented mentor of Athanasius. According to Sozomen, Bishop Alexander "invited Athanasius to be his commensal and secretary. He had been well educated, and was versed in grammar and rhetoric, and had already, while still a young man, and before reaching the episcopate, given proof to those who dwelt with him of his wisdom and acumen". Athanasius' earliest work,"Against the Heathen – On the Incarnation"(written before 319), bears traces of Origenist Alexandrian thought (such as repeatedly quoting Plato and used a definition from Aristotle's "Organon") but in an orthodox way. Athanasius was also familiar with the theories of various philosophical schools, and in particular with the developments of Neo-Platonism. Ultimately, Athanasius would modify the philosophical thought of the School of Alexandria away from the Origenist principles such as the "entirely allegorical interpretation of the text". Still, in later works, Athanasius quotes Homer more than once ("Hist. Ar. 68, Orat. iv. 29"). In his letter to Emperor Constantius, he presents a defense of himself bearing unmistakable traces of a study of Demosthenes "de Corona".
Athanasius knew Greek and admitted not knowing Hebrew e.g., the 39th Festal Letter of St. Athan.. The Old Testament passages he quotes frequently come from the Septuagint Greek translation. Only rarely did he use other Greek versions (to Aquila once in the "Ecthesis", to other versions once or twice on the Psalms), and his knowledge of the Old Testament was limited to the Septuagint. Nonetheless, during his later exile, with no access to a copy of the Scriptures, Athanasius could quote from memory every verse in the Old Testament with a supposed reference to the Trinity without missing any. The combination of Scriptural study and of Greek learning was characteristic of the famous Alexandrian School.
Rufinus (Hist. Eccl., I, xiv), relates how Bishop Alexander had invited fellow prelates for breakfast after a great religious function. As he waited for his guests by a window, he watched boys playing on the seashore below. He soon noticed that they were imitating the elaborate ritual of Christian baptism. He sent for the children and discovered that one of the boys (Athanasius) had acted as bishop. The real bishop (Alexander) determined to recognize the make-believe baptisms as genuine, and invited Athanasius and his playfellows to prepare for clerical careers.
Bishop (or Patriarch) Alexander ordained Athanasius a deacon in 319. In 325, Athanasius served as Alexander's secretary at the First Council of Nicaea. Already a recognized theologian and ascetic, he was the obvious choice to replace his aging mentor Alexander as the Patriarch of Alexandria, despite the opposition of the followers of Arius and Meletius of Lycopolis.
While still a deacon under Alexander's care (or early in his patriarchate as discussed below) Athanasius may have also become acquainted with some of the solitaries of the Egyptian desert, and in particular Anthony the Great, whose life he is said to have written.
Patriarch.
Athanasius' episcopate began on 9 May 328 as the Alexandrian Council, elected Athanasius to succeed the aged Alexander. That council also denounced various heresies and schisms, many of which continued to preoccupy his 45-year long episcopate (c. 8 June 328 – 2 May 373). Patriarch Athanasius spent over 17 years in five exiles ordered by four different Roman Emperors, not counting approximately six more incidents in which Athanasius fled Alexandria to escape people seeking to take his life. This gave rise to the expression "Athanasius contra mundum" or "Athanasius against the world". However, during his first years as bishop, Athanasius visited the churches of his territory, which at that time included all of Egypt and Libya. He established contacts with the hermits and monks of the desert, including Pachomius, which proved very valuable to him over the years. Shortly thereafter, Athanasius became occupied with the disputes with the Byzantine Empire and Arians which would occupy much of his life.
First exile: under Emperor Constantine, for 2.5 years Jul 335 – 22 Nov 337; in Trier (Germany).
Athanasius' first problem lay with Meletius of Lycopolis and his followers, who had failed to abide by the First Council of Nicaea. That council also anathematized Arius. Accused of mistreating Arians and Meletians, Athanasius answered those charges at a gathering of bishops in Tyre, the First Synod of Tyre, in 335. There, Eusebius of Nicomedia and other supporters of Arius deposed Athanasius. On 6 November, both sides of the dispute met with Emperor Constantine I in Constantinople. At that meeting, Athanasius was found guilty of threatening to interfere with the supply of grains from Egypt, and sent into exile at Trier in the Rhineland.
Second exile: under Emperor Constantius, for 7.5 years Apr 339 – 21 Oct 346; lived at Rome.
When Emperor Constantine I died, Athanasius was allowed to return to his See of Alexandria. Shortly thereafter, however, Constantine's son, the new Roman Emperor Constantius II, renewed the order for Athanasius' banishment in 338. Athanasius went to Rome, where he was under the protection of Constans, the Emperor of the West. During this time, Gregory of Cappadocia was installed as the Patriarch of Alexandria, usurping the absent Athanasius. Athanasius did, however, remain in contact with his people through his annual "Festal Letters", in which he also announced on which date Easter would be celebrated that year.
Moreover, in 340, one hundred bishops met at Alexandria, declared in favor of Athanasius, and vigorously rejected the criticisms of the Eusebian faction at Tyre. Plus, Pope Julius I wrote to the supporters of Arius strongly urging Athanasius's reinstatement, but that effort proved in vain. Pope Julius II called a synod in Rome in 341 to address the matter, which found Athanasius was found to be innocent of all the charges raised against him.
"Early in the year 343 we find the undaunted exile in Gaul, whither he had gone to consult the saintly Hosius, the great champion of orthodoxy in the West. The two together set out for the Council of Sardica which had been summoned in deference to the Roman pontiff's wishes. At this great gathering of prelates the case of Athanasius was taken up once more; and once more was his innocence reaffirmed. Two conciliar letters were prepared, one to the clergy and faithful of Alexandria, the other to the bishops of Egypt and Lybia, in which the will of the Council was made known. Meanwhile the Eusebian party had gone to Philippopolis / Shahba , where they issued an anathema against Athanasius and his supporters. The persecution against the orthodox party broke out with renewed vigour, and Constantius II was induced to prepare drastic measures against Athanasius and the priests who were devoted to him. Orders were given that if the Saint attempt to re-enter his episcopal see, he should be put to death. Athanasius, accordingly, withdrew from Sardica to Naissus in Mysia , where he celebrated the Easter festival of the year 344."
It was the Saintly Hosius, who was in the Chair at the Council of Sardica obviously positioned there by the Roman Pontiff's wishes. Even in the page; Hosius, we read: "Hosius presided in 343 at the fruitless synod of Sardica, which showed itself so hostile to Arianism; there and afterwards he spoke and wrote in favour of Athanasius...". It does not appear to be a "fruitless synod", for it was key in asserting the innocence of Athanasius, the "raison d'être" of the Council namely to put Athanasius "on trial".
In 346, when the hated replacement bishop Gregory died, Constans used his influence to allow Athanasius to return to Alexandria. Most Egyptians had come to view Athanasius as a national hero, and welcomed him. This began a "golden decade" of peace and prosperity, during which time Athanasius assembled several documents relating to his exiles and returns from exile in the "Apology Against the Arians". However, upon Constans' death in 350, another civil war broke out, which left pro-Arian Constantius as sole emperor. An Alexandria local council in 350 replaced (or reaffirmed) Athanasius in his see.
Third exile: under Emperor Constantius, for 6 years Feb 356 – 21 Feb 362; in the Egyptian desert.
Constantius, renewing his previous policies favoring the Arians, banished Athanasius from Alexandria once again. This was followed, in 356, by an attempt to arrest Athanasius during a vigil service. Athanasius fled to Upper Egypt, where he stayed in several monasteries and other houses. During this period, Athanasius completed his work "Four Orations against the Arians" and defended his own recent conduct in the "Apology to Constantius" and "Apology for His Flight". Constantius' persistence in his opposition to Athanasius, combined with reports Athanasius received about the persecution of non-Arians by the new Arian bishop George of Laodicea, prompted Athanasius to write his more emotional "History of the Arians", in which he described Constantius as a precursor of the Antichrist.
In 361, after the death of Emperor Constantius, shortly followed by the murder of the very unpopular Bishop George, Athanasius returned to his patriarchate. The following year he convened a council at Alexandria, and presided over it with Eusebius of Vercelli. Athanasius appealed for unity among all those who had faith in Christianity, even if they differed on matters of terminology. This prepared the groundwork for his definition of the orthodox doctrine of the Trinity. However, the council also was directed against those who denied the divinity of the Holy Spirit, the human soul of Christ, and Christ's divinity. Mild measures were agreed on for those heretic bishops who repented, but severe penance was decreed for the chief leaders of the major heresies.
Fourth exile: under Apostate Emperor Julian, 10 months Oct 362 – 5 Sept 363; in the Egyptian desert.
In 362, the new Emperor Julian, noted for his opposition to Christianity, ordered Athanasius to leave Alexandria once again. Athanasius left for Upper Egypt, remaining there with the Desert Fathers until Julian's death in 363.
Fifth exile: under Emperor Valens, 4 months Oct 365 – 31 Jan 366; in his father's tomb.
Two years later, the Emperor Valens, who favored the Arian position, in his turn exiled Athanasius. This time however, Athanasius simply left for the outskirts of Alexandria, where he stayed for only a few months before the local authorities convinced Valens to retract his order of exile. Some early reports state that Athanasius spent this period of exile in his family's ancestral tomb. He hid in his father's tomb in a Christian cemetery. ["It was during this period, the final exile, that he is said to have spent four months in hiding in his father's tomb (Soz., "Hist. Eccl.", VI, xii; Soc., "Hist. Eccl.", IV, xii)"].
Valens, who seems to have sincerely dreaded the possible consequences of another popular outbreak, within a few weeks issued orders allowing Athanasius to return to his episcopal see.
Final years and death: Feb 366 – 2 May 373
After returning to Alexandria in early 366, Athanasius spent his final years repairing all the damage done during the earlier years of violence, dissent, and exile. He resumed writing and preaching undisturbed, and characteristically re-emphasized the view of the Incarnation which had been defined at Nicaea.. On 2 May 373, having consecrated Peter II, one of his presbyters as his successor, Athanasius died quietly in his own bed, surrounded by his clergy and faithful supporters.
Works.
Polemical and theological works.
Athanasius was not a speculative theologian. As he stated in his "First Letters to Serapion", he held on to "the tradition, teaching, and faith proclaimed by the apostles and guarded by the fathers." He held that not only was the Son of God consubstantial with the Father, but so was the Holy Spirit, which had a great deal of influence in the development of later doctrines regarding the Trinity.
Athanasius' ""Letter Concerning the Decrees of the Council of Nicaea"" ("De Decretis"), is an important historical as well as theological account of the proceedings of that council, and another letter from 367 is the first known listing of the New Testament including all those books now accepted everywhere as the New Testament. (earlier similar lists vary by the omission or addition of a few books, see "Development of the New Testament canon").
Examples of Athanasius' polemical writings against his theological opponents include "Orations Against the Arians", his defence of the divinity of the Holy Spirit ("Letters to Serapion" in the 360s, and "On the Holy Spirit") against Macedonianism and On the Incarnation.
Athanasius also wrote a two-part "Against the Heathen" and "The Incarnation of the Word of God". Completed probably early in his life, before the Arian controversy, they constitute the first classic work of developed Orthodox theology. In the first part, Athanasius attacks several pagan practices and beliefs. The second part presents teachings on the redemption. Also in these books, Athanasius put forward the belief that the Son of God, the eternal Word through whom God created the world, entered that world in human form to lead men back into the harmony from which they had earlier fallen away.
His other important works include his "Letters to Serapion", which dealt with the divinity of the Holy Spirit. In a letter to Epictetus of Corinth, Athanasius anticipates future controversies in his defense of the humanity of Christ. Another of his letters, to Dracontius, urges that monk to leave the desert for the more active duties of a bishop.
Athanasius also wrote several works of Biblical exegesis, primarily of volumes in the Old Testament. Excerpts remain of his discussions concerning the Book of Genesis, the Song of Solomon, and Psalms.
Biographical and ascetic.
His biography of Anthony the Great entitled "Life of Antony" (Βίος καὶ Πολιτεία Πατρὸς Ἀντωνίου, "Vita Antonii") became his most widely read work. Translated into several languages, it played an important role in the spreading of the ascetic ideal in Eastern and Western Christianity. Depicting Anthony as an illiterate and holy man who through his existence in a primordial landscape has an absolute connection to the divine truth, the biography also resembles the life of his biographer Athanasius. It later served as an inspiration to Christian monastics in both the East and the West. The so-called Athanasian Creed dates from well after Athanasius's death and draws upon the phraseology of Augustine's "De trinitate".
Athanasius' works on ascetism also include a "Discourse on Virginity", a short work on "Love and Self-Control", and a treatise "On Sickness and Health" (of which only fragments remain).
Coptic author.
In Coptic literature, St. Athanasius is the first patriarch of Alexandria to use Coptic as well as Greek in his writings.
Misattributed works.
There are several other works ascribed to him, although not necessarily generally accepted as being his own work. These include the Athanasian creed, which is today generally seen as being of 5th-century Galician origin.
Quotes from St Athanasius.
"Jesus that I know as my Redeemer cannot be less than God" ~at the Council of Nicæa (c. 325)
"Both from the confession of the evil spirits and from the daily witness of His works, it is manifest, then, and let none presume to doubt it, that the Savior has raised His own body, and that He is very Son of God, having His being from God as from a Father, Whose Word and Wisdom and Whose Power He is. He it is Who in these latter days assumed a body for the salvation of us all, and taught the world concerning the Father. He it is Who has destroyed death and freely graced us all with incorruption through the promise of the resurrection, having raised His own body as its first-fruits, and displayed it by the sign of the cross as the monument to His victory over death and its corruption". – "The Incarnation of the Word", Chapter 5, The Resurrection (5:32)
"But for the searching and right understanding of the Scriptures there is need of a good life and a pure soul, and for Christian virtue to guide the mind to grasp, so far as human nature can, the truth concerning God the Word. One cannot possibly understand the teaching of the saints unless one has a pure mind and is trying to imitate their life. Anyone who wants to look at sunlight naturally wipes his eye clear first, in order to make, at any rate, some approximation to the purity of that on which he looks; and a person wishing to see a city or country goes to the place in order to do so. Similarly, anyone who wishes to understand the mind of the sacred writers must first cleanse his own life, and approach the saints by copying their deeds. Thus united to them in the fellowship of life, he will both understand the things revealed to them by God and, thenceforth escaping the peril that threatens sinners in the judgment, will receive that which is laid up for the saints in the kingdom of heaven." ~in his conclusion to "The Incarnation of the Word", Chapter 9, (9:57)
"These are fountains of salvation that they who thirst may be satisfied with the living words they contain. In these alone is proclaimed the doctrine of godliness. Let no man add to these, neither let him take out from these. For concerning these, the Lord put to shame the Sadducees, and said, 'Ye do err, not knowing the Scriptures' and He reproved the Jews, saying, 'Search the Scriptures, for these are they that testify of ME". ~describing the canon in his 39th Festal Letter (c.367)
"The Son of God became man so that we might become God." (Also phrased as "Christ became like man so that we might become like him.")
"He became what we are, so that He might make us what He is..." Incarnation of The Word, St. Athanasius, Section 54
"May God console you! ... What saddens you ... is the fact that others have occupied the churches by violence, while during this time you are on the outside. It is a fact that they have the premises – but you have the Apostolic Faith. They can occupy our churches, but they are outside the true Faith. You remain outside the places of worship, but the Faith dwells within you. Let us consider: what is more important, the place or the Faith? The true Faith, obviously. Who has lost and who has won in the struggle – the one who keeps the premises or the one who keeps the Faith? True, the premises are good when the Apostolic Faith is preached there; they are holy if everything takes place there in a holy way ...
You are the ones who are happy; you who remain within the Church by your Faith, who hold firmly to the foundations of the Faith which has come down to you from Apostolic Tradition. And if an execrable jealousy has tried to shake it on a number of occasions, it has not succeeded. They are the ones who have broken away from it in the present crisis. No one, ever, will prevail against your Faith, beloved Brothers. And we believe that God will give us our churches back some day.
Thus, the more violently they try to occupy the places of worship, the more they separate themselves from the Church. They claim that they represent the Church; but in reality, they are the ones who are expelling themselves from it and going astray. Even if Catholics faithful to Tradition are reduced to a handful, they are the ones who are the true Church of Jesus Christ." Letters of St. Athanasius to his flock... "Festal Letters"... CO. May 2014.
Veneration.
St Athanasius was originally buried in Alexandria, Egypt, but his remains were later transferred to the Chiesa di San Zaccaria in Venice, Italy. During Pope Shenouda III's visit to Rome from 4 to 10 May 1973, Pope Paul VI gave the Coptic Patriarch a relic of Athanasius, which he brought back to Egypt on 15 May. The relic is currently preserved under the new Saint Mark's Coptic Orthodox Cathedral in Cairo, Egypt. However, the majority of St Athanasius's corpse remains in the Venetian church.
All major Christian denominations which officially recognize saints venerate Athanasius. Western Christians observe his feast day on 2 May, the anniversary of his death. The Roman Catholic Church considers Athanasius a Doctor of the Church. For Coptic Christians, his feast day is Pashons 7 (now circa 15 May). Eastern Orthodox liturgical calendars remember St Athanasius on 18 January.
St. Gregory of Nazianzus (330–390, also a Doctor of the Church), said in Or.21: "When I praise Athanasius, virtue itself is my theme: for I name every virtue as often as I mention him who was possessed of all virtues. He was the true pillar of the Church. His life and conduct were the rule of bishops, and his doctrine the rule of the orthodox faith."
Some Eastern Orthodox churches use the following troparion (hymn) to St Athanasius:
Historical significance and controversies.
Opposition to Arianism.
In about 319, when Athanasius was a deacon, a presbyter named Arius came into a direct conflict with Alexander of Alexandria. It appears that Arius reproached Alexander for what he felt were misguided or heretical teachings being taught by the bishop. Arius’ theological views appear to have been firmly rooted in Alexandrian Christianity, and his Christological views were certainly not radical at all. He embraced a subordinationist Christology (that Christ was the divine Son( Logos ) of God made not begotten ), heavily influenced by Alexandrian thinkers like Origen, which was a common Christological view in Alexandria at the time. Support for Arius from powerful bishops like Eusebius of Caesarea and Eusebius of Nicomedia, further illustrate how Arius' subordinationist Christology was shared by other Christians in the Empire. Arius was subsequently excommunicated by Alexander, and he would begin to elicit the support of many bishops who agreed with his position.
New Testament canon.
St Athanasius is also the first person to identify the same 27 books of the New Testament that are in use today. Up until then, various similar lists of works to be read in churches were in use. A milestone in the evolution of the canon of New Testament books is his Easter letter from Alexandria, written in 367, usually referred to as his "39th Festal Letter". Pope Damasus I, the Bishop of Rome in 382, promulgated a list of books which contained a New Testament canon identical to that of Athanasius. A synod in Hippo in 393 repeated Athanasius' and Damasus' New Testament list (without the Epistle to the Hebrews), and a synod in Carthage in 397 repeated Athanasius' and Damasus' complete New Testament list.
Scholars debate whether Athanasius' list in 367 was the basis for the later lists. Because Athanasius' canon is the closest canon of any of the Church Fathers to the canon used by Protestant churches today, many Protestants point to Athanasius as the father of the canon. They are identical except that Athanasius includes the Book of Baruch and the Letter of Jeremiah and places the Book of Esther among the "7 books not in the canon but to be read" along with the Wisdom of Solomon, Sirach (Ecclesiasticus), Judith, Tobit, the Didache, and the Shepherd of Hermas. See the article, Biblical canon, for more details.
Birth year controversy.
Estimates of Athanasius' birth year vary from 293 to 296–298. The earlier date, 293, is sometimes assigned and apparently supported by the authority of a "Coptic Fragment" (published by Dr. O. von Lemm among the Mémoires de l'académie impériale des sciences de S. Péterbourg, 1888) and corroborated by the maturity revealed in his two earliest treatises "Contra Gentes" ("Against the Heathens") and "De Incarnatione" ("On the Incarnation"), which were admittedly written about the year 318 before Arianism had begun to make itself felt, as those writings do not show an awareness of Arianism. from "Athanasius", Catholic Encyclopedia, Cornelius Clifford, 1907.
"The Catholic Encyclopedia", however, states he was born around 296 and no later than 298. The argument for the later dates begins with analysis of the original collector of Athanasius' famed "Festal Epistles" (collected shortly after his death). That author stated that the Arians had accused St. Athanasius, among other charges, that he had not yet attained the canonical age (30) and thus could not be properly ordination as Patriarch of Alexandria in 328. Without retrying those disregarded accusations, Athanasius must have at least appeared close enough to 30 years old in 328 for the accusation to seem plausible. Furthermore, in two distinct passages ("Hist. Ar., lxiv", and "De Syn., xviii"), Athanasius does not recall from memory being a first hand witness to the onset of the great persecution by the Tetrarchy of Diocletian and Maximian in February 303. In referring to those events, he never mentions personal recollections, but falls back on tradition. This tends to indicate youth (age younger than 10) in 303. Finally, his parents were still alive in Alexandria in 358, which would also place the date of his birth later rather than earlier.
Character.
Athanasius has always been a controversial, if not divisive, figure. While some scholars praise him as an orthodox saint with great character, others see him as a power-hungry politician who employed questionable ecclesiastical tactics.
The Historian, Cornelius Clifford, says; "His career almost personifies a crisis in the history of Christianity; and he maybe said rather to have shaped the events in which he took part than to have been shaped by them." 
The greater majority of Church leaders and the Emperors fell into support for Arianism, so much so that Saint Jerome wrote of the period; "The whole world groaned and was amazed to find itself Arian". He even suffered an unjust excommunication from Pope Liberius (325-366) who was Arian or leaning towards the Arians. Athanasius stood virtually alone against the world.
Supporters of Athanasius.
Many Christian denominations revere Athanasius as a Saint, teacher, and father. They cite his defense of the Christology described in the first chapter of the Gospel of John and his significant theological works (C.S. Lewis calls "On the Incarnation of the Word of God" a "masterpiece") as evidence of his righteousness. They also emphasize his close relationship with St Anthony, who is almost universally revered throughout Christendom.
The Gospel of St. John and prticularly the first Chapter demonstrates the Divinity of Jesus, which is the greatest support of St Athanasius' stand. The Gospel of St John's first Chapter began to be said at the end of Mass, we believe as a result of Athanasius, and his life's stand, but quietly. The Last Gospel of The Mass, The Eucharist, St John, together with the prayer; "Placeat tibi", the Blessing, are all private devotions that have been gradually absorbed by the liturgical service. The beginning of St John's Gospel was much used as an object of special devotion throughout the Middle Ages. Nevertheless, the practice of saying it at the altar grew; eventually St. Pius V made this practice universal for the Roman Rite in his edition of the Missal. (1570). It became a firm custom with exceptions in using an other Gospel in use from 1920. So the Missals showed different last Gospel for certain Feast days. A Prayer Card for the St John's Gospel. Also: 
St Gregory Nazianzen, 330–390, begins Or. 21 with: "When I praise Athanasius, virtue itself is my theme: for I name every virtue as often as I mention him who was possessed of all virtues. He was the true pillar of the church. His life and conduct were the rule of bishops, and his doctrine the rule of the orthodox faith."
St Cyril of Alexandria, 370–444, In the first letter says: "Athanasius is one who can be trusted: he would not say anything that is not in accord with sacred scripture." (Ep 1).
Many modern historians point out that such a hostile attitude towards Athanasius is based on an unfair judgment of historical sources.
Saint Pope Pius X said in a letter to philosopher-friend and correspondent in the closing years of his life, (Epist. lxxi, ad Max.): "Let what was confessed by the fathers of Nicaea prevail".
The Historian, Cornelius Clifford, said in his account: "Athanasius was the greatest champion of Catholic belief on the subject of the Incarnation that the Church has ever known and in his lifetime earned the characteristic title of "Father of Orthodoxy", by which he has been distinguished ever since."
Ven. John Henry Newman described him as a "principal instrument, after the Apostles, by which the sacred truths of Christianity have been conveyed and secured to the world". [Letters..]
Critics of Athanasius.
Throughout most of his career, Athanasius had many detractors. Classical scholar Timothy Barnes relates contemporary allegations against Athanasius: from defiling an altar, to selling Church grain that had been meant to feed the poor for his own personal gain, and even violence and murder to suppress dissent. Athanasius used "Arian" to describe both followers of Arius, and as a derogatory polemical term for followers of ideas that he deemed as bad as Arius'. Athanasius called many of his opponents "Arian", except for Miletus.
Scholars now believe that the Arian Party was not monolithic, but held drastically different theological views that spanned the early Christian theological spectrum. They supported the tenets of Origenist thought and theology, but had little else in common. Moreover, many labelled "Arian" did not consider themselves followers of Arius. However, others point to the Council of Nicaea as proof in and of itself that Arianism was a real theological ideology.
Lawyer Richard E. Rubenstein suggests that Athanasius ascended to the rank of bishop in Alexandria under questionable circumstances because some questioned whether he had reached the minimum age of 30 years old, and further that Athanasius employed force when it suited his cause or personal interests. Thus, he argues that a small number of bishops who supported Athanasius held a private consecration to make him bishop.

</doc>
