<doc id="1884" url="http://en.wikipedia.org/wiki?curid=1884" title="ASCII art">
ASCII art

ASCII art is a graphic design technique that uses computers for presentation and consists of pictures pieced together from the 95 printable (from a total of 128) characters defined by the ASCII Standard from 1963 and ASCII compliant character sets with proprietary extended characters (beyond the 128 characters of standard 7-bit ASCII). The term is also loosely used to refer to text based visual art in general. ASCII art can be created with any text editor, and is often used with free-form languages. Most examples of ASCII art require a fixed-width font (non-proportional fonts, as on a traditional typewriter) such as Courier for presentation.
Among the oldest known examples of ASCII art are the
creations by computer-art pioneer Kenneth Knowlton from around 1966, who was working for Bell Labs at the time. "Studies in Perception I" by Ken Knowlton and Leon Harmon from 1966 shows some examples of their early ASCII art.
One of the main reasons ASCII art was born was because early printers often lacked graphics ability and thus characters were used in place of graphic marks. Also, to mark divisions between different print jobs from different users, bulk printers often used ASCII art to print large banners, making the division easier to spot so that the results could be more easily separated by a computer operator or clerk. ASCII art was also used in early e-mail when images could not be embedded. ASCII art can also be used for typesetting initials.
History.
Typewriter art.
Since 1867 typewriters have been used for creating visual art. The oldest known preserved example of typewriter art is a picture of a butterfly made in 1898 by Flora Stacey. 
Typewriter portraits by Hobart Reese gained attention in 1922.
Typewriter art was also called keyboard art. 
In the 1954 short film "Stamp Day for Superman", typewriter art was a feature of the plot.
TTY and RTTY.
TTY stands for "TeleTYpe" or "TeleTYpewriter" and is also known as Teleprinter or Teletype.
RTTY stands for Radioteletype; character sets such as Baudot code, which predated ASCII, were used. According to a chapter in the "RTTY Handbook", text images have been sent via teletypewriter as early as 1923. However, none of the "old" RTTY art has been discovered yet. What is known is that text images appeared frequently on radioteletype in the 1960s and the 1970s.
Line-printer art.
In the 1960s, Andries van Dam published a representation of an electronic circuit produced on an IBM 1403 line printer. At the same time, Kenneth Knowlton was producing realistic images, also on line printers, by overprinting several characters on top of one another.
ASCII art.
The widespread usage of ASCII art can be traced to the computer bulletin board systems of the late 1970s and early 1980s. The limitations of computers of that time period necessitated the use of text characters to represent images. Along with ASCII's use in communication, however, it also began to appear in the underground online art groups of the period. An ASCII comic is a form of webcomic which uses ASCII text to create images. In place of images in a regular comic, ASCII art is used, with the text or dialog usually placed underneath.
During the 1990s, graphical browsing and variable-width fonts became increasingly popular, leading to a decline in ASCII art. Despite this, ASCII art continued to survive through online MUDs, an acronym for "Multi-User Dungeon", (which are textual multiplayer role-playing video games), Internet Relay Chat, E-mail, message boards and other forms of online communication which commonly employ the needed fixed-width.
ANSI.
ASCII and more importantly, ANSI were staples of the early technological era; terminal systems relied on coherent presentation using color and control signals standard in the terminal protocols.
Over the years, warez groups began to enter the ASCII art scene. Warez groups usually release .nfo files with their software, cracks or other general software reverse-engineering releases. The ASCII art will usually include the warez group's name and maybe some ASCII borders on the outsides of the release notes, etc.
BBS systems were based on ASCII and ANSI art, as were most DOS and similar console applications, and the procursor to AOL.
Uses.
ASCII art is used wherever text can be more readily printed or transmitted than graphics, or in some cases, where the transmission of pictures is not possible. This includes typewriters, teleprinters, non-graphic computer terminals, printer separators, in early computer networking (e.g., BBSes), e-mail, and Usenet news messages. ASCII art is also used within the source code of computer programs for representation of company or product logos, and flow control or other diagrams. In some cases, the entire source code of a program is a piece of ASCII art – for instance, an entry to one of the earlier International Obfuscated C Code Contest is a program that adds numbers, but visually looks like a binary adder drawn in logic ports.
Examples of ASCII-style art predating the modern computer era can be found in the June 1939, July 1948 and October 1948 editions of Popular Mechanics.
"0verkill" is a 2D platform multiplayer shooter game designed entirely in colour ASCII art. MPlayer and VLC media player can display videos as ASCII art. ASCII art is used in the making of DOS-based ZZT games.
Many game walkthrough guides come as part of a basic .txt file; this file often contains the name of the game in ASCII art. Such as below, word art is created using backslashes and other ASCII values in order to create the illusion of 3D.
Types and styles.
Different techniques could be used in ASCII art to obtain different artistic effects. Electronic circuits and diagrams were implemented by typewriter or teletype and provided the pretense for ASCII.
Line art, for creating shapes: 
Solid art, for creating filled objects:
Shading, using symbols with various intensities for creating gradients or contrasts:
Combinations of the above, often used as signatures, for example, at the end of an email:
As-Pixel Characters, use combinations of ░ , █ , ▄ and ▀ to make pictures:<br>
Emoticons and verticons.
The simplest forms of ASCII art are combinations of two or three characters for expressing emotion in text. They are commonly referred to as 'emoticon', 'smilie', or 'smiley'.
There is another type of one-line ASCII art that does not require the mental rotation of pictures, which is widely known in Japan as kaomoji (literally "face characters".) Traditionally, they are referred to as "ASCII face".
More complex examples use several lines of text to draw large symbols or more complex figures.
Popular smileys.
Hundreds of different text smileys were developed over time, but only a few were generally accepted, used and understood.
ASCII comic.
An ASCII comic is a form of webcomic.
The Adventures of Nerd Boy.
The Adventures of Nerd Boy, or just Nerd Boy is an ASCII comic by Joaquim Gândara between 6 August 2001 and 17 July 2007, consisting of 600 strips. They were posted to ASCII art newsgroup alt.ascii-art and on the website. Some strips have been translated to Polish and French.
Styles of the computer underground text art scene.
Atari 400/800 ATASCII.
The Atari 400/800 which were released in 1979 did not follow the ASCII standard and had its own character set, called ATASCII. The emergence of ATASCII art coincided with the growing popularity of BBS Systems caused by availability of the acoustic couplers that were compatible with the 8-bit home computers. ATASCII text animations are also referred to as "break animations" by the Atari sceners.
C-64 PETSCII.
The Commodore 64, which was released in 1982, also did not follow the ASCII standard. The C-64 character set is called PETSCII, an extended form of ASCII-1963. As with the Atari's ATASCII art, C-64 fans developed a similar scene that used PETSCII for their creations.
"Block ASCII" / "High ASCII" style ASCII art on the IBM PC.
So-called "block ASCII" or "high ASCII" uses the extended characters of the 8-bit code page 437, which is a proprietary standard introduced by IBM in 1979 (ANSI Standard x3.16) for the IBM PC DOS and MS-DOS operating systems. "Block ASCIIs" were widely used on the PC during the 1990s until the Internet replaced BBSes as the main communication platform. Until then, "block ASCIIs" dominated the PC Text Art Scene.
The first art scene group that focused on the extended character set of the PC in their art work was called "Aces of ANSI Art," or "AAA." Some members left in 1990, and formed a group called ACiD, "ANSI Creators in Demand." In that same year the second major underground art scene group was founded, ICE, "Insane Creators Enterprise".
There is some debate between ASCII and block ASCII artist, with "Hardcore" ASCII artists maintaining that block ASCII art is in fact not ANSI art, because it does not use the 128 characters of the original ASCII standard. On the other hand, block ASCII artists argue that if their art uses only characters of the computers character set, then it is to be called ASCII, regardless if the character set is proprietary or not.
Microsoft Windows does not support the ANSI Standard x3.16. One can view block ASCIIs with a text editor using the font "Terminal", but it will not look exactly as it was intended by the artist. With a special ASCII/ANSI viewer, such as ACiDView for Windows (see ASCII and ANSI art viewers), one can see block ASCII and ANSI files properly. An example that illustrates the difference in appearance is part of this article. Alternatively, one could look at the file using the Type command in the command prompt.
"Amiga"/"Oldskool" style ASCII art.
In the art scene one popular ASCII style that used the 7-bit standard ASCII character set was the so-called "Oldskool" Style. It is also called "Amiga style", due to its origin and widespread use on the Commodore Amiga Computers. The style uses primarily the characters: _/\-+=.()<>:. The "oldskool" art looks more like the outlined drawings of shapes than real pictures.
This is an example of "Amiga style" (also referred to as "old school" or "oldskool" style) scene ASCII art.
The Amiga ASCII Scene surfaced in 1992, seven years after the introduction of the Commodore Amiga 1000. The Commodore 64 PETSCII scene did not make the transition to the Commodore Amiga as the C64 demo and warez scenes did. Among the first Amiga ASCII art groups were ART, Epsilon Design, Upper Class, Unreal (later known as "DeZign"). This means that the text art scene on the Amiga was actually younger than the text art scene on the PC. The Amiga artists also did not call their ASCII art style "Oldskool". That term was introduced on the PC. When and by whom is unknown and lost in history.
The Amiga style ASCII artwork was most often released in the form of a single text file, which included all the artwork (usually requested), with some design parts in between, as opposed to the PC art scene where the art work was released as a ZIP archive with separate text files for each piece. Furthermore, the releases were usually called "ASCII collections" and not "art packs" like on the IBM PC.
In text editors.
This kind of ASCII art is handmade in a text editor. Popular editors used to make this kind of ASCII art include CygnusEditor aka CED (Amiga) and EditPlus2 (PC).
Oldskool font example from the PC, which was taken from the ASCII Editor FIGlet.
Newskool style ASCII art.
"Newskool" is a popular form of ASCII art which capitalizes on character strings like "$#Xxo". In spite of its name, the style is not "new"; on the contrary, it was very old but fell out of favor and was replaced by "Oldskool" and "Block" style ASCII art. It was dubbed "Newskool" upon its comeback and renewed popularity at the end of the 1990s.
Newskool changed significantly as the result of the introduction of extended proprietary characters. The classic 7-bit standard ASCII characters remain predominant, but the extended characters are often used for "fine tuning" and "tweaking". The style developed further after the introduction and adaptation of Unicode.
Methods for generating ASCII art.
While some prefer to use a simple text editor to produce ASCII art, specialized programs, such as JavE have been developed that often simulate the features and tools in bitmap image editors. For Block ASCII art and ANSI art the artist almost always uses a special text editor, because the required characters are not available on a standard keyboard.
The special text editors have sets of special characters assigned to existing keys on the keyboard. Popular MS DOS based editors, such as TheDraw and ACiDDraw had multiple sets of different special characters mapped to the F-Keys to make the use of those characters easier for the artist who can switch between individual sets of characters via basic keyboard shortcuts. PabloDraw is one of the very few special ASCII/ANSI art editors that were developed for MS Windows XP.
Image to text conversion.
Other programs allow one to automatically convert an image to text characters, which is a special case of vector quantization. A method is to sample the image down to grayscale with less than 8-bit precision, and then assign a character for each value. Such ASCII art generators often allow users to choose the intensity and contrast of the generated image.
Examples of converted images are given below.
This is one of the earliest forms of ASCII art, dating back to the early days of the 1960s minicomputers and teletypes. During the 1970s it was popular in malls to get a t-shirt with a photograph printed in ASCII art on it from an automated kiosk manned by a computer. With the advent of the web and HTML and CSS, many ASCII conversion programs will now quantize to a full RGB colorspace, enabling colorized ASCII images.
Still images or movies can also be converted to ASCII on various Linux and UNIX computers using the aalib (black and white) or libcaca (colour) graphics device driver, or the VLC media player under Windows, Linux or OS X; all of which render the screen using ASCII symbols instead of pixels. See also O'Reilly article "Watch Videos in ASCII art".
There are also a number of smartphone applications, such as ASCII cam for Android, that generate ASCII art in real-time using input from the phone's camera. These applications typically allow the ASCII art to be saved as either a text file or as an image made up of ASCII text.
Non fixed-width ASCII.
Most ASCII art is created using a monospace font, where all characters are identical in width (Courier is a popular monospace font). Early computers in use when ASCII art came into vogue had monospace fonts for screen and printer displays. Today most of the more commonly used fonts in word processors, web browsers and other programs are proportional fonts, such as Helvetica or Times Roman, where different widths are used for different characters. ASCII art drawn for a fixed width font will usually appear distorted, or even unrecognizable when displayed in a proportional font.
Some ASCII artists have produced art for display in proportional fonts. These ASCIIs, rather than using a purely shade-based correspondence, use characters for slopes and borders and use block shading. These ASCIIs generally offer greater precision and attention to detail than fixed-width ASCIIs for a lower character count, although they are not as universally accessible since they are usually relatively font-specific.
Animated ASCII art.
Animated ASCII art started in 1970 from so-called VT100 animations produced on vt100 terminals. These animations were simply text with cursor movement instructions, deleting and erasing the characters necessary to appear animated. Usually, they represented a long hand-crafted process undertaken by a single person to tell a story.
Contemporary web browser revitalized animated ASCII art again. It became possible to display animated ASCII art via JavaScript or Java applets. Static ASCII art pictures are loaded and displayed one after another, creating the animation, very similar to how movie projectors unreel film reel and project the individual pictures on the big screen at movie theaters. A new term was born: "ASCIImation" – another name of Animated ASCII Art. A seminal work in this arena is the Star Wars ASCIImation. More complicated routines in JavaScript generate more elaborate ASCIImations showing effects like Morphing effects, star field emulations, fading effects and calculated images, such as mandelbrot fractal animations.
There are now many tools and programs that can transform raster images into text symbols; some of these tools can operate on streaming video. For example, the music video for pop singer Beck Hansen's song "Black Tambourine" is made up entirely of ASCII characters that approximate the original footage.
Other text-based visual art.
There are a variety of other types of art using text symbols from character sets other than ASCII and/or some form of color coding. Despite not being pure ASCII, these are still often referred to as "ASCII art". The character set portion designed specifically for drawing is known as the line drawing characters or pseudo-graphics.
ANSI art.
The IBM PC graphics hardware in text mode uses 16 bits per character. It supports a variety of configurations, but in its default mode under DOS they are used to give 256 glyphs from one of the IBM PC code pages (Code page 437 by default), 16 foreground colors, eight background colors, and a flash option. Such art can be loaded into screen memory directly. ANSI.SYS, if loaded, also allows such art to be placed on screen by outputting escape sequences that indicate movements of the screen cursor and color/flash changes. If this method is used then the art becomes known as ANSI art. The IBM PC code pages also include characters intended for simple drawing which often made this art appear much cleaner than that made with more traditional character sets. Plain text files are also seen with these characters, though they have become far less common since Windows GUI text editors (using the Windows ANSI code page) have largely replaced DOS based ones.
Shift_JIS.
A large character selection and the availability of fixed-width characters allow Japanese users to use Shift JIS as a text-based art on Japanese websites.
Special circumstances of Japan.
Japanese mainly refer to ASCII-art (AA) as Shift-JIS Art in Japan.
In this background,Independently generation author of the Japanese original 
in Japan, the ASCII-NET(People with disabilities of related Bulletin board system.) Mr.," the author eastern emoticons. The PC communications in June 20, 1986 00:28:26(JST) From the "face mark" was published 
Art by the derived has an eastern emoticon, has been recognized as a Character of actors, that a reason.
In other words, the "ASCII", does not refer to the American Standard Code, refers to ASCII-NET's art as ASCII Corporation.
"See List of common emoticons#Eastern".
Unicode.
Unicode would seem to offer the ultimate flexibility in producing text based art with its huge variety of characters. However, finding a suitable fixed-width font is likely to be difficult if a significant subset of Unicode is desired. (Modern UNIX-style operating systems do provide complete fixed-width Unicode fonts, e.g. for xterm. Windows has the Courier New font which includes characters like ┌╥─╨┐♥☺Ƹ̵̡Ӝ̵̨̄Ʒ) Also, the common practice of rendering Unicode with a mixture of variable width fonts is likely to make predictable display hard if more than a tiny subset of Unicode is used. ≽ʌⱷ҅ᴥⱷʌ≼ is an adequate representation of a cat's face in a font with varying character widths.
Overprinting (surprint).
In the 1970s and early 1980s it was popular to produce a kind of text art that relied on overprinting – the overall darkness of a particular character space dependent on how many characters, as well as the choice of character, printed in a particular place. Thanks to the increased granularity of tone, photographs were often converted to this type of printout. Even manual typewriters or daisy wheel printers could be used. The technique has fallen from popularity since all cheap printers can easily print photographs, and a normal text file (or an e-mail message or Usenet posting) cannot represent overprinted text. However, something similar has emerged to replace it: shaded or colored ASCII art, using ANSI video terminal markup or color codes (such as those found in HTML, IRC, and many internet message boards) to add a bit more tone variation. In this way, it is possible to create ASCII art where the characters only differ in color.
Creation.
ASCII art text editors are used to create ASCII art from scratch, or to edit existing ASCII art files.
ASCII art may be created from an existing digital image using an ASCII art converter, an online tool or a software application that automatically converts an image into ASCII art, using vector quantization. Typically, this is done by sampling the image down to grayscale with less than 8-bit precision, so that each value corresponds to different ASCII character.

</doc>
<doc id="1887" url="http://en.wikipedia.org/wiki?curid=1887" title="Alexius">
Alexius

Alexius is the Latinized form of the given name Alexios (, polytonic , "defender", cf. Alexander), especially common in the later Byzantine Empire. Variants include Alexis with the Russian Aleksey and its Ukrainian counterpart Oleksa/Oleksiy deriving from this form. The female form is Alexia () and its variants such as Alessia. 
It may refer to:

</doc>
<doc id="1890" url="http://en.wikipedia.org/wiki?curid=1890" title="American English">
American English

American English is a set of dialects of the English language used mostly in the United States. Approximately two-thirds of the world's native speakers of English live in the United States. The predominant accent of American English that is most free from regional, ethnic, or cultural distinctions is the accent known as General American.
English is the most widely-spoken language in the United States. English is the common language used by the federal government and is considered the de facto language of the United States due to its widespread use. English has been given official status by 30 of the 50 state governments. As an example, under federal law, English is the official language of United States courts in Puerto Rico.
The use of English in the United States is a result of British colonization. The first wave of English-speaking settlers arrived in North America during the 17th century, followed by further migrations in the 18th and 19th centuries. Since then, American English has been influenced by the languages of West Africa, the Native American population, German, Irish, Spanish, and other languages of successive waves of immigrants to the U.S.
Phonology.
Compared with English as spoken in England, North American English is more homogeneous. Some distinctive accents can be found on the East Coast (for example, in eastern New England, New York City, and Philadelphia) partly because these areas were in close contact with England and imitated prestigious varieties of English at a time when these were undergoing changes. In addition, many speech communities on the East Coast have existed in their present locations for centuries, while the interior of the country was settled by people from all regions of the existing United States and developed a far more general linguistic pattern.
Studies on historical usage of English in the United States and the United Kingdom suggest that spoken American English did not simply evolve from British English, but rather retained many archaic features British English has since lost. Most North American speech is rhotic, as English was in most places in the 17th century. Rhoticity was further supported by Hiberno-English, West Country English and Scottish English as well as the fact most regions of England at this time also had rhotic accents. In most varieties of North American English, the sound corresponding to the letter "r" is an alveolar approximant or retroflex rather than a trill or a tap. The loss of syllable-final "r" in North America is confined mostly to the accents of eastern New England, New York City and surrounding areas and the coastal portions of the South, and African American Vernacular English.
In rural tidewater Virginia and eastern New England, 'r' is non-rhotic in accented (such as "bird", "work", "first", "birthday") as well as unaccented syllables, although this is declining among the younger generation of speakers. Dropping of syllable-final "r" sometimes happens in natively rhotic dialects if "r" is located in unaccented syllables or words and the next syllable or word begins in a consonant (for example, many North Americans drop the first 'r' in "particular"). In England, the lost "r" was often changed into (schwa), giving rise to a new class of falling diphthongs. Furthermore, the "er" sound of "fur" or "butter", is realized in AmE as a monophthongal r-colored vowel (stressed or unstressed as represented in the IPA). This does not happen in the non-rhotic varieties of North American speech.
Some other English changes in which most North American dialects do not participate:
On the other hand, North American English has undergone some sound changes not found in other varieties of English speech:
Some mergers found in most varieties of both American and British English include:
Vocabulary.
North America has given the English lexicon many thousands of words, meanings, and phrases. Several thousand are now used in English as spoken internationally.
Creation of an American lexicon.
The process of coining new lexical items started as soon as the colonists began borrowing names for unfamiliar flora, fauna, and topography from the Native American languages. Examples of such names are "opossum, raccoon, squash" and "moose" (from Algonquian). Other Native American loanwords, such as "wigwam" or "moccasin", describe articles in common use among Native Americans. The languages of the other colonising nations also added to the American vocabulary; for instance, "cookie", "cruller", "stoop", and "pit" (of a fruit) from Dutch; "angst, kindergarten, sauerkraut" from German, "levee, portage" ("carrying of boats or goods") and (probably) "gopher" from French; "barbecue, stevedore, and rodeo" from Spanish.
Among the earliest and most notable regular "English" additions to the American vocabulary, dating from the early days of colonization through the early 19th century, are terms describing the features of the North American landscape; for instance, "run, branch, fork, snag, bluff, gulch, neck" (of the woods), "barrens, bottomland, notch, knob, riffle, rapids, watergap, cutoff, trail, timberline" and "divide". Already existing words such as "creek, slough, sleet" and (in later use) "watershed" received new meanings that were unknown in England.
Other noteworthy American toponyms are found among loanwords; for example, "prairie, butte" (French); "bayou" (Choctaw via Louisiana French); "coulee" (Canadian French, but used also in Louisiana with a different meaning); "canyon, mesa, arroyo" (Spanish); "vlei, skate, kill" (Dutch, Hudson Valley).
The word "corn", used in England to refer to wheat (or any cereal), came to denote the plant "Zea mays", the most important crop in the U.S., originally named "Indian corn" by the earliest settlers; wheat, rye, barley, oats, etc. came to be collectively referred to as "grain". Other notable farm related vocabulary additions were the new meanings assumed by "barn" (not only a building for hay and grain storage, but also for housing livestock) and "team" (not just the horses, but also the vehicle along with them), as well as, in various periods, the terms "range, (corn) crib, truck, elevator, sharecropping" and "feedlot."
"Ranch," later applied to a house style, derives from Mexican Spanish; most Spanish contributions came after the War of 1812, with the opening of the West. Among these are, other than toponyms, "chaps" (from "chaparreras), plaza, lasso, bronco, , rodeo;" examples of "English" additions from the cowboy era are "bad man, maverick, chuck" ("food") and "Boot Hill;" from the California Gold Rush came such idioms as "hit pay dirt" or "strike it rich." The word "blizzard" probably originated in the West. A couple of notable late 18th century additions are the verb "belittle" and the noun "bid," both first used in writing by Thomas Jefferson.
With the new continent developed new forms of dwelling, and hence a large inventory of words designating real estate concepts "(land office, lot, outlands, waterfront," the verbs "locate" and "relocate, betterment, addition, subdivision)," types of property "(log cabin, adobe" in the 18th century; "frame house, apartment, tenement house, shack, " in the 19th century; "project, condominium, townhouse, split-level, mobile home, multi-family" in the 20th century), and parts thereof "(driveway, breezeway, backyard, dooryard; clapboard, siding, trim, baseboard; stoop" (from Dutch), "family room, den;" and, in recent years, "HVAC, central air, walkout basement)."
Ever since the American Revolution, a great number of terms connected with the U.S. political institutions have entered the language; examples are "run, gubernatorial, primary election, carpetbagger" (after the Civil War), "repeater", "lame duck" (a British term used originally in Banking) and "pork barrel." Some of these are internationally used (for example, "caucus, gerrymander, filibuster, exit poll)."
19th century onwards.
The development of industry and material innovations throughout the 19th and 20th centuries were the source of a massive stock of distinctive new words, phrases and idioms. Typical examples are the vocabulary of "railroading" (see further at rail terminology) and "transportation" terminology, ranging from names of roads (from "dirt roads" and "back roads" to "freeways" and "parkways)" to road infrastructure "(parking lot, overpass, rest area)," and from automotive terminology to "public transit" (for example, in the sentence ""riding" the "subway downtown""); such American introductions as "commuter" (from "commutation ticket), concourse, to board" (a vehicle), "to park, double-park" and "parallel park" (a car), "" or the noun "terminal" have long been used in all dialects of English.
Trades of various kinds have endowed (American) English with household words describing jobs and occupations "(bartender, longshoreman, patrolman, hobo, bouncer, bellhop, roustabout, white collar, blue collar, employee, boss" Dutch, "intern, busboy, mortician, senior citizen)," businesses and workplaces "(department store, supermarket, thrift store, gift shop, drugstore, motel, main street, gas station, hardware store, savings and loan, hock" from Dutch), as well as general concepts and innovations "(automated teller machine, smart card, cash register, dishwasher, reservation" at hotels, "pay envelope, movie, mileage, shortage, outage, blood bank)."
Already existing English words—such as "store, shop, dry goods, haberdashery, lumber"—underwent shifts in meaning; some—such as "mason, student, clerk", the verbs "can" (as in "canned goods"), "ship, fix, carry, enroll" (as in school), "run" (as in "run a business"), "release" and "haul"—were given new significations, while others (such as "tradesman)" have retained meanings that disappeared in England. From the world of business and finance came "breakeven, merger, , downsize, disintermediation, bottom line;" from sports terminology came, jargon aside, "Monday-morning quarterback, cheap shot, game plan" (football); "in the ballpark, out of left field, off base, hit and run," and many other idioms from baseball; gamblers coined "bluff, , ante, bottom dollar, raw deal, pass the buck, ace in the hole, freeze-out, showdown;" miners coined "bedrock, bonanza, peter out, pan out" and the verb "prospect" from the noun; and railroadmen are to be credited with "make the grade, sidetrack, head-on," and the verb "railroad." A number of Americanisms describing material innovations remained largely confined to North America: "elevator, ground, gasoline;" many automotive terms fall in this category, although many do not "(hatchback, sport utility vehicle, station wagon, tailgate, motorhome, truck, pickup truck, to exhaust)."
In addition to the above-mentioned loans from French, Spanish, Mexican Spanish, Dutch, and Native American languages, other accretions from foreign languages came with 19th and early 20th century immigration; notably, from Yiddish "(chutzpah, schmooze, tush") and German—"hamburger" and culinary terms like "frankfurter/franks, liverwurst, sauerkraut, wiener, deli(catessen); scram, kindergarten, gesundheit;" musical terminology "(whole note, half note," etc.); and apparently "cookbook, fresh" ("impudent") and "what gives?" Such constructions as "Are you coming with?" and "I like to dance" (for "I like dancing") may also be the result of German or Yiddish influence.
Finally, a large number of English colloquialisms from various periods are American in origin; some have lost their American flavor (from "OK" and "cool" to "nerd" and "24/7)," while others have not "(have a nice day, sure);" many are now distinctly old-fashioned "(swell, groovy)." Some English words now in general use, such as "hijacking, disc jockey, boost, bulldoze" and "jazz," originated as American slang. Among the many English idioms of U.S. origin are "get the hang of, bark up the wrong tree, keep tabs, run scared, take a backseat, have an edge over, stake a claim, take a shine to, in on the ground floor, bite off more than one can chew, off/on the wagon, stay put, inside track, stiff upper lip, bad hair day, throw a monkey wrench, under the weather, jump bail, come clean, come again?, it ain't over till it's over, what goes around comes around," and "will the real x please stand up?"
Morphology.
American English has always shown a marked tendency to use nouns as verbs. Examples of verbed nouns are "interview, advocate, vacuum, lobby, pressure, rear-end, transition, feature, profile, spearhead, skyrocket, showcase, service" (as a car), "corner, torch, exit" (as in "exit the lobby"), "factor" (in mathematics), "gun" ("shoot"), "author" (which disappeared in English around 1630 and was revived in the U.S. three centuries later) and, out of American material, "proposition, graft" (bribery), "bad-mouth, vacation, major, backpack, backtrack, intern, ticket" (traffic violations), "hassle, blacktop, peer-review, dope" and "OD", and, of course "verbed" as used at the start of this sentence.
Compounds coined in the U.S. are for instance "foothill, flatlands, badlands, landslide" (in all senses), "overview" (the noun), ", teenager, brainstorm, , hitchhike, smalltime, , frontman, lowbrow" and "highbrow, hell-bent, foolproof, nitpick, about-face" (later verbed), "upfront" (in all senses), "fixer-upper, no-show;" many of these are phrases used as adverbs or (often) hyphenated attributive adjectives: "non-profit, for-profit, free-for-all, ready-to-wear, catchall, low-down, down-and-out, down and dirty, in-your-face, nip and tuck;" many compound nouns and adjectives are open: "happy hour, fall guy, capital gain, road trip, wheat pit, head start, plea bargain;" some of these are colorful "(empty nester, loan shark, , buzz saw, ghetto blaster, dust bunny)," others are euphemistic "(differently abled (physically challenged), human resources, affirmative action, correctional facility)."
Many compound nouns have the form verb plus preposition: ", stopover, lineup, , tryout, spin-off, rundown" ("summary"), "shootout, holdup, hideout, comeback, cookout, kickback, makeover, takeover, rollback" ("decrease"), "rip-off, come-on, shoo-in, fix-up, tie-in, tie-up" ("stoppage"), "stand-in." These essentially are nouned phrasal verbs; some prepositional and phrasal verbs are in fact of American origin "(spell out, figure out, hold up, brace up, size up, rope in, back up/off/down/out, step down, miss out, kick around, cash in, rain out, check in" and "check out" (in all senses), "fill in" ("inform"), "kick in" or "throw in" ("contribute"), "square off, sock in, sock away, factor in/out, come down with, give up on, lay off" (from employment), "run into" and "across" ("meet"), "stop by, pass up, put up" (money), "set up" ("frame"), "trade in, pick up on, pick up after, lose out)."
Noun endings such as "-ee (retiree), -ery (bakery), -ster (gangster)" and "-cian (beautician)" are also particularly productive. Some verbs ending in "-ize" are of U.S. origin; for example, "fetishize, prioritize, burglarize, accessorize, itemize, editorialize, customize, notarize, weatherize, winterize, Mirandize;" and so are some back-formations "(locate, fine-tune, evolute, curate, donate, emote, upholster, peeve" and "enthuse)." Among syntactical constructions that arose in the U.S. are "as of" (with dates and times), "outside of, headed for, meet up with, back of, convince someone to, not about to" and "lack for."
Americanisms formed by alteration of some existing words include notably "pesky, phony, rambunctious, pry" (as in "pry open", from "prize), putter" (verb), "buddy, sundae, skeeter, sashay" and "kitty-corner." Adjectives that arose in the U.S. are for example, "lengthy, bossy, cute" and "cutesy, grounded" (of a child), "punk" (in all senses), "sticky" (of the weather), "through" (as in "through train", or meaning "finished"), and many colloquial forms such as "peppy" or "wacky". American blends include "motel, guesstimate, infomercial" and "televangelist."
English words that survived in the United States and not in the United Kingdom.
A number of words and meanings that originated in Middle English or Early Modern English and that have been in everyday use in the United States dropped out in most varieties of British English; some of these have cognates in Lowland Scots. Terms such as "fall" ("autumn"), "faucet" ("tap"), "diaper" ("nappy"), "candy" ("sweets"), "skillet", "eyeglasses" and "obligate" are often regarded as Americanisms. "Fall" for example came to denote the season in 16th century England, a contraction of Middle English expressions like "fall of the leaf" and "fall of the year".
During the 17th century, English immigration to the British colonies in North America was at its peak and the new settlers took the English language with them. While the term "fall" gradually became obsolete in Britain, it became the more common term in North America. "Gotten" (past participle of "get") is often considered to be an Americanism, although there are some areas of Britain, such as Lancashire and North East England, that still continue to use it and sometimes also use "putten" as the past participle for "put" (which is not done by most speakers of American English).
Other words and meanings, to various extents, were brought back to Britain, especially in the second half of the 20th century; these include "hire" ("to employ"), "quit" ("to stop", which spawned "quitter" in the U.S.), "I guess" (famously criticized by H. W. Fowler), "baggage", "hit" (a place), and the adverbs "overly" and "presently" ("currently"). Some of these, for example "monkey wrench" and "wastebasket", originated in 19th century Britain.
The mandative subjunctive (as in "the City Attorney suggested that the case "not be closed"") is livelier in American English than it is in British English. It appears in some areas as a spoken usage and is considered obligatory in contexts that are more formal. The adjectives "mad" meaning "angry", "smart" meaning "intelligent", and "sick" meaning "ill" are also more frequent in American (these meanings are also frequent in Hiberno-English) than British English.
Regional differences.
While written American English is standardized across the country, there are several recognizable variations in the spoken language, both in pronunciation and in vernacular vocabulary. "General American" is the name given to any American accent that is relatively free of noticeable regional influences.
Eastern seaboard.
After the Civil War, the settlement of the western territories by migrants from the Eastern U.S. led to dialect mixing and leveling, (koineization) so that regional dialects are most strongly differentiated along the Eastern seaboard. The Connecticut River and Long Island Sound is usually regarded as the southern/western extent of New England speech, which has its roots in the speech of the Puritans from East Anglia who settled in the Massachusetts Bay Colony.
The Potomac River generally divides a group of Northern coastal dialects from the beginning of the Coastal Southern dialect area; in between these two rivers several local variations exist, chief among them the one that prevails in and around New York City and northern New Jersey, which developed on a Dutch substratum after the English conquered New Amsterdam. The main features of Coastal Southern speech can be traced to the speech of the English from the West Country who settled in Virginia after leaving England at the time of the English Civil War.
Midwest.
A distinctive speech pattern also appears near the border between Canada and the United States, centered on the Great Lakes region (but only on the American side). This is the Inland North Dialect—the "standard Midwestern" speech that was the basis for General American in the mid-20th century (although it has been recently modified by the northern cities vowel shift). Those not from this area frequently confuse it with the North Midland dialect treated below, referring to both collectively as "Midwestern" in the Mid-Atlantic region or "Northern" in the Southern US. The so-called '"Minnesotan" dialect is also prevalent in the cultural Upper Midwest, and is characterized by influences from the German and Scandinavian settlers of the region (like "yah" for yes, pronounced similarly to "ja" in German, Norwegian and Swedish). In parts of Pennsylvania and Ohio, another dialect known as Pennsylvania Dutch English is also spoken.
Interior.
In the interior, the situation is very different. West of the Appalachian Mountains begins the broad zone of what is generally called "Midland" speech. This is divided into two discrete subdivisions, the North Midland that begins north of the Ohio River valley area, and the South Midland speech; sometimes the former is designated simply "Midland" and the latter is reckoned as "Highland Southern". The North Midland speech continues to expand westward until it becomes the closely related Western dialect which contains Pacific Northwest English as well as the well-known California English, although in the immediate San Francisco area some older speakers do not possess the cot–caught merger and thus retain the distinction between words such as cot and caught which reflects a historical Mid-Atlantic heritage.
The South Midland or Highland Southern dialect follows the Ohio River in a generally southwesterly direction, moves across Arkansas and Oklahoma west of the Mississippi, and peters out in West Texas. It is a version of the Midland speech that has assimilated some coastal Southern forms (outsiders often mistakenly believe South Midland speech and coastal South speech to be the same).
Although no longer region-specific, African American Vernacular English, which remains prevalent among African Americans, has a close relationship to Southern varieties of AmE and has greatly influenced everyday speech of many Americans.
The island state of Hawaii has a distinctive Hawaiian Pidgin.
Finally, dialect development in the United States has been notably influenced by the distinctive speech of such important cultural centers as Baltimore, Boston, Buffalo, Charleston, Cleveland, Chicago, Detroit, Miami, New Orleans, New York City, Philadelphia and Pittsburgh, which imposed their marks on the surrounding areas.
Differences between British and American English.
American English and British English (BrE) differ at the levels of phonology, phonetics, vocabulary, and, to a much lesser extent, grammar and orthography.
The first large American dictionary, "An American Dictionary of the English Language", was written by Noah Webster in 1828; Webster intended to show that the United States, which was a relatively new country at the time, spoke a different dialect from that of Britain.
Differences in grammar are relatively minor, and normally do not affect mutual intelligibility; these include: different use of some verbal auxiliaries; formal (rather than notional) agreement with collective nouns; different preferences for the past forms of a few verbs (for example, AmE/BrE: "learned"/"learnt", "burned"/"burnt", "snuck/sneaked", "dove/dived"); different prepositions and adverbs in certain contexts (for example, AmE "in school," BrE "at school"); and whether or not a definite article is used, in very few cases (AmE "to the hospital", BrE "to hospital"; contrast, however, AmE "actress Elizabeth Taylor", BrE "the actress Elizabeth Taylor"). Often, these differences are a matter of relative preferences rather than absolute rules; and most are not stable, since the two varieties are constantly influencing each other.
Differences in orthography are also trivial. Some of the forms that now serve to distinguish American from British spelling ("color" for "colour", "center" for "centre", "traveler" for "traveller", etc.) were introduced by Noah Webster himself; others are due to spelling tendencies in Britain from the 17th century until the present day (for example, "-ise" for "-ize", although the Oxford English Dictionary still prefers the "-ize" ending) and cases favored by the francophile tastes of 19th century Victorian England, which had little effect on AmE (for example, "programme" for "program", "manoeuvre" for "maneuver", "skilful" for "skillful", "cheque" for "check", etc.). One of the most common spelling differences, traceable to Webster, is that words ending in "-re" in BrE are rendered as "-er" in AmE (such as ""centre"" and ""center"", ""theatre"" and ""theater"", and ""metre"" and ""meter"").
AmE sometimes favors words that are morphologically more complex, whereas BrE uses clipped forms, such as AmE "transportation" and BrE "transport" or where the British form is a back-formation, such as AmE "burglarize" and BrE "burgle" (from "burglar"). It should, however, be noted that while individuals usually use one or the other, both forms will be widely understood and mostly used alongside each other within the two systems.

</doc>
<doc id="1893" url="http://en.wikipedia.org/wiki?curid=1893" title="Albert Spalding">
Albert Spalding

Albert Goodwill Spalding (September 2, 1850 – September 9, 1915) was an American pitcher, manager and executive in the early years of professional baseball, and the co-founder of A.G. Spalding sporting goods company. He played major league baseball between 1871 and 1878. In 1877, he became the first well-known player to use a fielding glove; such gloves were among the items sold at his sporting goods store.
After his retirement as a player, Spalding remained active with the Chicago White Stockings as president and part-owner. In the 1880s, he took players on the first world tour of baseball. With William Hulbert, Spalding organized the National League. He later called for the commission that investigated the origins of baseball and credited Abner Doubleday with creating the game. He also wrote the first set of official baseball rules.
Baseball career.
Player.
Having played baseball throughout his youth, Spalding first played competitively with the Rockford Pioneers, a youth team, which he joined in 1865. After pitching his team to a 26–2 victory over a local men's amateur team (the Mercantiles), he was approached at the age of 15 by another, the Forest Citys, for whom he played for two years. In the autumn of 1867 he accepted a $40 per week contract, nominally as a clerk, but really to play professionally for the Chicago Excelsiors, not an uncommon arrangement used to circumvent the rules of the time, which forbade the hiring of professional players. Following the formation of baseball's first professional organization, the National Association of Professional Base Ball Players (which became known as the National Association, the Association, or NA) in 1871, Spalding joined the Boston Red Stockings (precursor club to the modern Atlanta Braves) and was highly successful; winning 206 games (and losing only 53) as a pitcher and batting .323 as a hitter.
William Hulbert, principal owner of the Chicago White Stockings, did not like the loose organization of the National Association and the gambling element that influenced it, so he decided to create a new organization, which he dubbed the National League of Baseball Clubs. To aid him in this venture, Hulbert enlisted the help of Spalding. Playing to the pitcher's desire to return to his Midwestern roots and challenging Spalding's integrity, Hulbert convinced Spalding to sign a contract to play for the White Stockings (now known as the Chicago Cubs) in 1876. Spalding then coaxed teammates Deacon White, Ross Barnes and Cal McVey, as well as Philadelphia Athletics players Cap Anson and Bob Addy, to sign with Chicago. This was all done under complete secrecy during the playing season because players were all free agents in those days and they did not want their current club and especially the fans to know they were leaving to play elsewhere the next year. News of the signings by the Boston and Philadelphia players leaked to the press before the season ended and all of them faced verbal abuse and physical threats from the fans of those cities.
He was "the premier pitcher of the 1870s", leading the league in victories for each of his six full seasons as a professional. During each of those years he was his team's only pitcher. In 1876, Spalding won 47 games as the prime pitcher for the White Stockings and led them to win the first-ever National League pennant by a wide margin.
In 1877, Spalding began to use a glove to protect his catching hand. People had used gloves previously, but never had a star like Spalding used one. Spalding had an ulterior motive for doing so: he now owned a sporting goods store which sold baseball gloves and wearing one himself was good advertising for his business.
Spalding retired from playing baseball in 1878 at the age of 27, although he continued as president and part owner of the White Stockings and a major influence on the National League. Spalding's .796 career winning percentage (from an era when teams played about once or twice a week) is the highest ever achieved by a baseball pitcher.
Organizer and executive.
In the months after signing for Chicago, Hulbert and Spalding organized the National League by enlisting the four major teams in the East and the three other top teams in what was then considered to be the West. Joining Chicago initially were the leading teams from Cincinnati, Louisville, and St. Louis. The owners of these western clubs accompanied Hulbert and Spalding to New York where they secretly met with owners from New York, Philadelphia, Hartford, and Boston. Each signed the league's constitution, and the National League was officially born. "Spalding was thus involved in the transformation of baseball from a game of gentlemen athletes into a business and a professional sport." Although the National Association held on for a few more seasons, it was no longer recognized as the premier organization for professional baseball. Gradually, it faded out of existence and was replaced by myriad minor leagues and associations around the country.
In 1905, after Henry Chadwick wrote an article saying that baseball grew from the British sports of cricket and rounders, Spalding called for a commission to find out the real source of baseball. The commission called for citizens who knew anything about the founding of baseball to send in letters. After three years of searching, on December 30, 1907, Spalding received a letter that (erroneously) declared baseball to be the invention of Abner Doubleday. The commission, though, was biased, as Spalding would not appoint anyone to the commission if they believed the sport was somewhat related to the English sport of rounders. Just before the commission, in a letter to sportswriter Tim Murnane, Spalding noted, "Our good old American game of baseball must have an American Dad." The project, later called the Mills Commission, concluded that "Base Ball had its origins in the United States" and "the first scheme for playing baseball, according to the best evidence available to date, was devised by Abner Doubleday at Cooperstown, N.Y., in 1839."
Receiving the archives of Henry Chadwick in 1908, Spalding combined these records with his own memories (and biases) to write "America's National Game" (published 1911) which, despite its flaws, was probably the first scholarly account of the history of baseball.
Businessman.
In 1874 while Spalding was playing and organizing the league, Spalding and his brother Walter began a sporting goods store in Chicago, which grew rapidly (14 stores by 1901) and expanded into a manufacturer and distributor of all kinds of sporting equipment. The company became "synonymous with sporting goods" and is still a going concern.
Spalding published the first official rules guide for baseball. In it he stated that only Spalding balls could be used (previously, the quality of the balls used had been subpar). Spalding also founded the "Baseball Guide," which at the time was the most widely read baseball publication.
In 1888–1889, Spalding took a group of major league players around the world to promote baseball and Spalding sporting goods. This was the first-ever world baseball tour. Playing across the western U.S., the tour made stops in Hawaii (although no game was played), New Zealand, Australia, Ceylon, Egypt, Italy, France, and England. The tour returned to grand receptions in New York, Philadelphia, and Chicago. The tour included future Hall of Famers Cap Anson and John Montgomery Ward. While the players were on the tour, the National League instituted new rules regarding player pay that led to a revolt of players, led by Ward, who started the Players League the following season (1890). The league lasted one year, partially due to the anti-competitive tactics of Spalding to limit its success.
In 1900 Spalding was appointed by President McKinley as the USA's Commissioner at that year's Summer Olympic Games.
Other activities.
Spalding had been a prominent member of the Theosophical Society under William Quan Judge. In 1900, Spalding moved to San Diego with his newly acquired second wife, Elizabeth and became a prominent member and supporter of the Theosophical community Lomaland, which was being developed on Point Loma by Katherine Tingley. He built an estate in the Sunset Cliffs area of Point Loma where he lived with Elizabeth for the rest of his life. The Spaldings raised race horses and collected Chinese fine furniture and art.
The Spaldings had an extensive library which included many volumes on Theosophy, art, and literature. In 1907-1909 he was the driving force behind the development of a paved road, known as the "Point Loma boulevard", from downtown San Diego to Point Loma and Ocean Beach; the road also provided good access to Lomaland. It later provided the basis for California State Route 209. He proposed the project, supervised it on behalf of the city, and paid a portion of the cost out of his own pocket. He joined with George Marston and other civic-minded businessmen to purchase the site of the original Presidio of San Diego, which they developed as a historic park and eventually donated to the city of San Diego. He ran unsuccessfully for the United States Senate in 1910. He helped to organize the 1915 Panama-California Exposition, serving as second vice-president.
Death.
He died on September 9, 1915 in San Diego, and his ashes were scattered at his request.
Legacy.
He was elected to the Baseball Hall of Fame by the Veterans Committee in 1939, as one of the first inductees from the 19th century at that summer's opening ceremonies. His plaque in the Hall of Fame reads "Albert Goodwill Spalding. Organizational genius of baseball's pioneer days. Star pitcher of Forest City Club in late 1860s, 4-year champion Bostons 1871-1875 and manager-pitcher of champion Chicagos in National League's first year. Chicago president for 10 years. Organizer of baseball's first round-the-world tour in 1888."
His nephew, also named Albert Spalding, was a renowned violinist.

</doc>
<doc id="1894" url="http://en.wikipedia.org/wiki?curid=1894" title="Africa Alphabet">
Africa Alphabet

The Africa Alphabet (also International African Alphabet or IAI alphabet) was developed in 1928 under the lead of Diedrich Westermann. He developed it with a group of Africanists at the International Institute of African Languages and Cultures (later the IAI) in London. Its aim was to enable people to write all the African languages for practical and scientific purposes without diacritics. It is based on the International Phonetic Alphabet with a few differences, such as "j" and "y", which instead have the same (consonant) sound values as in English.
This alphabet has influenced development of orthographies of many African languages (serving "as the basis for the transcription" of about 60, by one count), but not all, and discussions of harmonization of systems of transcription that led to, among other things, adoption of the African reference alphabet.
The African Alphabet was used, with the International Phonetic Alphabet, as a basis for the World Orthography.

</doc>
<doc id="1896" url="http://en.wikipedia.org/wiki?curid=1896" title="Acquire">
Acquire

Acquire is a board game designed by Sid Sackson.
The game was originally published in 1962 by 3M as a part of their bookshelf games series. In most versions, the theme of the game is investing in hotel chains. In the 1990s Hasbro edition, the hotel chains were replaced by generic corporations, though the actual gameplay was unchanged. The game is currently published by Hasbro under the Avalon Hill brand, and the companies are once again hotel chains.
The object of the game is to earn the most money by developing and merging hotel chains. When a chain in which a player owns stock is acquired by a larger chain, players earn money based on the size of the acquired chain. At the end of the game, all players liquidate their stock in order to determine which player has the most money.
Components.
The components of the game have varied over the years. In particular, the tiles have been made from wood, plastic, and cardboard in various editions of the game. In the current 2008 version, the tiles are cardboard. The following components are included in all versions:
The array on the game board is arranged with lettered rows (A through I) and numbered columns (1 through 12). The 108 tiles correspond to each of the squares: 5E, 10B, and so forth.
Rules.
"Acquire" is a game for three to six players, though earlier editions included special rules for two players. Standard tournament games are played with four players.
Setup.
At the beginning of the game, each player receives $6000 in cash. Each player draws a tile and places it on the board. The player whose tile is in the topmost row (closest to row A) goes first. If more than one player selects a tile in that row, then the player whose tile is in the leftmost column (closest to 1) goes first. All players place these tiles on the board. Then, starting with the first player, each player draws six tiles.
Play of the game.
A turn consists of three steps:
Tile placement falls in one of four categories. The tile placed could be an orphan, adjacent to no other tile on the board. The tile could create a new chain of tiles, and the player who placed it on the board would have the opportunity to found a new chain. The tile could increase the length of an existing chain already on the board. Or the tile could link two chains, causing a merger of two or more chains. Since there are only seven hotel chains in the game, placing a tile that would create an eighth chain is not permitted.
When a player founds a chain, he receives one free share of stock in that chain. If, however, there are no shares left when the chain is founded, then the founding player does not receive the free share.
Chains are deemed "safe" if they have 11 or more links; placing a tile that would cause such a chain to be acquired by a larger chain is also not permitted.
After a player places a tile, and the results of that placement have been handled, he may purchase up to three shares of stock. A player may only purchase shares of stock in chains that have already been founded. The price of a share depends on the size of the chain, according to a chart that lists prices according to size. A player may purchase shares in one, two, or three existing chains (assuming at least three chains are currently in play), in any combination up to a total of three shares.
Finally, the player replaces the tile he played, ensuring that he has six tiles at the end of his turn.
Growing and merging chains.
A chain is a conglomeration of tiles that are linked to each other either horizontally or vertically but not diagonally. For example, adjacent to square 5F are squares 4F, 6F, 5E, and 5G, but not 6E or 4G. If there is a tile in 5F, then placing either tile 4F or 5G would result in founding a new hotel chain. A chain grows when a player increases the length of a chain. Suppose a chain consists of squares 8D, 8E, and 8F. Playing tile 9F would add to the length of the chain. Playing tile 9C would not.
Chains merge when a player places the tile that eliminates the empty space between them. Suppose there is a chain at 1A, 2A, 3A, and 4A, along with another chain at 6A and 7A. Placing tile 5A would cause these two chains to merge. When a merger occurs, the larger hotel chain always acquires the smaller hotel chain. That is, the hotel chain with more tiles will continue to exist and now grows to include the smaller hotel chain (after bonuses have been calculated according to the steps outlined below). If a tile is placed between two hotel chains of the same size, the individual player who places the tile decides which hotel chain remains on the board and which is acquired. In this situation, there are a number of strategic reasons why an individual player might select one hotel chain over another to be the one that remains on the board. However, often it is most advantageous for the player selecting to choose to let the more expensive chains remain on the board (and trade in their stock of the less expensive chain at the 2-to-1 ratio described below).
Mergers.
The merger is the mechanism by which the players compete. Mergers yield bonuses for the two shareholders who hold, respectively, the largest and second-largest interests in a chain. Mergers also give each player who holds any interest at all in a chain a chance to sell his stock or to trade it in for shares of the acquiring chain. A merger takes place in three steps:
If placing a tile causes three or four chains to merge, then the merger steps are handled between the largest and second-largest chain, then with the third-largest chain, and finally with the smallest chain.
Rules issues.
The rules allow any player to count the number of shares available in the bank. However, the rules do not specify whether a player should hold his shares of stock face up or face down. That is, the rules do not say whether one player may ask another how many shares of stock he or she owns in a particular chain. Whether this is public or private information should be agreed upon between players before the game begins.
The current rules do not provide for a two-player game. However, the stock market was used as a "third shareholder" in previous versions of the game. By this rule, a tile is drawn whenever a merger is declared. The number on the tile indicates how many shares the stock market owns in the company that is being acquired. The players must compete with the market as well as with each other in order to receive bonuses.
Ending the game.
Any player may declare the game over at any time during his turn if either of two conditions is true: one chain has 41 or more tiles, or there is at least one chain on the board and every chain on the board has 11 or more tiles. Upon declaring the game over, the player is allowed to complete his turn (including buying stock). Ending the game is optional - if he believes it is to his advantage not to end the game, he may refrain from doing so. Once the game ends, the minority and majority bonuses are paid to the minority and majority holders in each of the remaining chains; each player sells his or her shares of stock in each of the remaining chains; and the player with the most money wins. Because ending the game is optional, and a player may not realize that he can end it, it is unethical to say "good game", or in any other way indicate that the game could be ended, until after a player actually has ended the game.
Corner Cases.
These conditions can be used to handle unusual situations.
Awards.
The game was short-listed for the first Spiel des Jahres board game awards in 1979.
"GAMES" magazine has inducted "Acquire" into their buyers' guide Hall of Fame. The magazine's stated criteria for the Hall of Fame encompasses "games that have met or exceeded the highest standards of quality and play value and have been continuously in production for at least 10 years; i.e., classics."
It was inducted into the Academy of Adventure Gaming Arts & Design's Hall of Fame, along with Sackson, in 2011.
Online play.
"Acquire" is playable online through the GameTable Online site.
Acquire is also playable for free as NetAcquire. Various NetAcquire clients are available including Visual Basic (for Windows) by Kensit, JAVA (for all) by Tim Styer, and a Mac version by Nolan Waite. Details can be found at http://www.netacquire.ca.

</doc>
<doc id="1897" url="http://en.wikipedia.org/wiki?curid=1897" title="Australian English">
Australian English

Australian English (AusE, AuE, AusEng, en-AU) is a major variety of the English language and is used throughout Australia. Although English has no official status in the Constitution, Australian English is Australia's "de facto" official language and is the first language of the majority of the population.
Australian English began to diverge from British English after the founding of the colony of New South Wales in 1788 and was recognised as being different from British English by 1820. It arose from the intermingling of early settlers from a great variety of mutually intelligible dialectal regions of the British Isles and quickly developed into a distinct variety of English.
Australian English differs from other varieties of English in vocabulary, accent, pronunciation, register, grammar and spelling.
History.
The earliest form of Australian English was first spoken by the children of the colonists born into the colony of New South Wales. This very first generation of children created a new dialect that was to become the language of the nation. The Australian-born children in the new colony were exposed to a wide range of different dialects from all over the British Isles, in particular from Ireland and South East England.
The native-born children of the colony created the new dialect from factors present in the speech they heard around them, and provided an avenue for the expression of peer solidarity. Even when new settlers arrived, this new dialect was strong enough to deflect the influence of other patterns of speech.
A large part of the convict body were of Irish origin, 25% of the total convict population. Many had been arrested in Ireland, and some in Great Britain. Many, if not most of the Irish convicts spoke either no English at all, or spoke it poorly and rarely. There were other significant populations of convicts from non-English speaking areas of Britain, such as the Scottish Highlands and Wales.
Records from the early 19th century indicate the distinct dialect that had surfaced in the colonies since first settlement in 1788, with Peter Miller Cunningham's 1827 book "Two Years in New South Wales", describing the distinctive accent and vocabulary of the native born colonists, different from that of their parents and with a strong London influence. Anthony Burgess writes that "Australian English may be thought of as a kind of fossilised Cockney of the Dickensian era". 
The first of the Australian gold rushes, in the 1850s, began a large wave of immigration, during which about two per cent of the population of the United Kingdom emigrated to the colonies of New South Wales and Victoria. According to linguist Bruce Moore, "the major input of the various sounds that went into constructing the Australian accent was from south-east England".
Some elements of Aboriginal languages have been adopted by Australian English—mainly as names for places, flora and fauna (for example dingo) and local culture. Many such are localised, and do not form part of general Australian use, while others, such as "kangaroo", "boomerang", "budgerigar", "wallaby" and so on have become international. Other examples are "cooee" and "hard yakka". The former is used as a high-pitched call, for attracting attention, (pronounced ) which travels long distances. "Cooee" is also a notional distance: "if he's within cooee, we'll spot him". "Hard yakka" means "hard work" and is derived from "yakka", from the Jagera/Yagara language once spoken in the Brisbane region.
Also from there is the word "bung", from the Sydney pidgin English (and ultimately from the Sydney Aboriginal language), meaning "dead", with some extension to "broken" or "useless". Many towns or suburbs of Australia have also been influenced or named after Aboriginal words. The most well known example is the capital, Canberra, named after a local language word meaning "meeting place".
Among the changes starting in the 19th century was the introduction of words, spellings, terms and usages from North American English. The words imported included some later considered to be typically Australian, such as "bushwhacker" and "squatter".
This American influence continued with the popularity of American films, and with the influx of American military personnel in World War II; seen in the enduring persistence of such terms as "okay", "you guys" and "gee".
Phonology and pronunciation.
The primary way in which Australian English is distinctive from other varieties of English is through its unique pronunciation. It shares most similarity with other Southern Hemisphere accents, in particular New Zealand English. Like most dialects of English it is distinguished primarily by its vowel phonology.
Vowels.
The vowels of Australian English can be divided according to length. The long vowels, which include monophthongs and diphthongs, mostly correspond to the tense vowels used in analyses of Received Pronunciation (RP) as well as its centring diphthongs. The short vowels, consisting only of monophthongs, correspond to the RP lax vowels. There exist pairs of long and short vowels with overlapping vowel quality giving Australian English phonemic length distinction, which is unusual amongst the various dialects of English, though not unknown elsewhere, such as in regional south-eastern dialects of the UK and eastern seaboard dialects in the US. As with General American and New Zealand English, the weak-vowel merger is complete in Australian English: unstressed (sometimes written as or ) is merged into (schwa).
Consonants.
There is little variation with respect to the sets of consonants used in various English dialects. There are, however, variations in how these consonants are used. Australian English is no exception.
Australian English is non-rhotic; in other words, the sound does not appear at the end of a syllable or immediately before a consonant. However, a linking can occur when a word that has a final <r> in the spelling comes before another word that starts with a vowel. An intrusive may similarly be inserted before a vowel in words that do not have <r> in the spelling in certain environments, namely after the long vowel and after word final .
There is some degree of allophonic variation in the alveolar stops. As with North American English, Intervocalic alveolar flapping is a feature of Australian English: prevocalic and surface as the alveolar tap after sonorants other than , /m/as well as at the end of a word or morpheme before any vowel in the same breath group. For many speakers, and in the combinations - and - are also palatalised, thus - and -, as Australian is only very slightly retroflex, the tip remaining below the level of the bottom teeth in the same position as for ; it is also somewhat rounded ("to say 'r' the way Australians do you need to say 'w' at the same time"), where older English and have fallen together as . The wine–whine merger is complete in Australian English.
"Yod"-dropping occurs after , , , , , , , and , . Other cases of and , along with and , have coalesced to , , and respectively for many speakers. is generally retained in other consonant clusters.
Pronunciation.
Differences in stress, weak forms and standard pronunciation of isolated words occur between Australian English and other forms of English, which while noticeable do not impair intelligibility.
The affixes "-ary", "-ery", "-ory", "-bury", "-berry" and "-mony" (seen in words such as "necessary, mulberry" and "matrimony") can be pronounced either with a full vowel or a schwa. Although some words like "necessary" are almost universally pronounced with the full vowel, older generations of Australians are relatively likely to pronounce these affixes with a schwa while younger generations are relatively likely to use a full vowel.
Words ending in unstressed "-ile" derived from Latin adjectives ending in "-ilis" are pronounced with a full vowel (), so that "fertile" rhymes with "fur tile" rather than "turtle".
In addition, miscellaneous pronunciation differences exist when compared with other varieties of English in relation to seemingly random words. For example, the vowel in "yoghurt" is pronounced as ("long 'O'") rather than ("short o"). Similarly, "vitamin" is pronounced with ("long 'I'") in the first syllable, rather than ("short 'I'"). Despite this, "advertisement" is pronounced with . "Brooch" is pronounced with as opposed to , and "Anthony" with rather than .
Variation.
Academic research has shown that the most notable variation within Australian English is largely sociocultural. This is mostly evident in phonology, which is divided into three sociocultural varieties: "broad", "general" and "cultivated".
A limited range of word choices is strongly regional in nature. Consequently, the geographical background of individuals can be inferred, if they use words that are peculiar to particular Australian states or territories and, in some cases, even smaller regions.
In addition, some Australians speak creole languages derived from Australian English, such as Australian Kriol, Torres Strait Creole and Norfuk.
Sociocultural.
The "broad", "general" and "cultivated" accents form a continuum that reflects minute variations in the Australian accent. They can reflect the social class, education and urban or rural background of speakers, though such indicators are not always reliable. According to linguists, the general Australian variant emerged some time before 1900. Recent generations have seen a comparatively smaller proportion of the population speaking with the broad variant, along with the near extinction of the cultivated Australian accent. The growth and dominance of general Australian accents perhaps reflects its prominence on radio and television during the late 20th century.
Australian Aboriginal English is made up of a range of forms which developed differently in different parts of Australia, and are said to vary along a continuum, from forms close to Standard Australian English to more non-standard forms. There are distinctive features of accent, grammar, words and meanings, as well as language use.
The ethnocultural dialects are diverse accents in Australian English that are spoken by the minority groups, which are of non-English speaking background. A massive immigration from Asia has made a large increase in diversity and the will for people to show their cultural identity within the Australian context. These ethnocultural varieties contain features of General Australian English as adopted by the children of immigrants blended with some non-English language features, such as the Afro-Asiatic and Asian languages.
Regional variation.
Although Australian English is relatively homogeneous, some regional variations are notable. The dialects of English spoken in South Australia, Western Australia, New South Wales, Victoria, Tasmania, Queensland and the Torres Strait Islands differ slightly from each other. Differences exist both in terms of vocabulary and phonology.
Most regional differences come down to word usage. For example, swimming clothes are known as "cossies" or "swimmers" in New South Wales, "togs" in Queensland, and "bathers" in Victoria, Western Australia and South Australia; what is referred to as a "stroller" in most of Australia is called a "pusher" in Victoria and usually a "pram" in Western Australia. Preference for synonymous words also differs between states. For example, "garbage" (i.e. garbage bin, garbage truck) dominates over "rubbish" in New South Wales and Queensland, while "rubbish" is more popular in Victoria, Western Australia and South Australia. The word "footy" generally refers to the most popular football code in the particular state or territory; that is, rugby league in New South Wales and Queensland, and Australian rules football elsewhere. Beer glasses are also named differently in different states. Distinctive grammatical patterns exist such as the use of the interrogative "eh?".
There are some notable regional variations in the pronunciations of certain words. The extent to which the trap‑bath split has taken hold is one example. This phonological development is more advanced in South Australia, which had a different settlement chronology and type than other parts of the country. Words such as "dance", "advance", "plant", "graph", "example" and "answer" are pronounced far more frequently with the older (as in "mad") outside of South Australia, but with the British-influenced (as in "father") within South Australia. "L"-vocalisation is also more common in South Australia than other states. In Western Australian English the vowels in "near" and "square" are typically realised as centring diphthongs, whereas in the eastern states they may also be realised as monophthongs. A feature common in Victorian English is salary–celery merger. There is also regional variation in before .
Vocabulary.
Australian English has many words and idioms which are unique to the dialect and have been written on extensively, with the Macquarie Dictionary, widely regarded as the national standard, incorporating numerous Australian terms.
Internationally well-known examples of Australian terminology include "outback", meaning a remote, sparsely populated area, "the bush", meaning either a native forest or a country area in general, and "g'day", a greeting. "Dinkum", or "fair dinkum" means "true", or "is that true?", among other things, depending on context and inflection. The derivative "dinky-di" means "true" or devoted: a "dinky-di Aussie" is a "true Australian".
Australian poetry, such as "The Man from Snowy River" as well as folk songs such as "Waltzing Matilda", contain many historical Australian words and phrases that are understood by Australians even though some are not in common usage today.
Australian English, in common with several British English dialects (for example, Cockney, Scouse, Glaswegian and Geordie), uses the word "mate". Many words used by Australians were at one time used in England but have since fallen out of usage or changed in meaning there.
For example, "creek" in Australia, as in North America, means a stream or small river, whereas in the UK it means a small watercourse flowing into the sea; "paddock" in Australia means field, whereas in the UK it means a small enclosure for livestock; "bush" or "scrub" in Australia, as in North America, means a wooded area, whereas in England they are commonly used only in proper names (such as Shepherd's Bush and Wormwood Scrubs).
Litotes, such as "not bad", "not much" and "you're not wrong", are also used, as are diminutives, which are commonly used and are often used to indicate familiarity. Some common examples are "arvo" (afternoon), "barbie" (barbecue), "smoko" (cigarette break), "Aussie" (Australian) and "pressie" (present/gift). This may also be done with people's names to create nicknames (other English speaking countries create similar diminutives). For example, "Gazza" from Gary, or "Smitty" from John Smith. The use of the suffix "-o" originates in Irish Gaelic (Irish "ó"), which is both a postclitic and a suffix with much the same meaning as in Australian English.
In informal speech, incomplete comparisons are sometimes used, such as "sweet as" (as in "That car is sweet as."). "Full", "fully" or "heaps" may precede a word to act as an intensifier (as in "The waves at the beach were heaps good."). This was more common in regional Australia and South Australia but has been in common usage in urban Australia for decades. The suffix "-ly" is sometimes omitted in broader Australian English. For instance "real good" instead of "really good".
Australia's switch to the metric system in the 1970s changed the country's vocabulary of measurement from Imperial towards metric measures.
Comparison with other varieties.
Where British and American vocabulary differs, Australians sometimes favour an Australian English usage, as with footpath (for US sidewalk, UK pavement) or capsicum (for US bell pepper, UK green/red pepper). In other instances, it either shares a term with American English, as with truck (UK: lorry) or eggplant (UK: aubergine), or with British English, as with mobile phone (US: cell phone) or bonnet (US: hood).
A non-exhaustive selection of common British English terms not commonly used in Australian English include (Australian usage in brackets): artic/articulated lorry (semi-trailer); aubergine (eggplant); bank holiday (public holiday); bedsit (one-bedroom apartment); bespoke (custom); black pudding (blood sausage); cagoule (raincoat); candy floss (fairy floss); cash machine (automatic teller machine/ATM)); child-minder (babysitter); clingfilm (glad wrap/cling-wrap); courgette (zucchini); crisps (chips/potato chips); doddle (bludge); dungarees (overalls); dustbin (garbage/rubbish bin); dustcart (garbage/rubbish truck); duvet (doona); elastoplast/plaster (band-aid); estate car (station wagon); fairy cake (cupcake/patty cake); flannel ((face) washer/wash cloth); free phone (toll-free); football (soccer); high street (main street); hoover (vacuum cleaner); ice lolly (ice block/icy pole); kitchen roll (paper towel); lavatory (toilet); lorry (truck); off-licence (bottle shop); pavement (footpath); red/green pepper (capsicum); pillar box (mail box); plimsoll (sandshoe); pushchair (pram/stroller); saloon (sedan); sweets (lollies); utility room (laundry); Wellington boots (gumboots).
A non-exhaustive list of American English terms not commonly found in Australian English include: acclimate (acclimatise); aluminum (aluminium); bangs (fringe); bell pepper (capsicum); bellhop (hotel porter); broil (grill); burglarize (rob); busboy (included under the broader term of waiter/waitress; rarely, table clearer); candy (lolly); cell phone (mobile phone); cilantro (coriander); cookie (biscuit); counter-clockwise (anticlockwise); diaper (nappy); emergency brake (handbrake); faucet (tap); flashlight (torch); gasoline (petrol); hood (bonnet); jell-o (jelly); jelly (jam); math (maths); pacifier (dummy); parking lot (car park); popsicle (ice block/icy pole); railway ties (sleepers); row house (terrace house); scallion (spring onion); stickshift (manual transmission); streetcar (tram); takeout (takeaway); trash can (garbage/rubbish bin); trunk (boot); turn signal (indicator/blinker); vacation (holiday).
Terms shared by British and American English but not so commonly found in Australian English include: abroad (overseas); cooler/ice box (esky); pickup truck (ute); wildfire (bushfire).
In addition, a number of words in Australian English have different meanings to those ascribed in other varieties of English. Clothing-related examples are notable. "Pants" in Australian English refer to British English "trousers" but in British English refer to Australian English "underpants"; "vest" in Australian English refers to British English "waistcoat" but in British English refers to Australian English "singlet"; "thong" in both American and British English refers to underwear otherwise known as a "G-string" while in Australian English it refers to British and American English "flip-flop".
Grammar.
As with American English, but unlike British English, collective nouns are almost always singular in construction, e.g. "the government was unable to decide" as opposed to "the government were unable to decide".
In common with British English, the past tense and past participles of the verbs "learn", "spell" and "smell" are often irregular ("learnt", "spelt", "smelt").
"Shan't" and the use of "should" as in "I should be happy if...", common in upper-register British English, are almost never encountered in Australian (or North American) English.
While prepositions before days may be omitted in American English, i.e. "She resigned Thursday", they must be retained in Australian English, as in British English: "She resigned on Thursday". Ranges of dates use "to", i.e. "Monday to Friday", as with British English, rather than "Monday through Friday" in American English.
"River" generally follows the name of the river in question as in North America, i.e. "Darling River", rather than the British convention of coming before the name, e.g. "River Thames". Note in South Australia however, the British convention applies—for example, the "River Murray" or the "River Torrens".
When saying or writing out numbers, "and" is inserted before the tens and units, i.e. "one hundred and sixty-two", as with British practice. However Australians, like Americans, are more likely to pronounce numbers such as 1200 as "twelve hundred", rather than "one thousand two hundred".
As with American English, "on the weekend" and "studied medicine" are used rather than the British "at the weekend" and "read medicine".
Spelling, style and keyboards.
As in most English-speaking countries, there is no official governmental regulator or overseer of correct spelling and grammar. The "Macquarie Dictionary" is used by universities and other organisations as a standard for Australian English spelling. The "Style Manual: For Authors, Editors and Printers", the "Cambridge Guide to Australian English Usage" and the "Australian Guide to Legal Citation" are prominent style guides.
Australian spelling is closer to British than American spelling. As with British spelling, the "u" is retained in words such as "colour", "honour", "labour" and "favour". While the Macquarie Dictionary lists the "-our" ending and follows it with the "-or" ending as an acceptable variant, the latter are rarely found in usage today. Australian print media, including digital media, today strongly favour "-our" endings. A notable exception to this rule is the Australian Labor Party, which adopted the American spelling in 1912 as a result of "-or" spellings' comparative popularity at that time and American influence. Consistent with British spellings, "-re", rather than "-er", is the only listed variant in Australian dictionaries in words such as "theatre", "centre" and "manoeuvre". Unlike British English, which is split between "-ise" and "-ize" in words such as "organise" and "realise", with "-ize" favoured by the Oxford English Dictionary and "-ise" listed as a variant, "-ize" is rare in Australian English and designated as a variant by the Macquarie Dictionary. "Ae" and "oe" are often maintained in words such as "manoeuvre", "paedophilia" and "foetus" (excepting those listed below), however the Macquarie dictionary lists forms with "e" (e.g. pedophilia, fetus) as acceptable variants and notes a tendency within Australian English towards using only "e". Individual words spelt differently from British spelling, according to the "Macquarie Dictionary", include "program" (in all contexts) as opposed to "programme", "inquire" (for all meanings) and derivatives "inquired", "inquiring", "inquiry", "inquirer", etc. as opposed to "enquire" and derivatives, "analog" as opposed to "analogue" (as with American English, "analog" is used in the context of information transmission and "analogue" in the sense of "something analogous to"), "livable" as opposed to "liveable", "guerilla" as opposed to "guerrilla", "yoghurt" as opposed to "yogurt", "verandah" as opposed to "veranda", "sulfur" and derivatives "sulfide", "sulfidic" and "sulfuric" as opposed to "sulphur" and derivatives, "burqa" as opposed to "burka", "pastie" (food) as opposed to "pasty", "onto" "or" "on to" as opposed to "on to", "anytime" as opposed to "any time", "alright" "or" "all right" as opposed to "all right", and "anymore" as opposed to "any more". Both "acknowledgement" and "acknowledgment", as well as "abridgement" and "abridgment" are used, with the shorter forms being endorsed by Australian governments. "Okay", rather than "OK", is listed as the preferred variant.
Different spellings have existed throughout Australia's history. A pamphlet entitled "The So-Called "American Spelling"", published in Sydney some time in the 19th century, argued that "there is no valid etymological reason for the preservation of the "u" in such words as "honor", "labor", etc." The pamphlet also claimed that "the tendency of people in Australasia is to excise the u, and one of the Sydney morning papers habitually does this, while the other generally follows the older form." What are today regarded as American spellings were popular in Australia throughout the late 19th and early 20th centuries, with the Victorian Department of Education endorsing them into the 1970s and The Age newspaper until the 1990s. This influence can be seen in the spelling of the Australian Labor Party and also in some place names such as Victor Harbor. The "Concise Oxford English Dictionary" has been attributed with re-establishing the dominance of the British spellings in the 1920s and 1930s. For a short time during the late 20th century, Harry Lindgren's 1969 spelling reform proposal ("Spelling Reform 1" or "SR1") was popular in Australia and was adopted by the Australian government. SR1 calls for the short sound (as in "bet") to be spelt with E (for example "friend→frend, head→hed"). Many general interest paperbacks were printed in SR1.
Both single and double quotation marks are in use (with double quotation marks being far more common in print media), with logical (as opposed to typesetter's) punctuation. Spaced and unspaced em-dashes remain in mainstream use, as with American and Canadian English. The DD/MM/YYYY date format is used with Monday as the first day of the week (as with British practice), however the 12-hour clock is used almost universally (as in the United States).
There are two major English language keyboard layouts, the United States layout and the United Kingdom layout. Australia universally uses the United States keyboard. As such, Pound Sterling and Euro currency symbols do not appear on Australian keyboards, which also lack negation symbols and have punctuation symbols placed differently from the way they are placed on British keyboards.

</doc>
<doc id="1902" url="http://en.wikipedia.org/wiki?curid=1902" title="American Airlines Flight 77">
American Airlines Flight 77

American Airlines Flight 77 was a passenger flight which was hijacked by five men affiliated to al-Qaeda on September 11, 2001, as part of the September 11 attacks. They deliberately crashed it into the Pentagon in Arlington County, Virginia near Washington, D.C., killing all 64 people on board including the five hijackers and six crew as well as 125 people in the building. The Boeing 757-223 aircraft was flying American Airlines' daily scheduled morning transcontinental service from Washington Dulles International Airport, in Dulles, Virginia to Los Angeles International Airport in Los Angeles, California.
Less than 35 minutes into the flight, the hijackers stormed the cockpit. They forced the passengers, crew, and pilots to the rear of the aircraft. Hani Hanjour, one of the hijackers who was trained as a pilot, assumed control of the flight. Unknown to the hijackers, passengers aboard made telephone calls to loved ones and relayed information on the hijacking.
The hijackers crashed the aircraft into the western side of the Pentagon at 9:37 a.m. (EDT). Dozens of people witnessed the crash, and news sources began reporting on the incident within minutes. The impact severely damaged an area of the Pentagon and caused a large fire. At 10:10 a.m., a portion of the Pentagon collapsed; firefighters spent days trying to fully extinguish the blaze. The damaged sections of the Pentagon were rebuilt in 2002, with occupants moving back into the completed areas on August 15, 2002.
The 184 victims of the attack are memorialized in the Pentagon Memorial adjacent to the Pentagon. The park contains a bench for each of the victims, arranged according to their year of birth, ranging from 1930 (age 71) to 1998 (age 3).
Hijackers.
The hijackers on American Airlines Flight 77 were led by Hani Hanjour, who piloted the aircraft into the Pentagon. Hanjour first came to the United States in 1990.
Hanjour trained at the CRM Airline Training Center in Scottsdale, Arizona, earning his FAA commercial pilot's certificate in April 1999. He had wanted to be a commercial pilot for the Saudi national airline but was rejected when he applied to the civil aviation school in Jeddah in 1999. Hanjour's brother later explained that, frustrated at not finding a job, Hanjour "increasingly turned his attention toward religious texts and cassette tapes of militant Islamic preachers". Hanjour returned to Saudi Arabia after being certified as a pilot, but left again in late 1999, telling his family that he was going to the United Arab Emirates to work for an airline. Hanjour likely went to Afghanistan, where Al Qaeda recruits were screened for special skills they may have. Already having selected the Hamburg Cell members, Al Qaeda leaders selected Hanjour to lead the fourth team of hijackers.
Alec Station, the CIA's unit dedicated to tracking Osama bin Laden, had discovered that two of the other hijackers, al-Hazmi and al-Mihdhar, had multiple entry visas to the United States well before 9/11. Two FBI agents inside the unit tried to alert FBI headquarters, but CIA officers rebuffed them.
In December 2000, Hanjour arrived in San Diego, joining "muscle" hijackers Nawaf al-Hazmi and Khalid al-Mihdhar, who had been there since January 2000. Soon after arriving, Hanjour and Hazmi left for Mesa, Arizona, where Hanjour began refresher training at Arizona Aviation.
In April 2001, they relocated to Falls Church, Virginia, where they awaited the arrival of the remaining "muscle" hijackers. One of these men, Majed Moqed, arrived on May 2, 2001, with Flight 175 hijacker Ahmed al-Ghamdi from Dubai at Dulles International Airport. They moved into an apartment with Hazmi and Hanjour. While living in Falls Church, Hazmi attended the mosque in the community.
On May 21, 2001, Hanjour rented a room in Paterson, New Jersey, where he stayed with other hijackers through the end of August. The last Flight 77 "muscle" hijacker, Salem al-Hazmi, arrived on June 29, 2001, with Abdulaziz al-Omari (a hijacker of Flight 11) at John F. Kennedy International Airport from the United Arab Emirates. They stayed with Hanjour.
Hanjour received ground instruction and did practice flights at Air Fleet Training Systems in Teterboro, New Jersey, and at Caldwell Flight Academy in Fairfield, New Jersey. Hanjour moved out of the room in Paterson and arrived at the Valencia Motel in Laurel, Maryland, on September 2, 2001. While in Maryland, Hanjour and fellow hijackers trained at Gold's Gym in Greenbelt. On September 10, he completed a certification flight, using a terrain recognition system for navigation, at Congressional Air Charters in Gaithersburg, Maryland.
On September 10, Nawaf al-Hazmi, accompanied by other hijackers, checked into the Marriott in Herndon, Virginia, near Dulles Airport.
Suspected accomplices.
According to a U.S. State Department cable leaked in the Wikileaks dump in February 2010, the FBI has investigated another suspect, Mohammed al-Mansoori. He had associated with three Qatari citizens who flew from Los Angeles to London (via Washington) and Qatar on the eve of the attacks, after allegedly surveying the World Trade Center and the White House. U.S. law enforcement officials said that the data about the four men was "just one of many leads that were thoroughly investigated at the time and never led to terrorism charges". An official added that the three Qatari citizens have never been questioned by the FBI. Eleanor Hill, the former staff director for the congressional joint inquiry on the September 11 attacks, said the cable reinforces questions about the thoroughness of the FBI's investigation. She also said that the inquiry concluded that the hijackers had a support network that helped them in different ways.
The three Qatari men were booked to fly from Los Angeles to Washington on September 10, 2001, on the same plane that was hijacked and piloted into the Pentagon on the following day. Instead, they flew from Los Angeles to Qatar, via Washington and London. While the cable said that Mansoori was currently under investigation, U.S. law enforcement officials said that there was no active investigation of him or of the Qatari citizens mentioned in the cable.
Flight.
The American Airlines Flight 77 aircraft was a Boeing 757-223 (registration number N644AA). The flight crew included pilot Charles Burlingame, First Officer David Charlebois, and flight attendants Michele Heidenberger, Jennifer Lewis, Kenneth Lewis, and Renee May. The capacity of the aircraft was 188 passengers, but with 58 passengers on September 11, the load factor was 33 percent. American Airlines said that Tuesdays were the least-traveled day of the week, with the same load factor seen on Tuesdays in the previous three months for Flight 77.
Boarding and departure.
On the morning of September 11, 2001, the five hijackers arrived at Washington Dulles International Airport. At 07:15, Khalid al-Mihdhar and Majed Moqed checked in at the American Airlines ticket counter for Flight 77, arriving at the passenger security checkpoint a few minutes later at 07:18. Both men set off the metal detector and were put through secondary screening. Moqed continued to set off the alarm, so he was searched with a hand wand. The Hazmi brothers checked in together at the ticket counter at 07:29. Hani Hanjour checked in separately and arrived at the passenger security checkpoint at 07:35. Hanjour was followed minutes later at the checkpoint by Salem and Nawaf al-Hazmi, who also set off the metal detector's alarm. The screener at the checkpoint never resolved what set off the alarm. As seen in security footage later released, Nawaf Hazmi appeared to have an unidentified item in his back pocket. Utility knives up to four inches were permitted at the time by the Federal Aviation Administration (FAA) as carry-on items. The passenger security checkpoint at Dulles International Airport was operated by Argenbright Security, under contract with United Airlines.
The hijackers were all selected for extra screening of their checked bags. Hanjour, al-Mihdhar, and Moqed were chosen by the Computer Assisted Passenger Prescreening System criteria, while the brothers Nawaf and Salem al-Hazmi were selected because they did not provide adequate identification and were deemed suspicious by the airline check-in agent. Hanjour, Mihdhar, and Nawaf al-Hazmi did not check any bags for the flight. Checked bags belonging to Moqed and Salem al-Hazmi were held until they boarded the aircraft. By 07:50, the five hijackers, carrying knives and box cutters, had made it through the airport security checkpoint.
Flight 77 was scheduled to depart for Los Angeles at 08:10; 58 passengers boarded through Gate D26, including the five hijackers. Excluding the hijackers, of the 59 other passengers and crew on board, there were 26 men, 22 women, and five children ranging in age from three to eleven. On the flight, Hani Hanjour was seated up front in 1B, while Salem and Nawaf al-Hazmi were seated in first class in seats 5E and 5F. Majed Moqed and Khalid al-Mihdhar were seated further back in 12A and 12B, in economy class. Flight 77 left the gate on time and took off from Runway 30 at Dulles at .
Hijacking.
The 9/11 Commission estimated that the flight was hijacked between 08:51 and 08:54, shortly after American Airlines Flight 11 struck the World Trade Center and not too long after United Airlines Flight 175 had been hijacked. The last normal radio communications from the aircraft to air traffic control occurred at 08:50:51. Unlike the other three flights, there were no reports of anyone being stabbed or a bomb threat. At 08:54, the plane began to deviate from its normal, assigned flight path and turned south. The hijackers set the flight's autopilot heading for Washington, D.C. By 08:56, the flight was turned around, and the transponder had been disabled.
The FAA was aware at this point that there was an emergency on board the airplane. By this time, American Airlines Flight 11 had already crashed into the North Tower of the World Trade Center, and United Airlines Flight 175 was known to have been hijacked and was within minutes of striking the South Tower. After learning of this second hijacking involving an American Airlines aircraft and the hijacking involving United Airlines, American Airlines' Executive Vice President Gerard Arpey ordered a nationwide ground stop for the airline. The Indianapolis Air Traffic Control Center, as well as American Airlines dispatchers, made several failed attempts to contact the aircraft. At the time the airplane was hijacked, it was flying over an area of limited radar coverage. With air controllers unable to contact the flight by radio, an Indianapolis official declared that the Boeing 757 had possibly crashed at 09:09.
Two people on the aircraft made phone calls to contacts on the ground. At 09:12, flight attendant Renee May called her mother, Nancy May, in Las Vegas. During the call, which lasted nearly two minutes, May said her flight was being hijacked by six persons, and staff and passengers had been moved to the rear of the airplane. May asked her mother to contact American Airlines, which she and her husband promptly did; American Airlines was already aware of the hijacking. Between 09:16 and 09:26, passenger Barbara Olson called her husband, United States Solicitor General Theodore Olson, and reported that the airplane had been hijacked and that the assailants had box cutters and knives. She reported that the passengers, including the pilots, had been moved to the back of the cabin and that the hijackers were unaware of her call. A minute into the conversation, the call was cut off. Theodore Olson contacted the command center at the Department of Justice, and tried unsuccessfully to contact Attorney General John Ashcroft. About five minutes later, Barbara Olson called again, told her husband that the "pilot" (possibly Hanjour on the cabin intercom) had announced the flight was hijacked, and asked "what do I tell the pilot to do?" Ted Olson asked her location and she reported the plane was flying over a residential area. He told her of the attacks on the World Trade Center. Soon afterward, the call cut off again.
An airplane was detected again by Dulles controllers on radar screens as it approached Washington, turning and descending rapidly. Controllers initially thought this was a military fighter, due to its high speed and maneuvering. Reagan Airport controllers asked a passing Air National Guard Lockheed C-130 Hercules to identify and follow the aircraft. The pilot, Lt. Col. Steven O'Brien, told them it was a Boeing 757 or 767, and its silver fuselage meant it was probably an American Airlines jet. He had difficulty picking out the airplane in the "East Coast haze", but then saw a "huge" fireball, and initially assumed it had hit the ground. Approaching the Pentagon, he saw the impact site on the building's west side and reported to Reagan control, "Looks like that aircraft crashed into the Pentagon, sir."
Crash.
According to the 9/11 Commission Report, as Flight 77 was west-southwest of the Pentagon, it made a 330-degree turn. At the end of the turn, it was descending through , pointed toward the Pentagon and downtown Washington. Hani Hanjour advanced the throttles to maximum power and dived toward the Pentagon. While level above the ground and seconds from the crash, the wings knocked over five street lampposts and the right wing struck a portable generator, creating a smoke trail seconds before smashing into the Pentagon. Flight 77, flying at 530 mph (853 km/h, 237 m/s, or 460 knots) over the Navy Annex Building adjacent to Arlington National Cemetery, crashed into the western side of the Pentagon in Arlington County, Virginia, just south of Washington, D.C., at 09:37:46, killing all 64 people on board: 53 passengers, five hijackers, and six crew. The plane hit the Pentagon at the first-floor level, and at the moment of impact, the airplane was rolled slightly to the left, with the right wing elevated. The front part of the fuselage disintegrated on impact, while the mid and tail sections moved for another fraction of a second, with tail section debris penetrating furthest into the building. In all, the airplane took eight-tenths of a second to fully penetrate into the three outermost of the building's five rings and unleashed a fireball that rose above the building.
At the time of the attacks, approximately 18,000 people worked in the Pentagon, which was 4,000 fewer than before renovations began in 1998. The section of the Pentagon, which had recently been renovated at a cost of $250 million, housed the Naval Command Center and other Pentagon offices, as well as some unoccupied offices. The crash and subsequent fire penetrated three outer ring sections of the western side. The outermost ring section was largely destroyed, and a large section collapsed. One hundred and twenty-five people in the Pentagon died in the attack.
In all, there were 189 deaths at the Pentagon site, including the 125 in the Pentagon building in addition to the 64 on board the aircraft. Passenger Barbara Olson was en route to a taping of "Politically Incorrect". A group of children, their chaperones, and National Geographic Society staff members were also on board, embarking on an educational trip west to the Channel Islands National Marine Sanctuary near Santa Barbara, California. The fatalities at the Pentagon included 55 military personnel and 70 civilians. Of those 125 killed, 92 were on the first floor, 31 were on the second floor, and two were on the third. The Army suffered 75 fatalities—far more than any other branch. Another 106 injured were treated at area hospitals. Lieutenant General Timothy Maude, an Army Deputy Chief of Staff, was the highest-ranking military officer killed at the Pentagon; also killed was retired Rear Admiral Wilson Flagg, a passenger on the plane. LT Mari-Rae Sopper, JAGC, USNR, was also on board the flight, and was the first Navy Judge Advocate ever killed in action.
On the side where the plane hit, the Pentagon is bordered by Interstate 395 and Washington Boulevard. Motorist Mary Lyman, who was on I-395, saw the airplane pass over at a "steep angle toward the ground and going fast" and then saw the cloud of smoke from the Pentagon. Omar Campo, another witness, was cutting the grass on the other side of the road when the airplane flew over his head.
"I was cutting the grass and it came in screaming over my head. I felt the impact. The whole ground shook and the whole area was full of fire. I could never imagine I would see anything like that here".Afework Hagos, a computer programmer, was on his way to work and stuck in a traffic jam near the Pentagon when the airplane flew over. "There was a huge screaming noise and I got out of the car as the plane came over. Everybody was running away in different directions. It was tilting its wings up and down like it was trying to balance. It hit some lampposts on the way in." Daryl Donley witnessed the crash and took some of the first photographs of the site.
"USA Today" reporter Mike Walter was driving on Washington Boulevard when he witnessed the crash, which he recounted,
"I looked out my window and I saw this plane, this jet, an American Airlines jet, coming. And I thought, 'This doesn't add up, it's really low.' And I saw it. I mean it was like a cruise missile with wings. It went right there and slammed right into the Pentagon".Terrance Kean, who lived in a nearby apartment building, heard the noise of loud jet engines, glanced out his window, and saw a "very, very large passenger jet". He watched "it just plow right into the side of the Pentagon. The nose penetrated into the portico. And then it sort of disappeared, and there was fire and smoke everywhere." Tim Timmerman, who is a pilot himself, noticed American Airlines markings on the aircraft as he saw it hit the Pentagon. Other drivers on Washington Boulevard, Interstate 395, and Columbia Pike witnessed the crash, as did people in Pentagon City, Crystal City, and other nearby locations.
Former Georgetown University basketball coach John Thompson had originally booked a ticket on Flight 77. As he would tell the story many times in the following years, including a September 12, 2011 interview on Jim Rome's radio show, he had been scheduled to appear on that show on September 12, 2001. Thompson was planning to be in Las Vegas for a friend's birthday on September 13, and initially insisted on traveling to Rome's Los Angeles studio on the 11th. However, this did not work for the show, which wanted him to travel on the day of the show. After a Rome staffer personally assured Thompson that he would be able to travel from Los Angeles to Las Vegas immediately after the show, Thompson changed his travel plans. He felt the impact from the crash at his home near the Pentagon.
Rescue and recovery.
Rescue efforts began immediately after the crash. Almost all the successful rescues of survivors occurred within half an hour of the impact. Initially, rescue efforts were led by the military and civilian employees within the building. Within minutes, the first fire companies arrived and found these volunteers searching near the impact site. The firemen ordered them to leave as they were not properly equipped or trained to deal with the hazards. The Arlington County Fire Department (ACFD) assumed command of the immediate rescue operation within 10 minutes of the crash. ACFD Assistant Chief James Schwartz implemented an incident command system (ICS) to coordinate response efforts among multiple agencies. It took about an hour for the ICS structure to become fully operational. Firefighters from Fort Myer and Reagan National Airport arrived within minutes. Rescue and firefighting efforts were impeded by rumors of additional incoming planes. Chief Schwartz ordered two evacuations during the day in response to these rumors.
As firefighters attempted to extinguish the fires, they watched the building in fear of a structural collapse. One firefighter remarked that they "pretty much knew the building was going to collapse because it started making weird sounds and creaking". Officials saw a cornice of the building move and ordered an evacuation. Minutes later, at 10:10, the upper floors of the damaged area of the Pentagon collapsed. The collapsed area was about at its widest point and at its deepest. The amount of time between impact and collapse allowed everyone on the fourth and fifth levels to evacuate safely before the structure collapsed. After the collapse, the interior fires intensified, spreading through all five floors. After 11:00, firefighters mounted a two-pronged attack against the fires. Officials estimated temperatures of up to . While progress was made against the interior fires by late afternoon, firefighters realized a flammable layer of wood under the Pentagon's slate roof had caught fire and begun to spread. Typical firefighting tactics were rendered useless by the reinforced structure as firefighters were unable to reach the fire to extinguish it. Firefighters instead made firebreaks in the roof on September 12 to prevent further spreading. At 18:00 on the 12th, Arlington County issued a press release stating the fire was "controlled" but not fully "extinguished". Firefighters continued to put out smaller fires that ignited in the succeeding days.
Various pieces of aircraft debris were found within the wreckage at the Pentagon. While on fire and escaping from the Navy Command Center, Lt. Kevin Shaeffer observed a chunk of the aircraft's nose cone and the nose landing gear in the service road between rings B and C. Early in the morning on Friday, September 14, Fairfax County Urban Search and Rescue Team members Carlton Burkhammer and Brian Moravitz came across an "intact seat from the plane's cockpit", while paramedics and firefighters located the two black boxes near the punch out hole in the A-E drive, nearly into the building. The cockpit voice recorder was too badly damaged and charred to retrieve any information, though the flight data recorder yielded useful information. Investigators also found a part of Nawaf al-Hazmi's driver's license in the North Parking Lot rubble pile. Personal effects belonging to victims were found and taken to Fort Myer.
Remains.
Army engineers determined by 5:30 p.m. on the first day that no one remained alive in the damaged section of the building. In the days after the crash, news reports emerged that up to 800 people had died. Army troops from Fort Belvoir were the first teams to survey the interior of the crash site and noted the presence of human remains. Federal Emergency Management Agency (FEMA) Urban Search and Rescue teams, including Fairfax County Urban Search and Rescue assisted the search for remains, working through the National Interagency Incident Management System (NIIMS). Kevin Rimrodt, a Navy photographer surveying the Navy Command Center after the attacks, remarked that "there were so many bodies, I'd almost step on them. So I'd have to really take care to look backwards as I'm backing up in the dark, looking with a flashlight, making sure I'm not stepping on somebody". Debris from the Pentagon was taken to the Pentagon's north parking lot for more detailed search for remains and evidence.
Remains that were recovered from the Pentagon were photographed, and turned over to the Armed Forces Medical Examiner office, located at Dover Air Force Base in Delaware. The medical examiner's office was able to identify remains belonging to 179 of the victims. Investigators eventually identified 184 of the 189 people who died in the attack. The remains of the five hijackers were identified through a process of elimination, and were turned over as evidence to the Federal Bureau of Investigation (FBI). On September 21, the ACFD relinquished control of the crime scene to the FBI. The Washington Field Office, National Capital Response Squad (NCRS), and the Joint Terrorism Task Force (JTTF) led the crime scene investigation at the Pentagon.
By October 2, 2001, the search for evidence and remains was complete and the site was turned over to Pentagon officials. In 2002, the remains of 25 victims were buried collectively at Arlington National Cemetery, with a five-sided granite marker inscribed with the names of all the victims in the Pentagon. The ceremony also honored the five victims whose remains were never found.
Flight Data Recorder and Cockpit Voice Recorder.
At around 3:40 a.m on September 14, a paramedic and a firefighter who were searching through the debris of the impact site found two dark boxes, about by long. They called for an FBI agent, who in turn called for someone from the National Transportation Safety Board (NTSB). The NTSB employee confirmed that these were the flight recorders ("black boxes") from American Airlines Flight 77. Dick Bridges, deputy manager for Arlington County, Virginia, said the cockpit voice recorder that used magnetic tape was damaged on the outside and the flight data recorder that used a solid-state drive was charred. Bridges said the recorders were found "right where the plane came into the building."
The cockpit voice recorder that used magnetic tape was transported to the NTSB lab in Washington, D.C., to see what data was salvageable. In its report on the cockpit voice recorder, the NTSB identified the unit as an L-3 Communications, Fairchild Aviation Recorders model A-100A cockpit voice recorder; a device which records on magnetic tape. The NTSB reported that "The majority of the recording tape was fused into a solid block of charred plastic." No usable segments of tape were found inside the recorder. All the data from the flight data recorder that used a solid-state drive was recovered.
Continuity of operations.
At the moment of impact, Secretary of Defense Donald Rumsfeld was in his office on the other side of the Pentagon, away from the crash site. He ran to the site and assisted the injured. Rumsfeld returned to his office, and went to a conference room in the Executive Support Center where he joined a secure videoteleconference with Vice President Dick Cheney and other officials. On the day of the attacks, DoD officials considered moving their command operations to Site R, a backup facility in Pennsylvania. Secretary of Defense Rumsfeld insisted he remain at the Pentagon, and sent Deputy Secretary Paul Wolfowitz to Site R. The National Military Command Center (NMCC) continued to operate at the Pentagon, even as smoke entered the facility. Engineers and building managers manipulated the ventilation and other building systems that still functioned to draw smoke out of the NMCC and bring in fresh air.
During a press conference held inside the Pentagon at 18:42, Rumsfeld announced, "The Pentagon's functioning. It will be in business tomorrow." Pentagon employees returned the next day to offices in mostly unaffected areas of the building. By the end of September, more workers returned to the lightly damaged areas of the Pentagon.
Aftermath.
Early estimates on rebuilding the damaged section of the Pentagon were that it would take three years to complete. However, the project moved forward at an accelerated pace and was completed by the one-year anniversary of the attack. The rebuilt section of the Pentagon includes a small indoor memorial and chapel at the point of impact. An outdoor memorial, commissioned by the Pentagon and designed by Julie Beckman and Keith Kaseman, was completed on schedule for its dedication on September 11, 2008.
Security camera video.
On May 16, 2006, the Department of Defense released filmed footage that was recorded by a security camera of American Airlines Flight 77 crashing into the Pentagon, with a plane visible in one frame, as a "thin white blur" and an explosion following. The images were made public in response to a December 2004 Freedom of Information Act request by Judicial Watch. Some still images from the video had previously been released and publicly circulated, but this was the first official release of the edited video of the crash.
A nearby Citgo service station also had security cameras, but a video released on September 15, 2006 did not show the crash because the camera was pointed away from the crash site.
The Doubletree Hotel, located nearby in Crystal City, Virginia, also had a security camera video. On December 4, 2006, the FBI released the video in response to a FOIA lawsuit filed by Scott Bingham. The footage is "grainy and the focus is soft, but a rapidly growing tower of smoke is visible in the distance on the upper edge of the frame as the plane crashes into the building".
Memorials.
On September 12, 2002, Defense Secretary Donald Rumsfeld and General Richard Myers, Chairman of the Joint Chiefs of Staff, dedicated the Victims of Terrorist Attack on the Pentagon Memorial at Arlington National Cemetery. The memorial specifically honors the five individuals for whom no identifiable remains were found. This included Dana Falkenberg, age three, who was aboard American Airlines Flight 77 with her parents and older sister. A portion of the remains of 25 other victims are also buried at the site. The memorial is a pentagonal granite marker high. On five sides of the memorial along the top are inscribed the words "Victims of Terrorist Attack on the Pentagon September 11, 2001". Aluminum plaques, painted black, are inscribed with the names of the 184 victims of the terrorist attack. The site is located in Section 64, on a slight rise, which gives it a view of the Pentagon.
At the National September 11 Memorial, the names of the Pentagon victims are inscribed on the South Pool, on Panels S-1 and S-72 – S-76.

</doc>
<doc id="1905" url="http://en.wikipedia.org/wiki?curid=1905" title="Ambush">
Ambush

An ambush is a long-established military tactic, in which combatants take advantage of concealment and the element of surprise to attack unsuspecting enemy combatants from concealed positions, such as among dense underbrush or behind hilltops. Ambushes have been used consistently throughout history, from ancient to modern warfare.
History.
The use by early humans of the ambush may date as far back as two million years when anthropologists have recently suggested that ambush techniques were used to hunt large game. 
More recently, an ambush often might involve thousands of soldiers on a large scale, such as over a mountain pass. Ambushes appear many times in military history. One outstanding example from ancient times is the Battle of the Trebia river. Hannibal encamped within striking distance of the Romans with the Trebia River between them, and placed a strong force of cavalry and infantry in concealment, near the battle zone. He had noticed, says Polybius, a "“place between the two camps, flat indeed and treeless, but well adapted for an ambuscade, as it was traversed by a water-course with steep banks, densely overgrown with brambles and other thorny plants, and here he proposed to lay a stratagem to surprise the enemy”". 
When the Roman infantry became entangled in combat with his army, the hidden ambush force attacked the legionnaires in the rear. The result was slaughter and defeat for the Romans. Nevertheless the battle also displays the effects of good tactical discipline on the part of the ambushed force. Although most of the legions were lost, about 10,000 Romans cut their way through to safety, maintaining unit cohesion. This ability to maintain discipline and break out or maneuver away from a killing zone is a hallmark of good troops and training in any ambush situation. See Ranger reference below.
Another famous ambush was that sprung by Germanic warchief Arminius against the Romans at Battle of the Teutoburg Forest. This particular ambush was to have an impact on the course of Western history. The Germanic forces demonstrated several principles needed for a successful ambush. They took cover in difficult forested terrain, allowing the warriors time and space to mass without detection. They had the element of surprise, and this was also aided by the defection of Arminius from Roman ranks prior to the battle. They sprung the attack when the Romans were most vulnerable- when they had left their fortified camp, and were on the march in a pounding rainstorm. 
They did not dawdle at the hour of decision but attacked quickly, using a massive series of short, rapid, vicious charges against the length of the whole Roman line, with charging units sometimes withdrawing to the forest to regroup while others took their place. The Germans also made use of blocking obstacles, erecting a trench and earthen wall to hinder Roman movement along the route of the killing zone. The result was mass slaughter of the Romans, and the destruction of 3 legions. The Germanic victory caused a limit on Roman expansion in the West. Ultimately, it established the Rhine as the boundary of the Roman Empire for the next four hundred years, until the decline of the Roman influence in the West. The Roman Empire made no further concerted attempts to conquer Germania beyond the Rhine.
Procedure.
In modern warfare, an ambush is most often employed by ground troops up to platoon size against enemy targets, which may be other ground troops, or possibly vehicles. However, in some situations, especially when deep behind enemy lines, the actual attack will be carried out by a platoon, a company-sized unit will be deployed to support the attack group, setting up and maintaining a forward patrol harbour from which the attacking force will deploy, and to which they will retire after the attack.
Planning.
Ambushes are complex, multi-phase operations, and are, therefore, usually planned in some detail. First, a suitable killing zone is identified. This is the place where the ambush will be laid. It's generally a place where enemy units are expected to pass, and which gives reasonable cover for the deployment, execution, and extraction phases of the ambush patrol. A path along a wooded valley floor would be a typical example.
Ambush can be described geometrically as:

</doc>
<doc id="1908" url="http://en.wikipedia.org/wiki?curid=1908" title="Abzyme">
Abzyme

An abzyme (from antibody and enzyme), also called "catmab" (from "catalytic monoclonal antibody"), is a monoclonal antibody with catalytic activity. Molecules which are modified to gain new catalytic activity are called synzymes. Abzymes are usually artificial constructs, but are also found in normal humans (anti-vasoactive intestinal peptide autoantibodies) and in patients with autoimmune diseases such as systemic lupus erythematosus, where they can bind to and hydrolyze DNA. Abzymes are potential tools in biotechnology, e.g., to perform specific actions on DNA. They are also useful in hydrolysis of esters. Rate of hydrolysis was increased 100 times.
Enzymes function by lowering the activation energy of the transition state, thereby catalyzing the formation of an otherwise less-favorable molecular intermediate between reactants and products. If an antibody is developed to stabilize a molecule that's similar to an unstable intermediate of another (potentially unrelated) reaction, the developed antibody will enzymatically bind to and stabilize the intermediate state, thus catalyzing the reaction. A new and unique type of enzyme is produced.
HIV treatment.
In a June 2008 issue of the journal Autoimmunity Review, researchers S Planque, Sudhir Paul, Ph.D, and Yasuhiro Nishiyama, Ph.D of the University Of Texas Medical School at Houston announced that they have engineered an abzyme that degrades the superantigenic region of the gp120 CD4 binding site. This is the one part of the HIV virus outer coating that does not change, because it is the attachment point to T lymphocytes, the key cell in cell-mediated immunity. Once infected by HIV, patients produce antibodies to the more changeable parts of the viral coat. The antibodies are ineffective because of the virus' ability to change their coats rapidly. Because this protein gp120 is necessary for HIV to attach, it does not change across different strains and is a point of vulnerability across the entire range of the HIV variant population.
The abzyme does more than bind to the site, it actually destroys the site, rendering HIV inert, and then can attach to other viruses. A single abzyme can destroy thousands of HIV viruses. Human clinical trials will be the next step in producing treatment and perhaps even preventative vaccines and microbicides.

</doc>
<doc id="1909" url="http://en.wikipedia.org/wiki?curid=1909" title="Adaptive radiation">
Adaptive radiation

In evolutionary biology, adaptive radiation is a process in which organisms diversify rapidly into a multitude of new forms, particularly when a change in the environment makes new resources available, creates new challenges and opens environmental niches. Starting with a recent single ancestor, this process results in the speciation and phenotypic adaptation of an array of species exhibiting different morphological and physiological traits with which they can exploit a range of divergent environments.
Adaptive radiation, a characteristic example of cladogenesis, can be graphically illustrated as a "bush", or clade, of coexisting species (on the tree of life).
Identification.
Four features can be used to identify an adaptive radiation:
Causes.
Innovation.
The evolution of a novel feature may permit a clade to diversify by making new areas of morphospace accessible. A classic example is the evolution of a fourth cusp in the mammalian tooth. This trait permits a vast increase in the range of foodstuffs which can be fed on. Evolution of this character has thus increased the number of ecological niches available to mammals. The trait arose a number of times in different groups during the Cenozoic, and in each instance was immediately followed by an adaptive radiation. Birds find other ways to provide for each other, i.e. the evolution of flight opened new avenues for evolution to explore, initiating an adaptive radiation.
Other examples include placental gestation (for eutherian mammals), or bipedal locomotion (in hominins).
Opportunity.
Adaptive radiations often occur as a result of an organism arising in an environment with unoccupied niches, such as a newly formed lake or isolated island chain. The colonizing population may diversify rapidly to take advantage of all possible niches.
In Lake Victoria, an isolated lake which formed recently in the African rift valley, over 300 species of cichlid fish adaptively radiated from one parent species in just 15,000 years.
Adaptive radiations commonly follow mass extinctions: following an extinction, many niches are left vacant. A classic example of this is the replacement of the non-avian dinosaurs with mammals at the end of the Cretaceous, and of brachiopods by bivalves at the Permo-Triassic boundary.

</doc>
<doc id="1910" url="http://en.wikipedia.org/wiki?curid=1910" title="Agarose gel electrophoresis">
Agarose gel electrophoresis

Agarose gel electrophoresis is a method of gel electrophoresis used in biochemistry, molecular biology, and clinical chemistry to separate a mixed population of DNA or proteins in a matrix of agarose. The proteins may be separated by charge and or size (IEF agarose, essentially size independent), and the DNA and RNA fragments by length. Biomolecules are separated by applying an electric field to move the negatively charged molecules through an agarose matrix, and the biomolecules are separated by size in the agarose gel matrix.
Agarose gels are easy to cast and is particularly suitable for separating larger DNA of size range most often encountered in laboratories, which accounts for the popularity of its use. The separated DNA may be viewed with stain, most commonly under UV light, and the DNA fragments can be extracted from the gel with relative ease. Most agarose gels used are between 0.7 - 2% dissolved in a suitable electrophoresis buffer.
Properties of agarose gel.
Agarose gel is a three-dimensional matrix formed of helical agarose molecules in supercoiled bundles that are aggregated into three-dimensional structures with channels and pores through which biomolecules can pass. The 3-D structure is held together with hydrogen bonds and can therefore be disrupted by heating back to a liquid state. The melting temperature is different from the gelling temperature, depending on the sources, agarose gel has a gelling temperature of 35-42°C and a melting temperature of 85-95°C. Low-melting and low-gelling agaroses made through chemical modifications are also available.
Agarose gel has large pore size and good gel strength that made it particularly suitable as an anticonvection medium for the electrophoresis of DNA and large protein molecules. The pore size of a 1% gel has been estimated from 100 nm to 200-500 nm, and its gel strength allows gels as dilute as 0.15% to form slab for gel electrophoresis. Agarose gel has lower resolving power than polyacrylamide gel for DNA but has a greater range of separation, and is therefore used for DNA fragments of usually 50-20,000 bp in size. The limit of resolution for standard agarose gel electrophoresis is around 750 kb, but resolution of over 6 Mb is possible with pulsed field gel electrophoresis (PFGE). It can also be used to separate large protein, and it is the preferred matrix for the gel electrophoresis of particles with effective radii larger than 5-10 nm. A 0.9% agarose gel has pores large enough for the entry of bacteriophage T4.
The agarose polymer contains charged groups, in particular pyruvate and sulphate. These negatively-charged groups create a flow of water in the opposite direction to the movement of DNA in a process called electroendosmosis (EEO), and can therefore retard the movement of DNA and cause blurring of bands. Higher concentration gel would have higher electroosmotic flow. Low EEO agarose is therefore generally preferred for use in agarose gel electrophoresis of nucleic acids, but high EEO agarose may be used for other purposes. The lower sulphate content of low EEO agarose, particularly low-melting point (LMP) agarose, is also beneficial in cases where the DNA extracted from gel is to be used for further manipulation as the presence of contaminating sulphate may affect some subsequent procedures, such as ligation and PCR. Zero EEO agaroses however are undesirable for some applications as they may be made by adding positively-charged group and such groups can affect subsequent enzyme reactions.
Migration of nucleic acids in agarose gel.
Factors affect migration of nucleic acid in gel.
A number of factors can affect the migration of nucleic acids: the dimension of the gel pores (gel concentration), size of DNA being electrophoresed, the voltage used, the ionic strength of the buffer, and the concentration intercalating dye such as ethidium bromide if used during electrophoresis.
Smaller molecules travel faster than larger molecules in gel, and double-stranded DNA moves at a rate that is inversely proportional to the log10 of the number of base pairs. This relationship however breaks down with very large DNA fragments, and separation of very large DNA fragments requires the use of pulsed field gel electrophoresis (PFGE).
For standard agarose gel electrophoresis, larger molecules are resolved better using a low concentration gel while smaller molecules separate better at high concentration gel. High concentrations gel however requires longer run times (sometimes days).
The movement of the DNA may be affected by the conformation of the DNA molecule, for example, supercoiled DNA usually moves faster than relaxed DNA because it is tightly coiled and hence more compact. In a normal plasmid DNA preparation, multiple forms of DNA may be present. Gel electrophoresis of the plasmids would normally show the negatively supercoiled form as the main band, while nicked DNA (open circular form) and the relaxed closed circular form appears as minor bands. The rate at which the various forms move however can change using different electrophoresis conditions, and the mobility of larger circular DNA may be more strongly affected than linear DNA by the pore size of the gel.
Ethidium bromide which intercalates into circular DNA can change the charge, length, as well as the superhelicity of the DNA molecule, therefore its presence in gel during electrophoresis can affect its movement. Agarose gel electrophoresis can be used to resolve circular DNA with different supercoiling topology.
DNA damage due to increased cross-linking will also reduce electrophoretic DNA migration in a dose-dependent way.
The rate of migration of the DNA is proportional to the voltage applied, i.e. the higher the voltage, the faster the DNA moves. The resolution of large DNA fragments however is lower at high voltage. The mobility of DNA may also change in an unsteady field - in a field that is periodically reversed, the mobility of DNA of a particular size may drop significantly at a particular cycling frequency. This phenomenon can result in band inversion in field inversion gel electrophoresis (FIGE), whereby larger DNA fragments move faster than smaller ones.
Mechanism of migration and separation.
The negative charge of its phosphate backbone moves the DNA towards the positively-charged anode during electrophoresis. However, the migration of DNA molecules in solution, in the absence of a gel matrix, is independent of molecular weight during electrophoresis. The gel matrix is therefore responsible for the separation of DNA by size during electrophoresis, and a number of models exist to explain the mechanism of separation of biomolecules in gel matrix. A widely accepted one is the Ogston model which treats the polymer matrix as a sieve. A globular protein or a random coil DNA moves through the interconnected pores, and the movement of larger molecules is more likely to be impeded and slowed down by collisions with the gel matrix, and the molecules of different sizes can therefore be separated in this sieving process.
The Ogston model however breaks down for large molecules whereby the pores are significantly smaller than size of the molecule. For DNA molecules of size greater than 1 kb, a reptation model (or its variants) is most commonly used. This model assumes that the DNA can crawl in a "snake-like" fashion (hence "reptation") through the pores as an elongated molecule. At higher electric field strength, this turned into a biased reptation model, whereby the leading end of the molecule become strongly biased in the forward direction and pulls the rest of the molecule along. Real-time fluorescence microscopy of stained molecules, however, showed more subtle dynamics during electrophoresis, with the DNA showing considerable elasticity as it alternately stretching in the direction of the applied field and then contracting into a ball, or becoming hooked into a U-shape when it gets caught on the polymer fibres.
General procedure.
The details of an agarose gel electrophoresis experiment may vary depending on methods, but most follow a general procedure.
Casting of gel.
The gel is prepared by dissolving the agarose powder in an appropriate buffer, such as TAE or TBE, to be used in electrophoresis. The agarose is dispersed in the buffer before heating it to near-boiling point, but avoid boiling. The melted agarose is allowed to cool sufficiently before pouring the solution into a cast as the cast may warp or crack if the agarose solution is too hot. A comb is placed in the cast to create wells for loading sample, and the gel should be completely set before use.
The concentration of gel affects the resolution of DNA separation. For a standard agarose gel electrophoresis, a 0.8% gives good separation or resolution of large 5–10kb DNA fragments, while 2% gel gives good resolution for small 0.2–1kb fragments. 1% gels are common for many applications. The concentration is measured in weight of agarose over volume of buffer used. High percentage gels are often brittle and may not set evenly, while low percentage gels (01.-0.2%) are fragile and not easy to handle. Low-melting-point (LMP) agarose gels are also more fragile than normal agarose gel. PFGE and FIGE are often done with high percentage agarose gels.
Loading of samples.
Once the gel has set, the comb is removed, leaving wells where DNA samples can be loaded. Loading buffer is mixed with the DNA sample before the mixture is loaded into the wells. The loading buffer contains a dense compound, which may be glycerol, sucrose, or Ficoll, that raises the density of the sample so that the DNA sample may sink to the bottom of the well. If the DNA sample contains residual ethanol after its preparation, it may float out of the well. The loading buffer also include colored dyes such as xylene cyanol and bromophenol blue used to monitor the progress of the electrophoresis. The DNA samples are loaded using a pipette.
Electrophoresis.
Agarose gel electrophoresis is most commonly done horizontally in a submarine mode whereby the slab gel is completely submerged in buffer during electrophoresis. It is also possible, but less common, to perform the electrophoresis vertically, as well as horizontally with the gel raised on agarose legs using the appropriate apparatus. The buffer used in the gel is the same as the running buffer in the electrophoresis tank, which is why electrophoresis in the submarine mode is possible with agarose gel.
For optimal resolution of DNA > 2kb in size in standard gel electrophoresis, 5 to 8 V/cm is recommended (the distance in cm refers to the distance between electrodes, therefore this recommended voltage would be 5 to 8 multiply by the distance between the electrodes in cm). Voltage may also be limited by the fact that it heats the gel and may cause the gel to melt if it is run at high voltage for a prolonged period, especially if the gel used is LMP agarose gel. Too high a voltage may also reduce resolution, as well as causing band streaking for large DNA molecules. Too low a voltage may lead to broadening of band for small DNA fragments due to dispersion and diffusion.
Since DNA is not visible in natural light, the progress of the electrophoresis is monitored using colored dyes. Xylene cyanol (light blue color) comigrates large DNA fragments, while Bromophenol blue (dark blue) comigrates with the smaller fragments. Less commonly-used dyes include Cresol Red and Orange G which migrate ahead of bromophenol blue. A DNA marker is also run together for the estimation of the molecular weight of the DNA fragments. Note however that the size of a circular DNA like plasmids cannot be accurately gauged using standard markers unless it has been linearized by restriction digest, alternatively a supercoiled DNA marker may be used.
Staining and visualization.
DNA as well as RNA are normally visualized by staining with ethidium bromide, which intercalates into the major grooves of the DNA and fluoresces under UV light. The ethidium bromide may be added to the agarose solution before it gels, or the DNA gel may be stained later after electrophoresis. Destaining of the gel is not necessary but may produce better images. Other methods of staining are available, examples are SYBR Green, GelRed, methylene blue, brilliant cresyl blue, Nile blue sulphate, and crystal violet. SYBR Green, GelRed and other similar commercial products are sold as safer alternatives to ethidium bromide as it has been shown to be mutagenic in Ames test, although the carcinogenicity of ethidium bromide has not actually been established. SYBR Green requires the use of a blue-light transilluminator. DNA stained with crystal violet can be viewed under natural light without the use of a UV transillumintor which is an advantage, however it may not produce a strong band.
When stained with ethidium bromide, the gel is viewed with an ultraviolet (UV) transilluminator. Standard transilluminators use wavelengths of 302/312-nm (UV-B), however exposure of DNA to UV radiations for as little as 45 seconds can produce damages to DNA and affect subsequent procedures, for example reducing the efficiency of transformation, "in vitro" transcription, and PCR. Exposure of the DNA to UV radiations therefore should be limited. Using higher wavelength of 365 nm (UV-A range) causes less damage to the DNA but also produces much weaker fluorescence with ethidium bromide. Where multiple wavelengths can be selected in the transillumintor, the shorter wavelength would be used to capture images, while the longer wavelength should be used when it is necessary to work on the gel for any extended period of time.
The transilluminator apparatus may also contain image capture devices, such as a digital or polaroid camera, that allow an image of the gel to be taken or printed.
Downstream procedures.
The separated DNA bands are often used for further procedures, and a DNA band may be cut out of the gel as a slice, dissolved and purified. The gels may also be used for blotting techniques.
Buffers.
In general, the ideal buffer should have good conductivity, produce less heat and have a long life. There are a number of buffers used for agarose electrophoresis. The most common being, for nucleic acids Tris/Acetate/EDTA (TAE), Tris/Borate/EDTA (TBE). Many other buffers have been proposed, e.g. lithium borate (LB), which is almost never used, based on Pubmed citations, iso electric histidine, pK matched goods buffers, etc.; in most cases the purported rationale is lower current (less heat) and or matched ion mobilities, which leads to longer buffer life. Tris-phosphate buffer has high buffering capacity but cannot be used if DNA extracted is to be used in phosphate sensitive reaction. Borate is problematic; Borate can polymerize, and/or interact with cis diols such as those found in RNA. TAE has the lowest buffering capacity but provides the best resolution for larger DNA. This means a lower voltage and more time, but a better product. LB is relatively new and is ineffective in resolving fragments larger than 5 kbp; However, with its low conductivity, a much higher voltage could be used (up to 35 V/cm), which means a shorter analysis time for routine electrophoresis. As low as one base pair size difference could be resolved in 3% agarose gel with an extremely low conductivity medium (1 mM Lithium borate). The buffers used contain EDTA to inactivate many nucleases which require divalent cation for their function.
Applications.
Agarose gels are easily cast and handled compared to other matrices and nucleic acids are not chemically altered during electrophoresis. Samples are also easily recovered. After the experiment is finished, the resulting gel can be stored in a plastic bag in a refrigerator.
Electrophoresis is performed in buffer solutions to reduce pH changes due to the electric field, which is important because the charge of DNA and RNA depends on pH, but running for too long can exhaust the buffering capacity of the solution. Further, different preparations of genetic material may not migrate consistently with each other, for morphological or other reasons.

</doc>
<doc id="1911" url="http://en.wikipedia.org/wiki?curid=1911" title="Allele">
Allele

An allele ( or ), or allel, is one of a number of alternative forms of the same gene or same genetic locus. It is the alternative form of a gene for a character producing different effects. Sometimes, different alleles can result in different observable phenotypic traits, such as different pigmentation. However, many genetic variations result in little or no observable variation.
Most multicellular organisms have two sets of chromosomes; that is, they are diploid. These chromosomes are referred to as homologous chromosomes. Diploid organisms have one copy of each gene (and, therefore, one allele) on each chromosome. If both alleles are the same, they and the organism are homozygous and the organisms are homozygotes. If the alleles are different, they and the organism are heterozygous and the organisms are heterozygotes.
The word "allele" is a short form of allelomorph ("other form"), which was used in the early days of genetics to describe variant forms of a gene detected as different phenotypes. It derives from the Greek prefix "ἀλλήλ", "allel", meaning "reciprocal" or "each other", which itself is related to the Greek adjective ἄλλος (allos; cognate with Latin "alius"), meaning "other".
Dominant and recessive alleles.
In many cases, genotypic interactions between the two alleles at a locus can be described as dominant or recessive, according to which of the two homozygous genotypes the phenotype of the heterozygote most resembles. Where the heterozygote is indistinguishable from one of the homozygotes, the allele involved is said to be dominant to the other, which is said to be recessive to the former. The degree and pattern of dominance varies among loci. For a further discussion see Dominance (genetics). This type of interaction was first formally described by Gregor Mendel. However, many traits defy this simple categorization and the phenotypes are modeled by polygenic inheritance.
The term "wild type" allele is sometimes used to describe an allele that is thought to contribute to the typical phenotypic character as seen in "wild" populations of organisms, such as fruit flies ("Drosophila melanogaster"). Such a "wild type" allele was historically regarded as dominant, common, and normal, in contrast to "mutant" alleles regarded as recessive, rare, and frequently deleterious. It was commonly thought that most individuals were homozygous for the "wild type" allele at most gene loci, and that any alternative "mutant" allele was found in homozygous form in a small minority of "affected" individuals, often as genetic diseases, and more frequently in heterozygous form in "carriers" for the mutant allele. It is now appreciated that most or all gene loci are highly polymorphic, with multiple alleles, whose frequencies vary from population to population, and that a great deal of genetic variation is hidden in the form of alleles that do not produce obvious phenotypic differences.
Multiple alleles.
A population or species of organisms typically includes multiple alleles at each locus among various individuals. Allelic variation at a locus is measurable as the number of alleles (polymorphism) present, or the proportion of heterozygotes in the population.
For example, at the gene locus for the ABO blood type carbohydrate antigens in humans, classical genetics recognizes three alleles, IA, IB, and ii, that determine compatibility of blood transfusions. Any individual has one of six possible genotypes (IAIA, IAi, IBIB, IBi, IAIB, and ii) that produce one of four possible phenotypes: "Type A" (produced by IAIA homozygous and IAi heterozygous genotypes), "Type B" (produced by IBIB homozygous and IBi heterozygous genotypes), "Type AB" produced by IAIB heterozygous genotype, and "Type O" produced by ii homozygous genotype. It is now known that each of the A, B, and O alleles is actually a class of multiple alleles with different DNA sequences that produce proteins with identical properties: more than 70 alleles are known at the ABO locus. An individual with "Type A" blood may be an AO heterozygote, an AA homozygote, or an AA heterozygote with two different "A" alleles.
Allele and genotype frequencies.
The frequency of alleles in a diploid population can be used to predict the frequencies of the corresponding genotypes (see Hardy-Weinberg principle). For a simple model, with two alleles:
where "p" is the frequency of one allele and "q" is the frequency of the alternative allele, which necessarily sum to unity. Then, "p"2 is the fraction of the population homozygous for the first allele, 2"pq" is the fraction of heterozygotes, and "q"2 is the fraction homozygous for the alternative allele. If the first allele is dominant to the second then the fraction of the population that will show the dominant phenotype is "p"2 + 2"pq", and the fraction with the recessive phenotype is "q"2.
With three alleles:
In the case of multiple alleles at a diploid locus, the number of possible genotypes (G) with a number of alleles (a) is given by the expression:
Allelic variation in genetic disorders.
A number of genetic disorders are caused when an individual inherits two recessive alleles for a single-gene trait. Recessive genetic disorders include Albinism, Cystic Fibrosis, Galactosemia, Phenylketonuria (PKU), and Tay-Sachs Disease. Other disorders are also due to recessive alleles, but because the gene locus is located on the X chromosome, so that males have only one copy (that is, they are hemizygous), they are more frequent in males than in females. Examples include red-green color blindness and Fragile X syndrome.
Other disorders, such as Huntington disease, occur when an individual inherits only one dominant allele.

</doc>
<doc id="1912" url="http://en.wikipedia.org/wiki?curid=1912" title="Ampicillin">
Ampicillin

Ampicillin is an antibiotic useful for the treatment of a number of bacterial infections. It is a beta-lactam antibiotic that is part of the aminopenicillin family and is roughly equivalent to amoxicillin in terms activity. It is taken by mouth. It is active against many Gram-(+) and Gram-(-) bacteria.
It is effective for ear infections and respiratory infections such as sinusitis caused by bacteria, acute exacerbations of COPD, and epiglottis. It is also sometimes used for the treatment of urinary tract infections, meningitis, and salmonella infections, but resistance to ampicillin is increasingly common among the bacteria responsible for these infections. 
Common side effects include rash, diarrhea, nausea and vomiting. It is not useful for the treatment of viral infections. 
It is on the World Health Organization's List of Essential Medicines, a list of the most important medication needed in a basic health system.
Medical uses.
Ampicillin is active against Gram-(+) bacteria including "Streptococcus pneumoniae", "Streptococcus pyogenes", "Staphylococcus aureus" (but not methicillin-resistant strains), and some "Enterococci". Activity against Gram-(-) bacteria includes "Neisseria meningitidis", some "Haemophilus influenzae", and some Enterobacteriaceae. Its spectrum of activity is enhanced by co-administration of sulbactam, a drug that inhibits beta lactamase, an enzyme produced by bacteria to inactivate ampicillin and related antibiotics.
It is used for the treatment of infections known to be or highly likely to be caused by these bacteria. These include common respiratory infections including sinusitis, bronchitis, and pharyngitis, as well as otitis media. In combination with vancomycin (which provides coverage of ampicillin-resistant pneumococci), it is effective for the treatment of bacterial meningitis. It is also used for gastrointestinal infections caused by consuming contaminated water or food, such as "Salmonella", "Shigella", and "Listeriosis".
Ampicillin is a first-line agent for the treatment of infections caused by "Enterococci". The bacteria are an important cause of healthcare-associated infections such as endocarditis, meningitis, and catheter-associated urinary tract infections that are typically resistant to other antibiotics.
Adverse Effects.
Ampicillin is relatively non-toxic. Its most common side effects include rash, diarrhea, nausea and vomiting. In very rare cases it causes severe side effects such as anaphylaxis and "Clostridium difficile" diarrhea.
Mechanism of action.
Belonging to the penicillin group of beta-lactam antibiotics, ampicillin is able to penetrate Gram-positive and some Gram-negative bacteria. It differs from penicillin G, or benzylpenicillin, only by the presence of an amino group. That amino group helps the drug penetrate the outer membrane of Gram-negative bacteria.
Ampicillin acts as an irreversible inhibitor of the enzyme transpeptidase, which is needed by bacteria to make their cell walls. It inhibits the third and final stage of bacterial cell wall synthesis in binary fission, which ultimately leads to cell lysis. Ampicillin is bacteriocidal.
History.
Ampicillin has been used extensively to treat bacterial infections since 1961. Until the introduction of ampicillin by the British company Beecham, penicillin therapies had only been effective against Gram-positive organisms such as staphylococci and streptococci. Ampicillin (originally branded as 'Penbritin') also demonstrated activity against Gram-negative organisms such as "H. influenzae", coliforms and "Proteus" spp. 

</doc>
<doc id="1913" url="http://en.wikipedia.org/wiki?curid=1913" title="Annealing">
Annealing

Annealing may refer to:

</doc>
<doc id="1914" url="http://en.wikipedia.org/wiki?curid=1914" title="Antibiotic resistance">
Antibiotic resistance

Antibiotic resistance is a form of drug resistance whereby some (or, less commonly, all) sub-populations of a microorganism, usually a bacterial species, are able to survive after exposure to one or more antibiotics; pathogens resistant to multiple antibiotics are considered "multidrug resistant" (MDR) or, more colloquially, superbugs.
Antibiotic resistance is a serious and growing phenomenon in contemporary medicine and has emerged as one of the pre-eminent public health concerns of the 21st century, in particular as it pertains to pathogenic organisms (the term is especially relevant to organisms that cause disease in humans). A World Health Organization report released April 30, 2014 states, "this serious threat is no longer a prediction for the future, it is happening right now in every region of the world and has the potential to affect anyone, of any age, in any country. Antibiotic resistance–when bacteria change so antibiotics no longer work in people who need them to treat infections–is now a major threat to public health."
In the simplest cases, drug-resistant organisms may have acquired resistance to first-line antibiotics, thereby necessitating the use of second-line agents. Typically, a first-line agent is selected on the basis of several factors including safety, availability, and cost; a second-line agent is usually broader in spectrum, has a less favourable risk-benefit profile, and is more expensive or, in dire circumstances, may be locally unavailable. In the case of some MDR pathogens, resistance to second- and even third-line antibiotics is, thus, sequentially acquired, a case quintessentially illustrated by "Staphylococcus aureus" in some nosocomial settings. Some pathogens, such as "Pseudomonas aeruginosa", also possess a high level of intrinsic resistance.
It may take the form of a spontaneous or induced genetic mutation, or the acquisition of resistance genes from other bacterial species by horizontal gene transfer via conjugation, transduction, or transformation. Many antibiotic resistance genes reside on transmissible plasmids, facilitating their transfer. Exposure to an antibiotic naturally selects for the survival of the organisms with the genes for resistance. In this way, a gene for antibiotic resistance may readily spread through an ecosystem of bacteria. Antibiotic-resistance plasmids frequently contain genes conferring resistance to several different antibiotics. This is not the case for "Mycobacterium tuberculosis", the bacteria that causes Tuberculosis, since evidence is lacking for whether these bacteria have plasmids. Also "M. tuberculosis" lack the opportunity to interact with other bacteria in order to share plasmids.
Genes for resistance to antibiotics, like the antibiotics themselves, are ancient. However, the increasing prevalence of antibiotic-resistant bacterial infections seen in clinical practice stems from antibiotic use both within human medicine and veterinary medicine. Any use of antibiotics can increase selective pressure in a population of bacteria to allow the resistant bacteria to thrive and the susceptible bacteria to die off. As resistance towards antibiotics becomes more common, a greater need for alternative treatments arises. However, despite a push for new antibiotic therapies, there has been a continued decline in the number of newly approved drugs. Antibiotic resistance therefore poses a significant problem.
The growing prevalence and incidence of infections due to MDR pathogens is epitomised by the increasing number of familiar acronyms used to describe the causative agent and sometimes the infection; of these, MRSA is probably the most well-known, but others including VISA (vancomycin-intermediate "S. aureus"), VRSA (vancomycin-resistant "S. aureus"), ESBL (Extended spectrum beta-lactamase), VRE (Vancomycin-resistant "Enterococcus") and MRAB (Multidrug-resistant "A. baumannii") are prominent examples. Nosocomial infections overwhelmingly dominate cases where MDR pathogens are implicated, but multidrug-resistant infections are also becoming increasingly common in the community.
Although there were low levels of preexisting antibiotic-resistant bacteria before the widespread use of antibiotics, evolutionary pressure from their use has played a role in the development of multidrug-resistant varieties and the spread of resistance between bacterial species. In medicine, the major problem of the emergence of resistant bacteria is due to misuse and overuse of antibiotics. In some countries, antibiotics are sold over the counter without a prescription, which also leads to the creation of resistant strains. Other practices contributing to resistance include antibiotic use in livestock feed to promote faster growth. Household use of antibacterials in soaps and other products, although not clearly contributing to resistance, is also discouraged (as not being effective at infection control). Unsound practices in the pharmaceutical manufacturing industry can also contribute towards the likelihood of creating antibiotic-resistant strains. The procedures and clinical practice during the period of drug treatment are frequently flawed — usually no steps are taken to isolate the patient to prevent re-infection or infection by a new pathogen, negating the goal of complete destruction by the end of the course (see Healthcare-associated infections and Infection control).
Certain antibiotic classes are highly associated with colonisation with "superbugs" compared to other antibiotic classes. A superbug, also called multiresistant, is a bacterium that carries several resistance genes. The risk for colonisation increases if there is a lack of susceptibility (resistance) of the superbugs to the antibiotic used and high tissue penetration, as well as broad-spectrum activity against "good bacteria". In the case of MRSA, increased rates of MRSA infections are seen with glycopeptides, cephalosporins, and especially quinolones. In the case of colonisation with "Clostridium difficile", the high-risk antibiotics include cephalosporins and in particular quinolones and clindamycin.
Of antibiotics used in the United States in 1997, half were used in humans and half in animals; in 2013, 80% were used in animals.
Natural occurrence.
There is evidence that naturally occurring antibiotic resistance is common. The genes that confer this resistance are known as the environmental resistome. These genes may be transferred from non-disease-causing bacteria to those that do cause disease, leading to clinically significant antibiotic resistance.
In 1952, an experiment conducted by Joshua and Esther Lederberg showed that penicillin-resistant bacteria existed before penicillin treatment. While experimenting at the University of Wisconsin-Madison, Joshua Lederberg and his graduate student Norton Zinder also demonstrated preexistent bacterial resistance to streptomycin. In 1962, the presence of penicillinase was detected in dormant "Bacillus licheniformis" endospores, revived from dried soil on the roots of plants, preserved since 1689 in the British Museum. Six strains of "Clostridium", found in the bowels of William Braine and John Hartnell (members of the Franklin Expedition) showed resistance to cefoxitin and clindamycin. It was suggested that penicillinase may have emerged as a defense mechanism for bacteria in their habitats, such as the case of penicillinase-rich "Staphylococcus aureus", living with penicillin-producing "Trichophyton", however this was deemed circumstantial. Search for a penicillinase ancestor has focused on the class of proteins that must be "a priori" capable of specific combination with penicillin. The resistance to cefoxitin and clindamycin in turn was attributed to Braine's and Hartnell's contact with microorganisms that naturally produce them or random mutation in the chromosomes of "Clostridium" strains. Nonetheless there is evidence that heavy metals and some pollutants may select for antibiotic-resistant bacteria, generating a constant source of them in small numbers.
In medicine.
In 1945, in his Nobel lecture "Penicillin," Alexander Fleming warned against the use of sub‐therapeutic doses of antibiotics – "bought by anyone in the shops" without a prescription:
Inappropriate prescribing of antibiotics has been attributed to a number of causes, including people insisting on antibiotics, physicians prescribing them as they feel they do not have time to explain why they are not necessary, and physicians not knowing when to prescribe antibiotics or being overly cautious for medical and/or legal reasons. For example, a third of people believe that antibiotics are effective for the common cold, and the common cold is the most common reason antibiotics are prescribed even though antibiotics are completely useless against viruses.
The number of persons inappropriately prescribed antibiotics is a greater factor in the increasing rates of bacterial resistance than non-compliance with antibiotic protocol among those prescribed the drugs, although the non-compliance rate is also high. A single regimen of antibiotics even in compliant patients leads to a greater risk of resistant organisms to that antibiotic in the person for a month to possibly a year.
Antibiotic resistance has been shown to increase with duration of treatment; therefore, as long as a clinically effective lower limit is observed (that depends upon the organism and antibiotic in question), the use by the medical community of shorter courses of antibiotics is likely to decrease rates of resistance, reduce cost, and have better outcomes due to fewer complications such as C. difficile infection and diarrhea. In some situations a short course is inferior to a long course. One pediatric study found that with one antibiotic a short course was more effective, but with a different antibiotic, a longer course was more effective.
A WHO report incorporating data from 114 countries recommends people help tackle resistance by using antibiotics only when prescribed by a doctor and completing the full prescription, even if they feel better. Some infections require treatment long after symptoms are gone, and in all cases, an insufficient course of antibiotics may lead to relapse (with an infection that is now more antibiotic resistant). Some researchers have found however that antibiotics can often be safely stopped 72 hours after symptoms resolve. Because patients may feel better before the infection is eradicated, doctors must provide instructions to patients so they know when it is safe to stop taking a prescription. Some researchers advocate doctors' using a very short course of antibiotics, reevaluating the patient after a few days, and stopping treatment if there are no longer clinical signs of infection.
A large number of people do not finish a course of antibiotics primarily because they feel better (varying from 10% to 44%, depending on the country). Compliance with once-daily antibiotics is better than with twice-daily antibiotics. Patients taking less than the required dosage or failing to take their doses within the prescribed timing results in decreased concentration of antibiotics in the bloodstream and tissues, and, in turn, exposure of bacteria to suboptimal antibiotic concentrations increases the frequency of antibiotic resistant organisms, however factors within the intensive care unit setting such as mechanical ventilation and multiple underlying diseases also appeared to contribute to bacterial resistance. These nosocomial pneumonia patients represented a situation where there was relatively little contribution of host defense to outcome, and therefore may not be applicable to otherwise healthy individuals taking antibiotics.
Antibiotic-tolerant states may depend on physiological adaptations without direct connections to antibiotic target activity or to drug uptake, efflux, or inactivation.
Poor hand hygiene by hospital staff has been associated with the spread of resistant organisms, and an increase in hand washing compliance results in decreased rates of these organisms.
The improper use of antibiotics and therapeutic treatments can often be attributed to the presence of structural violence in particular regions. Socioeconomic factors such as race and poverty affect the accessibility of and adherence to drug therapy. The efficacy of treatment programs for these drug-resistant strains depends on whether or not programmatic improvements take into account the effects of structural violence.
Role of other animals.
The emergence of antibiotic-resistant microorganisms in human medicine is primarily the result of the use of antibiotics in humans, although the use of antibiotics in animals is also partly responsible.
Antibiotic drugs are used non-therapeutically in animals that are intended for human consumption, such as cattle, pigs, chickens, fish, etc. Since the last third of the 20th century, there has been extensive use of antibiotics in animal husbandry. Some of these drugs are not considered significant for use in humans, because of their lack of either efficacy or purpose in humans (such as the use of ionopores in ruminants), or because that drug has mostly gone out of use in humans (such as sulfa drugs due to widespread allergic reactions and antibiotic resistance among human pathogens). Others however are used in both animals and humans, including penicillin and some forms of tetracycline. Historically, regulation of antibiotic use in food animals has been limited to limiting drug residues in meat, egg, and milk products, rather than by direct concern over the development of antibiotic resistance. This mirrors the primary concerns in human medicine, where, in general, researchers and doctors were more concerned about effective but non-toxic doses of drugs rather than antibiotic resistance.
The resistant bacteria which antibiotic exposure selects in animals can be transmitted to humans via three pathways, those being through the consumption of animal products (milk, meat, eggs, etc.), from close or direct contact with animals or other humans, or through the environment. In the first pathway, food preservation methods can help eliminate, decrease, or prevent the growth of bacteria in some food classes. Evidence for the transfer of antibiotic- resistant microorganisms from animals to humans has been scant, and most evidence shows that pathogens of concern in human populations originated in humans and are maintained there, with rare cases of transference to humans. The use of antibiotics in animals can be classified into different use patterns according to its purpose. The most accepted classification discriminates therapeutic, prophylactic, metaphylactic, and growth promotion uses of antibiotics. All four patterns select for bacterial resistance, since antibiotic resistance is a natural evolutionary process, but the non-therapeutic uses expose larger number of animals, and therefore of bacteria, for more extended periods, and at lower doses. They therefore greatly increase the cross-section for the evolution of resistance.
The World Health Organization concluded that inappropriate use of antibiotics in animal husbandry is an underlying contributor to the emergence and spread of antibiotic-resistant germs, and that the use of antibiotics as growth promoters in animal feeds should be prohibited, in the absence of risk assessments. Regarding this matter, the OIE has added to the Terrestrial Animal Health Code a series of guidelines with recommendations to its members for the creation and harmonization of national antimicrobial resistance surveillance and monitoring programs, monitoring of the quantities of antibiotics used in animal husbandry, and recommendations to ensure the proper and prudent use of antibiotic substances. Another guideline introduced in the terrestrial Animal Health Code is the implementation of methodologies that help to establish risk factors associated with this worldwide concern. The OIE concluded that risk assessments should be performed in order to assess, manage and define the possible health risks of antibiotic resistance in human and animal populations.
In the world, antibiotics are widely used on animals. As in human medicine, antibiotics can often be bought without prescription and veterinary supervision for use on pets and livestock. Bacteria remaining in these animals are likely to be resistant to the antibiotics used, and may be passed into the environment by the excretion and secretion of materials such as milk, feces, urine, saliva, semen, lochia, etc. The actual impact of these resistant germs depends on their specific type and on the animal or organism they henceforth infect. Some germs, such as tetanus, are toxic regardless of their antibiotic-resistant status. (It is useful to remember that antibiotics are not used in treatment of all diseases caused by bacteria. Tetanus, as an example, is prevented by vaccine and is extremely difficult to treat once symptoms appear.)
In 1998, European Union health ministers voted to ban four antibiotics widely used to promote animal growth (despite their scientific panel's recommendations). Regulation banning the use of antibiotics in European feed, with the exception of two antibiotics in poultry feeds, became effective in 2006. In Scandinavia, there is evidence that the ban has led to a lower prevalence of antimicrobial resistance in (nonhazardous) animal bacterial populations. However, a corresponding change in antibiotic-resistance cases among humans has not yet been demonstrated.
In the United States.
In the United States, the United States Department of Agriculture (USDA) and the Food and Drug Administration (FDA) collect data on antibiotic use in animals and humans. In research studies, occasional animal-to-human spread of drug-resistant organisms has been demonstrated. Antibiotics and other drugs are used in U.S. animal feed to promote animal productivity. In particular, poultry feed and water is a common route of administration of drugs, due to higher overall costs when drugs are administered by handling animals individually. In general, animals that appear ill are not permitted to be slaughtered for human consumption in the United States.
Growing U.S. consumer concern about using antibiotics in animal feed has led to a niche market of "antibiotic-free" animal products, but this small market is unlikely to change entrenched, industry-wide practices.
In 2001, the Union of Concerned Scientists estimated that greater than 70% of the antibiotics used in the U.S. are given to food animals (for example, chickens, pigs, and cattle), in the absence of disease. The amounts given are termed "sub-therapeutic", i.e., insufficient to combat disease. Despite no diagnosis of disease, the administration of these drugs (most of which are not significant to human medicine) results in decreased mortality and morbidity and increased growth in the animals so treated. It is theorized that sub-therapeutic dosages kills some, but not all, of the bacterial organisms in the animal — likely leaving those that are naturally antibiotic-resistant. Studies have shown, however, that, in essence, the overall population levels of bacteria are unchanged; only the mix of bacteria is affected.
The actual mechanism by which sub-therapeutic antibiotic feed additives serve as growth promoters is thus unclear. Some people have speculated that animals and fowl may have sub-clinical infections, which would be cured by low levels of antibiotics in feed, thereby allowing the creatures to thrive. No convincing evidence has been advanced for this theory, and the bacterial load in an animal is essentially unchanged by use of antibiotic feed additives. The mechanism of growth promotion is therefore probably something other than "killing off the bad bugs."
In 2000, the FDA announced their intention to revoke approval of fluoroquinolone use in poultry production because of substantial evidence linking it to the emergence of fluoroquinolone-resistant "Campylobacter" infections in humans. Legal challenges from the food animal and pharmaceutical industries delayed the final decision to do so until 2006. Fluroquinolones have been banned from extra-label use in food animals in the USA since 2007. However, they remain widely used in companion and exotic animals.
In 2001, "National Hog Farmer" magazine warned U.S. producers that "C. difficile" "is sweeping the industry, killing many piglets" (Neutkens D; "New Clostridium Claiming Baby Pigs"). In 2006, a study by the USDA's National Animal Health Monitoring System further investigated the prevalence of "C. difficile" in hog farms. The study, which covered hog farms of a size typical of those producing 94% of US swine, found the prevalence of "C. difficile" "relatively low (11.4%)" and that there was no difference in region or in size of farm. Human infection with "C. difficile" (either drug-resistant or not) is most commonly associated with the use of strong antibiotics in hospitalized humans, and is not associated with humans in contact with farm animals.
During 2007, two federal bills (S. 549 and H.R. 962) aimed at phasing out "nontherapeutic" antibiotics in U.S. food animal production. The Senate bill, introduced by Sen. Edward "Ted" Kennedy, died. The House bill, introduced by Rep. Louise Slaughter, died after being referred to Committee.
In the United States, the FDA first determined in 1977 that there is evidence of emergence of antibiotic-resistant bacterial strains in livestock. The long-established practice of permitting OTC sales of antibiotics (including penicillin and other drugs) to lay animal owners for administration to their own animals nonetheless continued in all states. In March 2012, the United States District Court for the Southern District of New York, ruling in an action brought by the Natural Resources Defense Council and others, ordered the FDA to revoke approvals for the use of antibiotics in livestock that violated FDA regulations. On April 11, 2012 the FDA announced a voluntary program to phase out unsupervised use of drugs as feed additives and convert approved over-the-counter uses for antibiotics to prescription use only, requiring veterinarian supervision of their use and a prescription. In December 2013, the FDA announced the commencement of these steps to phase out the use of antibiotics for the purposes of promoting livestock growth.
Environmental impact.
Antibiotics have been polluting the environment since their introduction through human waste (medication, farming), animals, and the pharmaceutical industry. Along with antibiotic waste, resistant bacteria follow, thus introducing antibiotic-resistant bacteria into the environment. As bacteria replicate quickly, the resistant bacteria that enter the environment replicate their resistance genes as they continue to divide. In addition, bacteria carrying resistance genes have the ability to spread those genes to other species via horizontal gene transfer. Therefore, even if the specific antibiotic is no longer introduced into the environment, antibiotic-resistance genes will persist through the bacteria that have since replicated without continuous exposure.
A study the Poudre River (Colorado, United States) implicated wastewater treatment plants, as well as animal-feeding operations in the dispersal of antibiotic-resistance genes into the environment. This research was done using molecular signatures in order to determine the sources, and the location at the Poudre River was chosen due to lack of other anthropogenic influences upstream. The study indicates that monitoring of antibiotic-resistance genes may be useful in determining not only the point of origin of their release but also how these genes persist in the environment. In addition, studying physical and chemical methods of treatment may alleviate pressure of antibiotic-resistance genes in the environment, and thus their entry back into human contact.
Mechanisms of antibiotic resistance.
Antibiotic resistance can be a result of horizontal gene transfer, and also of unlinked point mutations in the pathogen genome at a rate of about 1 in 108 per chromosomal replication. The antibiotic action against the pathogen can be seen as an environmental pressure. Those bacteria with a mutation that allows them to survive live to reproduce. They then pass this trait to their offspring, which leads to the evolution of a fully resistant colony.
The four main mechanisms by which microorganisms exhibit resistance to antimicrobials are:
There are three known mechanisms of fluoroquinolone resistance. Some types of efflux pumps can act to decrease intracellular quinolone concentration. In Gram-negative bacteria, plasmid-mediated resistance genes produce proteins that can bind to DNA gyrase, protecting it from the action of quinolones. Finally, mutations at key sites in DNA gyrase or topoisomerase IV can decrease their binding affinity to quinolones, decreasing the drug's effectiveness. Research has shown the bacterial protein LexA may play a key role in the acquisition of bacterial mutations giving resistance to quinolones and rifampicin.
Antibiotic resistance can also be introduced artificially into a microorganism through laboratory protocols, sometimes used as a selectable marker to examine the mechanisms of gene transfer or to identify individuals that absorbed a piece of DNA that included the resistance gene and another gene of interest. A recent study demonstrated that the extent of horizontal gene transfer among "Staphylococcus" is much greater than previously expected—and encompasses genes with functions beyond antibiotic resistance and virulence, and beyond genes residing within the mobile genetic elements.
For a long time, it has been thought that, for a microorganism to become resistant to an antibiotic, it must be in a large population. However, recent findings show that there is no necessity of large populations of bacteria for the appearance of antibiotic resistance. We know now that small populations of E.coli in an antibiotic gradient can become resistant. Any heterogeneous environment with respect to nutrient and antibiotic gradients may facilitate the development of antibiotic resistance in small bacterial populations and this is also true for the human body. Researchers hypothesize that the mechanism of resistance development is based on four SNP mutations in the genome of E.coli produced by the gradient of antibiotic. These mutations confer the bacteria emergence of antibiotic resistance.
A common misconception is that a person can become resistant to certain antibiotics. It is a strain of microorganism that can become resistant, not a person's body.
Resistant pathogens.
"Staphylococcus aureus".
"Staphylococcus aureus" (colloquially known as "Staph aureus" or a "Staph infection") is one of the major resistant pathogens. Found on the mucous membranes and the human skin of around a third of the population, it is extremely adaptable to antibiotic pressure. It was one of the earlier bacteria in which penicillin resistance was found—in 1947, just four years after the drug started being mass-produced. Methicillin was then the antibiotic of choice, but has since been replaced by oxacillin due to significant kidney toxicity. Methicillin-resistant "Staphylococcus aureus" (MRSA) was first detected in Britain in 1961, and is now "quite common" in hospitals. MRSA was responsible for 37% of fatal cases of sepsis in the UK in 1999, up from 4% in 1991. Half of all "S. aureus" infections in the US are resistant to penicillin, methicillin, tetracycline and erythromycin.
This left vancomycin as the only effective agent available at the time. However, strains with intermediate (4-8 μg/ml) levels of resistance, termed glycopeptide-intermediate "Staphylococcus aureus" (GISA) or vancomycin-intermediate "Staphylococcus aureus" (VISA), began appearing in the late 1990s. The first identified case was in Japan in 1996, and strains have since been found in hospitals in England, France and the US. The first documented strain with complete (>16 μg/ml) resistance to vancomycin, termed vancomycin-resistant "Staphylococcus aureus" (VRSA) appeared in the United States in 2002. However, in 2011, a variant of vancomycin has been tested that binds to the lactate variation and also binds well to the original target, thus reinstating potent antimicrobial activity.
A new class of antibiotics, oxazolidinones, became available in the 1990s, and the first commercially available oxazolidinone, linezolid, is comparable to vancomycin in effectiveness against MRSA. Linezolid-resistance in "S. aureus" was reported in 2001.
Community-acquired MRSA (CA-MRSA) has now emerged as an epidemic that is responsible for rapidly progressive, fatal diseases, including necrotizing pneumonia, severe sepsis, and necrotizing fasciitis. MRSA is the most frequently identified antimicrobial drug-resistant pathogen in US hospitals. The epidemiology of infections caused by MRSA is rapidly changing. In the past 10 years, infections caused by this organism have emerged in the community. The two MRSA clones in the United States most closely associated with community outbreaks, USA400 (MW2 strain, ST1 lineage) and USA300, often contain Panton-Valentine leukocidin (PVL) genes and, more frequently, have been associated with skin and soft tissue infections. Outbreaks of CA-MRSA infections have been reported in correctional facilities, among athletic teams, among military recruits, in newborn nurseries, and among men that have sex with men. CA-MRSA infections now appear endemic in many urban regions and cause most CA-"S. aureus" infections.
"Streptococcus" and "Enterococcus".
"Streptococcus pyogenes" (Group A "Streptococcus": GAS) infections can usually be treated with many different antibiotics. Early treatment may reduce the risk of death from invasive group A streptococcal disease. However, even the best medical care does not prevent death in every case. For those with very severe illness, supportive care in an intensive-care unit may be needed. For persons with necrotizing fasciitis, surgery often is needed to remove damaged tissue. Strains of "S. pyogenes" resistant to macrolide antibiotics have emerged; however, all strains remain uniformly susceptible to penicillin.
Resistance of "Streptococcus pneumoniae" to penicillin and other beta-lactams is increasing worldwide. The major mechanism of resistance involves the introduction of mutations in genes encoding penicillin-binding proteins. Selective pressure is thought to play an important role, and use of beta-lactam antibiotics has been implicated as a risk factor for infection and colonization. "S. pneumoniae" is responsible for pneumonia, bacteremia, otitis media, meningitis, sinusitis, peritonitis and arthritis.
Multidrug-resistant "Enterococcus faecalis" and "Enterococcus faecium" are associated with nosocomial infections. Among these strains, penicillin-resistant "Enterococcus" was seen in 1983, vancomycin-resistant "Enterococcus" in 1987, and linezolid-resistant "Enterococcus" in the late 1990s.
"Pseudomonas aeruginosa".
"Pseudomonas aeruginosa" is a highly prevalent opportunistic pathogen. One of the most worrisome characteristics of "P. aeruginosa" is its low antibiotic susceptibility, which is attributable to a concerted action of multidrug efflux pumps with chromosomally encoded antibiotic resistance genes (for example, "mexAB-oprM", "mexXY", etc.) and the low permeability of the bacterial cellular envelopes. "Pseudomonas aeruginosa" has the ability to produce 4-hydroxy-2-alkylquinolines (HAQs) and it has been found that HAQs have prooxidant effects, and overexpressing modestly increased susceptibility to antibiotics.
The study experimented with the "Pseudomonas aeruginosa" biofilms and found that a disruption of relA and spoT genes produced an inactivation of the Stringent response (SR) in cells with nutrient limitation, which provides cells be more susceptible to antibiotics.
"Clostridium difficile".
"Clostridium difficile" is a nosocomial pathogen that causes diarrheal disease in hospitals world wide. Clindamycin-resistant "C. difficile" was reported as the causative agent of large outbreaks of diarrheal disease in hospitals in New York, Arizona, Florida and Massachusetts between 1989 and 1992. Geographically dispersed outbreaks of "C. difficile" strains resistant to fluoroquinolone antibiotics, such as ciprofloxacin and levofloxacin, were also reported in North America in 2005.
"Salmonella" and "E. coli".
Infection with "Escherichia coli" and "Salmonella" can result from the consumption of contaminated food and water. When both bacteria are spread, serious health conditions arise. Many people are hospitalized each year after becoming infected, with some dying as a result. Since 1993, some strains of "E. coli" have become resistant to multiple types of fluoroquinolone antibiotics.
"Acinetobacter baumannii".
On November 5, 2004, the Centers for Disease Control and Prevention (CDC) reported an increasing number of "Acinetobacter baumannii" bloodstream infections in patients at military medical facilities in which service members injured in the Iraq/Kuwait region during Operation Iraqi Freedom and in Afghanistan during Operation Enduring Freedom were treated. Most of these showed multidrug resistance (MRAB), with a few isolates resistant to all drugs tested.
"Klebsiella pneumoniae".
Klebsiella pneumoniae carbapenemase (KPC)-producing bacteria are a group of emerging highly drug-resistant Gram-negative bacilli causing infections associated with significant morbidity and mortality whose incidence is rapidly increasing in a variety of clinical settings around the world. "Klebsiella pneumoniae" includes numerous mechanisms for antibiotic resistance, many of which are located on highly mobile genetic elements. Carbapenem antibiotics (heretofore often the treatment of last resort for resistant infections) are generally not effective against KPC-producing organisms.
"Mycobacterium tuberculosis".
Tuberculosis is increasing across the globe, especially in developing countries, over the past few years. TB resistant to antibiotics is called MDR TB (Multidrug Resistant TB). Globally, MDR TB causes 150,000 deaths annually. The rise of the HIV/AIDS epidemic has contributed to this.
TB was considered one of the most prevalent diseases, and did not have a cure until the discovery of Streptomycin by Selman Waksman in 1943. However, the bacteria soon developed resistance. Since then, drugs such as isoniazid and rifampin have been used. M. tuberculosis develops resistance to drugs by spontaneous mutations in its genomes. Resistance to one drug is common, and this is why treatment is usually done with more than one drug. Extensively Drug-Resistant TB (XDR TB) is TB that is also resistant to the second line of drugs.
Resistance of "Mycobacterium tuberculosis" to isoniazid, rifampin, and other common treatments has become an increasingly relevant clinical challenge. (For more on Drug-Resistant TB, visit the Multi-drug-resistant tuberculosis page.)
"Neisseria gonorrhoeae".
Neisseria gonorrhoeae is a sexually transmitted pathogen that can cause pelvic pain, pain on urination, penile, and vaginal discharge, as well as systemic symptoms in human infection. The bacteria was first identified in 1879, although some Biblical scholars believe that references to the disease can be found as early as Parshat Metzora of the Old Testament.
Treatment with penicillin in the 1940s proved helpful, but by the 1970s resistant strains predominated. Resistance to penicillin has developed through two mechanisms: chomasomally mediated resistance (CMRNG) and penicillinase-mediated resistance (PPNG). CMRNG involves stepwise mutation of penA, which codes for the penicilin-binding protein (PBP-2); mtr, which encodes an efflux pump to remove penicilin from the cell; and penB, which encodes the bacterial cell wall porins. PPNG involves the acquisition of a plasmid-borne beta-lactamase.
Fluoroquinolones were a useful next-line treatment until resistance was achieved through efflux pumps and mutations to the gyrA gene, which encodes DNA gyrase. Third-generation cephalosporins have been used to treat gonorrhoea since 2007, although resistant strains have emerged. Strains of Neisseria gonorrhoea have also been found to be resistant to tetracyclines and aminoglycosides. Neisseria gonorrheoea has a high affinity for horizontal gene transfer, and as a result, the existence of any strain resistant to a given drug could spread easily across strains.
Today, injectable ceftriaxone is used, sometimes in combination with azithromycin or doxycycline.
Ways to fight antibiotic resistance.
World Health Organization recommendations.
An April 30, 2014, report by the WHO makes the following recommendations for how to tackle antibiotic resistance:
Preventing infections.
Rational use of antibiotics may reduce the chances of development of opportunistic infection by antibiotic-resistant bacteria due to dysbacteriosis. Our immune systems will cure minor bacterial infections on their own. If we give it the chance without relying on antibiotics to cure a small infection, it will be less likely to become immune or resistant to the antibiotic. It is also important to note that antibiotics will not cure viral infections such as colds and the flu, and taking an antibiotic unnecessarily to treat a viral infection can lead to the resistance of antibiotics In one study, the use of fluoroquinolones is clearly associated with "Clostridium difficile" infection, which is a leading cause of nosocomial diarrhea in the United States, and a major cause of death, worldwide.
Vaccines do not have the problem of resistance because a vaccine enhances the body's natural defenses, whereas an antibiotic operates separately from the body's normal defenses. Nevertheless, new strains that escape immunity induced by vaccines may evolve; for example, an updated influenza vaccine is needed each year.
While theoretically promising, antistaphylococcal vaccines have shown limited efficacy, because of immunological variation between "Staphylococcus" species, and the limited duration of effectiveness of the antibodies produced. Development and testing of more effective vaccines is underway.
The Australian Commonwealth Scientific and Industrial Research Organisation (CSIRO), realizing the need for the reduction of antibiotic use, has been working on two alternatives. One alternative is to prevent diseases by adding cytokines instead of antibiotics to animal feed. These proteins are made in the animal body "naturally" after a disease and are not antibiotics, so they do not contribute to the problem of antibiotic resistance. Furthermore, studies on using cytokines have shown they also enhance the growth of animals like the antibiotics now used, but without the drawbacks of nontherapeutic antibiotic use. Cytokines have the potential to achieve the animal growth rates traditionally sought by the use of antibiotics without the contribution of antibiotic resistance associated with the widespread nontherapeutic uses of antibiotics currently used in the food animal production industries. In addition, CSIRO is working on vaccines for diseases.
Phage therapy.
Phage therapy, an approach that has been extensively researched and used as a therapeutic agent for over 60 years, especially in the Soviet Union, represents a potentially significant but currently underdeveloped approach to the treatment of bacterial disease. Phage therapy was widely used in the United States until the discovery of antibiotics, in the early 1940s. Bacteriophages or "phages" are viruses that invade bacterial cells and, in the case of lytic phages, disrupt bacterial metabolism and cause the bacterium to lyse. Phage therapy is the therapeutic use of lytic bacteriophages to treat pathogenic bacterial infections.
Bacteriophage therapy is a potentially important alternative to antibiotics in the current era of multidrug-resistant pathogens. A review of studies that dealt with the therapeutic use of phages from 1966 to 1996 and few latest ongoing phage therapy projects via internet showed: Phages were used topically, orally or systemically in Polish and Soviet studies. The success rate found in these studies was 80–95%, with few gastrointestinal or allergic side-effects. British studies also demonstrated significant efficacy of phages against "Escherichia coli", "Acinetobacter" spp., "Pseudomonas" spp., and "Staphylococcus aureus". US studies dealt with improving the bioavailability of phage. Phage therapy may prove as an important alternative to antibiotics for treating multidrug-resistant pathogens.
Discovery of the structure of the viral protein PlyC is allowing researchers to understand the way it kills a significant range of pathogenic bacteria.
Research.
Development pipeline.
Since the discovery of antibiotics, research and development (R&D) efforts have provided new drugs in time to treat bacteria that became resistant to older antibiotics, but in the 2000s there has been concern that development has slowed enough that seriously ill patients may run out of treatment options. Another concern is that doctors may become reluctant to perform routine surgeries due to the increased risk of harmful infection. Backup treatments can have serious side-effects; for example, treatment of multi-drug-resistant tuberculosis can cause deafness and insanity. The potential crisis at hand is the result of a marked decrease in industry R&D. Poor financial investment in antibiotic research has exacerbated the situation. In 2011, Pfizer, one of the last major pharmaceutical companies developing new antibiotics, shut down its primary research effort, citing poor shareholder returns relative to drugs for chronic illnesses.
In the United States, drug companies and the administration of President Barack Obama have been proposing changing the standards by which the FDA approves antibiotics targeted at resistant organisms. On 12 December 2013, the Antibiotic Development to Advance Patient Treatment (ADAPT) Act of 2013 was introduced in the U.S. Congress. The ADAPT Act aims to fast-track the drug development in order to combat the growing public health threat of 'superbugs'. Under this Act, the FDA can approve antibiotics and antifungals needed for life-threatening infections based on data from smaller clinical trials. The CDC will reinforce the monitoring of the use of antibiotics that treat serious and life-threatening infections and the emerging resistance, and make the data publicly available. The FDA antibiotics labeling process, 'Susceptibility Test Interpretive Criteria for Microbial Organisms' or 'breakpoints' is also streamlined to allow the most up-to-date and cutting-edge data available to healthcare professionals under the new Act.
The U.S. National Institutes of Health plans to fund a new research network on the issue up to $62 million from 2013 to 2019. Using authority created by the Pandemic and All Hazards Preparedness Act of 2006, the Biomedical Advanced Research and Development Authority in the U.S. Department of Health and Human Services announced that it will spend between $40 million and $200 million in funding for R&D on new antibiotic drugs under development by GlaxoSmithKline.
Mechanism.
In research published on October 17, 2008 in "Cell", a team of scientists pinpointed the place on bacteria where the antibiotic myxopyronin launches its attack, and why that attack is successful. The myxopyronin binds to and inhibits the crucial bacterial enzyme RNA polymerase. The myxopyronin changes the structure of the switch-2 segment of the enzyme, inhibiting its function of reading and transmitting DNA code. This prevents RNA polymerase from delivering genetic information to the ribosomes, causing the bacteria to die.
In 2012, a team of the University of Leipzig modified a peptide found in honeybees. It is effective against 37 types of bacteria.
One major cause of antibiotic resistance is the increased pumping activity of microbial ABC transporters, which diminishes the effective drug concentration inside the microbial cell. ABC transporter inhibitors that can be used in combination with current antimicrobials are being tested in clinical trials and are available for therapeutic regimens.
Applications.
Antibiotic resistance is an important tool for genetic engineering. By constructing a plasmid that contains an antibiotic-resistance gene as well as the gene being engineered or expressed, a researcher can ensure that, when bacteria replicate, only the copies that carry the plasmid survive. This ensures that the gene being manipulated passes along when the bacteria replicates.
In general, the most commonly used antibiotics in genetic engineering are "older" antibiotics that have largely fallen out of use in clinical practice. These include:
In industry, the use of antibiotic resistance is disfavored, since maintaining bacterial cultures would require feeding them large quantities of antibiotics. Instead, the use of auxotrophic bacterial strains (and function-replacement plasmids) is preferred.

</doc>
<doc id="1915" url="http://en.wikipedia.org/wiki?curid=1915" title="Antigen">
Antigen

The antigen may originate from within the body ("self") or from the external environment ("non-self"). The immune system is usually non-reactive against "self" antigens under normal conditions and is supposed to identify and attack only "non-self" invaders from the outside world or modified/harmful substances present in the body under distressed conditions.
Cells present their antigenic structures to the immune system via a histocompatibility molecule. Depending on the antigen presented and the type of the histocompatibility molecule, several types of immune cells can become activated. Antigen was originally a structural molecule that binds specifically to the antibody, but the term now also refers to any molecule or molecular fragment that can be recognized by highly variable antigen receptors (B-cell receptor or T-cell receptor) of the adaptive immune system. For T-Cell Receptor (TCR) recognition, it must be processed into small fragments inside the cell and presented to a T-cell receptor by major histocompatibility complex (MHC). Antigen by itself is not capable to elicit the immune response without the help of an Immunologic adjuvant. The essential role of the adjuvant component of vaccines in the activation of innate immune system is so-called immunologist's dirty little secret as originally described by Charles Janeway.
An immunogen is in analogy to the antigen a substance (or a mixture of substances) that is able to provoke an immune response if injected to the body. An immunogen is able to initiate an indispensable innate immune response first, later leading to the activation of the adaptive immune response, whereas an antigen is able to bind the highly variable immunoreceptor products (B-cell receptor or T-cell receptor) once these have been produced. The overlapping concepts of immunogenicity and antigenicity are, therefore, subtly different. According to current textbook notions:
Immunogenicity is the ability to induce a humoral and/or cell-mediated immune response
Antigenicity is the ability to combine specifically with the final products of the immune response (i.e. secreted antibodies and/or surface receptors on T-cells). Although all immunogenic molecules are also antigenic, the reverse is not true.
At the molecular level, an antigen can be characterized by its ability to be bound by the variable Fab region of an antibody. Note also that different antibodies have the potential to discriminate between specific epitopes present on the surface of the antigen (as illustrated in the Figure). Hapten is a small molecule that changes the structure of an antigenic epitope. In order to induce an immune response, it has to be attached to a large carrier molecule such as protein. 
Antigens are usually proteins and polysaccharides, less frequently also lipids. This includes parts (coats, capsules, cell walls, flagella, fimbrae, and toxins) of bacteria, viruses, and other microorganisms. Lipids and nucleic acids are antigenic only when combined with proteins and polysaccharides. Non-microbial exogenous (non-self) antigens can include pollen, egg white, and proteins from transplanted tissues and organs or on the surface of transfused blood cells. Vaccines are examples of antigens in an immunogenic form, which are to be intentionally administered to induce the memory function of adaptive immune system toward the antigens of the pathogen invading the recipient.
Origin of the term antigen.
In 1899, Ladislas Deutsch (Laszlo Detre) (1874–1939) named the hypothetical substances halfway between bacterial constituents and antibodies "substances immunogenes ou antigenes" (antigenic or immunogenic substances). He originally believed those substances to be precursors of antibodies, just as zymogen is a precursor of an enzyme. But, by 1903, he understood that an antigen induces the production of immune bodies (antibodies) and wrote that the word "antigen" is a contraction of Antisomatogen(= "Immunkörperbildner"). The Oxford English Dictionary indicates that the logical construction should be "anti(body)-gen".
Origin of antigens.
Antigens can be classified in order of their class.
Exogenous antigens.
Exogenous antigens are antigens that have entered the body from the outside, for example by inhalation, ingestion, or injection. The immune system's response to exogenous antigens is often subclinical. By endocytosis or phagocytosis, exogenous antigens are taken into the antigen-presenting cells (APCs) and processed into fragments. APCs then present the fragments to T helper cells (CD4+) by the use of class II histocompatibility molecules on their surface. Some T cells are specific for the peptide:MHC complex. They become activated and start to secrete cytokines. Cytokines are substances that can activate cytotoxic T lymphocytes (CTL), antibody-secreting B cells, macrophages, and other particles.
Some antigens start out as exogenontigens, and later become endogenous (for example, intracellular viruses). Intracellular antigens can again be released back into circulation upon the destruction of the infected cell.
Endogenous antigens.
Endogenous antigens are antigens that have been generated within previously normal cells as a result of normal cell metabolism, or because of viral or intracellular bacterial infection. The fragments are then presented on the cell surface in the complex with MHC class I molecules. If activated cytotoxic CD8+ T cells recognize them, the T cells begin to secrete various toxins that cause the lysis or apoptosis of the infected cell. In order to keep the cytotoxic cells from killing cells just for presenting self-proteins, self-reactive T cells are deleted from the repertoire as a result of tolerance (also known as negative selection). Endogenous antigens include xenogenic (heterologous), autologous and idiotypic or allogenic (homologous) antigens.
Autoantigens.
An autoantigen is usually a normal protein or complex of proteins (and sometimes DNA or RNA) that is recognized by the immune system of patients suffering from a specific autoimmune disease. These antigens should not be, under normal conditions, the target of the immune system, but, due mainly to genetic and environmental factors, the normal immunological tolerance for such an antigen has been lost in these patients.
Tumor antigens.
"Tumor antigens" or "neoantigens" are those antigens that are presented by MHC I or MHC II molecules on the surface of tumor cells. These antigens can sometimes be presented by tumor cells and never by the normal ones. In this case, they are called tumor-specific antigens (TSAs) and, in general, result from a tumor-specific mutation. More common are antigens that are presented by tumor cells and normal cells, and they are called tumor-associated antigens (TAAs). Cytotoxic T lymphocytes that recognize these antigens may be able to destroy the tumor cells before they proliferate or metastasize.
Tumor antigens can also be on the surface of the tumor in the form of, for example, a mutated receptor, in which case they will be recognized by B cells.
Nativity.
A native antigen is an antigen that is not yet processed by an APC to smaller parts. T cells cannot bind native antigens, but require that they be processed by APCs, whereas B cells can be activated by native ones.
Antigenic specificity.
Antigen(ic) specificity is the ability of the host cells to recognize an antigen specifically as a unique molecular entity and distinguish it from another with exquisite precision. Antigen specificity is due primarily to the side-chain conformations of the antigen. It is a measurement, although the degree of specificity may not be easy to measure, and need not be linear or of the nature of a rate-limited step or equation.

</doc>
<doc id="1916" url="http://en.wikipedia.org/wiki?curid=1916" title="Autosome">
Autosome

An autosome is a chromosome that is not an allosome (i.e., not a sex chromosome). Autosomes appear in pairs whose members have the same form but differ from other pairs in a diploid cell, whereas members of an allosome pair may differ from one another and thereby determine sex. The DNA in autosomes is collectively known as atDNA or auDNA.
For example, humans have a diploid genome that usually contains 22 pairs of autosomes and one allosome pair (46 chromosomes total). The autosome pairs are labeled with numbers (1-22 in humans) roughly in order of their sizes in base pairs, while allosomes are labeled with their letters. By contrast, the allosome pair consists of two X chromosomes in females or one X and one Y chromosome in males. (Unusual combinations of XYY, XXY, XXX, XXXX, XXXXX or XXYY, among other allosome combinations, are known to occur and usually cause developmental abnormalities.) 

</doc>
<doc id="1919" url="http://en.wikipedia.org/wiki?curid=1919" title="Antwerp (disambiguation)">
Antwerp (disambiguation)

Antwerp is the name of a city, a district and a province in Flanders, Belgium:
Antwerp is also the name of a number of places 

</doc>
<doc id="1920" url="http://en.wikipedia.org/wiki?curid=1920" title="Aquila">
Aquila

Aquila is the Latin and Romance languages word for "eagle". Specifically, it may refer to:

</doc>
<doc id="1921" url="http://en.wikipedia.org/wiki?curid=1921" title="Al-Qaeda">
Al-Qaeda

Al-Qaeda ( ; ', , translation: "The Base" and alternatively spelled al-Qaida and sometimes al-Qa'ida) is a global militant Islamist and takfiri organization founded by Osama bin Laden, Abdullah Azzam, and several other militants, at some point between August 1988 and late 1989, with its origins being traceable to the Soviet war in Afghanistan. It operates as a network comprising both a multinational, stateless army See also:
Al-Qaeda has attacked civilian and military targets in various countries, including the September 11 attacks, 1998 U.S. embassy bombings and the 2002 Bali bombings. The U.S. government responded to the September 11 attacks by launching the War on Terror. With the loss of key leaders, culminating in the death of Osama bin Laden, al-Qaeda's operations have devolved from actions that were controlled from the top-down, to actions by franchise associated groups, to actions of lone wolf operators.
Characteristic techniques employed by al-Qaeda include suicide attacks and simultaneous bombings of different targets. Activities ascribed to it may involve members of the movement, who have taken a pledge of loyalty to Osama bin Laden, or the much more numerous "al-Qaeda-linked" individuals who have undergone training in one of its camps in Afghanistan, Pakistan, Iraq or Sudan, but who have not taken any pledge. Al-Qaeda ideologues envision a complete break from all foreign influences in Muslim countries, and the creation of a new world-wide Islamic caliphate. Among the beliefs ascribed to Al-Qaeda members is the conviction that a Christian–Jewish alliance is conspiring to destroy Islam. As Salafist jihadists, they believe that the killing of civilians is religiously sanctioned, and they ignore any aspect of religious scripture which might be interpreted as forbidding the murder of civilians and internecine fighting. Al-Qaeda also opposes man-made laws, and wants to replace them with a strict form of sharia law.
Al-Qaeda is also responsible for instigating sectarian violence among Muslims. Al-Qaeda is intolerant of non-Sunni branches of Islam and denounces them by means of excommunications called "takfir". Al-Qaeda leaders regard liberal Muslims, Shias, Sufis and other sects as heretics and have attacked their mosques and gatherings. Examples of sectarian attacks include the Yazidi community bombings, the Sadr City bombings, the Ashoura Massacre and the April 2007 Baghdad bombings. The group is led by the Egyptian theologian Ayman al-Zawahiri.
Organization.
Al-Qaeda's management philosophy has been described as "centralization of decision and decentralization of execution." It is thought that al-Qaeda's leadership, following the War on Terror, has "become geographically isolated", leading to the "emergence of decentralized leadership" of regional groups using the al-Qaeda "brand".
Many terrorism experts do not believe that the global jihadist movement is driven at every level by al-Qaeda's leadership. Although bin Laden still held considerable ideological sway over some Muslim extremists before his death, experts argue that al-Qaeda has fragmented over the years into a variety of regional movements that have little connection with one another. Marc Sageman, a psychiatrist and former Central Intelligence Agency (CIA) officer, said that al-Qaeda is now just a "loose label for a movement that seems to target the West". "There is no umbrella organisation. We like to create a mythical entity called [al-Qaeda] in our minds, but that is not the reality we are dealing with."
This view mirrors the account given by Osama bin Laden in his October 2001 interview with Tayseer Allouni:
"... this matter isn't about any specific person and... is not about the al-Qa`idah Organization. We are the children of an Islamic Nation, with Prophet Muhammad as its leader, our Lord is one... and all the true believers are brothers. So the situation isn't like the West portrays it, that there is an 'organization' with a specific name (such as 'al-Qa`idah') and so on. That particular name is very old. It was born without any intention from us. Brother Abu Ubaida... created a military base to train the young men to fight against the vicious, arrogant, brutal, terrorizing Soviet empire... So this place was called 'The Base' ['Al-Qa`idah', as in a training base, so this name grew and became. We aren't separated from this nation. We are the children of a nation, and we are an inseparable part of it, and from those public *** which spread from the far east, from the Philippines, to Indonesia, to Malaysia, to India, to Pakistan, reaching Mauritania... and so we discuss the conscience of this nation."
Others, however, see al-Qaeda as an integrated network that is strongly led from the Pakistani tribal areas and has a powerful strategic purpose. Bruce Hoffman, a terrorism expert at Georgetown University, said "It amazes me that people don't think there is a clear adversary out there, and that our adversary does not have a strategic approach."
Al-Qaeda has the following direct affiliates:
Al-Qaeda has the following indirect affiliates:
Leadership.
Information mostly acquired from Jamal al-Fadl provided American authorities with a rough picture of how the group was organized. While the veracity of the information provided by al-Fadl and the motivation for his cooperation are both disputed, American authorities base much of their current knowledge of al-Qaeda on his testimony.
Osama bin Laden was the most historically notable emir, or commander, and Senior Operations Chief of al-Qaeda prior to his assassination on May 1, 2011 by US forces. Ayman al-Zawahiri, al-Qaeda's Deputy Operations Chief prior to bin Laden's death, assumed the role of commander, according to an announcement by al-Qaeda on June 16, 2011. He replaced Saif al-Adel, who had served as interim commander.
Bin Laden was advised by a Shura Council, which consists of senior al-Qaeda members, estimated by Western officials to consist of 20–30 people.
Atiyah Abd al-Rahman was alleged to be second in command prior to his death on August 22, 2011.
On June 5, 2012, Pakistan intelligence officials announced that al-Rahman's alleged successor Abu Yahya al-Libi had been killed in Pakistan.
Al-Qaeda's network was built from scratch as a conspiratorial network that draws on leaders of all its regional nodes "as and when necessary to serve as an integral part of its high command."
Command structure.
When asked about the possibility of al-Qaeda's connection to the July 7, 2005 London bombings in 2005, Metropolitan Police Commissioner Sir Ian Blair said: "Al-Qaeda is not an organization. Al-Qaeda is a way of working... but this has the hallmark of that approach... al-Qaeda clearly has the ability to provide training... to provide expertise... and I think that is what has occurred here."
On August 13, 2005, however, "The Independent" newspaper, quoting police and MI5 investigations, reported that the July 7 bombers had acted independently of an al-Qaeda terror mastermind someplace abroad.
What exactly al-Qaeda is, or was, remains in dispute. Certainly, it has been obliged to evolve and adapt in the aftermath of 9/11 and the launch of the 'war on terror'.
Nasser al-Bahri, who was Osama bin Laden's bodyguard for four years in the run-up to 9/11 gives a highly detailed description of how the organization functioned at that time in his memoir. He describes its formal administrative structure and vast arsenal, as well as day-to-day life as a member.
However, author and journalist Adam Curtis argues that the idea of al-Qaeda as a formal organization is primarily an American invention. Curtis contends the name "al-Qaeda" was first brought to the attention of the public in the 2001 trial of bin Laden and the four men accused of the 1998 US embassy bombings in East Africa:
The reality was that bin Laden and Ayman al-Zawahiri had become the focus of a loose association of disillusioned Islamist militants who were attracted by the new strategy. But there was no organization. These were militants who mostly planned their own operations and looked to bin Laden for funding and assistance. He was not their commander. There is also no evidence that bin Laden used the term "al-Qaeda" to refer to the name of a group until after September 11 attacks, when he realized that this was the term the Americans had given it.
As a matter of law, the US Department of Justice needed to show that bin Laden was the leader of a criminal organization in order to charge him "in absentia" under the Racketeer Influenced and Corrupt Organizations Act, also known as the RICO statutes. The name of the organization and details of its structure were provided in the testimony of Jamal al-Fadl, who said he was a founding member of the organization and a former employee of bin Laden. Questions about the reliability of al-Fadl's testimony have been raised by a number of sources because of his history of dishonesty, and because he was delivering it as part of a plea bargain agreement after being convicted of conspiring to attack U.S. military establishments. Sam Schmidt, one of his defense lawyers, said:
There were selective portions of al-Fadl's testimony that I believe was false, to help support the picture that he helped the Americans join together. I think he lied in a number of specific testimony about a unified image of what this organization was. It made al-Qaeda the new Mafia or the new Communists. It made them identifiable as a group and therefore made it easier to prosecute any person associated with al-Qaeda for any acts or statements made by bin Laden.
Field operatives.
The number of individuals in the organization who have undergone proper military training, and are capable of commanding insurgent forces, is largely unknown. Documents captured in the raid on bin Laden compound in 2011, show that the core Al-Qaeda membership in 2002 was 170. In 2006, it was estimated that al-Qaeda had several thousand commanders embedded in 40 different countries. As of 2009, it was believed that no more than 200–300 members were still active commanders.
According to the award-winning 2004 BBC documentary "The Power of Nightmares", al-Qaeda was so weakly linked together that it was hard to say it existed apart from bin Laden and a small clique of close associates. The lack of any significant numbers of convicted al-Qaeda members, despite a large number of arrests on terrorism charges, was cited by the documentary as a reason to doubt whether a widespread entity that met the description of al-Qaeda existed.
Insurgent forces.
According to Robert Cassidy, al-Qaeda controls two separate forces deployed alongside insurgents in Iraq and Pakistan. The first, numbering in the tens of thousands, was "organized, trained, and equipped as insurgent combat forces" in the Soviet-Afghan war. It was made up primarily of foreign "mujahideen" from Saudi Arabia and Yemen. Many went on to fight in Bosnia and Somalia for global "jihad". Another group, approximately 10,000 strong, live in Western states and have received rudimentary combat training.
Other analysts have described al-Qaeda's rank and file as being "predominantly Arab," in its first years of operation, and now also includes "other peoples" as of 2007. It has been estimated that 62% of al-Qaeda members have university education.
Financing.
Some financing for al-Qaeda in the 1990s came from the personal wealth of Osama bin Laden. By 2001 Afghanistan had become politically complex and mired. With many financial sources for al-Qaeda, bin Laden's financing role may have become comparatively minor. Sources in 2001 could also have included Jamaa Al-Islamiyya and Islamic Jihad, both associated with Afghan-based Egyptians. Other sources of income in 2001 included the heroin trade and donations from supporters in Kuwait, Saudi Arabia and other Islamic countries. A WikiLeaks released memo from the United States Secretary of State sent in 2009 asserted that the primary source of funding of Sunni terrorist groups worldwide was Saudi Arabia.
Strategy.
On March 11, 2005, "Al-Quds Al-Arabi" published extracts from Saif al-Adel's document "Al Qaeda's Strategy to the Year 2020". Abdel Bari Atwan summarizes this strategy as comprising five stages to rid the Ummah from all forms of oppression:
Atwan also noted, regarding the collapse of the U.S., "If this sounds far-fetched, it is sobering to consider that this virtually describes the downfall of the Soviet Union."
Name.
In Arabic, "al-Qaeda" has four syllables ("", or ). However, since two of the Arabic consonants in the name (the voiceless uvular plosive and the voiced pharyngeal fricative ) are not phones found in the English language, the closest naturalized English pronunciations include , and . al-Qaeda's name can also be transliterated as al-Qaida, al-Qa'ida, el-Qaida, or al-Qaeda.
The name comes from the Arabic noun "qā'idah", which means "foundation" or "basis", and can also refer to a military base. The initial "al-" is the Arabic definite article "the", hence "the base".
Bin Laden explained the origin of the term in a videotaped interview with Al Jazeera journalist Tayseer Alouni in October 2001:
The name 'al-Qaeda' was established a long time ago by mere chance. The late Abu Ebeida El-Banashiri established the training camps for our "mujahedeen" against Russia's terrorism. We used to call the training camp al-Qaeda. The name stayed.
It has been argued that two documents seized from the Sarajevo office of the Benevolence International Foundation prove that the name was not simply adopted by the "mujahid" movement and that a group called al-Qaeda was established in August 1988. Both of these documents contain minutes of meetings held to establish a new military group, and contain the term "al-Qaeda".
Former British Foreign Secretary Robin Cook wrote that the word al-Qaeda should be translated as "the database", and originally referred to the computer file of the thousands of "mujahideen" militants who were recruited and trained with CIA help to defeat the Russians. In April 2002, the group assumed the name "Qa'idat al-Jihad", which means "the base of Jihad". According to Diaa Rashwan, this was "apparently as a result of the merger of the overseas branch of Egypt's al-Jihad (Egyptian Islamist Jihad, or EIJ) group, led by Ayman al-Zawahiri, with the groups Bin Laden brought under his control after his return to Afghanistan in the mid-1990s."
Ideology.
The radical Islamist movement in general and al-Qaeda in particular developed during the Islamic revival and Islamist movement of the last three decades of the 20th century, along with less extreme movements.
Some have argued that "without the writings" of Islamic author and thinker Sayyid Qutb, "al-Qaeda would not have existed." Qutb preached that because of the lack of "sharia" law, the Muslim world was no longer Muslim, having reverted to pre-Islamic ignorance known as "jahiliyyah".
To restore Islam, he said a vanguard movement of righteous Muslims was needed to establish "true Islamic states", implement "sharia", and rid the Muslim world of any non-Muslim influences, such as concepts like socialism and nationalism. Enemies of Islam in Qutb's view included "treacherous Orientalists" and "world Jewry", who plotted "conspiracies" and "wicked" opposed Islam.
In the words of Mohammed Jamal Khalifa, a close college friend of bin Laden: Islam is different from any other religion; it's a way of life. We and bin Laden were trying to understand what Islam has to say about how we eat, who we marry, how we talk. We read Sayyid Qutb. He was the one who most affected our generation.
Qutb had an even greater influence on bin Laden's mentor and another leading member of al-Qaeda, Ayman al-Zawahiri. Zawahiri's uncle and maternal family patriarch, Mafouz Azzam, was Qutb's student, then protégé, then personal lawyer, and finally executor of his estate—one of the last people to see Qutb before his execution. "Young Ayman al-Zawahiri heard again and again from his beloved uncle Mahfouz about the purity of Qutb's character and the torment he had endured in prison." Zawahiri paid homage to Qutb in his work "Knights under the Prophet's Banner."
One of the most powerful of Qutb's ideas was that many who said they were Muslims were not. Rather, they were apostates. That not only gave jihadists "a legal loophole around the prohibition of killing another Muslim," but made "it a religious obligation to execute" these self-professed Muslims. These alleged apostates included leaders of Muslim countries, since they failed to enforce "sharia" law.
Religious compatibility.
Abdel Bari Atwan writes that:
History.
The Guardian has described five distinct phases in the development of al-Qaeda: beginnings in the late 1980s, a "wilderness" period in 1990–96, its "heyday" in 1996–2001, a network period from 2001 to 2005, and a period of fragmentation from 2005 to today.
Jihad in Afghanistan.
The origins of al-Qaeda as a network inspiring terrorism around the world and training operatives can be traced to the Soviet War in Afghanistan (December 1979 – February 1989). The U.S. viewed the conflict in Afghanistan, with the Afghan Marxists and allied Soviet troops on one side and the native Afghan "mujahideen", some of whom were radical Islamic militants, on the other, as a blatant case of Soviet expansionism and aggression. A CIA program called Operation Cyclone channeled funds through Pakistan's Inter-Services Intelligence agency to the Afghan Mujahideen who were fighting the Soviet occupation.
At the same time, a growing number of Arab "mujahideen" joined the "jihad" against the Afghan Marxist regime, facilitated by international Muslim organizations, particularly the Maktab al-Khidamat, which was funded by the Saudi Arabia government as well as by individual Muslims (particularly Saudi businessmen who were approached by bin Laden). Together, these sources donated some $600 million a year to jihad.
In 1984, Maktab al-Khidamat (MAK), or the "Services Office", a Muslim organization founded to raise and channel funds and recruit foreign "mujahideen" for the war against the Soviets in Afghanistan, was established in Peshawar, Pakistan, by bin Laden and Abdullah Yusuf Azzam, a Palestinian Islamic scholar and member of the Muslim Brotherhood. MAK organized guest houses in Peshawar, near the Afghan border, and gathered supplies for the construction of paramilitary training camps to prepare foreign recruits for the Afghan war front. Bin Laden became a "major financier" of the "mujahideen", spending his own money and using his connections with "the Saudi royal family and the petro-billionaires of the Gulf" to influence public opinion about the war and raise additional funds.
From 1986, MAK began to set up a network of recruiting offices in the U.S., the hub of which was the Al Kifah Refugee Center at the Farouq Mosque on Brooklyn's Atlantic Avenue. Among notable figures at the Brooklyn center were "double agent" Ali Mohamed, whom FBI special agent Jack Cloonan called "bin Laden's first trainer," and "Blind Sheikh" Omar Abdel-Rahman, a leading recruiter of "mujahideen" for Afghanistan. Al-Qaeda evolved from MAK.
Azzam and bin Laden began to establish camps in Afghanistan in 1987.
U.S. government financial support for the Afghan Islamic militants was substantial. Aid to Gulbuddin Hekmatyar, an Afghan "mujahideen" leader and founder and leader of the Hezb-e Islami radical Islamic militant faction, alone amounted "by the most conservative estimates" to $600 million. Later, in the early 1990s, after the U.S. had withdrawn support, Hekmatyar "worked closely" with bin Laden. In addition to receiving hundreds of millions of dollars in American aid, Hekmatyar was the recipient of the lion's share of Saudi aid. There is evidence that the CIA supported Hekmatyar's drug trade activities by giving him immunity for his opium trafficking, which financed the operation of his militant faction.
MAK and foreign "mujahideen" volunteers, or "Afghan Arabs," did not play a major role in the war. While over 250,000 Afghan "mujahideen" fought the Soviets and the communist Afghan government, it is estimated that were never more than 2,000 foreign "mujahideen" in the field at any one time. Nonetheless, foreign "mujahideen" volunteers came from 43 countries, and the total number that participated in the Afghan movement between 1982 and 1992 is reported to have been 35,000. Bin Laden played a central role in organizing training camps for the foreign Muslim volunteers.
The Soviet Union finally withdrew from Afghanistan in 1989. To the surprise of many, Mohammad Najibullah's communist Afghan government hung on for three more years, before being overrun by elements of the "mujahideen". With "mujahideen" leaders unable to agree on a structure for governance, chaos ensued, with constantly reorganizing alliances fighting for control of ill-defined territories, leaving the country devastated.
Expanding operations.
Toward the end of the Soviet military mission in Afghanistan, some "mujahideen" wanted to expand their operations to include Islamist struggles in other parts of the world, such as Israel and Kashmir. A number of overlapping and interrelated organizations were formed, to further those aspirations.
One of these was the organization that would eventually be called al-Qaeda, formed by bin Laden with an initial meeting held on August 11, 1988.
Notes of a meeting of bin Laden and others on August 20, 1988, indicate al-Qaeda was a formal group by that time: "basically an organized Islamic faction, its goal is to lift the word of God, to make His religion victorious." A list of requirements for membership itemized the following: listening ability, good manners, obedience, and making a pledge ("bayat") to follow one's superiors.
In his memoir, bin Laden's former bodyguard, Nasser al-Bahri, gives the only publicly available description of the ritual of giving "bayat" when he swore his allegiance to the al-Qaeda chief.
According to Wright, the group's real name wasn't used in public pronouncements because "its existence was still a closely held secret." His research suggests that al-Qaeda was formed at an August 11, 1988, meeting between "several senior leaders" of Egyptian Islamic Jihad, Abdullah Azzam, and bin Laden, where it was agreed to join bin Laden's money with the expertise of the Islamic Jihad organization and take up the jihadist cause elsewhere after the Soviets withdrew from Afghanistan.
Bin Laden wished to establish non-military operations in other parts of the world; Azzam, in contrast, wanted to remain focused on military campaigns. After Azzam was assassinated in 1989, the MAK split, with a significant number joining bin Laden's organization.
In November 1989, Ali Mohamed, a former special forces Sergeant stationed at Fort Bragg, North Carolina, left military service and moved to California. He traveled to Afghanistan and Pakistan and became "deeply involved with bin Laden's plans."
A year later, on November 8, 1990, the FBI raided the New Jersey home of Ali Mohammed's associate El Sayyid Nosair, discovering a great deal of evidence of terrorist plots, including plans to blow up New York City skyscrapers. Nosair was eventually convicted in connection to the 1993 World Trade Center bombing. In 1991, Ali Mohammed is said to have helped orchestrate bin Laden's relocation to Sudan.
Gulf War and the start of U.S. enmity.
Following the Soviet Union's withdrawal from Afghanistan in February 1989, bin Laden returned to Saudi Arabia. The Iraqi invasion of Kuwait in August 1990 had put the Kingdom and its ruling House of Saud at risk. The world's most valuable oil fields were within easy striking distance of Iraqi forces in Kuwait, and Saddam's call to pan-Arab/Islamism could potentially rally internal dissent.
In the face of a seemingly massive Iraqi military presence, Saudi Arabia's own forces were well armed but far outnumbered. Bin Laden offered the services of his "mujahideen" to King Fahd to protect Saudi Arabia from the Iraqi army. The Saudi monarch refused bin Laden's offer, opting instead to allow U.S. and allied forces to deploy troops into Saudi territory.
The deployment angered bin Laden, as he believed the presence of foreign troops in the "land of the two mosques" (Mecca and Medina) profaned sacred soil. After speaking publicly against the Saudi government for harboring American troops, he was banished and forced to live in exile in Sudan.
Sudan.
From around 1992 to 1996, al-Qaeda and bin Laden based themselves in Sudan at the invitation of Islamist theoretician Hassan al-Turabi. The move followed an Islamist coup d'état in Sudan, led by Colonel Omar al-Bashir, who professed a commitment to reordering Muslim political values. During this time, bin Laden assisted the Sudanese government, bought or set up various business enterprises, and established camps where insurgents trained.
A key turning point for bin Laden, further pitting him against the Sauds, occurred in 1993 when Saudi Arabia gave support for the Oslo Accords, which set a path for peace between Israel and Palestinians.
Zawahiri and the EIJ, who served as the core of al-Qaeda but also engaged in separate operations against the Egyptian government, had bad luck in Sudan. In 1993, a young schoolgirl was killed in an unsuccessful EIJ attempt on the life of the Egyptian prime minister, Atef Sedki. Egyptian public opinion turned against Islamist bombings, and the police arrested 280 of al-Jihad's members and executed 6.
Due to bin Laden's continuous verbal assault on King Fahd of Saudi Arabia, on March 5, 1994 Fahd sent an emissary to Sudan demanding bin Laden's passport; bin Laden's Saudi citizenship was also revoked. His family was persuaded to cut off his monthly stipend, $7 million ($ today) a year, and his Saudi assets were frozen. His family publicly disowned him. There is controversy over whether and to what extent he continued to garner support from members of his family and/or the Saudi government.
In June 1995, an even more ill-fated attempt to assassinate Egyptian president Mubarak led to the expulsion of EIJ, and in May 1996, of bin Laden, by the Sudanese government.
According to Pakistani-American businessman Mansoor Ijaz, the Sudanese government offered the Clinton Administration numerous opportunities to arrest bin Laden. Those opportunities were met positively by Secretary of State Madeleine Albright, but spurned when Susan Rice and counter-terrorism czar Richard Clarke persuaded National Security Advisor Sandy Berger to overrule Albright. Ijaz’s claims appeared in numerous Op-Ed pieces, including one in the "Los Angeles Times" and one in "The Washington Post" co-written with former Ambassador to Sudan Timothy M. Carney. Similar allegations have been made by "Vanity Fair" contributing editor David Rose, and Richard Miniter, author of "Losing bin Laden", in a November 2003 interview with "World".
Several sources dispute Ijaz's claim, including the National Commission on Terrorist Attacks on the U.S. (the 9–11 Commission), which concluded in part: Sudan's minister of defense, Fatih Erwa, has claimed that Sudan offered to hand Bin Ladin over to the U.S. The Commission has found no credible evidence that this was so. Ambassador Carney had instructions only to push the Sudanese to expel Bin Ladin. Ambassador Carney had no legal basis to ask for more from the Sudanese since, at the time, there was no indictment out-standing.
Refuge in Afghanistan.
After the Soviet withdrawal, Afghanistan was effectively ungoverned for seven years and plagued by constant infighting between former allies and various "mujahideen" groups.
Throughout the 1990s, a new force began to emerge. The origins of the Taliban (literally "students") lay in the children of Afghanistan, many of them orphaned by the war, and many of whom had been educated in the rapidly expanding network of Islamic schools (madrassas) either in Kandahar or in the refugee camps on the Afghan-Pakistani border.
According to Ahmed Rashid, five leaders of the Taliban were graduates of Darul Uloom Haqqania, a madrassa in the small town of Akora Khattak. The town is situated near Peshawar in Pakistan, but largely attended by Afghan refugees. This institution reflected Salafi beliefs in its teachings, and much of its funding came from private donations from wealthy Arabs. Bin Laden's contacts were still laundering most of these donations, using "unscrupulous" Islamic banks to transfer the money to an "array" of charities which serve as front groups for al-Qaeda, or transporting cash-filled suitcases straight into Pakistan. Another four of the Taliban's leaders attended a similarly funded and influenced madrassa in Kandahar.
Many of the "mujahideen" who later joined the Taliban fought alongside Afghan warlord Mohammad Nabi Mohammadi's Harkat i Inqilabi group at the time of the Russian invasion. This group also enjoyed the loyalty of most Afghan Arab fighters.
The continuing internecine strife between various factions, and accompanying lawlessness following the Soviet withdrawal, enabled the growing and well-disciplined Taliban to expand their control over territory in Afghanistan, and it came to establish an enclave which it called the Islamic Emirate of Afghanistan. In 1994, it captured the regional center of Kandahar, and after making rapid territorial gains thereafter, conquered the capital city Kabul in September 1996.
After the Sudanese made it clear, in May 1996, that bin Laden would never be welcome to return, Taliban-controlled Afghanistan—with previously established connections between the groups, administered with a shared militancy, and largely isolated from American political influence and military power—provided a perfect location for al-Qaeda to relocate its headquarters. Al-Qaeda enjoyed the Taliban's protection and a measure of legitimacy as part of their Ministry of Defense, although only Pakistan, Saudi Arabia, and the United Arab Emirates recognized the Taliban as the legitimate government of Afghanistan.
While in Afghanistan, the Taliban government tasked al-Qaeda with the training of Brigade 055, an elite part of the Taliban's army from 1997–2001. The Brigade was made up of mostly foreign fighters, many veterans from the Soviet Invasion, and all under the same basic ideology of the mujahideen. In November 2001, as Operation Enduring Freedom had toppled the Taliban government, many Brigade 055 fighters were captured or killed, and those that survived were thought to head into Pakistan along with bin Laden.
By the end of 2008, some sources reported that the Taliban had severed any remaining ties with al-Qaeda, while others cast doubt on this. According to senior U.S. military intelligence officials, there were fewer than 100 members of al-Qaeda remaining in Afghanistan in 2009.
Call for global jihad.
Around 1994, the Salafi groups waging "jihad" in Bosnia entered into a seemingly irreversible decline. As they grew less and less aggressive, groups such as EIJ began to drift away from the Salafi cause in Europe. Al-Qaeda decided to step in and assumed control of around 80% of the terrorist cells in Bosnia in late 1995.
At the same time, al-Qaeda ideologues instructed the network's recruiters to look for "Jihadi international", Muslims who believed that "jihad" must be fought on a global level. The concept of a "global Salafi "jihad"" had been around since at least the early 1980s. Several groups had formed for the explicit purpose of driving non-Muslims out of every Muslim land, at the same time, and with maximum carnage. This was, however, a fundamentally defensive strategy.
Al-Qaeda sought to open the "offensive phase" of the global Salafi "jihad". Bosnian Islamists in 2006 called for "solidarity with Islamic causes around the world", supporting the insurgents in Kashmir and Iraq as well as the groups fighting for a Palestinian state.
Fatwas.
In 1996, al-Qaeda announced its "jihad" to expel foreign troops and interests from what they considered Islamic lands. Bin Laden issued a "fatwa" (binding religious edict), which amounted to a public declaration of war against the U.S. and its allies, and began to refocus al-Qaeda's resources on large-scale, propagandist strikes.
On February 23, 1998, bin Laden and Ayman al-Zawahiri, a leader of Egyptian Islamic Jihad, along with three other Islamist leaders, co-signed and issued a "fatwa" calling on Muslims to kill Americans and their allies where they can, when they can. Under the banner of the World Islamic Front for Combat Against the Jews and Crusaders, they declared:
ruling to kill the Americans and their allies—civilians and military—is an individual duty for every Muslim who can do it in any country in which it is possible to do it, in order to liberate the al-Aqsa Mosque [in Jerusalem and the holy mosque Mecca from their grip, and in order for their armies to move out of all the lands of Islam, defeated and unable to threaten any Muslim. This is in accordance with the words of Almighty Allah, 'and fight the pagans all together as they fight you all together,' and 'fight them until there is no more tumult or oppression, and there prevail justice and faith in Allah'.
Neither bin Laden nor al-Zawahiri possessed the traditional Islamic scholarly qualifications to issue a "fatwa". However, they rejected the authority of the contemporary "ulema" (which they saw as the paid servants of "jahiliyya" rulers), and took it upon themselves. Former Russian FSB agent Alexander Litvinenko, who was later killed, said that the FSB trained al-Zawahiri in a camp in Dagestan eight months before the 1998 "fatwa".
Iraq.
Al-Qaeda is Sunni, and often attacked the Iraqi Shia majority in an attempt to incite sectarian violence and greater chaos in the country. Al-Zarqawi purportedly declared an all-out war on Shiites while claiming responsibility for Shiite mosque bombings. The same month, a statement claiming to be by AQI rejected as "fake" a letter allegedly written by al-Zawahiri, in which he appears to question the insurgents' tactic of indiscriminately attacking Shiites in Iraq. In a December 2007 video, al-Zawahiri defended the Islamic State in Iraq, but distanced himself from the attacks against civilians committed by "hypocrites and traitors existing among the ranks".
U.S. and Iraqi officials accused AQI of trying to slide Iraq into a full-scale civil war between Iraq's majority Shiites and minority Sunni Arabs, with an orchestrated campaign of civilian massacres and a number of provocative attacks against high-profile religious targets. With attacks such as the 2003 Imam Ali Mosque bombing, the 2004 Day of Ashura and Karbala and Najaf bombings, the 2006 first al-Askari Mosque bombing in Samarra, the deadly single-day series of bombings in which at least 215 people were killed in Baghdad's Shiite district of Sadr City, and the second al-Askari bombing in 2007, they provoked Shiite militias to unleash a wave of retaliatory attacks, resulting in death squad-style killings and spiraling further sectarian violence which escalated in 2006 and brought Iraq to the brink of violent anarchy in 2007. In 2008, sectarian bombings blamed on al-Qaeda killed at least 42 people at the Imam Husayn Shrine in Karbala in March, and at least 51 people at a bus stop in Baghdad in June.
As of 2014, the group controls the city of Falluja.
Somalia and Yemen.
In Somalia, al-Qaeda agents had been collaborating closely with its Somali wing, which was created from the al-Shabaab group. In February 2012, al-Shabaab officially joined al-Qaeda, declaring loyalty in a joint video. The Somalian al-Qaeda actively recruit children for suicide-bomber training, and export young people to participate in military actions against Americans at the AfPak border.
The percentage of terrorist attacks in the West originating from the Afghanistan-Pakistan (AfPak) border declined considerably from almost 100% to 75% in 2007, and to 50% in 2010, as al-Qaeda shifted to Somalia and Yemen. While al-Qaeda leaders are hiding in the tribal areas along the AfPak border, the middle-tier of the movement display heightened activity in Somalia and Yemen.
“We know that South Asia is no longer their primary base,” a U.S. defense agency source said. “They are looking for a hide-out in other parts of the world, and continue to expand their organization.“
In January 2009, al-Qaeda’s division in Saudi Arabia merged with its Yemeni wing to form al-Qaeda in the Arabian Peninsula. Centered in Yemen, the group takes advantage of the country's poor economy, demography and domestic security. In August 2009, they made the first assassination attempt against a member of the Saudi royal dynasty in decades. President Obama asked his Yemen counterpart Ali Abdullah Saleh to ensure closer cooperation with the U.S. in the struggle against the growing activity of al-Qaeda in Yemen, and promised to send additional aid. Because of the wars in Iraq and Afghanistan, the U.S. was unable to pay sufficient attention to Somalia and Yemen, which could cause problems in the near future. In December 2011, U.S. Secretary of Defense Leon Panetta said that the U.S. operations against al-Qaeda "are now concentrating on key groups in Yemen, Somalia and North Africa." Al-Qaeda in the Arabian Peninsula claimed responsibility for the 2009 bombing attack on Northwest Airlines Flight 253 by Umar Farouk Abdulmutallab. The group released photos of Abdulmutallab smiling in a white shirt and white Islamic skullcap, with the al-Qaeda in Arabian Peninsula banner in the background.
United States operations.
In December 1998, the Director of Central Intelligence Counterterrorism Center reported to the president that al-Qaeda was preparing for attacks in the USA, including the training of personnel to hijack aircraft. On September 11, 2001, al-Qaeda attacked the United States, hijacking four airliners and deliberately crashing them. The attackers killed 2,977 people.
U.S. officials called Anwar al-Awlaki an "example of al-Qaeda reach into" the U.S. in 2008 after probes into his ties to the September 11 attacks hijackers. A former FBI agent identifies Awlaki as a known "senior recruiter for al-Qaeda", and a spiritual motivator. Awlaki's sermons in the U.S. were attended by three of the 9/11 hijackers, as well as accused Fort Hood shooter Nidal Malik Hasan. U.S. intelligence intercepted emails from Hasan to Awlaki between December 2008 and early 2009. On his website, Awlaki has praised Hasan's actions in the Fort Hood shooting.
An unnamed official claimed there was good reason to believe Awlaki "has been involved in very serious terrorist activities since leaving the U.S. 9/11, including plotting attacks against America and our allies.” In addition, "Christmas Day bomber" Umar Farouk Abdulmutallab said al-Awlaki was one of his al-Qaeda trainers, meeting with him and involved in planning or preparing the attack, and provided religious justification for it, according to unnamed U.S. intelligence officials. In March 2010, alAwlaki said in a videotape delivered to CNN that jihad against America was binding upon himself and every other able Muslim.
U.S. President Barack Obama approved the targeted killing of al-Awlaki by April 2010, making al-Awlaki the first U.S. citizen ever placed on the CIA target list. That required the consent of the U.S. National Security Council, and officials said it was appropriate for an individual who posed an imminent danger to national security. In May 2010, Faisal Shahzad, who pleaded guilty to the 2010 Times Square car bombing attempt, told interrogators he was "inspired by" al-Awlaki, and sources said Shahzad had made contact with al-Awlaki over the internet. Representative Jane Harman called him "terrorist number one", and "Investor's Business Daily" called him "the world's most dangerous man". In July 2010, the U.S. Treasury Department added him to its list of Specially Designated Global Terrorists, and the UN added him to its list of individuals associated with al-Qaeda. In August 2010, al-Awlaki's father initiated a lawsuit against the U.S. government with the American Civil Liberties Union, challenging its order to kill al-Awlaki. In October 2010, U.S. and U.K. officials linked al-Awlaki to the 2010 cargo plane bomb plot. In September 2011, he was killed in a targeted killing drone attack in Yemen. It was reported on March 16, 2012 that Osama bin Laden plotted to kill United States President Barack Obama.
Death of Osama bin Laden.
On May 1, 2011 in Washington, D.C. (May 2, Pakistan Standard Time), U.S. President Barack Obama announced that Osama bin Laden had been killed by "a small team of Americans" acting under Obama's direct orders, in a covert operation in Abbottabad, Pakistan, about north of Islamabad. According to U.S. officials a team of 20–25 US Navy SEALs under the command of the Joint Special Operations Command and working with the CIA stormed bin Laden's compound in two helicopters. Bin Laden and those with him were killed during a firefight in which U.S. forces experienced no injuries or casualties. According to one US official the attack was carried out without the knowledge or consent of the Pakistani authorities. In Pakistan some people were reported to be shocked at the unauthorized incursion by US armed forces. The site is a few miles from the Pakistan Military Academy in Kakul. In his broadcast announcement President Obama said that U.S. forces "took care to avoid civilian casualties."
Details soon emerged that three men and a woman were killed along with bin Laden, the woman being killed when she was “used as a shield by a male combatant”. DNA from bin Laden's body, compared with DNA samples on record from his dead sister, confirmed bin Laden's identity. The body was recovered by the US military and was in its custody until, according to one US official, his body was buried at sea according to Islamic traditions. One U.S. official stated that "finding a country willing to accept the remains of the world's most wanted terrorist would have been difficult." U.S State Department issued a "Worldwide caution" for Americans following bin Laden's death and U.S Diplomatic facilities everywhere were placed on high alert, a senior U.S official said. Crowds gathered outside the White House and in New York City's Times Square to celebrate bin Laden's death.
Syria.
In 2003, President Bashar Al-Assad revealed in an interview with a Kuwaiti newspaper that he doubted the organization of Al-Qaeda even existed. He was quoted as saying, "Is there really an entity called al-Qaeda? Was it in Afghanistan? Does it exist now?" He went on further to remark about Bin Laden commenting, he "cannot talk on the phone or use the Internet, but he can direct communications to the four corners of the world? This is illogical."
Following the mass protests that took place later in 2011 demanding the resignation of Al-Assad, Al-Qaeda affiliated organizations and Sunni sympathizers soon began to constitute the most effective fighting force in the Syrian opposition. Until then, al-Qaeda's presence in Syria was not worth mentioning, but its growth thereafter was rapid. Groups such as the Al-Nusra Front and the Islamic State of Iraq and the Levant (ISIS; sometimes ISIL) have recruited many foreign Mujahideen to train and fight in what has gradually become a highly sectarian war. Ideologically, the Syrian Civil War has served the interests of Al-Qaeda as it pits a mainly Sunni opposition against a Shia backed Alawite regime. Viewing Shia Islam as heretical, Al-Qaeda and other fundamentalist Sunni militant groups have invested heavily in the civil conflict, actively backing and supporting the Syrian Opposition despite its clashes with moderate opposition groups such as the Free Syrian Army (FSA).
On February 2, 2014, Al-Qaeda distanced itself from ISIS and its actions in Syria.
Attacks.
[[File:RecentAlQaedaAttacks.svg|thumb|360px|Map of major attacks attributed to al-Qaeda:
1. The Pentagon, US – Sep 11, 2001
2. World Trade Center, US – Sep 11, 2001
3. Istanbul, Turkey – Nov 15, 2003; Nov 20, 2003
4. Aden, Yemen – Oct 12, 2000
5. Nairobi, Kenya – Aug 7, 1998
6. Dar es Salaam, Tanzania – Aug 7, 1998]]
Al-Qaeda has carried out a total of six major terrorist attacks, four of them in its jihad against America. In each case the leadership planned the attack years in advance, arranging for the shipment of weapons and explosives and using its privatized businesses to provide operatives with safehouses and false identities.
Al-Qaeda usually does not disburse funds for attacks, and very rarely makes wire transfers.
1992.
On December 29, 1992, al-Qaeda's first terrorist attack took place as two bombs were detonated in Aden, Yemen. The first target was the Movenpick Hotel and the second was the parking lot of the Goldmohur Hotel.
The bombings were an attempt to eliminate American soldiers on their way to Somalia to take part in the international famine relief effort, Operation Restore Hope. Internally, al-Qaeda considered the bombing a victory that frightened the Americans away, but in the U.S. the attack was barely noticed.
No Americans were killed because the soldiers were staying in a different hotel altogether, and they went on to Somalia as scheduled. However little noticed, the attack was pivotal as it was the beginning of al-Qaeda's change in direction, from fighting armies to killing civilians. Two people were killed in the bombing, an Australian tourist and a Yemeni hotel worker. Seven others, mostly Yemenis, were severely injured.
Two fatwas are said to have been appointed by the most theologically knowledgeable of al-Qaeda's members, Mamdouh Mahmud Salim, to justify the killings according to Islamic law. Salim referred to a famous fatwa appointed by Ibn Taymiyyah, a 13th-century scholar much admired by Wahhabis, which sanctioned resistance by any means during the Mongol invasions.
1993 World Trade Center bombing.
In 1993, Ramzi Yousef used a truck bomb to attack the World Trade Center in New York City. The attack was intended to break the foundation of Tower One knocking it into Tower Two, bringing the entire complex down.
Yousef hoped this would kill 250,000 people. The towers shook and swayed but the foundation held and he succeeded in killing only six people (although he injured 1,042 others and caused nearly $300 million in property damage).
After the attack, Yousef fled to Pakistan and later moved to Manila. There he began developing the Bojinka plot plans to implode a dozen American airliners simultaneously, to assassinate Pope John Paul II and President Bill Clinton, and to crash a private plane into CIA headquarters. He was later captured in Pakistan.
None of the U.S. government's indictments against bin Laden have suggested that he had any connection with this bombing, but Ramzi Yousef is known to have attended a terrorist training camp in Afghanistan. After his capture, Yousef declared that his primary justification for the attack was to punish the U.S. for its support for the Israeli occupation of Palestinian territories and made no mention of any religious motivations. A follow-up attack was planned by Omar Abdel-Rahman - the New York City landmark bomb plot. However, the plot was foiled by the authorities.
Late 1990s.
In 1996, bin Laden personally engineered a plot to assassinate Clinton while the president was in Manila for the Asia-Pacific Economic Cooperation. However, intelligence agents intercepted a message just minutes before the motorcade was to leave, and alerted the U.S. Secret Service. Agents later discovered a bomb planted under a bridge.
The 1998 U.S. embassy bombings in East Africa resulted in upward of 300 deaths, mostly locals. A barrage of cruise missiles launched by the U.S. military in response devastated an al-Qaeda base in Khost, Afghanistan, but the network's capacity was unharmed. In late 1999/2000, Al-Qaeda planned attacks to coincide with the millennium, masterminded by Abu Zubaydah and involving Abu Qatada, which would include the bombing Christian holy sites in Jordan, the bombing of Los Angeles International Airport by Ahmed Ressam, and the bombing of the .
In October 2000, al-Qaeda militants in Yemen bombed the missile destroyer "U.S.S. Cole" in a suicide attack, killing 17 U.S. servicemen and damaging the vessel while it lay offshore. Inspired by the success of such a brazen attack, al-Qaeda's command core began to prepare for an attack on the U.S. itself.
September 11 attacks.
The September 11, 2001 attacks were the most devastating terrorist acts in American history, killing approximately 3,000 people. Two commercial airliners were deliberately flown into the World Trade Center towers, a third into The Pentagon, and a fourth, originally intended to target the United States Capitol, crashed in a field in Stonycreek Township near Shanksville, Pennsylvania.
The attacks were conducted by al-Qaeda, acting in accord with the 1998 "fatwa" issued against the U.S. and its allies by military forces under the command of bin Laden, al-Zawahiri, and others. Evidence points to suicide squads led by al-Qaeda military commander Mohamed Atta as the culprits of the attacks, with bin Laden, Ayman al-Zawahiri, Khalid Sheikh Mohammed, and Hambali as the key planners and part of the political and military command.
Messages issued by bin Laden after September 11, 2001 praised the attacks, and explained their motivation while denying any involvement. Bin Laden legitimized the attacks by identifying grievances felt by both mainstream and Islamist Muslims, such as the general perception that the U.S. was actively oppressing Muslims.
Bin Laden asserted that America was massacring Muslims in 'Palestine, Chechnya, Kashmir and Iraq' and that Muslims should retain the 'right to attack in reprisal'. He also claimed the 9/11 attacks were not targeted at women and children, but 'America's icons of military and economic power'.
Evidence has since come to light that the original targets for the attack may have been nuclear power stations on the east coast of the U.S. The targets were later altered by al-Qaeda, as it was feared that such an attack "might get out of hand".
Designation as terrorist organization.
Al-Qaeda has been designated a "terrorist organization" by the following countries and international organizations:
War on Terrorism.
In the immediate aftermath of the attacks, the U.S. government decided to respond militarily, and began to prepare its armed forces to overthrow the Taliban regime it believed was harboring al-Qaeda. Before the U.S. attacked, it offered Taliban leader Mullah Omar a chance to surrender bin Laden and his top associates. The first forces to be inserted into Afghanistan were Paramilitary Officers from the CIA's elite Special Activities Division (SAD).
The Taliban offered to turn over bin Laden to a neutral country for trial if the U.S. would provide evidence of bin Laden's complicity in the attacks. U.S. President George W. Bush responded by saying: "We know he's guilty. Turn him over", and British Prime Minister Tony Blair warned the Taliban regime: "Surrender bin Laden, or surrender power".
Soon thereafter the U.S. and its allies invaded Afghanistan, and together with the Afghan Northern Alliance removed the Taliban government in the war in Afghanistan.
As a result of the U.S. using its special forces and providing air support for the Northern Alliance ground forces, both Taliban and al-Qaeda training camps were destroyed, and much of the operating structure of al-Qaeda is believed to have been disrupted. After being driven from their key positions in the Tora Bora area of Afghanistan, many al-Qaeda fighters tried to regroup in the rugged Gardez region of the nation.
Again, under the cover of intense aerial bombardment, U.S. infantry and local Afghan forces attacked, shattering the al-Qaeda position and killing or capturing many of the militants. By early 2002, al-Qaeda had been dealt a serious blow to its operational capacity, and the Afghan invasion appeared an initial success. Nevertheless, a significant Taliban insurgency remains in Afghanistan, and al-Qaeda's top two leaders, bin Laden and al-Zawahiri, evaded capture.
Debate raged about the exact nature of al-Qaeda's role in the 9/11 attacks, and after the U.S. invasion began, the U.S. State Department also released a videotape showing bin Laden speaking with a small group of associates somewhere in Afghanistan shortly before the Taliban was removed from power. Although its authenticity has been questioned by some, the tape appears to implicate bin Laden and al-Qaeda in the September 11 attacks and was aired on many television channels all over the world, with an accompanying English translation provided by the U.S. Defense Department.
In September 2004, the US government 9/11 Commission investigating the September 11 attacks officially concluded that the attacks were conceived and implemented by al-Qaeda operatives. In October 2004, bin Laden appeared to claim responsibility for the attacks in a videotape released through Al Jazeera, saying he was inspired by Israeli attacks on high-rises in the 1982 invasion of Lebanon: "As I looked at those demolished towers in Lebanon, it entered my mind that we should punish the oppressor in kind and that we should destroy towers in America in order that they taste some of what we tasted and so that they be deterred from killing our women and children."
By the end of 2004, the U.S. government proclaimed that two-thirds of the most senior al-Qaeda figures from 2001 had been captured and interrogated by the CIA: Abu Zubaydah, Ramzi bin al-Shibh and Abd al-Rahim al-Nashiri in 2002; Khalid Sheikh Mohammed in 2003; and Saif al Islam el Masry in 2004. Mohammed Atef and several others were killed. The West was criticised for not being able to comprehend or deal with Al-Qaida despite more than a decade of the war. This also meant no progress has been made in global state security.
Activities.
Africa.
Al-Qaeda involvement in Africa has included a number of bombing attacks in North Africa, as well as supporting parties in civil wars in Eritrea and Somalia. From 1991 to 1996, bin Laden and other al-Qaeda leaders were based in Sudan.
Islamist rebels in the Sahara calling themselves al-Qaeda in the Islamic Maghreb have stepped up their violence in recent years. French officials say the rebels have no real links to the al-Qaeda leadership, but this is a matter of some dispute in the international press and amongst security analysts. It seems likely that bin Laden approved the group's name in late 2006, and the rebels "took on the al Qaeda franchise label", almost a year before the violence began to escalate.
In Mali, the Ansar Dine faction was also reported as an ally of Al-Qaeda in 2013. The Ansar al Dine faction aligned themselves with the AQIM.
Following the Libyan Civil War, the removal of Gaddafi and the ensuing period of post-civil war violence in Libya allowed various Islamist militant organizations affiliated with Al-Qaeda to expand their operations in the region. The 2012 Benghazi attack, which resulted in the death of U.S. Ambassador J. Christopher Stevens and 3 other Americans, is suspected of having been carried out by various Jihadist networks, such as Al-Qaeda in the Islamic Maghreb, Ansar al-Sharia and several other Al-Qaeda affiliated groups. The capture of Nazih Abdul-Hamed al-Ruqai, a senior Al-Qaeda operative wanted by the United States for his involvement in the 1998 United States embassy bombings, on October 5, 2013 by U.S. Navy Seals, FBI and CIA agents illustrates the importance the U.S. and other Western allies have placed on North Africa.
Europe.
Before the 9/11 attacks and the U.S. invasion of Afghanistan, recruits at Al-Qaeda training camps who had Western backgrounds were especially sought after by Al-Qaeda's military wing for conducting operations overseas. Language skills and knowledge of Western culture were generally found among recruits from Europe, such was the case with Mohamed Atta, an Egyptian national studying in Germany at the time of his training, and other members of the Hamburg Cell. Osama bin Laden and Mohamed Atef would later designate Atta as the ringleader of the 9/11 hijackers. Following the attacks, Western intelligence agencies determined that Al-Qaeda cells operating in Europe had aided the hijackers with financing and communications with the central leadership based in Afghanistan.
In 2003, Islamists carried out a series of bombings in Istanbul killing fifty-seven people and injuring seven hundred. Seventy-four people were charged by the Turkish authorities. Some had previously met bin Laden, and though they specifically declined to pledge allegiance to al-Qaeda they asked for its blessing and help.
In 2009, three Londoners, Tanvir Hussain, Assad Sarwar and Ahmed Abdullah Ali, were convicted of conspiring to detonate bombs disguised as soft drinks on seven airplanes bound for Canada and the U.S. The massively complex police and MI5 investigation of the plot involved more than a year of surveillance work conducted by over two hundred officers. British and U.S. officials said the plan—unlike many recent homegrown European terrorist plots—was directly linked to al-Qaeda and guided by senior Islamic militants in Pakistan.
In 2012, Russian Intelligence indicated that al-Qaeda had given a call for "forest jihad" and has been starting massive forest fires as part of a strategy of "thousand cuts".
Arab world.
Following Yemeni unification in 1990, Wahhabi networks began moving missionaries into the country in an effort to subvert the capitalist north. Although it is unlikely bin Laden or Saudi al-Qaeda were directly involved, the personal connections they made would be established over the next decade and used in the "USS Cole" bombing. Concerns grow over Al Qaeda's group in Yemen.
In Iraq, al-Qaeda forces loosely associated with the leadership were embedded in the Jama'at al-Tawhid wal-Jihad organization commanded by Abu Musab al-Zarqawi. Specializing in suicide operations, they have been a "key driver" of the Sunni insurgency. Although they played a small part in the overall insurgency, between 30% and 42% of all suicide bombings which took place in the early years were claimed by Zarqawi's organization. Reports have indicated that oversights such as the failure to control access to the Qa'qaa munitions factory in Yusufiyah have allowed large quantities of munitions to fall into the hands of al-Qaida. In November 2010, the Islamic State of Iraq militant group, which is linked to al-Qaeda in Iraq, threatened to "exterminate Iraqi Christians".
Significantly, it was not until the late 1990s that al-Qaeda began training Palestinians. This is not to suggest that resistance fighters are underrepresented in the network as a number of Palestinians, mostly coming from Jordan, wanted to join and have risen to serve high-profile roles in Afghanistan. Rather, large groups such as Hamas and Palestinian Islamic Jihad—which cooperate with al-Qaeda in many respects—have had difficulties accepting a strategic alliance, fearing that al-Qaeda will co-opt their smaller cells. This may have changed recently, as Israeli security and intelligence services believe al-Qaeda has managed to infiltrate operatives from the Occupied Territories into Israel, and is waiting for the right time to mount an attack.
Kashmir.
Bin Laden and Ayman al-Zawahiri consider India to be a part of the ‘Crusader-Zionist-Hindu’ conspiracy against the Islamic world. According to the 2005 report 'Al Qaeda: Profile and Threat Assessment' by Congressional Research Service, bin Laden was involved in training militants for Jihad in Kashmir while living in Sudan in the early nineties. By 2001, Kashmiri militant group Harkat-ul-Mujahideen had become a part of the al-Qaeda coalition. According to the United Nations High Commissioner for Refugees al-Qaeda was thought to have established bases in Pakistan-administered Kashmir (in Azad Kashmir, and to some extent in Gilgit–Baltistan) during the 1999 Kargil War and continued to operate there with tacit approval of Pakistan's Intelligence services.
Many of the militants active in Kashmir were trained in the same Madrasahs as Taliban and al-Qaeda. Fazlur Rehman Khalil of Kashmiri militant group Harkat-ul-Mujahideen was a signatory of al-Qaeda's 1998 declaration of Jihad against America and its allies. In a 'Letter to American People' written by bin Laden in 2002 he stated that one of the reasons he was fighting America is because of its support to India on the Kashmir issue. In November 2001, Kathmandu airport went on high alert after threats that bin Laden planned to hijack a plane from there and crash it into a target in New Delhi. In 2002, U.S. Secretary of Defense Donald Rumsfeld, on a trip to Delhi, suggested that al-Qaeda was active in Kashmir though he did not have any hard evidence. He proposed hi tech ground sensors along the line of control to prevent militants from infiltrating into Indian administered Kashmir.
An investigation in 2002 unearthed evidence that al-Qaeda and its affiliates were prospering in Pakistan-administered Kashmir with tacit approval of Pakistan's National Intelligence agency Inter-Services Intelligence In 2002, a special team of Special Air Service and Delta Force was sent into Indian Administered Kashmir to hunt for bin Laden after reports that he was being sheltered by Kashmiri militant group Harkat-ul-Mujahideen which had previously been responsible for 1995 Kidnapping of western tourists in Kashmir. Britain's highest ranking al-Qaeda operative Rangzieb Ahmed had previously fought in Kashmir with the group Harkat-ul-Mujahideen and spent time in Indian prison after being captured in Kashmir.
U.S. officials believe that al-Qaeda was helping organize a campaign of terror in Kashmir in order to provoke conflict between India and Pakistan. Their strategy was to force Pakistan to move its troops to the border with India, thereby relieving pressure on al-Qaeda elements hiding in northwestern Pakistan. In 2006 al-Qaeda claimed they had established a wing in Kashmir; this has worried the Indian government. However the Indian Army Lt. Gen. H.S. Panag, GOC-in-C Northern Command, said to reporters that the army has ruled out the presence of al-Qaeda in Indian-administered Jammu and Kashmir; furthermore he said that there is nothing that can verify reports from the media of al-Qaeda presence in the state. He however stated that al-Qaeda had strong ties with Kashmiri militant groups Lashkar-e-Taiba and Jaish-e-Mohammed based in Pakistan. It has been noted that Waziristan has now become the new battlefield for Kashmiri militants fighting NATO in support of al-Qaeda and Taliban. Dhiren Barot, who wrote the "Army of Madinah In Kashmir" and was an al-Qaeda operative convicted for involvement in the 2004 financial buildings plot, had received training in weapons and explosives at a militant training camp in Kashmir.
Maulana Masood Azhar, the founder of another Kashmiri group Jaish-e-Mohammed, is believed to have met bin Laden several times and received funding from him. In 2002, Jaish-e-Mohammed organized the kidnapping and murder of Daniel Pearl in an operation run in conjunction with al-Qaeda and funded by bin Laden. According to American counter-terrorism expert Bruce Riedel, al-Qaeda and Taliban were closely involved in the 1999 hijacking of Indian Airlines Flight 814 to Kandahar which led to the release of Maulana Masood Azhar & Ahmed Omar Saeed Sheikh from an Indian prison in exchange for the passengers. This hijacking, Riedel stated, was rightly described by then Indian Foreign minister Jaswant Singh as a 'dress rehearsal' for September 11 attacks. Bin Laden personally welcomed Azhar and threw a lavish party in his honor after his release, according to Abu Jandal, bodyguard of bin Laden. Ahmed Omar Saeed Sheikh, who had been in Indian prison for his role in 1994 kidnappings of Western tourists in India, went on to murder Daniel Pearl and was sentenced to death by Pakistan. Al-Qaeda operative Rashid Rauf, who was one of the accused in 2006 transatlantic aircraft plot, was related to Maulana Masood Azhar by marriage.
Lashkar-e-Taiba, a Kashmiri militant group which is thought to be behind 2008 Mumbai attacks, is also known to have strong ties to senior al-Qaeda leaders living in Pakistan. In Late 2002, top al-Qaeda operative Abu Zubaydah was arrested while being sheltered by Lashkar-e-Taiba in a safe house in Faisalabad. The FBI believes that al-Qaeda and Lashkar have been 'intertwined' for a long time while the CIA has said that al-Qaeda funds Lashkar-e-Taiba. French investigating magistrate Jean-Louis Bruguière, who was the top French counter-terrorism official, told Reuters in 2009 that 'Lashkar-e-Taiba is no longer a Pakistani movement with only a Kashmir political or military agenda. Lashkar-e-Taiba is a member of al-Qaeda.'
In a video released in 2008, senior al-Qaeda operative American-born Adam Yahiye Gadahn stated that "victory in Kashmir has been delayed for years; it is the liberation of the jihad there from this interference which, Allah willing, will be the first step towards victory over the Hindu occupiers of that Islam land."
In September 2009, a U.S. drone strike reportedly killed Ilyas Kashmiri who was the chief of Harkat-ul-Jihad al-Islami, a Kashmiri militant group associated with al-Qaeda. Kashmiri was described by Bruce Riedel as a 'prominent' al-Qaeda member while others have described him as head of military operations for al-Qaeda. Kashmiri was also charged by the U.S. in a plot against Jyllands-Posten, the Danish newspaper which was at the center of Jyllands-Posten Muhammad cartoons controversy. U.S. officials also believe that Kashmiri was involved in the Camp Chapman attack against the CIA. In January 2010, Indian authorities notified Britain of an al-Qaeda plot to hijack an Indian airlines or Air India plane and crash it into a British city. This information was uncovered from interrogation of Amjad Khwaja, an operative of Harkat-ul-Jihad al-Islami, who had been arrested in India.
In January 2010, U.S. Defense secretary Robert Gates, while on a visit to Pakistan, stated that al-Qaeda was seeking to destabilize the region and planning to provoke a nuclear war between India and Pakistan.
Internet.
Timothy L. Thomas claims that in the wake of its evacuation from Afghanistan, al-Qaeda and its successors have migrated online to escape detection in an atmosphere of increased international vigilance. As a result, the organization's use of the Internet has grown more sophisticated, encompassing financing, recruitment, networking, mobilization, publicity, as well as information dissemination, gathering and sharing.
Abu Ayyub al-Masri’s al-Qaeda movement in Iraq regularly releases short videos glorifying the activity of jihadist suicide bombers. In addition, both before and after the death of Abu Musab al-Zarqawi (the former leader of al-Qaeda in Iraq), the umbrella organization to which al-Qaeda in Iraq belongs, the Mujahideen Shura Council, has a regular presence on the Web.
The range of multimedia content includes guerrilla training clips, stills of victims about to be murdered, testimonials of suicide bombers, and videos that show participation in jihad through stylized portraits of mosques and musical scores. A website associated with al-Qaeda posted a video of captured American entrepreneur Nick Berg being decapitated in Iraq. Other decapitation videos and pictures, including those of Paul Johnson, Kim Sun-il, and Daniel Pearl, were first posted on jihadist websites.
In December 2004 an audio message claiming to be from bin Laden was posted directly to a website, rather than sending a copy to al Jazeera as he had done in the past.
Al-Qaeda turned to the Internet for release of its videos in order to be certain it would be available unedited, rather than risk the possibility of al Jazeera editors editing the videos and cutting out anything critical of the Saudi royal family. Bin Laden's December 2004 message was much more vehement than usual in this speech, lasting over an hour.
In the past, Alneda.com and Jehad.net were perhaps the most significant al-Qaeda websites. Alneda was initially taken down by American Jon Messner, but the operators resisted by shifting the site to various servers and strategically shifting content.
The U.S. is currently attempting to extradite a British information technology specialist, Babar Ahmad, on charges of operating a network of English-language al-Qaeda websites, such as Azzam.com. Ahmad's extradition is opposed by various British Muslim organizations, such as the Muslim Association of Britain.
Online Communications.
In 2007, Al-Qaeda released Mujahedeen Secrets, encryption software used for online and cellular communications. A later version, Mujahideen Secrets 2, was released in 2008.
Aviation network.
Al-Qaeda is believed to be operating a clandestine aviation network including “several Boeing 727 aircraft”, turboprops and executive jets, according to a Reuters story. Based on a U.S. Department of Homeland Security report, the story said that al-Qaeda is possibly using aircraft to transport drugs and weapons from South America to various unstable countries in West Africa. A Boeing 727 can carry up to 10 tons of cargo. The drugs eventually are smuggled to Europe for distribution and sale, and the weapons are used in conflicts in Africa and possibly elsewhere. Gunmen with links to al-Qaeda have been increasingly kidnapping some Europeans for ransom. The profits from the drug and weapon sales, and kidnappings can, in turn, fund more militant activities.
Involvement in military conflicts.
The following is a list of military conflicts in which Al-Qaeda and its direct affiliates have taken part militarily. 
Alleged CIA involvement.
Munir Akram, Permanent Representative of Pakistan to the United Nations from 2002 to 2008, wrote in a letter published in the New York Times on January 19, 2008:
The strategy to support the Afghans against Soviet military intervention was evolved by several intelligence agencies, including the C.I.A. and Inter-Services Intelligence, or ISI. After the Soviet withdrawal, the Western powers walked away from the region, leaving behind 40,000 militants imported from several countries to wage the anti-Soviet jihad. Pakistan was left to face the blowback of extremism, drugs and guns.
A variety of sources—CNN journalist Peter Bergen, Pakistani ISI Brigadier Mohammad Yousaf, and CIA operatives involved in the Afghan program, such as Vincent Cannistraro—deny that the CIA or other American officials had contact with the foreign mujahideen or bin Laden, let alone armed, trained, coached or indoctrinated them.
Bergen and others argue that there was no need to recruit foreigners unfamiliar with the local language, customs or lay of the land since there were a quarter of a million local Afghans willing to fight; that foreign mujahideen themselves had no need for American funds since they received several hundred million dollars a year from non-American, Muslim sources; that Americans could not have trained mujahideen because Pakistani officials would not allow more than a handful of them to operate in Pakistan and none in Afghanistan; and that the Afghan Arabs were almost invariably militant Islamists reflexively hostile to Westerners whether or not the Westerners were helping the Muslim Afghans.
According to Bergen, known for conducting the first television interview with bin Laden in 1997, the idea that "the CIA funded bin Laden or trained bin Laden... a folk myth. There's no evidence of this... Bin Laden had his own money, he was anti-American and he was operating secretly and independently... The real story here is the CIA didn't really have a clue about who this guy was until 1996 when they set up a unit to really start tracking him." But Bergen conceded that, in one "strange incident," the CIA appeared to give visa help to mujahideen-recruiter Omar Abdel-Rahman.
In his widely praised account of al-Qaeda, English journalist Jason Burke wrote:
Broader influence.
Anders Behring Breivik, the perpetrator of the 2011 Norway attacks, was inspired by al-Qaeda, calling it "the most successful revolutionary movement in the world." While admitting different aims, he sought to "create a European version of al-Qaida."
Criticism.
According to a number of sources there has been a "wave of revulsion" against al-Qaeda and its affiliates by "religious scholars, former fighters and militants" alarmed by al-Qaeda's takfir and killing of Muslims in Muslim countries, especially Iraq.
Noman Benotman, a former Afghan Arab and militant of the Libyan Islamic Fighting Group, went public with an open letter of criticism to Ayman al-Zawahiri in November 2007 after persuading imprisoned senior leadership of his former group to enter into peace negotiations with the Libyan regime. While Ayman al-Zawahiri announced the affiliation of the group with al-Qaeda in November 2007, the Libyan government released 90 members of the group from prison several months later after "they were said to have renounced violence."
In 2007, around the sixth anniversary of the September 11 attacks and a couple of months before "Rationalizing Jihad" first appeared in the newspapers, the Saudi sheikh Salman al-Ouda delivered a personal rebuke to bin Laden. Al-Ouda, a religious scholar and one of the fathers of the Sahwa, the fundamentalist awakening movement that swept through Saudi Arabia in the 1980s, is a widely respected critic of jihadism. Al-Ouda addressed al-Qaeda's leader on television asking him
My brother Osama, how much blood has been spilt? How many innocent people, children, elderly, and women have been killed... in the name of al-Qaeda? Will you be happy to meet God Almighty carrying the burden of these hundreds of thousands or millions victims on your back?
According to Pew polls, support for al-Qaeda has been slightly dropped for parts of the Muslim world in the years before 2008. The numbers supporting suicide bombings in Indonesia, Lebanon, and Bangladesh, for instance, have dropped by half or more in the last five years. In Saudi Arabia, only 10 percent now have a favorable view of al-Qaeda, according to a December poll by Terror Free Tomorrow, a Washington-based think tank.
In 2007, the imprisoned Sayyed Imam Al-Sharif, an influential Afghan Arab, "ideological godfather of al-Qaeda", and former supporter of takfir, sensationally withdrew his support from al-Qaeda with a book "Wathiqat Tarshid Al-'Aml Al-Jihadi fi Misr w'Al-'Alam" ("Rationalizing Jihad in Egypt and the World").
Although once associated with al-Qaeda, in September 2009 LIFG completed a new "code" for jihad, a 417-page religious document entitled "Corrective Studies". Given its credibility and the fact that several other prominent Jihadists in the Middle East have turned against al-Qaeda, the LIFG's about face may be an important step toward staunching al-Qaeda's recruitment.
See also.
Publications:

</doc>
<doc id="1923" url="http://en.wikipedia.org/wiki?curid=1923" title="Alessandro Volta">
Alessandro Volta

Alessandro Giuseppe Antonio Anastasio Volta (February 18, 1745 – March 5, 1827) was an Italian physicist known for the invention of the battery in the 1800s.
Early life and works.
Volta was born in Como, a town in present-day northern Italy (near the Swiss border) on February 18, 1745. In 1774, he became a professor of physics at the Royal School in Como. A year later, he improved and popularized the electrophorus, a device that produced static electricity. His promotion of it was so extensive that he is often credited with its invention, even though a machine operating on the same principle was described in 1762 by the Swedish experimenter Johan Wilcke.
In the years between 1776–78, Volta studied the chemistry of gases. He discovered methane after reading a paper by Benjamin Franklin of America on "flammable air", and Volta searched for it carefully in Italy. In November 1776, he found methane at Lake Maggiore, and by 1778 he managed to isolate methane. He devised experiments such as the ignition of methane by an electric spark in a closed vessel. Volta also studied what we now call electrical capacitance, developing separate means to study both electrical potential ("V" ) and charge ("Q" ), and discovering that for a given object, they are proportional. This may be called Volta's Law of capacitance, and it was for this work the unit of electrical potential has been named the volt.
In 1779 he became a professor of experimental physics at the University of Pavia, a chair that he occupied for almost 40 years. In 1794, Volta married an aristocratic lady also from Como, Teresa Peregrini, with whom he raised three sons: Giovanni, Flaminio and Zanino.
Volta and Galvani.
Luigi Galvani discovered something he named "animal electricity" when two different metals were connected in series with the frog's leg and to one another. Volta realized that the frog's leg served as both a conductor of electricity (what we would now call an electrolyte) and as a detector of electricity. He replaced the frog's leg with brine-soaked paper, and detected the flow of electricity by other means familiar to him from his previous studies. In this way he discovered the electrochemical series, and the law that the electromotive force (emf) of a galvanic cell, consisting of a pair of metal electrodes separated by electrolyte, is the difference between their two electrode potentials (thus, two identical electrodes and a common electrolyte give zero net emf). This may be called Volta's Law of the electrochemical series.
In 1800 as the result of a professional disagreement over the galvanic response advocated by Galvani, he invented the voltaic pile, an early electric battery, which produced a steady electric current. Volta had determined that the most effective pair of dissimilar metals to produce electricity was zinc and silver. Initially he experimented with individual cells in series, each cell being a wine goblet filled with brine into which the two dissimilar electrodes were dipped. The voltaic pile replaced the goblets with cardboard soaked in brine.
First battery.
In announcing his discovery of his voltaic pile, Volta paid tribute to the influences of William Nicholson, Tiberius Cavallo, and Abraham Bennet.
The battery made by Volta is credited as the first electrochemical cell. It consists of two electrodes: one made of zinc, the other of copper. The electrolyte is either sulfuric acid mixed with water or a form of saltwater brine. The electrolyte exists in the form 2H+ and SO42−. The zinc, which is higher than both copper and hydrogen in the electrochemical series, reacts with the negatively charged sulfate (SO42−). The positively charged hydrogen ions (protons) capture electrons from the copper, forming bubbles of hydrogen gas, H2. This makes the zinc rod the negative electrode and the copper rod the positive electrode.
Thus, there are two terminals, and an electric current will flow if they are connected. The chemical reactions in this voltaic cell are as follows:
The copper does not react, but rather it functions as an electrode for the electric current.
However, this cell also has some disadvantages. It is unsafe to handle, since sulfuric acid, even if diluted, can be hazardous. Also, the power of the cell diminishes over time because the hydrogen gas is not released. Instead, it accumulates on the surface of the zinc electrode and forms a barrier between the metal and the electrolyte solution.
Last years and retirement.
In honor of his work, Volta was made a count by Napoleon Bonaparte in 1801. Furthermore, his image was depicted upon the Italian 10,000 lira note (no longer in circulation, since the lira has been replaced by the euro) along with a sketch of his well-known voltaic pile.
Volta retired in 1819 to his estate in Camnago, a frazione of Como, Italy, now named "Camnago Volta" in his honor. He died there on March 5, 1827. Volta's remains were also buried in Camnago Volta.
Volta's legacy is celebrated by the Tempio Voltiano memorial located in the public gardens by the lake. There is also a museum which has been built in his honor, and it exhibits some of the original equipment that Volta used to conduct experiments. Not far away stands the Villa Olmo, which houses the Voltian Foundation, an organization promoting scientific activities. Volta carried out his experimental studies and produced his first inventions near Como.
Religious beliefs.
Volta was raised as a Catholic and for all of his life continued to maintain his belief. Because he was not ordained a clergyman, like his family expected, he was sometimes accused of being irreligious and some people have speculated about his possible unbelief, stressing that "he did not join the Church", or that he virtually "ignored the church's call". Nevertheless, he casted out doubts with a declaration of faith in which he said:
I do not understand how anyone can doubt the sincerity and constancy of my attachment to the rehgion which I profess, the Roman, Catholic and Apostolic religion in which I was born and brought up, and of which 1 have always made confession, externally and internally... I have, indeed, and only too often, failed in the performance of those good works which are the mark of a Catholic Christian, and I have been guilty of many sins: but through the special mercy of God I have never, as far as I know, wavered in my faith. In this faith I recognise a pure gift of God, a supernatural grace ; but I have not neglected those human means which confirm belief, and overthrow the doubts which at times arise. I studied attentively the grounds and basis of religion, the works of apologists and assailants, the reasons for and against, and I can say that the result of such study is to clothe religion with such a degree of probability, even for the merely natural reason, that every spirit unperverted by sin and passion, every naturally noble spirit must love and accept it. May this confession which has been asked from me and which I willingly give, written and subscribed by my own hand, with authority to show it to whomsoever you will, for I am not ashamed of the Gospel, may it produce some good fruit.
Publications.
"De vi attractiva ignis electrici" (1769) ("On the attractive force of electric fire")

</doc>
<doc id="1924" url="http://en.wikipedia.org/wiki?curid=1924" title="Argo Navis">
Argo Navis

Argo Navis (or simply Argo) was a large constellation in the southern sky that has since been divided into three constellations. It represented the "Argo", the ship used by Jason and the Argonauts in Greek mythology. The abbreviation was "Arg" and the genitive was "Argus Navis".
Argo Navis is the only one of the 48 constellations listed by the 2nd century astronomer Ptolemy that is no longer officially recognized as a constellation. It was unwieldy due to its enormous size: were it still considered a single constellation, it would be the largest of all. In 1752, the French astronomer Nicolas Louis de Lacaille subdivided it into Carina (the keel, or the hull, of the ship), Puppis (the poop deck, or stern), Vela (the sails), and, according to some, Pyxis (the compass, formerly the mast). When Argo Navis was split, its Bayer designations were also split. Carina has the α, β and ε, Vela has γ and δ, Puppis has ζ, and so on.
The constellation Pyxis (the mariner's compass) occupies an area which in antiquity was considered part of Argo's mast (called Malus). While its Bayer designations are separate from those of Carina, Puppis and Vela, having its own α, β and γ, for example, various modern authorities hold that Pyxis was in fact part of the Greek conception of Argo Navis.
The Maori had several names for what was the constellation Argo, including "Te Waka-o-Tamarereti", "Te Kohi-a-Autahi", and "Te Kohi".

</doc>
<doc id="1925" url="http://en.wikipedia.org/wiki?curid=1925" title="Andromeda (mythology)">
Andromeda (mythology)

In Greek mythology, Andromeda is the daughter of Cepheus, an Aethiopian king, and Cassiopeia. When Cassiopeia's hubris leads her to boast that Andromeda is more beautiful than the Nereids, Poseidon, influenced by Hades sends a sea monster, Cetus, to ravage Aethiopia as divine punishment. Andromeda is stripped and chained naked to a rock as a sacrifice to sate the monster, but is saved from death by Perseus.
Her name is the Latinized form of the Greek ("Androméda") or ("Andromédē"): "ruler of men", from ("anēr, andrós") "man", and "medon", "ruler".
As a subject, Andromeda has been popular in art since classical times; it is one of several Greek myths of a Greek hero's rescue of the intended victim of an archaic Hieros gamos (sacred marriage), giving rise to the "princess and dragon" motif. From the Renaissance, interest revived in the original story, typically as derived from Ovid's account.
Mythology.
In Greek mythology, Andromeda was the daughter of Cepheus and Cassiopeia, king and queen of the North African kingdom of Aethiopia.
Her mother Cassiopeia boasted that her daughter was more beautiful than the Nereids, the nymph-daughters of the sea god Nereus and often seen accompanying Poseidon. To punish the queen for her arrogance, Poseidon, brother to Zeus and god of the sea, sent a sea monster named Cetus to ravage the coast of Aethiopia including the kingdom of the vain queen. The desperate king consulted the Oracle of Apollo, who announced that no respite would be found until the king sacrificed his daughter, Andromeda, to the monster. Stripped naked, she was chained to a rock on the coast.
Perseus was returning from having slain the Gorgon Medusa. After he happened upon the chained Andromeda, he approached Cetus while invisible (for he was wearing Hades's helm), and killed the sea monster. He set Andromeda free, and married her in spite of her having been previously promised to her uncle Phineus. At the wedding a quarrel took place between the rivals and Phineus was turned to stone by the sight of the Gorgon's head.
Andromeda followed her husband, first to his native island of Serifos, where he rescued his mother Danaë, and then to Tiryns in Argos. Together, they became the ancestors of the family of the "Perseidae" through the line of their son Perses. Perseus and Andromeda had seven sons: Perses, Alcaeus, Heleus, Mestor, Sthenelus, Electryon, and Cynurus as well as two daughters, Autochthe and Gorgophone. Their descendants ruled Mycenae from Electryon down to Eurystheus, after whom Atreus attained the kingdom, and would also include the great hero Heracles. According to this mythology, Perseus is the ancestor of the Persians.
At the port city of Jaffa (today part of Tel Aviv) an outcrop of rocks near the harbour has been associated with the place of Andromeda's chaining and rescue by the traveler Pausanias, the geographer Strabo and the historian of the Jews Josephus.
After Andromeda's death, as Euripides had promised Athena at the end of his "Andromeda", produced in 412 BCE, the goddess placed her among the constellations in the northern sky, near Perseus and Cassiopeia; the constellation Andromeda, so known since antiquity, is named after her.
Constellations.
Andromeda is represented in the northern sky by the constellation Andromeda, which contains the Andromeda Galaxy.
Four constellations are associated with the myth. Viewing the fainter stars visible to the naked eye, the constellations are rendered as:
Other constellations related to the story are:
Portrayals of the myth.
Italian composer Salvatore Sciarrino composed an hour-long operatic drama called Perseo e Andromeda in 2000.
Sophocles and Euripides (and in more modern times, Corneille) made the story the subject of tragedies, and its incidents were represented in numerous ancient works of art. Jean-Baptiste Lully's opera, "Persée", also dramatizes the myth.
Andromeda has been the subject of numerous ancient and modern works of art, including, Rembrandt's "Andromeda Chained to the Rocks", one of Titian's "poesies" (Wallace Collection), and compositions by Joachim Wtewael (Louvre), Veronese (Rennes), Rubens, Ingres, and Gustave Moreau. From the Renaissance onward the chained nude figure of Andromeda typically was the centre of interest, and often she was shown alone, fearfully awaiting the monster.
In year 1973, animated film called "Perseus" (20 minutes) was made in the Soviet Union as part of Soviet Union animated film collection called "Legends and mуths of Ancient Greece".
The 1981 film "Clash of the Titans" retells the story of Perseus, Andromeda, and Cassiopeia, but makes a few changes (notably Cassiopeia boasts that her daughter is more beautiful than Thetis as opposed to the Nereids as a group). Thetis was a Nereid, but also the future mother of Achilles. Andromeda and Perseus meet and fall in love after he saves her soul from the enslavement of Thetis' hideous son, Calibos, whereas in the myth, they simply meet as Perseus returns home from having slain Medusa. In the film, the monster is called a kraken, although it is depicted as a lizard-like creature rather than a squid; and combining two elements of the myth, Perseus defeats the sea monster by showing it Medusa's face, turning the monster into stone. Andromeda is depicted as being strong-willed and independent, whereas in the stories she is only really mentioned as being the princess whom Perseus saves from the sea monster. Andromeda was portrayed by Judi Bowker in this film.
Andromeda also features in the 2010 film "Clash of the Titans", a remake of the 1981 version. Several changes were made in regard to the myth, most notably that Perseus did not marry Andromeda after he rescued her from the sea monster. Andromeda was portrayed by Alexa Davalos. The character was played by Rosamund Pike in the sequel "Wrath of the Titans", the second of a planned trilogy. In the end of the sequel, Perseus and Andromeda begin a relationship.
In the Japanese anime "Saint Seiya" the character, Shun, represents the Andromeda constellation using chains as his main weapons, reminiscent of Andromeda being chained before she was saved by Perseus. In order to attain the Andromeda Cloth, he was chained between two large pillars of rock and he had to overcome the chains before the tide came in and killed him, also reminiscent of this myth.
Andromeda appears in Disney's "" as a new student of "Prometheus Academy" which Hercules and other characters from Greek mythology attend.
In "The Sea of Monsters", the second book in the "Percy Jackson & the Olympians" series, a cruise ship which serves as living space for Kronos's army called "The Princess Andromeda" is named after her.

</doc>
<doc id="1926" url="http://en.wikipedia.org/wiki?curid=1926" title="Antlia">
Antlia

Antlia (; from Ancient Greek "ἀντλία") is a constellation in the southern sky. Its name means "pump" and it specifically represents an air pump. The constellation was created in the 18th century from an undesignated region of sky, so the stars comprising Antlia are faint. Antlia is bordered by Hydra the sea snake, Pyxis the compass, Vela the sails, and Centaurus the centaur. This group of constellations is prominent in the southern sky in late winter and spring. NGC 2997, a spiral galaxy, and the Antlia Dwarf Galaxy lie within Antlia's borders.
History.
Antlia was created in 1756 by the French astronomer Abbé Nicolas Louis de Lacaille, who created fourteen constellations for the southern sky to fill some faint regions. Though Antlia was technically visible to ancient Greek astronomers, its stars were too faint to have been included in any constellations. Because of this, its main stars have no particular pattern and it is devoid of bright deep-sky objects. It was originally named "Antlia pneumatica" ("Machine Pneumatique" in French) to commemorate the air pump invented by the French physicist Denis Papin. 
Lacaille and Johann Bode each depicted Antlia differently, as either the single-cylinder vacuum pump used in Papin's initial experiments, or the more advanced double-cylinder version. The International Astronomical Union subsequently adopted it as one of the 88 modern constellations. There is no mythology attached to Antlia as Lacaille discontinued the tradition of giving names from mythology to constellations and instead chose names mostly from scientific instruments.
According to some, the most prominent stars that now comprise Antlia were once included within the ancient constellation Argo Navis, the Ship of the Argonauts, which due to its immense size was split into several smaller constellations by French astronomer Nicolas Louis de Lacaille in 1763. However, given the faintness and obscurity of its stars, most authorities do not believe that the ancient Greeks included Antlia as part of their classical depiction of Argo Navis.
Notable features.
Deep-sky objects.
Because it occupies a part of the celestial sphere that faces away from the Milky Way, Antlia contains very few deep-sky objects. It contains no globular clusters, no planetary nebulae, and no open clusters. However, it does contain several galaxies. 
NGC 2997 is a loose face-on spiral galaxy of type Sc. It is the brightest galaxy in Antlia at an integrated magnitude of 10.6. Though nondescript in most amateur telescopes, it presents bright clusters of young stars and many dark dust lanes in photographs.
The Antlia Dwarf, a 14.8m dwarf spheroidal galaxy that belongs to our Local Group of galaxies. It was discovered only as recently as 1997.
In non-Western astronomy.
Chinese astronomers were able to view what is modern Antlia from their latitudes, and incorporated its stars into two different constellations. Several stars in the southern part of Antlia were a portion of ""Dong'ou"", which represented an area in southern China. Furthermore, epsilon Antliae, eta Antliae, and theta Antliae were incorporated into the celestial temple, which also contained stars from modern Pyxis.

</doc>
<doc id="1927" url="http://en.wikipedia.org/wiki?curid=1927" title="Ara (constellation)">
Ara (constellation)

Ara is a southern constellation situated between Scorpius and Triangulum Australe. Its name is Latin for "altar". Ara was one of the 48 Greek constellations described by the 2nd century astronomer Ptolemy, and it remains one of the 88 modern constellations defined by the International Astronomical Union.
Notable features.
Stars.
Ara contains part of the Milky Way to the south of Scorpius and thus has rich star fields.
The constellation's stars have no names in Western culture, but the Chinese call α Arae "Choo" ("club" or "staff"), and ε Arae "Tso Kang", meaning 'left guard'.
Deep-sky objects.
The northwest corner of Ara is crossed by the Milky Way and contains several open clusters (notably NGC 6200) and diffuse nebulae (including the bright cluster/nebula pair NGC 6188 and NGC 6193). The brightest of the globular clusters, sixth magnitude NGC 6397, lies at a distance of just , making it one of the closest globular cluster to the solar system.
Although Ara lies close to the heart of the Milky Way, two spiral galaxies (NGC 6215 and NGC 6221) are visible near star η Arae.
Planetary Nebulae.
The Stingray Nebula (Hen 3-1357), the youngest known planetary nebula as of 2010, formed in Ara; the light from its formation was first observable around 1987.
Last, but not least; there is also NGC 6326. A planetary nebula that might have a binary system at its center.
Illustrations.
In illustrations, Ara is usually depicted as an altar with its smoke 'rising' southward. However, depictions of Ara often vary in their details. In the early days of printing, a 1482 woodcut of Gaius Julius Hyginus's classic "Poeticon Astronomicon" depicts the altar as surrounded by demons. Johann Bayer in 1603 depicted Ara as an altar with burning incense; the flames rise southward as in most atlases. Hyginus also depicted Ara as an altar with burning incense, though his Ara featured devils on either side of the flames. However, Willem Blaeu, a Dutch uranographer active in the 16th and 17th centuries, drew Ara as an altar designed for sacrifice, with a burning animal offering. Unlike most depictions, the smoke from Blaeu's Ara rises northward, represented by Alpha Arae. A more unusual depiction of Ara comes from Aratus, a Greek uranographer, in 270 BCE. He drew Ara as a lighthouse, where Alpha. Beta, Epsilon, and Zeta Arae represent the base, and Eta Arae represents the flames at the lighthouse's light.
Mythology.
In ancient Greek mythology, Ara was identified as the altar where the gods first made offerings and formed an alliance before defeating the Titans. The nearby Milky Way represents the smoke rising from the offerings on the altar.
Equivalents.
In Chinese astronomy, the stars of the constellation Ara lie within "The Azure Dragon of the East" (東方青龍, "Dōng Fāng Qīng Lóng").
Namesakes.
USS Ara (AK-136) was a United States Navy Crater class cargo ship named after the constellation.
Bibliography.
Online sources

</doc>
<doc id="1928" url="http://en.wikipedia.org/wiki?curid=1928" title="Auriga">
Auriga

Auriga can refer to:

</doc>
<doc id="1930" url="http://en.wikipedia.org/wiki?curid=1930" title="Arkansas">
Arkansas

Arkansas ( ) is a state located in the Southern region of the United States. Its name is of Siouan derivation, denoting the Quapaw Indians. The state's diverse geography ranges from the mountainous regions of the Ozark and the Ouachita Mountains, which make up the U.S. Interior Highlands, to the densely forested land in the south known as the Arkansas Timberlands, to the eastern lowlands along the Mississippi River and the Arkansas Delta. Known as "the Natural State", the diverse regions of Arkansas offer residents and tourists a variety of opportunities for outdoor recreation.
Arkansas is the 29th largest in square miles and the 32nd most populous of the 50 United States. The capital and most populous city is Little Rock, located in the central portion of the state, a hub for transportation, business, culture, and government. The northwestern corner of the state, including the Fayetteville–Springdale–Rogers Metropolitan Area and Fort Smith metropolitan area, is also an important population, education, and economic center. The largest city in the eastern part of the state is Jonesboro.
The Territory of Arkansas was admitted to the Union as the 25th state on June 15, 1836. Arkansas withdrew from the United States and joined the Confederate States of America during the Civil War. Upon returning to the Union, the state would continue to suffer due to its earlier reliance on slavery and the plantation economy, causing the state to fall behind economically and socially. White rural interests continued to dominate the state's politics until the Civil Rights movement in the mid-20th century. Arkansas began to diversify its economy following World War II and now relies on its service industry as well as aircraft, poultry, steel and tourism in addition to cotton and rice.
The culture of Arkansas is observable in museums, theaters, novels, television shows, restaurants and athletic venues across the state. Despite a plethora of cultural, economic, and recreational opportunities, Arkansas is often stereotyped as a "poor, banjo-picking hillbilly" state, a reputation dating back to early accounts of the territory by frontiersmen in the early 1800s. Arkansas's enduring image has earned the state "a special place in the American consciousness", but it has in reality produced such prominent figures as William Fulbright, Bill Clinton, Douglas MacArthur, Sam Walton and Johnny Cash.
Etymology.
The name Arkansas derives from the same root as the name for the state of Kansas. The Kansa tribe of Native Americans are closely associated with the Sioux tribes of the Great Plains. The word "Arkansas" itself is a French pronunciation ("Arcansas") of a Quapaw (a related "Kaw" tribe) word, "akakaze", meaning "land of downriver people" or the Sioux word "akakaze" meaning "people of the south wind".
In 1881, the pronunciation of Arkansas with the final "s" being silent was made official by an act of the state legislature after a dispute arose between Arkansas's then-two U.S. senators as one favored the pronunciation as while the other favored .
In 2007, the state legislature passed a non-binding resolution declaring the possessive form of the state's name to be "Arkansas's" which has been followed increasingly by the state government.
Geography.
Boundaries.
Arkansas borders Louisiana to the south, Texas to the southwest, Oklahoma to the west, Missouri to the north, and Tennessee and Mississippi on the east. The United States Census Bureau classifies Arkansas as a southern state, sub-categorized among the West South Central States. The Mississippi River forms most of Arkansas's eastern border, except in Clay and Greene, counties where the St. Francis River forms the western boundary of the Missouri Bootheel, and in many places where the current channel of the Mississippi has meandered from where its original legal designation. The state line along the Mississippi River is indeterminate along much of the eastern border with Mississippi due to these meanders.
Terrain.
Arkansas can generally be split into two halves, the highlands in the northwest half and the lowlands of the southeastern half. The highlands are part of the Southern Interior Highlands, including The Ozarks and the Ouachita Mountains. The southern lowlands include the Gulf Coastal Plain and the Arkansas Delta. This dual split is somewhat simplistic, however, and thus usually yields to general regions named northwest, southwest, northeast, southeast, or central Arkansas. These directionally named regions are also not defined along county lines and are also broad. Arkansas has seven distinct natural regions: the Ozark Mountains, Ouachita Mountains, Arkansas River Valley, Gulf Coastal Plain, Crowley's Ridge, and the Arkansas Delta, with Central Arkansas sometimes included as a blend of multiple regions.
The southeastern part of Arkansas along the Mississippi Alluvial Plain is sometimes called the Arkansas Delta. This region is a flat landscape of rich alluvial soils formed by repeated flooding of the adjacent Mississippi. Farther away from the river, in the southeast portion of the state, the Grand Prairie consists of a more undulating landscape. Both are fertile agricultural areas. The Delta region is bisected by an unusual geological formation known as Crowley's Ridge. A narrow band of rolling hills, Crowley's Ridge rises from above the surrounding alluvial plain and underlies many of the major towns of eastern Arkansas.
Northwest Arkansas is part of the Ozark Plateau including the Ozark Mountains, to the south are the Ouachita Mountains, and these regions are divided by the Arkansas River; the southern and eastern parts of Arkansas are called the Lowlands. These mountain ranges are part of the U.S. Interior Highlands region, the only major mountainous region between the Rocky Mountains and the Appalachian Mountains. The highest point in the state is Mount Magazine in the Ouachita Mountains; it rises to above sea level.
Hydrology.
Arkansas has many rivers, lakes, and reservoirs within or along its borders. Major tributaries of the Mississippi River include the Arkansas River, White River, and St. Francis River. The Arkansas is fed by the Mulberry River, and Fourche LaFave River in the Arkansas River Valley, which is also home to Lake Dardanelle. The Buffalo River, Little Red River, Black River and Cache River all serve as tributaries to the White River, which also empties into the Mississippi. The Saline River, Little Missouri River, Bayou Bartholomew, and the Caddo River all serve as tributaries to the Ouachita River in south Arkansas, which eventually empties into the Mississippi in Louisiana. The Red River briefly serves as the state's boundary with Texas. Arkansas has few natural lakes but many major reservoirs, including Bull Shoals Lake, Lake Ouachita, Greers Ferry Lake, Millwood Lake, Beaver Lake, Norfork Lake, DeGray Lake, and Lake Conway.
Arkansas is home to many caves, such as Blanchard Springs Caverns. More than 43,000 Native American living, hunting and tool making sites, many of them Pre-Columbian burial mounds and rock shelters, have been cataloged by the State Archeologist. Crater of Diamonds State Park near Murfreesboro is the world's only diamond-bearing site accessible to the public for digging. Arkansas is home to a dozen Wilderness Areas totaling . These areas are set aside for outdoor recreation and are open to hunting, fishing, hiking, and primitive camping. No mechanized vehicles nor developed campgrounds are allowed in these areas.
Flora and fauna.
Arkansas is divided into three broad ecoregions, the "Ozark, Ouachita-Appalachian Forests", "Mississippi Alluvial and Southeast USA Coastal Plains", and the "Southeastern USA Plains". The state is further divided into seven subregions: the Arkansas Valley, Boston Mountains, Mississippi Alluvial Plain, Mississippi Valley Loess Plain, Ozark Highlands, Ouachita Mountains, and the South Central Plains. A 2010 United States Forest Service survey determined of Arkansas's land is forestland, or 56% of the state's total area. Dominant species in Arkansas's forests include "Quercus" (oak), "Carya" (hickory), "Pinus echinata" (shortleaf pine) and "Pinus taeda" (Loblolly pine).
Arkansas's plant life varies with its climate and elevation. The pine belt stretching from the Arkansas delta to Texas consists of dense oak-hickory-pine growth. Lumbering and paper milling activity is active throughout the region. In eastern Arkansas, one can find "Taxodium " (cypress), "Quercus nigra" (water oaks), and hickories with their roots submerged in the Mississippi Valley bayous indicative of the deep south. Nearby Crowley's Ridge is only home of the tulip tree in the state, and generally hosts more northeastern plant life such as the beech tree. The northwestern highlands are covered in an oak-hickory mixture, with Ozark white cedars, "cornus" (dogwoods), and "Cercis canadensis" (redbuds) also present. The higher peaks in the Arkansas River Valley play host to scores of ferns, including the "Woodsia scopulina" and "Adiantum" (maidenhair fern) on Mount Magazine.
Climate.
Arkansas generally has a humid subtropical climate, which borders on humid continental in some northern highland areas. While not bordering the Gulf of Mexico, Arkansas is still close enough to this warm, large body of water for it to influence the weather in the state. Generally, Arkansas has hot, humid summers and cold, slightly drier winters. In Little Rock, the daily high temperatures average around with lows around in July. In January highs average around and lows around . In Siloam Springs in the northwest part of the state, the average high and low temperatures in July are and in January the average high and lows are . Annual precipitation throughout the state averages between about ; somewhat wetter in the south and drier in the northern part of the state. Snowfall is infrequent but most common in the northern half of the state. The half of the state south of Little Rock is more apt to see ice storms. Arkansas' all-time record high is at Ozark on August 10, 1936; the all-time record low is at Gravette, on February 13, 1905.
Arkansas is known for extreme weather and many storms. A typical year will see thunderstorms, tornadoes, hail, snow and ice storms. Between both the Great Plains and the Gulf States, Arkansas receives around 60 days of thunderstorms. A few of the most destructive tornadoes in U.S. history have struck the state. While being sufficiently away from the coast to be safe from a direct hit from a hurricane, Arkansas can often get the remnants of a tropical system which dumps tremendous amounts of rain in a short time and often spawns smaller tornadoes.
History.
Early Arkansas through territorial period.
Prior to European settlement of North America, Arkansas was inhabited by the Caddo, Osage, and Quapaw people. Explorers to visit the state include Hernando de Soto in 1541, Jacques Marquette and Louis Jolliet in 1673, and Robert La Salle and Henri de Tonti in 1681. Originally a Quapaw village, Arkansas Post was the first European settlement upon its establishment by de Tonti in 1686, in the name of King Louis XIV of France. As Europeans settled the east coast, many other Native American tribes were relocated to Arkansas. Settlers, including fur trappers, moved to Arkansas in the early 18th century. These people used Arkansas Post as a home base and entrepôt. During the colonial period, Arkansas changed hands between France and Spain following the Seven Years' War, although neither showed interest in the remote settlement of Arkansas Post. In April 1783, Arkansas saw its only battle of the American Revolutionary War, a brief siege of the post by British Captain James Colbert with the assistance of the Choctaw and Chickasaw. The early Spanish or French explorers of the state gave it its name, which is probably a phonetic spelling of the Illinois tribe's name for the Quapaw people, who lived downriver from them.
Napoleon Bonaparte sold French Louisiana to the United States in 1803, including all of Arkansas, in a transaction known today as the Louisiana Purchase, although French soldiers remained at Arkansas Post. Following the purchase, the balanced give-and-take relationship between settlers and Native Americans began to change all along the frontier, including in Arkansas. Following a controversy over allowing slavery in the territory, the Territory of Arkansas was organized on July 4, 1819. Gradual emancipation in Arkansas was struck down by one vote, the Speaker of the House Henry Clay, allowing Arkansas to organize as a slave territory. Slavery became a wedge issue in Arkansas, forming a geographic divide that remained for decades. The owners and operators of the cotton plantation economy in southeast Arkansas firmly supported slavery, as slave labor was perceived by them to be the best or "only" economically viable method of harvesting their crop. The "hill country" of northwest Arkansas was unable to grow cotton and relied on a cash-scarce, subsistence farming economy. Native American removals began in earnest during the territorial period, with final Quapaw removal complete by 1833. The capital was relocated from Arkansas Post to Little Rock in 1821, during the territorial period.
Statehood, Civil War and Reconstruction.
When Arkansas applied for statehood, the slavery issue was again raised in Washington DC. Congress eventually approved the Arkansas Constitution after a 25-hour session, admitting Arkansas on June 15, 1836 as the 25th state and the 13th slave state, having a population of about 60,000. Arkansas struggled with taxation to support its new state government, a problem made worse by a state banking scandal and worse yet by the Panic of 1837. In early antebellum Arkansas, the southeast Arkansas economy grew rapidly on the backs of slaves. On the eve of the Civil War in 1860, enslaved African Americans numbered 111,115 people, just over 25% of the state's population. However, plantation agriculture would ultimately set the state and region behind the nation for decades. The growth of southeast Arkansas also caused a rift to form between the northwest and southeast.
Many politicians were elected to office from the Family, the Southern rights political force in antebellum Arkansas, but the populace generally wanted to avoid a civil war. When the Gulf states seceded in early 1861, Arkansas voted to remain in the Union. Arkansas did not secede until Abraham Lincoln demanded Arkansas troops be sent to Fort Sumter to quell the rebellion there. The following month a state convention voted to terminate Arkansas's membership in the Union and join the Confederate States of America. Arkansas held a very important position for the Rebels, maintaining control of the Mississippi River and surrounding Southern states. The bloody Battle of Wilson's Creek just across the border in Missouri shocked many Arkansans who thought the war would be a quick and decisive Southern victory. Battles early in the war took place in northwest Arkansas, including the Battle of Cane Hill, Battle of Pea Ridge, and Battle of Prairie Grove. Union General Samuel Curtis swept across the state to Helena in 1862. Little Rock was captured the following year, forcing the Confederate capital to move to Hot Springs, and then again to Washington from 1863-1865, for the remainder of the war. Throughout the state, guerrilla warfare ravaged the countryside and destroyed cities. Passion for the Confederate cause waned after implementation of unpopular programs like a draft, high taxes, and martial law.
Under the Military Reconstruction Act, Congress declared Arkansas restored to the Union in June 1868. The Republican-controlled reconstruction legislature established universal male suffrage (though temporarily disfranchising all former Confederates, who were mostly Democrats), a public education system, and passed general issues to improve the state and help more of the population. The state soon came under almost exclusive control of the Radical Republicans, (those who moved in from the North being derided as "carpetbaggers" based on allegations of corruption), and led by Governor Powell Clayton, they presided over a time of great upheaval and racial violence in the state between Republican state militia and the Ku Klux Klan.
In 1874, the Brooks-Baxter War, a political struggle between factions of the Republican Party shook Little Rock and the state governorship. It was settled only when President Ulysses S. Grant ordered Joseph Brooks to disperse his militant supporters.
Following the Brooks-Baxter War, a new state constitution was ratified, re-enfranchising former Confederates.
In 1881, the Arkansas state legislature enacted a bill that adopted an official pronunciation of the state's name, to combat a controversy then simmering. (See Law and Government below.)
After Reconstruction, the state began to receive more immigrants and migrants. Chinese, Italian, and Syrian men were recruited for farm labor in the developing Delta region. None of these nationalities stayed long at farm labor; the Chinese especially quickly became small merchants in towns around the Delta. Some early 20th-century immigration included people from eastern Europe. Together, these immigrants made the Delta more diverse than the rest of the state. In the same years, some black migrants moved into the area because of opportunities to develop the bottomlands and own their own property. Many Chinese became such successful merchants in small towns that they were able to educate their children at college.
Construction of railroads enabled more farmers to get their products to market. It also brought new development into different parts of the state, including the Ozarks, where some areas were developed as resorts. In a few years at the end of the 19th century, for instance, Eureka Springs in Carroll County grew to 10,000 people, rapidly becoming a tourist destination and the fourth-largest city of the state. It featured newly constructed, elegant resort hotels and spas planned around its natural springs, considered to have healthful properties. The town's attractions included horse racing and other entertainment. It appealed to a wide variety of classes, becoming almost as popular as Hot Springs.
In the late 1880s, the worsening agricultural depression catalyzed Populist and third party movements, leading to interracial coalitions. Struggling to stay in power, in the 1890s the Democrats in Arkansas followed other Southern states in passing legislation and constitutional amendments that disfranchised blacks and poor whites. Democrats wanted to prevent their alliance. In 1891 state legislators passed a requirement for a literacy test, knowing that many blacks and whites would be excluded, at a time when more than 25% of the population could neither read nor write. In 1892 they amended the state constitution to include a poll tax and more complex residency requirements, both of which adversely affected poor people and sharecroppers, and forced them from voter rolls.
By 1900 the Democratic Party expanded use of the white primary in county and state elections, further denying blacks a part in the political process. Only in the primary was there any competition among candidates, as Democrats held all the power. The state was a Democratic one-party state for decades, until after the federal Civil Rights Act of 1964 and Voting Rights Act of 1965 were passed by Congress.
Between 1905 and 1911, Arkansas began to receive a small immigration of German, Slovak, and Scots-Irish from Europe. The German and Slovak peoples settled in the eastern part of the state known as the Prairie, and the Irish founded small communities in the southeast part of the state. The Germans were mostly Lutheran and the Slovaks were primarily Catholic. The Irish were mostly Protestant from Ulster, of Scots and Northern Borders descent.
After the Supreme Court's decision in "Brown "v." Board of Education of Topeka, Kansas" in 1954, the Little Rock Nine brought Arkansas to national attention when the Federal government intervened to protect African-American students trying to integrate a high school in the Arkansas capital. Governor Orval Faubus ordered the Arkansas National Guard to aid segregationists in preventing nine African-American students from enrolling at Little Rock's Central High School. After attempting three times to contact Faubus, President Dwight D. Eisenhower sent 1000 troops from the active-duty 101st Airborne Division to escort and protect the African-American students as they entered school on September 25, 1957. In defiance of federal court orders to integrate, the governor and city of Little Rock decided to close the high schools for the remainder of the school year. By the fall of 1959, the Little Rock high schools were completely integrated.
Bill Clinton, the 42nd President of the United States, was born in Hope, Arkansas. Before his presidency, Clinton served as the 40th and 42nd Governor of Arkansas, a total of nearly 12 years.
Cities and towns.
Little Rock has been Arkansas's capital city since 1821 when it replaced Arkansas Post as the capitol of the Territory of Arkansas. The state capitol was moved to Hot Springs and later Washington during the Civil War when the Union armies threatened the city in 1862, and state government did not return to Little Rock until after the war ended. Today, the Little Rock–North Little Rock–Conway metropolitan area is the largest in the state, with a population of 724,385 in 2013.
The Fayetteville–Springdale–Rogers Metropolitan Area is the second-largest metropolitan area in Arkansas, growing at the fastest rate due to the influx of businesses and the growth of the University of Arkansas and Walmart.
The state has nine cities with populations above 50,000 (based on 2010 census). In descending order of size they are Little Rock, Fort Smith, Fayetteville, Springdale, Jonesboro, North Little Rock, Conway, Rogers, and Pine Bluff. Of these, only Fort Smith and Jonesboro are outside the two largest metropolitan areas. Other notable cities include Hot Springs, Bentonville, Texarkana, Sherwood, Jacksonville, Russellville, Bella Vista, West Memphis, Paragould, Cabot, Searcy, Van Buren, El Dorado, Blytheville, Harrison, and Mountain Home.
Demographics.
Population.
The United States Census Bureau estimates that the population of Arkansas was 2,949,132 on July 1, 2012, a 1.1% increase since the 2010 United States Census.
As of 2012, Arkansas has an estimated population of 2,949,132. From fewer than 15,000 in 1820, Arkansas's population grew to 52,240 during a special census in 1835, far exceeding the 40,000 required to apply for statehood. Following statehood in 1836, the population doubled each decade until the 1870 Census conducted following the Civil War. The state recorded growth in each successive decade, although slowing until recording losses in the 1950 and 1960 Censuses. This outmigration was a result of multiple factors, including mechanization on the farm reducing the number of laborers needed and young educated people leaving the state due to a lack of non-farming industry in the state. Arkansas again began to grow, recording positive growth rates ever since and exceeding the 2 million mark during the 1980 Census. Arkansas's current rate of change, age distributions, and gender distributions mirror national averages. Minority group data also approximates national averages, with the exception of persons of Hispanic or Latino origin approximately 10% below the national percentage in Arkansas. The center of population of Arkansas for 2000 was located in Perry County, near Nogal.
Race and ancestry.
In terms of race and ethnicity, the state was 80.1% White (74.2% non-Hispanic White), 15.6% Black or African American, 0.9% American Indian and Alaska Native, 1.3% Asian, and 1.8% from Two or More Races. Hispanics or Latinos of any race made up 6.6% of the population.
As of 2011, 39.0% of Arkansas's population younger than age 1 were minorities.
European Americans have a strong presence in the northwestern Ozarks and the central part of the state. African Americans live mainly in the southern and eastern parts of the state. Arkansans of Irish, English and German ancestry are mostly found in the far northwestern Ozarks near the Missouri border. Ancestors of the Irish in the Ozarks were chiefly Scots-Irish, Protestants from Northern Ireland, the Scottish lowlands and northern England part of the largest group of immigrants from Great Britain and Ireland before the American Revolution. English and Scots-Irish immigrants settled throughout the backcountry of the South and in the more mountainous areas. Americans of English stock are found throughout the state.
The principal ancestries of Arkansas's residents in 2010 were surveyed to be the following:
Many of the people of American ancestry have some English descent and some have Scots-Irish descent. Their families have been in the state so long, in many cases since before statehood, that they choose to identify simply as having American ancestry or do not in fact know their own ancestry. Their ancestry primarily goes back to the original 13 colonies and for this reason many of them today simply claim American ancestry. Many people who identify themselves as Irish descent are in fact of Scots-Irish descent.
According to the 2006–2008 American Community Survey, 93.8% of Arkansas' population (over the age of five) spoke only English at home. About 4.5% of the state's population spoke Spanish at home. About 0.7% of the state's population spoke any other Indo-European languages. About 0.8% of the state's population spoke an Asian language, and 0.2% spoke other languages.
Religion.
Arkansas, like most other Southern states, is part of the Bible Belt and is predominantly Protestant. The largest denominations by number of adherents in 2010 were the Southern Baptist Convention with 661,382; the United Methodist Church with 158,574; non-denominational Evangelical Protestants with 129,638; and the Catholic Church with 122,662. However, there are some residents of the state who live by other religions such as Wiccan, Pagan, Islam, Hinduism, Buddhism and still others who prefer no religious denomination.
Economy.
Once a state with a cashless society in the uplands and plantation agriculture in the lowlands, Arkansas's economy has evolved and diversified to meet the needs of today's consumer. The state's gross domestic product (GDP) was $105 billion in 2010. Six Fortune 500 companies are based in Arkansas, including the world's #1 retailer, Walmart. The per capita personal income in 2010 was $36,027, ranking forty-fifth in the nation. The three-year median household income from 2009-11 was $39,806, ranking forty-ninth in the nation. The state's agriculture outputs are poultry and eggs, soybeans, sorghum, cattle, cotton, rice, hogs, and milk. Its industrial outputs are food processing, electric equipment, fabricated metal products, machinery, and paper products. Mines in Arkansas produce natural gas, oil, crushed stone, bromine, and vanadium. According to CNBC, Arkansas currently ranks as the 20th best state for business, with the 2nd-lowest cost of doing business, 5th-lowest cost of living, 11th best workforce, 20th-best economic climate, 28th-best educated workforce, 31st-best infrastructure and the 32nd-friendliest regulatory environment. Arkansas gained twelve spots in the best state for business rankings since 2011.
As of April 2013 the state's unemployment rate is 7.5%
Industry and commerce.
Arkansas's earliest industries were fur trading and agriculture, with development of cotton plantations in the areas near the Mississippi River. They were dependent on slave labor through the American Civil War.
Today only approximately 3% of the population is employed in the agricultural sector, it remains a major part of the state's economy, ranking 13th in the nation in the value of products sold. The state is the U.S.'s largest producer of rice, broilers, and turkeys, and ranks in the top three for cotton, pullets, and aquaculture (catfish). Forestry remains strong in the Arkansas Timberlands, and the state ranks fourth nationally and first in the South in softwood lumber production. In recent years, automobile parts manufacturers have opened factories in eastern Arkansas to support auto plants in other states. Bauxite was formerly a large part of the state's economy, mined mostly around Saline County.
Tourism is also very important to the Arkansas economy; the official state nickname "The Natural State" was created for state tourism advertising in the 1970s, and is still used to this day. The state maintains 52 state parks and the National Park Service maintains seven properties in Arkansas, including the nation's first National Park, Hot Springs National Park. The completion of the William Jefferson Clinton Presidential Library in Little Rock has drawn many visitors to the city and revitalized the nearby River Market District. Many cities also hold festivals which draw tourists to the culture of Arkansas, such as King Biscuit Blues Festival, Ozark Folk Festival, Toad Suck Daze, and Tontitown Grape Festival.
Culture.
The culture of Arkansas is available to all in various forms, whether it be architecture, literature, or fine and performing arts. The state's culture also includes distinct cuisine, dialect, and traditional festivals. Sports are also very important to the culture of Arkansas, ranging from football, baseball, and basketball to hunting and fishing. Perhaps the best-known piece of Arkansas's culture is the stereotype of its citizens as shiftless hillbillies. The reputation began when the state was characterized by early explorers as a savage wilderness full of outlaws and thieves. The most enduring icon of Arkansas's hillbilly reputation is "The Arkansas Traveller", a painted depiction of a folk tale from the 1840s. Although intended to represent the divide between rich southeastern plantation Arkansas planters and the poor northwestern hill country, the meaning was twisted to represent a Northerner lost in the Ozarks on a white horse asking a backwoods Arkansan for directions. The state also suffers from the racial stigma common to former Confederate states, with historical events such as the Little Rock Nine adding to Arkansas's enduring image.
Art and history museums display pieces of cultural value for Arkansans and tourists to enjoy. Crystal Bridges Museum of American Art in Bentonville is the most popular with 604,000 visitors in 2012, its first year. The museum includes walking trails and educational opportunities in addition to displaying over 450 works covering five centuries of American art. Several historic town sites have been restored as Arkansas state parks, including Historic Washington State Park, Powhatan Historic State Park, and Davidsonville Historic State Park.
Arkansas features a variety of native music across the state, ranging from the blues heritage of West Memphis and Helena-West Helena to rockabilly, bluegrass, and folk music from the Ozarks. Festivals such as the King Biscuit Blues Festival and Bikes, Blues, and BBQ pay homage to the history of blues in the state. The Ozark Folk Festival in Mountain View is a celebration of Ozark culture and often features folk and bluegrass musicians. Literature set in Arkansas such as "I Know Why the Caged Bird Sings" by Maya Angelou and "A Painted House" by John Grisham describe the culture at various time periods.
Sports and recreation.
Sports are an integral part of the culture of Arkansas, and her residents enjoy participating in and spectating various events throughout the year. One of the oldest sports in Arkansas is hunting. The state created the Arkansas Game and Fish Commission in 1915 to regulate and enforce hunting. Today a significant portion of Arkansas's population participates in hunting duck in the Mississippi flyway and deer across the state. Millions of acres of public land are available for both bow and modern gun hunters.
Fishing has always been popular in Arkansas, and the sport and the state have benefited from the creation of reservoirs across the state. Following the completion of Norfork Dam, the Norfork Tailwater and the White River have become a destination for trout fishers. Several smaller retirement communities such as Bull Shoals, Hot Springs Village, and Fairfield Bay have flourished due to their position on a fishing lake. The Buffalo National River has been preserved in its natural state by the National Park Service and is frequented by fly fishers annually.
Football, especially collegiate football, has always been important to Arkansans. College football in Arkansas began from humble beginnings. The University of Arkansas first fielded a team in 1894 when football was a very dangerous game. Calling the Hogs is a cheer that shows support for the Razorbacks, one of the two FBS teams in the state. High school football also began to grow in Arkansas in the early 20th century. Over the years, many Arkansans have looked to the Razorbacks football team as the public image of the state. Following the Little Rock Nine integration crisis at Little Rock Central High School, Arkansans looked to the successful Razorback teams in the following years to repair the state's reputation. Although the University of Arkansas is based in Fayetteville, the Razorbacks have always played at least two games per season at War Memorial Stadium in Little Rock in an effort to keep fan support in central and south Arkansas. Arkansas State University joined the University of Arkansas in the Football Bowl Subdivision in 1992 after playing in lower divisions for nearly two decades. However, the two schools have never played each other, due to the University of Arkansas' policy of not playing intrastate games. Six of Arkansas' smaller colleges play in the Great American Conference, with University of Arkansas at Pine Bluff playing in the Southwestern Athletic Conference and University of Central Arkansas competing in the Southland Conference.
Baseball runs deep in Arkansas and has been popular since before the state hosted Major League Baseball (MLB) spring training in Hot Springs from 1886-1920s. Today, two minor league teams are based in the state. The Arkansas Travelers play at Dickey-Stephens Park in North Little Rock, and the Northwest Arkansas Naturals play in Arvest Ballpark in Springdale. Both teams compete in the Texas League.
Health.
Arkansans, as with many Southern states, have a high incidence of premature death, infant mortality, cardiovascular deaths, and occupational fatalities compared to the rest of the United States. The state is tied for 43rd with New York in percentage of adults who regularly exercise. Arkansas is usually ranked as one of the least healthy states due to high obesity, smoking, and sedentary lifestyle rates. The state also has an uninsured rate of 18%, ranking it 37th in the nation. Uninsured individuals often obtain care that is more expensive and less effective, increasing the cost of health care across the state and compounding the problem.
The Arkansas Clean Indoor Air Act went into effect in 2006, a statewide smoking ban excluding bars and some restaurants.
Healthcare in Arkansas is provided by a network of hospitals as members of the Arkansas Hospital Association. Major institutions with multiple branches include Baptist Health, Community Health Systems, and HealthSouth. The University of Arkansas for Medical Sciences (UAMS) in Little Rock operates the UAMS Medical Center, a teaching hospital ranked as high performing nationally in cancer and nephrology. The pediatric division of UAMS Medical Center is known as Arkansas Children's Hospital, nationally ranked in pediatric cardiology and heart surgery. Together, these two institutions are the state's only Level I trauma centers.
Education.
Arkansas ranks as the 32nd smartest state on the Morgan Quitno Smartest State Award, 44th in percentage of residents with at least a high school diploma, and 48th in percentage of bachelor's degree attainment. However, Arkansas has been making major strides recently in education reform. "Education Week" has praised the state, ranking Arkansas in the top 10 of their Quality Counts Education Rankings every year since 2009 while scoring it in the top 5 during 2012 and 2013. Arkansas specifically received an A in Transition and Policy Making for progress in this area consisting of early-childhood education, college readiness, and career readiness. Governor Mike Beebe has made improving education a major issue through his attempts to spend more on education. Through reforms, the state is now a leader in requiring curricula designed to prepare students for postsecondary education, rewarding teachers for student achievement, and providing incentives for principals who work in lower-tier schools.
In 2010 Arkansas students earned an average score of 20.3 on the ACT exam, just below the national average of 21. These results were expected due to the large increase in the number of students taking the exam since the establishment of the Academic Challenge Scholarship. Top high schools receiving recognition from the U.S. News & World Report are spread across the state, including Haas Hall Academy in Fayetteville, KIPP Delta Collegiate in Helena-West Helena, Bentonville, Rogers, Rogers Heritage, Valley Springs, Searcy, and McCrory. A total of 81 Arkansas high schools were ranked by the U.S. News & World Report in 2012.
The state supports a network of public universities and colleges, including two major university systems: Arkansas State University System and University of Arkansas System. The University of Arkansas, flagship campus of the University of Arkansas System in Fayetteville was ranked #63 among public schools in the nation by "U.S. News & World Report". Other public institutions include Arkansas Tech University, Henderson State University, Southern Arkansas University, and University of Central Arkansas across the state. It is also home to 11 private colleges and universities including Hendrix College, one of the nation's top 100 liberal arts colleges, according to U.S. News & World Report.
Transportation.
Transportation in Arkansas is overseen by the Arkansas State Highway and Transportation Department (AHTD), headquartered in Little Rock. Several main corridors pass through Little Rock, including Interstate 30 (I-30) and I-40 (the nation’s 3rd-busiest trucking corridor). In northeast Arkansas, I-55 travels north from Memphis to Missouri, with a new spur to Jonesboro (I-555). Northwest Arkansas is served by I-540 from Fort Smith to Bella Vista, which is a segment of future I-49. The state also has the 13th largest state highway system in the nation.
Arkansas is served by of railroad track divided among twenty-six railroad companies including three Class I railroads. Freight railroads are concentrated in southeast Arkansas to serve the industries in the region. The Texas Eagle, an Amtrak passenger train, serves five stations in the state Walnut Ridge, Little Rock, Malvern, Arkadelphia, and Texarkana.
Arkansas also benefits from the use of its rivers for commerce. The Mississippi River and Arkansas River are both major rivers. The United States Army Corps of Engineers maintains the McClellan-Kerr Arkansas River Navigation System, allowing barge traffic up the Arkansas River to the Port of Catoosa in Tulsa, Oklahoma.
There are four airports with commercial service: Little Rock National Airport, Northwest Arkansas Regional Airport, Fort Smith Regional Airport, and Texarkana Regional Airport, with dozens of smaller airports in the state.
Public transit and community transport services for the elderly or those with developmental disabilities are provided by agencies such as the Central Arkansas Transit Authority and the Ozark Regional Transit, organizations that are part of the Arkansas Transit Association.
Law and government.
As with the federal government of the United States, political power in Arkansas is divided into three branches: executive, legislative, and judicial. Each officer's term is four years long. Office holders are term-limited to two full terms plus any partial terms before the first full term.
Executive.
The current Governor of Arkansas is Mike Beebe, a Democrat, who was elected on November 7, 2006. Beebe was reelected to his second and final term in 2010 which will expire January 13, 2015. The six other elected executive positions in Arkansas are lieutenant governor, secretary of state, attorney general, treasurer, auditor, and land commissioner. The governor also appoints qualified individuals to lead various state boards, committees, and departments. Arkansas governors served two-year terms until a referendum lengthened the term to four years, effective with the 1986 general election.
In Arkansas, the lieutenant governor is elected separately from the governor and thus can be from a different political party.
Legislative.
The Arkansas General Assembly is the state's bicameral bodies of legislators, composed of the Senate and House of Representatives. The Senate contains 35 members from districts of approximately equal population. These districts are redrawn decennially with each US census, and in election years ending in "2", the entire body is put up for reelection. Following the election, half of the seats are designated as two-year seats and will be up for reelection again in two years, these "half-terms" do not count against a legislator's term limits. The remaining half serve a full four-year term. This staggers elections such that half the body is up for re-election every two years and allows for complete body turnover following redistricting. Arkansas voters selected a 21-14 Republican majority in the Senate in 2012. Arkansas House members can serve a maximum of three two-year terms. House districts are redistricted by the Arkansas Board of Apportionment. Following the 2012 elections, Republicans gained a 51-49 majority in the House of Representatives.
The Republican Party majority status in the Arkansas State House of Representatives following the 2012 elections is the party's first since 1874. Arkansas was the last state of the old Confederacy to never have Republicans control either chamber of its house since the Civil War.
Following the term limits changes, studies have show that lobbyist have become less influential in state politics, but legislative staff, not subject to term limits, have acquired additional power and influence due to the high rate of elected official turnover.
Judicial.
Arkansas's judicial branch has five court systems: Arkansas Supreme Court, Arkansas Court of Appeals, Circuit Courts, District Courts and City Courts.
Most cases begin in district court, which is subdivided into state district court and local district court. State district courts exercise district-wide jurisdiction over the districts created by the General Assembly, and local district courts are presided over by part-time judges who may privately practice law. There are currently 25 state district court judges presiding over 15 districts, with more districts to be created in 2013 and 2017. There are 28 judicial circuits of Circuit Court, with each contains five subdivisions: criminal, civil, probate, domestic relations, and juvenile court. The jurisdiction of the Arkansas Court of Appeals is determined by the Arkansas Supreme Court, and there is no right of appeal from the Court of Appeals to the high court. However, the Arkansas Supreme Court can review Court of Appeals cases upon application by either a party to the litigation, upon request by the Court of Appeals, or if the Arkansas Supreme Court feels the case should have been initially assigned to it. The twelve judges of the Arkansas Court of Appeals are elected from judicial districts to renewable six-year terms.
The Arkansas Supreme Court is the court of last resort in the state, composed of seven justices elected to eight-year terms. Established by the Arkansas Constitution in 1836, the court's decisions can be appealed to only the Supreme Court of the United States.
Federal.
One of Arkansas's U.S. Senators is Democrat Mark Pryor, and the other one is Republican John Boozman. The state has four seats in U.S. House of Representatives. All four seats are held by Republicans: Rick Crawford (1st district), Tim Griffin (2nd district), Steve Womack (3rd district), and Tom Cotton (4th district).
Politics.
Arkansas Governor Bill Clinton brought national attention to the state with a long speech at the 1988 Democratic National Convention endorsing Michael Dukakis. Pundits suggested the speech would ruin Clinton's political career, but instead, Clinton won the Democratic nomination for President the following cycle. Presenting himself as a "New Democrat" and using incumbent George H. W. Bush's against him, Clinton won the 1992 presidential election (43.0% of the vote) against Republican Bush (37.4% of the vote) and billionaire populist Ross Perot, who ran as an independent (18.9% of the vote).
Most Republican strength lies mainly in the northwestern part of the state, particularly Fort Smith and Bentonville, as well as North Central Arkansas around the Mountain Home area. In the latter area, Republicans have been known to get 90 percent or more of the vote. The rest of the state is more Democratic. Arkansas has only elected two Republicans to the U.S. Senate since Reconstruction, Tim Hutchinson, who was defeated after one term by Mark Pryor and John Boozman, who defeated incumbent Blanche Lincoln. Prior to 2013, the General Assembly had not been controlled by the Republican Party since Reconstruction with the GOP holding a 51-seat majority in the state House and a 21-seat (of 35) in the state Senate following victories in 2012. Arkansas was one of just three states among the states of the former Confederacy that sent two Democrats to the U.S. Senate (the others being Florida and Virginia) during the first decade of the 21st century.
In 2010, Republicans captured three of the state's four seats in the U.S. House of Representatives. In 2012, Republicans won election for all four House seats. Arkansas holds the distinction of having a U.S. House delegation composed of military veterans (Rick Crawford - Army; Tim Griffin - Army Reserve; Steve Womack - Army National Guard, Tom Cotton- Army).
Reflecting the state's large evangelical population, the state has a strong social conservative bent. Under the Arkansas Constitution Arkansas is a right to work state, its voters passed a ban on same-sex marriage with 75% voting yes, and the state is one of a handful with legislation on its books banning abortion in the event "Roe v. Wade" is ever overturned.
Attractions.
Arkansas is home to many areas protected by the National Park System. These include:

</doc>
<doc id="1931" url="http://en.wikipedia.org/wiki?curid=1931" title="Atmosphere (disambiguation)">
Atmosphere (disambiguation)

An atmosphere is a gas layer around a celestial body.
Atmosphere may also refer to:

</doc>
<doc id="1933" url="http://en.wikipedia.org/wiki?curid=1933" title="Apus">
Apus

Apus is a faint constellation in the southern sky, first defined in the late 16th century. Its name means "no feet" in Greek, and it represents a bird-of-paradise (which were once believed to lack feet). It is bordered by Triangulum Australe, Circinus, Musca, Chamaeleon, Octans, Pavo and Ara. Its genitive is "Apodis".
History.
Apus was one of twelve constellations created by Petrus Plancius from the observations of Pieter Dirkszoon Keyser and Frederick de Houtman and it first appeared on a 35 cm diameter celestial globe published in 1597 (or 1598) in Amsterdam by Plancius with Jodocus Hondius. Plancius called the constellation ""Paradysvogel Apis Indica""; the first word is Dutch for 'bird of paradise,' of genus Pteridophora, but the others are Latin for "Indian Bee," although "apis" (Latin for "bee") is presumably an error for "avis" or "bird".
The name "Apus" is derived from the Greek "apous", meaning "without feet", which referred to the Western conception of a bird-of-paradise as one without feet, a misconception perpetuated by the fact that the only specimens available in the West had both feet and wings removed. These specimens began to arrive in Europe in 1522, when the survivors of Ferdinand Magellan's expedition brought them home.
After its introduction on Plancius's globe, the first known depiction of the constellation in a celestial atlas was in Johann Bayer's "Uranometria" of 1603, where it was called "Apis Indica".
It was suggested by Richard Allen that Houtmann, who observed the southern constellations from the island of Sumatra, took his ideas for the formation of Apus (as well as Phoenix and Indus) from the Chinese, who knew these stars as the "Little Wonder Bird.". Ridpath, however, disputes this possibility, arguing that the southern constellations were introduced later than Allen believed, and by different people altogether.
Notable features.
The most prominent deep-sky objects in Apus include the globular clusters NGC 6101 and IC 4499 as well as the spiral galaxy IC 4633.
Equivalents.
When the Ming Dynasty Chinese astronomer Xu Guangqi adapted the European southern hemisphere constellations to the Chinese system in "The Southern Asterisms", he combined Apus with some of the stars in Octans to form the "Exotic Bird" (異雀, "Yìquè").

</doc>
<doc id="1934" url="http://en.wikipedia.org/wiki?curid=1934" title="Abadan, Iran">
Abadan, Iran

Âbâdân () is a city in and the capital of Abadan County, Khuzestan Province, Iran. It lies on Abadan Island ( long, 3–19 km or 2–12 miles wide, the island is bounded in the west by the Arvand waterway and to the east by the Bahmanshir outlet of the Karun River otherwise known as Shatt al-Arab), from the Persian Gulf, near the Iraqi-Iran border.
The civilian population of the city dropped close to zero during the eight years of the Iran–Iraq War (1980–88). The 1986 census recorded only 6 people. In 1992, 84,774 had returned to live in the city. By 2001, the population had jumped to 206,073, and it was 217,988, in 48,061 families, according to 2006 census. Abadan Refinery is one of the largest in the world.
Etymology.
The earliest mention of the island of Abadan, if not the port itself is found in works of the geographer Marcian, who renders the name "Apphadana". Earlier, the classical geographer, Ptolemy notes "Apphana" as an island off the mouth of the Tigris (which is, where the modern Island of Abadan is located). An etymology for this name is presented by 'B. Farahvashi" to be derived from the Persian word "ab" (water) and the root "pā" (guard, watch) thus "coastguard station").
In the Islamic times, a pseudo-etymology was produced by the historian Ahmad ibn Yahya al-Baladhuri (d.892) quoting a folk story that the town was presumably founded by one ""Abbad bin Hosayn" from the Arabian Tribe of Banu Tamim", who established a garrison there during the governorship of "Hajjaj" in the Ummayad period.
In the subsequent centuries, the Persian version of the name had begun to come into general use before it was adopted by official decree in 1935.
History.
Abadan is thought to have been further developed into a major port city under the Abbasids' rule. In this time period, it was a commercial source of salt and woven mats. The siltation of the river delta forced the town further away from water; In the 14th century, however, Ibn Battutah described Abadan just as a small port in a flat salty plain. Politically, Abadan was often the subject of dispute between the nearby states; in 1847, Persia acquired it, in which state Abadan has remained since. From the 17th century onward, the island of Abadan was part of the lands of the Arab "Ka'ab" (Bani Kaab) tribe. One section of this tribe, "Mohaysen", had its headquarters at "Mohammara"(present-day Khorramshahr), until the removal of Shaikh Khaz'al Khan in 1924.
It was not until the 20th century that rich oil fields were discovered in the area. In 1910, the population had been around 400. The Anglo-Persian Oil Company built their first pipeline terminus oil refinery in Abadan, starting in 1909 and completing it in 1913 (see Abadan Refinery). By 1938, it was the largest in the world. To this day it remains a vast facility for refining petroleum. The facilities necessitated an equally vast population: more than 220,000 people in 1956. In 1951, Iran nationalized all oil properties and refining ground to a stop on the island. It wasn't until 1954, that a settlement was reached, which allows a group of international oil companies to manage the production and refining on the island.
Only 9% of managers (of the oil company) were from Khuzestan. The proportion of natives of Tehran, the Caspian, Azarbaijan and Kurdistan rose from 4% of blue collar workers to 22% of white collar workers to 45% of managers. Thus while Persian-speakers were concentrated on the lower rungs of the work force, managers tended to be brought in from some distance.
During World War II, Abadan was a major logistics center for Lend-Lease aircraft being sent to the Soviet Union by the United States.
On 19 August 1978—the anniversary of the US backed pro-Shah coup d'état which overthrew the nationalists and popular Iranian prime minister, Dr. Mohammed Mossadegh—the Cinema Rex, a movie theatre in Abadan, Iran, was set ablaze by four Islamic Revolution sympathizers in an attempt to help the cause of Iran's Islamic Revolution. The local Abadan police had taken notice, and became suspicious of Hossein Takbali-zadeh and his accomplices, and had started following the arsonists as they were entering Cinema Rex. The police decided to continue their surveillance and track the group after they left the movie theater. This incident ended up led to the Cinema Rex Fire, where over 350 people perished. At the trial, Hossein Takbali-zadeh stated that his three accomplices by the three names Faraj, Falah, and Yadollah had all burned in the fire. The reformist Sobhe Emrooz newspaper in one of its editorials revealed that the Cinema Rex was burned down by the radical Islamists. The newspaper was shut down immediately after.
In September 1980, Abadan was almost overrun during a surprise attack on Khuzestan by Iraq, marking the beginning of the Iran–Iraq War. For 18 months Abadan was besieged, but never captured, by Iraqi forces. Much of the city, including the oil refinery which was the world's largest refinery with capacity of 680,000 barrels per day, was badly damaged or destroyed by the siege and by bombing. Previous to the war, the city's civilian population was about 300,000, but before it was over, most of the populace had sought refuge elsewhere in Iran.
After the war, the biggest concern was the rebuilding of Abadan's oil refinery. In 1993, the refinery began limited operation & and the port reopened. By 1997, the refinery reached the same rate of production it was at before the war. Recently, Abandan has been the site of major labor activity as workers at the oil refineries in the city have staged walkouts and strikes to protest non-payment of wages and the political situation in the country.
Recent events.
To honor the 100th anniversary of the refining of oil in Abadan, city officials are planning an "oil museum." The Abadan oil refinery was featured on the reverse side of Iran's 100-rial banknotes printed in 1965 and from 1971 to 1973.
Places of interest.
The Abadan Institute of Technology was established in Abadan in 1939. The school specialized in engineering and petroleum chemistry, and was designed to train staff for the refinery in town. The school's name has since changed several times, but since 1989 has been considered a branch campus of the Petroleum University of Technology, centered in Tehran.
There is an international airport in Abadan. It is represented by the IATA airport code ABD.
Climate.
The climate in Abadan is arid (Köppen climate classification "BWh") and similar to Baghdad's, but slightly hotter due to Abadan's lower latitude. Summers are dry and extremely hot, with temperatures above 45 degrees almost daily. Winters are mildly wet and spring-like, though subject to cold spells. Winter temperatures are around 16–20 degrees. The world's highest unconfirmed temperature was a temperature flare up during a heat burst in June 1967, with a temperature of . Reliable measurements in the city range from .

</doc>
<doc id="1935" url="http://en.wikipedia.org/wiki?curid=1935" title="Attorney">
Attorney

Attorney may refer to:

</doc>
<doc id="1937" url="http://en.wikipedia.org/wiki?curid=1937" title="Alexander Fleming">
Alexander Fleming

Sir Alexander Fleming, FRSE, FRS, FRCS(Eng) (6 August 188111 March 1955) was a Scottish biologist, pharmacologist and botanist. He wrote many articles on bacteriology, immunology, and chemotherapy. His best-known discoveries are the enzyme lysozyme in 1923 and the antibiotic substance penicillin from the mould "Penicillium notatum" in 1928, for which he shared the Nobel Prize in Physiology or Medicine in 1945 with Howard Florey and Ernst Boris Chain.
Biography.
Early life and education.
Fleming was born on 6 August 1881 at Lochfield farm near Darvel, in Ayrshire, Scotland. He was the third of the four children of farmer Hugh Fleming (1816–1888) from his second marriage to Grace Stirling Morton (1848–1928), the daughter of a neighbouring farmer. Hugh Fleming had four surviving children from his first marriage. He was 59 at the time of his second marriage, and died when Alexander (known as Alec) was seven.
Fleming went to Loudoun Moor School and Darvel School, and earned a two-year scholarship to Kilmarnock Academy before moving to London, where he attended the Royal Polytechnic Institution. After working in a shipping office for four years, the twenty-year-old Fleming inherited some money from an uncle, John Fleming. His elder brother, Tom, was already a physician and suggested to his younger sibling that he should follow the same career, and so in 1903, the younger Alexander enrolled at St Mary's Hospital Medical School in Paddington; he qualified with an MBBS degree from the school with distinction in 1906.
Fleming had been a private in the London Scottish Regiment of the Volunteer Force since 1900, and had been a member of the rifle club at the medical school. The captain of the club, wishing to retain Fleming in the team suggested that he join the research department at St Mary's, where he became assistant bacteriologist to Sir Almroth Wright, a pioneer in vaccine therapy and immunology. In 1908, he gained a BSc degree with Gold Medal in Bacteriology, and became a lecturer at St Mary's until 1914. On 23 December 1915, Fleming married a trained nurse, Sarah Marion McElroy of Killala, County Mayo, Ireland.
Fleming served throughout World War I as a captain in the Royal Army Medical Corps, and was Mentioned in Dispatches. He and many of his colleagues worked in battlefield hospitals at the Western Front in France. In 1918 he returned to St Mary's Hospital, where he was elected Professor of Bacteriology of the University of London in 1928.
Research.
Work before penicillin.
Following World War I, Fleming actively searched for anti-bacterial agents, having witnessed the death of many soldiers from sepsis resulting from infected wounds. Antiseptics killed the patients' immunological defences more effectively than they killed the invading bacteria. In an article he submitted for the medical journal "The Lancet" during World War I, Fleming described an ingenious experiment, which he was able to conduct as a result of his own glass blowing skills, in which he explained why antiseptics were killing more soldiers than infection itself during World War I. Antiseptics worked well on the surface, but deep wounds tended to shelter anaerobic bacteria from the antiseptic agent, and antiseptics seemed to remove beneficial agents produced that protected the patients in these cases at least as well as they removed bacteria, and did nothing to remove the bacteria that were out of reach. Sir Almroth Wright strongly supported Fleming's findings, but despite this, most army physicians over the course of the war continued to use antiseptics even in cases where this worsened the condition of the patients.
Accidental discovery.
"When I woke up just after dawn on September 28, 1928, I certainly didn't plan to revolutionise all medicine by discovering the world's first antibiotic, or bacteria killer," Fleming would later say, "But I suppose that was exactly what I did."
By 1927, Fleming was investigating the properties of staphylococci. He was already well-known from his earlier work, and had developed a reputation as a brilliant researcher, but his laboratory was often untidy. On 3 September 1928, Fleming returned to his laboratory having spent August on holiday with his family. Before leaving, he had stacked all his cultures of staphylococci on a bench in a corner of his laboratory. On returning, Fleming noticed that one culture was contaminated with a fungus, and that the colonies of staphylococci immediately surrounding the fungus had been destroyed, whereas other staphylococci colonies farther away were normal, famously remarking ""That's funny"". Fleming showed the contaminated culture to his former assistant Merlin Price, who reminded him, ""That's how you discovered lysozyme."" Fleming grew the mould in a pure culture and found that it produced a substance that killed a number of disease-causing bacteria. He identified the mould as being from the "Penicillium" genus, and, after some months of calling it ""mould juice"", named the substance it released "penicillin" on 7 March 1929. The laboratory in which Fleming discovered and tested penicillin is preserved as the Alexander Fleming Laboratory Museum in St. Mary's Hospital, Paddington.
He investigated its positive anti-bacterial effect on many organisms, and noticed that it affected bacteria such as staphylococci and many other Gram-positive pathogens that cause scarlet fever, pneumonia, meningitis and diphtheria, but not typhoid fever or paratyphoid fever, which are caused by Gram-negative bacteria, for which he was seeking a cure at the time. It also affected "Neisseria gonorrhoeae," which causes gonorrhoea although this bacterium is Gram-negative.
Fleming published his discovery in 1929, in the British "Journal of Experimental Pathology," but little attention was paid to his article. Fleming continued his investigations, but found that cultivating "penicillium" was quite difficult, and that after having grown the mould, it was even more difficult to isolate the antibiotic agent. Fleming's impression was that because of the problem of producing it in quantity, and because its action appeared to be rather slow, penicillin would not be important in treating infection. Fleming also became convinced that penicillin would not last long enough in the human body ("in vivo") to kill bacteria effectively. Many clinical tests were inconclusive, probably because it had been used as a surface antiseptic. In the 1930s, Fleming’s trials occasionally showed more promise, and he continued, until 1940, to try to interest a chemist skilled enough to further refine usable penicillin. Fleming finally abandoned penicillin, and not long after he did, Howard Florey and Ernst Boris Chain at the Radcliffe Infirmary in Oxford took up researching and mass-producing it, with funds from the U.S. and British governments. They started mass production after the bombing of Pearl Harbor. By D-Day in 1944, enough penicillin had been produced to treat all the wounded in the Allied forces.
Purification and stabilisation.
In Oxford, Ernst Boris Chain and Edward Abraham discovered how to isolate and concentrate penicillin. Abraham was the first to propose the correct structure of penicillin. Shortly after the team published its first results in 1940, Fleming telephoned Howard Florey, Chain's head of department, to say that he would be visiting within the next few days. When Chain heard that he was coming, he remarked ""Good God! I thought he was dead.""
Norman Heatley suggested transferring the active ingredient of penicillin back into water by changing its acidity. This produced enough of the drug to begin testing on animals. There were many more people involved in the Oxford team, and at one point the entire Dunn School was involved in its production.
After the team had developed a method of purifying penicillin to an effective first stable form in 1940, several clinical trials ensued, and their amazing success inspired the team to develop methods for mass production and mass distribution in 1945.
Fleming was modest about his part in the development of penicillin, describing his fame as the ""Fleming Myth"" and he praised Florey and Chain for transforming the laboratory curiosity into a practical drug. Fleming was the first to discover the properties of the active substance, giving him the privilege of naming it: penicillin. He also kept, grew, and distributed the original mould for twelve years, and continued until 1940 to try to get help from any chemist who had enough skill to make penicillin. But Sir Henry Harris said in 1998: ""Without Fleming, no Chain; without Chain, no Florey; without Florey, no Heatley; without Heatley, no penicillin.""
Antibiotics.
Fleming's accidental discovery and isolation of penicillin in September 1928 marks the start of modern antibiotics. Before that, several scientists had published or pointed out that mould or "penicillium sp." were able to inhibit bacterial growth, and even to cure bacterial infections in animals. Ernest Duchesne in 1897 in his thesis "Contribution to the study of vital competition in micro-organisms: antagonism between moulds and microbes", or also Clodomiro Picado Twight whose work at Institut Pasteur in 1923 on the inhibiting action of fungi of the "Penicillin sp" genre in the growth of staphylococci drew little interest from the direction of the Institut at the time. Fleming was the first to push these studies further by isolating the penicillin, and by being motivated enough to promote his discovery at a larger scale. Fleming also discovered very early that bacteria developed antibiotic resistance whenever too little penicillin was used or when it was used for too short a period. Almroth Wright had predicted antibiotic resistance even before it was noticed during experiments. Fleming cautioned about the use of penicillin in his many speeches around the world. He cautioned not to use penicillin unless there was a properly diagnosed reason for it to be used, and that if it were used, never to use too little, or for too short a period, since these are the circumstances under which bacterial resistance to antibiotics develops.
Myths.
The popular story of Winston Churchill's father paying for Fleming's education after Fleming's father saved young Winston from death is false. According to the biography, "Penicillin Man: Alexander Fleming and the Antibiotic Revolution" by Kevin Brown, Alexander Fleming, in a letter to his friend and colleague Andre Gratia, described this as ""A wondrous fable."" Nor did he save Winston Churchill himself during World War II. Churchill was saved by Lord Moran, using sulphonamides, since he had no experience with penicillin, when Churchill fell ill in Carthage in Tunisia in 1943. The "Daily Telegraph" and the "Morning Post" on 21 December 1943 wrote that he had been saved by penicillin. He was saved by the new sulphonamide drug, Sulphapyridine, known at the time under the research code M&B 693, discovered and produced by May & Baker Ltd, Dagenham, Essex – a subsidiary of the French group Rhône-Poulenc. In a subsequent radio broadcast, Churchill referred to the new drug as ""This admirable M&B."" It is highly probable that the correct information about the sulphonamide did not reach the newspapers because, since the original sulphonamide antibacterial, Prontosil, had been a discovery by the German laboratory Bayer, and as Britain was at war with Germany at the time, it was thought better to raise British morale by associating Churchill's cure with the British discovery, penicillin.
Personal life.
Fleming's first wife, Sarah, died in 1949. Their only child, Robert Fleming, became a general medical practitioner. After Sarah's death, Fleming married Dr. Amalia Koutsouri-Vourekas, a Greek colleague at St. Mary's, on 9 April 1953; she died in 1986.
Death.
On 11 March 1955, Fleming died at his home in London of a heart attack. He was buried in St Paul's Cathedral.
Honours, awards and achievements.
His discovery of penicillin had changed the world of modern medicine by introducing the age of useful antibiotics; penicillin has saved, and is still saving, millions of people around the world.
The laboratory at St Mary's Hospital where Fleming discovered penicillin is home to the Fleming Museum, a popular London attraction. His alma mater, St Mary's Hospital Medical School, merged with Imperial College London in 1988. The Sir Alexander Fleming Building on the South Kensington campus was opened in 1998 and is now one of the main preclinical teaching sites of the Imperial College School of Medicine.
His other alma mater, the Royal Polytechnic Institution (now the University of Westminster) has named one of its student halls of residence "Alexander Fleming House", which is near to Old Street.

</doc>
<doc id="1938" url="http://en.wikipedia.org/wiki?curid=1938" title="Andrew Carnegie">
Andrew Carnegie

Andrew Carnegie ( , but commonly or ; November 25, 1835 – August 11, 1919) was a Scottish-American industrialist who led the enormous expansion of the American steel industry in the late 19th century. He was also one of the highest profile philanthropists of his era; his 1889 article proclaiming "The Gospel of Wealth" called on the rich to use their wealth to improve society, and stimulated a wave of philanthropy.
Carnegie was born in Dunfermline, Scotland, and emigrated to the United States with his very poor parents in 1848. Carnegie started as a telegrapher and by the 1860s had investments in railroads, railroad sleeping cars, bridges and oil derricks. He built further wealth as a bond salesman raising money for American enterprise in Europe. He built Pittsburgh's Carnegie Steel Company, which he sold to J.P. Morgan in 1901 for $480 million (the equivalent of approximately $ billion in ), creating the U.S. Steel Corporation. Carnegie devoted the remainder of his life to large-scale philanthropy, with special emphasis on local libraries, world peace, education and scientific research. With the fortune he made from business, he built Carnegie Hall, and founded the Carnegie Corporation of New York, Carnegie Endowment for International Peace, Carnegie Institution for Science, Carnegie Trust for the Universities of Scotland, Carnegie Hero Fund, Carnegie Mellon University and the Carnegie Museums of Pittsburgh, among others. His life has often been referred to as a true "rags to riches" story.
Biography.
Early life.
Andrew Carnegie was born in Dunfermline, Scotland, in a typical weaver's cottage with only one main room, consisting of half the ground floor which was shared with the neighboring weaver's family. The main room served as a living room, dining room and bedroom. He was named after his legal grandfather. In 1836, the family moved to a larger house in Edgar Street (opposite Reid's Park), following the demand for more heavy damask from which his father, William Carnegie, benefited. His uncle, George Lauder, whom he referred to as "Dod", introduced him to the writings of Robert Burns and historical Scottish heroes such as Robert the Bruce, William Wallace, and Rob Roy. Falling on very hard times as a handloom weaver and with the country in starvation, William Carnegie decided to move with his family to Allegheny, Pennsylvania, in the United States in 1848 for the prospect of a better life. Andrew's family had to borrow money in order to migrate. Allegheny was a very poor area. His first job at age 13 in 1848 was as a bobbin boy, changing spools of thread in a cotton mill 12 hours a day, 6 days a week in a Pittsburgh cotton factory. His starting wage was $1.20 per week. Andrew's father, William Carnegie, started off working in a cotton mill but then would earn money weaving and peddling linens. His mother, Margaret Morrison Carnegie, earned money by binding shoes.
Railroads.
In 1850, Carnegie became a telegraph messenger boy in the Pittsburgh Office of the Ohio Telegraph Company, at $2.50 per week, following the recommendation of his uncle. His new job gave him many benefits including free admission to the local theater. This made him appreciate Shakespeare's work.
He was a very hard worker and would memorize all of the locations of Pittsburgh's businesses and the faces of important men. He made many connections this way. He also paid close attention to his work, and quickly learned to distinguish the differing sounds the incoming telegraph signals produced. He developed the ability to translate signals by ear, without using the paper slip, and within a year was promoted as an operator. Carnegie's education and passion for reading was given a great boost by Colonel James Anderson, who opened his personal library of 400 volumes to working boys each Saturday night. Carnegie was a consistent borrower and a "self-made man" in both his economic development and his intellectual and cultural development. His capacity, his willingness for hard work, his perseverance, and his alertness soon brought forth opportunities.
Starting in 1853, Thomas A. Scott of the Pennsylvania Railroad Company employed Carnegie as a secretary/telegraph operator at a salary of $4.00 per week. At age 18, the precocious youth began a rapid advance through the company, becoming the superintendent of the Pittsburgh Division. His employment by the Pennsylvania Railroad Company would be vital to his later success. The railroads were the first big businesses in America, and the Pennsylvania was one of the largest of them all. Carnegie learned much about management and cost control during these years, and from Scott in particular.
Scott also helped him with his first investments. Many of these were part of the corruption indulged in by Scott and the Pennsylvania's president, J. Edgar Thomson, which consisted of inside trading in companies that the railroad did business with, or payoffs made by contracting parties "as part of a quid pro quo". In 1855, Scott made it possible for Carnegie to invest $500 in the Adams Express, which contracted with the Pennsylvania to carry its messengers. The money was secured by his mother's placing a $500 mortgage on the family's $700 home, but the opportunity was available only because of Carnegie's close relationship with Scott. A few years later, he received a few shares in T.T. Woodruff's sleeping car company, as a reward for holding shares that Woodruff had given to Scott and Thomson, as a payoff. Reinvesting his returns in such inside investments in railroad-related industries: (iron, bridges, and rails), Carnegie slowly accumulated capital, the basis for his later success. Throughout his later career, he made use of his close connections to Thomson and Scott, as he established businesses that supplied rails and bridges to the railroad, offering the two men a stake in his enterprises.
1860–1865: The Civil War.
Before the Civil War, Carnegie arranged a merger between Woodruff's company and that of George Pullman, the inventor of a sleeping car for first class travel which facilitated business travel at distances over . The investment proved a great success and a source of profit for Woodruff and Carnegie. The young Carnegie continued to work for the Pennsylvania's Tom Scott, and introduced several improvements in the service.
In spring 1861, Carnegie was appointed by Scott, who was now Assistant Secretary of War in charge of military transportation, as Superintendent of the Military Railways and the Union Government's telegraph lines in the East. Carnegie helped open the rail lines into Washington D.C. that the rebels had cut; he rode the locomotive pulling the first brigade of Union troops to reach Washington D.C. Following the defeat of Union forces at Bull Run, he personally supervised the transportation of the defeated forces. Under his organization, the telegraph service rendered efficient service to the Union cause and significantly assisted in the eventual victory. Carnegie later joked that he was "the first casualty of the war" when he gained a scar on his cheek from freeing a trapped telegraph wire.
Defeat of the Confederacy required vast supplies of munitions, as well as railroads (and telegraph lines) to deliver the goods. The war demonstrated how integral the industries were to American success.
Keystone Bridge Company.
In 1864, Carnegie invested $40,000 in Story Farm on Oil Creek in Venango County, Pennsylvania. In one year, the farm yielded over $1,000,000 in cash dividends, and petroleum from oil wells on the property sold profitably. The demand for iron products, such as armor for gunboats, cannon, and shells, as well as a hundred other industrial products, made Pittsburgh a center of wartime production. Carnegie worked with others in establishing a steel rolling mill, and steel production and control of industry became the source of his fortune. Carnegie had some investments in the iron industry before the war.
After the war, Carnegie left the railroads to devote all his energies to the ironworks trade. Carnegie worked to develop several iron works, eventually forming The Keystone Bridge Works and the Union Ironworks, in Pittsburgh. Although he had left the Pennsylvania Railroad Company, he remained closely connected to its management, namely Thomas A. Scott and J. Edgar Thomson. He used his connection to the two men to acquire contracts for his Keystone Bridge Company and the rails produced by his ironworks. He also gave stock to Scott and Thomson in his businesses, and the Pennsylvania was his best customer. When he built his first steel plant, he made a point of naming it after Thomson. As well as having good business sense, Carnegie possessed charm and literary knowledge. He was invited to many important social functions—functions that Carnegie exploited to his own advantage.
Carnegie believed in using his fortune for others and doing more than making money. He wrote: 
Industrialist.
1885–1900: Steel empire.
Carnegie's mother had not wanted him to get married. After she died in 1886, Carnegie married Louise Whitfield, who was more than 20 years his junior. In 1897, the couple had their only child, a daughter, whom they named after Carnegie's mother, Margaret.
Carnegie made his fortune in the steel industry, controlling the most extensive integrated iron and steel operations ever owned by an individual in the United States. One of his two great innovations was in the cheap and efficient mass production of steel by adopting and adapting the Bessemer process for steel making. Sir Henry Bessemer had invented the furnace which allowed the high carbon content of pig iron to be burnt away in a controlled and rapid way. The steel price dropped as a direct result, and Bessemer steel was rapidly adopted for railway lines and girders for buildings and bridges.
The second was in his vertical integration of all suppliers of raw materials. In the late 1880s, Carnegie Steel was the largest manufacturer of pig iron, steel rails, and coke in the world, with a capacity to produce approximately 2,000 tons of pig metal per day. In 1888, Carnegie bought the rival Homestead Steel Works, which included an extensive plant served by tributary coal and iron fields, a 425-mile (685 km) long railway, and a line of lake steamships. Carnegie combined his assets and those of his associates in 1892 with the launching of the Carnegie Steel Company.
By 1889, the U.S. output of steel exceeded that of the UK, and Carnegie owned a large part of it. Carnegie's empire grew to include the J. Edgar Thomson Steel Works, (named for John Edgar Thomson, Carnegie's former boss and president of the Pennsylvania Railroad), Pittsburgh Bessemer Steel Works, the Lucy Furnaces, the Union Iron Mills, the Union Mill (Wilson, Walker & County), the Keystone Bridge Works, the Hartman Steel Works, the Frick Coke Company, and the Scotia ore mines. Carnegie, through Keystone, supplied the steel for and owned shares in the landmark Eads Bridge project across the Mississippi River at St. Louis, Missouri (completed 1874). This project was an important proof-of-concept for steel technology, which marked the opening of a new steel market.
1901: U.S. Steel.
In 1901, Carnegie was 66 years of age and considering retirement. He reformed his enterprises into conventional joint stock corporations as preparation to this end. John Pierpont Morgan was a banker and perhaps America's most important financial deal maker. He had observed how efficiently Carnegie produced profit. He envisioned an integrated steel industry that would cut costs, lower prices to consumers, produce in greater quantities and raise wages to workers. To this end, he needed to buy out Carnegie and several other major producers and integrate them into one company, thereby eliminating duplication and waste. He concluded negotiations on March 2, 1901, and formed the United States Steel Corporation. It was the first corporation in the world with a market capitalization over $1 billion.
The buyout, secretly negotiated by Charles M. Schwab (no relation to Charles R. Schwab), was the largest such industrial takeover in United States history to date. The holdings were incorporated in the United States Steel Corporation, a trust organized by Morgan, and Carnegie retired from business. His steel enterprises were bought out at a figure equivalent to 12 times their annual earnings—$480 million (presently, $) which at the time was the largest ever personal commercial transaction.
Carnegie's share of this amounted to $225,639,000 (presently, $), which was paid to Carnegie in the form of 5%, 50-year gold bonds. The letter agreeing to sell his share was signed on February 26, 1901. On March 2, the circular formally filing the organization and capitalization (at $1,400,000,000—4% of U.S. national wealth at the time) of the United States Steel Corporation actually completed the contract. The bonds were to be delivered within two weeks to the Hudson Trust Company of Hoboken, New Jersey, in trust to Robert A. Franks, Carnegie's business secretary. There, a special vault was built to house the physical bulk of nearly $230,000,000 worth of bonds. It was said that "...Carnegie never wanted to see or touch these bonds that represented the fruition of his business career. It was as if he feared that if he looked upon them they might vanish like the gossamer gold of the leprechaun. Let them lie safe in a vault in New Jersey, safe from the New York tax assessors, until he was ready to dispose of them..."
Scholar and activist.
1880–1900.
Carnegie continued his business career; some of his literary intentions were fulfilled. He befriended English poet Matthew Arnold, English philosopher Herbert Spencer, and American humorist Mark Twain, as well as being in correspondence and acquaintance with most of the U.S. Presidents, statesmen, and notable writers.
Carnegie constructed commodious swimming-baths for the people of his hometown in Dunfermline in 1879. In the following year, Carnegie gave $40,000 for the establishment of a free library in Dunfermline. In 1884, he gave $50,000 to Bellevue Hospital Medical College (now part of New York University Medical Center) to found a histological laboratory, now called the Carnegie Laboratory.
In 1881, Carnegie took his family, including his 70 year-old mother, on a trip to the United Kingdom. They toured Scotland by coach, and enjoyed several receptions en route. The highlight for them all was a triumphal return to Dunfermline, where Carnegie's mother laid the foundation stone of a Carnegie library for which he donated the money. Carnegie's criticism of British society did not mean dislike; on the contrary, one of Carnegie's ambitions was to act as a catalyst for a close association between the English-speaking peoples. To this end, in the early 1880s in partnership with Samuel Storey, he purchased numerous newspapers in England, all of which were to advocate the abolition of the monarchy and the establishment of "the British Republic". Carnegie's charm aided by his great wealth meant that he had many British friends, including Prime Minister William Ewart Gladstone.
In 1886, Carnegie's younger brother Thomas died at age 43. Success in the business continued, however. While owning steel works, Carnegie had purchased at low cost the most valuable of the iron ore fields around Lake Superior. The same year Carnegie became a figure of controversy. Following his tour of the UK, he wrote about his experiences in a book entitled "An American Four-in-hand in Britain". Although still actively involved in running his many businesses, Carnegie had become a regular contributor to numerous magazines, most notably "The Nineteenth Century", under the editorship of James Knowles, and the influential "North American Review", led by editor Lloyd Bryce.
In 1886, Carnegie wrote his most radical work to date, entitled "Triumphant Democracy". Liberal in its use of statistics to make its arguments, the book argued his view that the American republican system of government was superior to the British monarchical system. It gave a highly favorable and idealized view of American progress and criticized the British royal family. The cover depicted an upended royal crown and a broken scepter. The book created considerable controversy in the UK. The book made many Americans appreciate their country's economic progress and sold over 40,000 copies, mostly in the U.S.
In 1889, Carnegie published "Wealth" in the June issue of the "North American Review". After reading it, Gladstone requested its publication in England, where it appeared as "The Gospel of Wealth" in the "Pall Mall Gazette". The article was the subject of much discussion. Carnegie argued that the life of a wealthy industrialist should comprise two parts. The first part was the gathering and the accumulation of wealth. The second part was for the subsequent distribution of this wealth to benevolent causes. The philanthropy was key to making the life worthwhile.
Carnegie was a well-regarded writer. He published three books on travel.
Anti-imperialism.
While Carnegie did not comment on British imperialism, he very strongly opposed the idea of American colonies. He strongly opposed the annexation of the Philippines, almost to the point of supporting William Jennings Bryan against McKinley in 1900. In 1898, Carnegie tried to arrange for independence for the Philippines. As the end of the Spanish American War neared, the United States bought the Philippines from Spain for $20 million. To counter what he perceived as imperialism on the part of the United States, Carnegie personally offered $20 million to the Philippines so that the Filipino people could buy their independence from the United States. However, nothing came of the offer. In 1898 Carnegie joined the American Anti-Imperialist League, in opposition to the U.S. annexation of the Philippines. Its membership included former presidents of the United States Grover Cleveland and Benjamin Harrison and literary figures like Mark Twain.
1901–1919: Philanthropist.
Carnegie spent his last years as a philanthropist. From 1901 forward, public attention was turned from the shrewd business acumen which had enabled Carnegie to accumulate such a fortune, to the public-spirited way in which he devoted himself to utilizing it on philanthropic projects. He had written about his views on social subjects and the responsibilities of great wealth in "Triumphant Democracy" (1886) and "Gospel of Wealth" (1889). Carnegie bought Skibo Castle in Scotland, and made his home partly there and partly in New York. He then devoted his life to providing the capital for purposes of public interest and social and educational advancement.
He was a powerful supporter of the movement for spelling reform as a means of promoting the spread of the English language.
Among his many philanthropic efforts, the establishment of public libraries throughout the United States, Britain, Canada and other English-speaking countries was especially prominent. In this special driving interest and project of his he was inspired by a visit and tour he made with Mr. Enoch Pratt (1808-1896), formerly of Massachusetts but who made his fortune in Baltimore and ran his various mercantile and financial businesses very thriftily. Pratt in turn had been inspired and helped by his friend and fellow Bay Stater, George Peabody, (1795-1869) who also had made his fortune in the "Monumental City" of Baltimore before moving to New York and London to expand his empire as the richest man in America before the Civil War. Later he too endowed several institutions, schools, libraries and foundations in his home commonwealth, and also in Baltimore with his Peabody Institute in 1857, completed in 1866, with added library wings a decade later and several educational foundations throughout the Old South. Several decades later, Carnegie's visit with Mr. Pratt for several days; resting and dining in his city mansion, then touring, visiting and talking with staff and ordinary citizen patrons of the newly established Enoch Pratt Free Library (1886) impressed the Scotsman deeply and years later he was always heard to proclaim that "Pratt was my guide and inspiration". The first Carnegie library opened in 1883 in Dunfermline. His method was to build and equip, but only on condition that the local authority matched that by providing the land and a budget for operation and maintenance. To secure local interest, in 1885, he gave $500,000 to Pittsburgh for a public library, and in 1886, he gave $250,000 to Allegheny City for a music hall and library; and $250,000 to Edinburgh for a free library. In total Carnegie funded some 3,000 libraries, located in 47 US states, and also in Canada, the United Kingdom, what is now the Republic of Ireland, Australia, New Zealand, the West Indies, and Fiji. He also donated £50,000 to help set up the University of Birmingham in 1899. In the early 20th Century, a decade after Mr. Pratt's death, when expansion and city revenues grew tight, Carnegie returned the favor and endowed a large sum to permit the building of many Carnegie Libraries in the Enoch Pratt system in Baltimore and enabled EPFL to expand through the next quarter-century to meet the needs of the growing city and supply neighborhood branches for its annexed suburbs.
As Van Slyck (1991) showed, the last years of the 19th century saw acceptance of the idea that free libraries should be available to the American public. But the design of the idealized free library was the subject of prolonged and heated debate. On one hand, the library profession called for designs that supported efficiency in administration and operation; on the other, wealthy philanthropists favored buildings that reinforced the paternalistic metaphor and enhanced civic pride. Between 1886 and 1917, Carnegie reformed both library philanthropy and library design, encouraging a closer correspondence between the two.
He gave $2 million in 1900 to start the Carnegie Institute of Technology (CIT) at Pittsburgh and the same amount in 1902 to found the Carnegie Institution at Washington, D.C. He later contributed more to these and other schools. CIT is now known as Carnegie Mellon University after it merged with the Mellon Institute of Industrial Research. Carnegie also served on the Board of Cornell University.
In 1911, Carnegie became a sympathetic benefactor to George Ellery Hale, who was trying to build the Hooker Telescope at Mount Wilson, and donated an additional ten million dollars to the Carnegie Institution with the following suggestion to expedite the construction of the telescope: "I hope the work at Mount Wilson will be vigorously pushed, because I am so anxious to hear the expected results from it. I should like to be satisfied before I depart, that we are going to repay to the old land some part of the debt we owe them by revealing more clearly than ever to them the new heavens." The telescope saw first light on November 2, 1917, with Carnegie still alive.
In Scotland, he gave $10 million in 1901 to establish the Carnegie Trust for the Universities of Scotland. It was created by a deed which he signed on June 7, 1901, and it was incorporated by Royal Charter on August 21, 1902. The Trust was funded by a gift of $10 million (a then unprecedented sum: at the time, total government assistance to all four Scottish universities was about £50,000 a year) and its aim was to improve and extend the opportunities for scientific research in the Scottish universities and to enable the deserving and qualified youth of Scotland to attend a university. He was subsequently elected Lord Rector of University of St. Andrews in December 1901. He also donated large sums of money to Dunfermline, the place of his birth. In addition to a library, Carnegie also bought the private estate which became Pittencrieff Park and opened it to all members of the public, establishing the Carnegie Dunfermline Trust to benefit the people of Dunfermline. A statue of him stands there today. He gave a further $10 million in 1913 to endow the Carnegie United Kingdom Trust, a grant-making foundation.
Carnegie also established large pension funds in 1901 for his former employees at Homestead and, in 1905, for American college professors. The latter fund evolved into TIAA-CREF. One critical requirement was that church-related schools had to sever their religious connections to get his money.
His interest in music led him to fund construction of 7,000 church organs. He built and owned Carnegie Hall in New York City.
Carnegie was a large benefactor of the Tuskegee Institute under Booker T. Washington for African-American education. He helped Washington create the National Negro Business League.
He founded the Carnegie Hero Fund for the United States and Canada in 1904 (a few years later also established in the United Kingdom, Switzerland, Norway, Sweden, France, Italy, the Netherlands, Belgium, Denmark, and Germany) for the recognition of deeds of heroism. Carnegie contributed $1,500,000 in 1903 for the erection of the Peace Palace at The Hague; and he donated $150,000 for a Pan-American Palace in Washington as a home for the International Bureau of American Republics.
Carnegie was honored for his philanthropy and support of the arts by initiation as an honorary member of Phi Mu Alpha Sinfonia Fraternity on October 14, 1917, at the New England Conservatory of Music in Boston, Massachusetts. The fraternity's mission reflects Carnegie's values by developing young men to share their talents to create harmony in the world.
By the standards of 19th century tycoons, Carnegie was not a particularly ruthless man but a humanitarian with enough acquisitiveness to go in the ruthless pursuit of money; on the other hand, the contrast between his life and the lives of many of his own workers and of the poor, in general, was stark. "Maybe with the giving away of his money," commented biographer Joseph Wall, "he would justify what he had done to get that money."
To some, Carnegie represents the idea of the American dream. He was an immigrant from Scotland who came to America and became successful. He is not only known for his successes but his enormous amounts of philanthropist works, not only to charities but also to promote democracy and independence to colonized countries.
Death.
Carnegie died on August 11, 1919, in Lenox, Massachusetts, of bronchial pneumonia. He had already given away $350,695,653 (approximately $4.8 billion, adjusted to 2010 figures) of his wealth. At his death, his last $30,000,000 was given to foundations, charities, and to pensioners. He was buried at the Sleepy Hollow Cemetery in North Tarrytown, New York. The grave site is located on the Arcadia Hebron plot of land at the corner of Summit Avenue and Dingle Road. Carnegie is buried only a few yards away from union organizer Samuel Gompers, another important figure of industry in the Gilded Age.
Controversies.
1889: Johnstown Flood.
Carnegie was one of more than 50 members of the South Fork Fishing and Hunting Club, which has been blamed for the Johnstown Flood that killed 2,209 people in 1889.
At the suggestion of his friend Benjamin Ruff, Carnegie's partner Henry Clay Frick had formed the exclusive South Fork Fishing and Hunting Club high above Johnstown, Pennsylvania. The sixty-odd club members were the leading business tycoons of Western Pennsylvania and included among their number Frick's best friend, Andrew Mellon, his attorneys Philander Knox and James Hay Reed, as well as Frick's business partner, Carnegie. High above the city, near the small town of South Fork, the South Fork Dam was originally built between 1838 and 1853 by the Commonwealth of Pennsylvania as part of a canal system to be used as a reservoir for a canal basin in Johnstown. With the coming-of-age of railroads superseding canal barge transport, the lake was abandoned by the Commonwealth, sold to the Pennsylvania Railroad, and sold again to private interests and eventually came to be owned by the South Fork Fishing and Hunting Club in 1881. Prior to the flood, speculators had purchased the abandoned reservoir, made less than well-engineered repairs to the old dam, raised the lake level, built cottages and a clubhouse, and created the South Fork Fishing and Hunting Club. Less than 20 miles downstream from the dam sat the city of Johnstown.
The dam was high and long. Between 1881 when the club was opened, and 1889, the dam frequently sprang leaks and was patched, mostly with mud and straw. Additionally, a previous owner removed and sold for scrap the 3 cast iron discharge pipes that previously allowed a controlled release of water. There had been some speculation as to the dam's integrity, and concerns had been raised by the head of the Cambria Iron Works downstream in Johnstown. Such repair work, a reduction in height, and unusually high snowmelt and heavy spring rains combined to cause the dam to give way on May 31, 1889 resulting in twenty million tons of water sweeping down the valley causing the Johnstown Flood. When word of the dam's failure was telegraphed to Pittsburgh, Frick and other members of the South Fork Fishing and Hunting Club gathered to form the Pittsburgh Relief Committee for assistance to the flood victims as well as determining never to speak publicly about the club or the flood. This strategy was a success, and Knox and Reed were able to fend off all lawsuits that would have placed blame upon the club's members.
Although Cambria Iron and Steel's facilities were heavily damaged by the flood, they returned to full production within a year. After the flood, Carnegie built Johnstown a new library to replace the one built by Cambria's chief legal counsel Cyrus Elder, which was destroyed in the flood. The Carnegie-donated library is now owned by the Johnstown Area Heritage Association, and houses the Flood Museum.
1892: Homestead Strike.
The Homestead Strike was a bloody labor confrontation lasting 143 days in 1892, one of the most serious in U.S. history. The conflict was centered on Carnegie Steel's main plant in Homestead, Pennsylvania, and grew out of a dispute between the National Amalgamated Association of Iron and Steel Workers of the United States and the Carnegie Steel Company.
Carnegie left on a trip to Scotland before the unrest peaked. In doing so, Carnegie left mediation of the dispute in the hands of his associate and partner Henry Clay Frick. Frick was well known in industrial circles for maintaining staunch anti-union sensibilities.
After a recent increase in profits by 60%, the company refused to raise workers' pay by more than 30%. When some of the workers demanded the full 60%, management locked the union out. Workers considered the stoppage a "lockout" by management and not a "strike" by workers. As such, the workers would have been well within their rights to protest, and subsequent government action would have been a set of criminal procedures designed to crush what was seen as a pivotal demonstration of the growing labor rights movement, strongly opposed by management. Frick brought in thousands of strikebreakers to work the steel mills and Pinkerton agents to safeguard them.
On July 6, the arrival of a force of 300 Pinkerton agents from New York City and Chicago resulted in a fight in which 10 men—seven strikers and three Pinkertons—were killed and hundreds were injured. Pennsylvania Governor Robert Pattison ordered two brigades of state militia to the strike site. Then, allegedly in response to the fight between the striking workers and the Pinkertons, anarchist Alexander Berkman shot at Frick in an attempted assassination, wounding Frick. While not directly connected to the strike, Berkman was tied in for the assassination attempt. According to Berkman, "...with the elimination of Frick, responsibility for Homestead conditions would rest with Carnegie." Afterwards, the company successfully resumed operations with non-union immigrant employees in place of the Homestead plant workers, and Carnegie returned to the United States. However, Carnegie's reputation was permanently damaged by the Homestead events.
Philosophy.
Andrew Carnegie Dictum.
In his final days, Carnegie suffered from bronchial pneumonia. Before his death on August 11, 1919, Carnegie had donated $350,695,654 for various causes. The "Andrew Carnegie Dictum" was:
Carnegie was involved in philanthropic causes, but he kept himself away from religious circles. He wanted to be identified by the world as a "positivist". He was highly influenced in public life by John Bright.
On wealth.
As early as 1868, at age 33, he drafted a memo to himself. He wrote: "...The amassing of wealth is one of the worse species of idolatry. No idol more debasing than the worship of money."
In order to avoid degrading himself, he wrote in the same memo he would retire at age 35 to pursue the practice of philanthropic giving for "...the man who dies thus rich dies disgraced." However, he did not begin his philanthropic work in all earnest until 1881, with the gift of a library to his hometown of Dunfermline, Scotland.
Carnegie wrote "The Gospel of Wealth", an article in which he stated his belief that the rich should use their wealth to help enrich society.
The following is taken from one of Carnegie's memos to himself: 
Intellectual influences.
Carnegie claimed to be a champion of evolutionary thought particularly the work of Herbert Spencer, even declaring Spencer his teacher. Though Carnegie claims to be a disciple of Spencer many of his actions went against the ideas espoused by Spencer.
Spencerian evolution was for individual rights and against government interference. Furthermore, Spencerian evolution held that those unfit to sustain themselves must be allowed to perish. Spencer believed that just as there were many varieties of beetles, respectively modified to existence in a particular place in nature, so too had human society “spontaneously fallen into division of labour”. Individuals who survived to this, the latest and highest stage of evolutionary progress would be “those in whom the power of self-preservation is the greatest—are the select of their generation.” Moreover, Spencer perceived governmental authority as borrowed from the people to perform the transitory aims of establishing social cohesion, insurance of rights, and security. Spencerian ‘survival of the fittest’ firmly credits any provisions made to assist the weak, unskilled, poor and distressed to be an imprudent disservice to evolution. Spencer insisted people should resist for the benefit of collective humanity as these severe fate singles out the weak, debauched, and disabled.
Andrew Carnegie’s political and economic focus of during the late nineteenth and early twentieth century was the defense of laissez faire economics. Carnegie emphatically resisted government intrusion in commerce, as well as government-sponsored charities. Carnegie believed the concentration of capital was essential for societal progress and should be encouraged. Carnegie was an ardent supporter of commercial “survival of the fittest” and sought to attain immunity from business challenges by dominating all phases of the steel manufacturing procedure. Carnegie’s determination to lower costs included cutting labor expenses as well. In a notably Spencerian manner, Carnegie argued that unions impeded the natural reduction of prices by pushing up costs, which blocked evolutionary progress. Carnegie felt that unions represented the narrow interest of the few while his actions benefited the entire community.
On the surface, Andrew Carnegie appears to be a strict laissez-faire capitalist and follower of Herbert Spencer, often referring to himself as a disciple of Spencer. Conversely, Carnegie a titan of industry seems to embody all of the qualities of Spencerian survival of the fittest. The two men enjoyed a mutual respect for one another and maintained correspondence until Spencer’s death in 1903. There are however, some major discrepancies between Spencer’s capitalist evolutionary conceptions and Andrew Carnegie’s capitalist practices.
Spencer wrote that in production the advantages of the superior individual is comparatively minor, and thus acceptable, yet the benefit that dominance provides those who control a large segment of production might be hazardous to competition. Spencer feared that an absence of “sympathetic self-restraint” of those with too much power could lead to the ruin of his competitors. He did not think free market competition necessitated competitive warfare. Furthermore, Spencer argued that individuals with superior resources who deliberately used investment schemes to put competitor out of business were committing acts of “commercial murder”. Carnegie built his wealth in the steel industry by maintaining an extensively integrated operating system. Carnegie also bought out some regional competitors, and merged with others, usually maintaining the majority shares in the companies. Over the course of twenty years, Carnegie’s steel properties grew to include the Edgar Thomson Steel Works, the Lucy Furnace Works, the Union Iron Mills, the Homestead Works, the Keystone Bridge Works, the Hartman Steel Works, the Frick Coke Company, and the Scotia ore mines among many other industry related assets. Furthermore, Carnegie’s success was due to his convenient relationship with the railroad industries, which not only relied on steel for track, but were also making money from steel transport. The steel and railroad barons worked closely to negotiate prices instead of free market competition determinations.
Besides Carnegie’s market manipulation, United States trade tariffs were also working in favor of the steel industry. Carnegie spent energy and resources lobbying congress for a continuation of favorable tariffs from which he earned millions of dollars a year. Carnegie tried to keep this information concealed, but legal document released in 1900, during proceeding with the ex-chairman of Carnegie Steel Henry Clay Frick revealed how favorable the tariffs had been. Herbert Spencer absolutely was against government interference in business in the form of regulatory limitation, taxes, and tariffs as well. Spencer saw tariffs as a form of taxation that levied against the majority in service to “the benefit of a small minority of manufacturers and artisans”.
Despite Carnegie's personal dedication to Herbert Spencer as a friend, his adherence to Spencer’s political and economic ideas is more contentious. In particular, it appears Carnegie either misunderstood or intentionally misrepresented some of Spencer's principal arguments. Spencer remarked upon his first visit to Carnegie's steel mills in Pittsburgh, which Carnegie saw as the manifestation of Spencer's philosophy, "Six months' residence here would justify suicide."
On the subject of charity Andrew Carnegie's actions diverged in the most significant and complex manner from Herbert Spencer's philosophies. In his 1854 essay Manners and Fashion, Spencer referred to public education as “Old schemes”. He went on to declare that public schools and colleges, fill the heads of students with inept useless knowledge, which excludes useful knowledge. Spencer stated that he trusted no organization of any kind, “political, religious, literary, philanthropic”, and believed that as they expanded in influence so too did its regulations expand. In addition Spencer thought that as all institutions grow they become evermore corrupted by the influence of power and money. The institution eventually loses its “original spirit, and sinks into a lifeless mechanism”. Spencer insisted that all forms of philanthropy uplift the poor and downtrodden were reckless and incompetent. Spencer thought any attempt to prevent “the really salutary sufferings” of the less fortunate “bequeath to posterity a continually increasing curse”. Carnegie, a self-proclaimed devotee of Spencer, testified to Congress on February 5, 1915: "My business is to do as much good in the world as I can; I have retired from all other business."
Carnegie held that societal progress relied on individuals who maintained moral obligations to themselves and to society. Furthermore, he believed that charity supplied the means for those who wish to improve themselves to achieve their goals. Carnegie urged other wealthy people to contribute to society in the form of parks, works of art, libraries and other endeavors that improve the community and contribute to the “lasting good.” Carnegie also held a strong opinion against inherited wealth. Carnegie believed that the sons of prosperous businesspersons were rarely as talented as their fathers. By leaving large sums of money to their children, wealthy business leaders were wasting resources that could be used to benefit society. Most notably, Carnegie believed that the future leaders of society would rise from the ranks the poor. Carnegie strongly believed in this because he had risen from the bottom. He believed the poor possessed an advantage over the wealthy because they receive greater attention from their parents and are taught better work ethics.
Religion and world view.
Witnessing sectarianism and strife in 19th century Scotland regarding religion and philosophy, Carnegie kept his distance from organized religion and theism. Carnegie instead preferred to see things through naturalistic and scientific terms stating, "Not only had I got rid of the theology and the supernatural, but I had found the truth of evolution."
Later in life, Carnegie's firm opposition to religion softened. For many years he was a member of Madison Avenue Presbyterian Church, pastored from 1905 to 1926 by Social Gospel exponent Henry Sloane Coffin, while his wife and daughter belonged to the Brick Presbyterian Church. He also prepared (but did not deliver) an address in which he professed a belief in "an Infinite and Eternal Energy from which all things proceed".
World peace.
Influenced by his "favorite living hero in public life", the British liberal, John Bright, Carnegie started his efforts in pursuit of world peace at a young age. His motto, "All is well since all grows better", served not only as a good rationalization of his successful business career but also in his view of international relations.
Despite his efforts towards international peace, Carnegie faced many dilemmas on his quest. These dilemmas are often regarded as conflicts between his view on international relations and his other loyalties. Throughout the 1880s and 1890s, for example, Carnegie allowed his steel works to fill large orders of armor plate for the building of an enlarged and modernized United States Navy; while he opposed American oversea expansion. 
On the matter of American colonial expansion, Carnegie had always thought it is an unwise gesture for the United States. He did not oppose the annexation of the Hawaiian islands or Puerto Rico, but he opposed the annexation of the Philippines. Carnegie believed that it involved a denial of the fundamental democratic principle, and he also urged William McKinley to withdraw American troops and allow the Filipinos to live with their independence. This act well impressed the other American anti-imperialists, who soon elected him vice-president of the Anti-Imperialist League.
After he sold his steel company in 1901, Carnegie was able to get fully involved into the acts for the peace cause, both financially and personally. He gave away much of his fortunes= to various peace-keeping agencies in order to keep them growing. When his friend, the British publicist William T. Stead, asked him to create a new organization for the goal of a peace and arbitration society, his reply was as such:
Carnegie believed that it is the effort and will of the people, that maintains the peace in international relations. Money is just a push for the act. If world peace depended solely on financial support, it would not seem a goal, but more like an act of pity.
The creation of the Carnegie Endowment for International Peace in 1910 was regarded as a milestone on the road to the ultimate goal of abolition of war. Beyond a gift of $10 million for peace promotion, Carnegie also encouraged the "scientific" investigation of the various causes of war, and the adoption of judicial methods that should eventually eliminate them. He believed that the Endowment exists to promote information on the nations' rights and responsibilities under existing international law and to encourage other conferences to codify this law.
In 1914, on the eve of the First World War, Carnegie founded the Church Peace Union (CPU), a group of leaders in religion, academia, and politics. Through the CPU, Carnegie hoped to mobilize the world's churches, religious organizations, and other spiritual and moral resources to join in promoting moral leadership to put an end to war forever. For its inaugural international event, the CPU sponsored a conference to be held on August 1, 1914, on the shores of Lake Constance in southern Germany. As the delegates made their way to the conference by train, Germany was invading Belgium.
Despite its inauspicious beginning, the CPU thrived. Today its focus is on ethics and it is known as the Carnegie Council for Ethics in International Affairs, an independent, nonpartisan, nonprofit organization, whose mission is to be the voice for ethics in international affairs.
The outbreak of the First World War was clearly a shock to Carnegie and his optimistic view on world peace. Although his promotion of anti-imperialism and world peace had all failed, and the Carnegie Endowment had not fulfilled his expectations, his beliefs and ideas on international relations had helped build the foundation of the League of Nations after his death, which took world peace to another level.
Writings.
Carnegie was a frequent contributor to periodicals on labor issues. In addition to "Triumphant Democracy" (1886), and "The Gospel of Wealth" (1889), he also wrote "An American Four-in-hand in Britain" (1883), "Round the World" (1884), "The Empire of Business" (1902), "The Secret of Business is the Management of Men" (1903), "James Watt" (1905) in the Famous Scots Series, "Problems of Today" (1907), and his posthumously published autobiography "Autobiography of Andrew Carnegie" (1920).
Legacy and honors.
Carnegie received the honorary Doctor of Laws (DLL) from the University of Glasgow in June 1901, and received the Freedom of the City of Glasgow "in recognition of his munificence" later the same year.
Carnegie's personal papers reside at the Library of Congress Manuscript Division.
The Carnegie Collections of the Columbia University Rare Book and Manuscript Library consist of the archives of the following organizations founded by Carnegie: The Carnegie Corporation of New York (CCNY); The Carnegie Endowment for International Peace (CEIP); the Carnegie Foundation for the Advancement of Teaching (CFAT);The Carnegie Council on Ethics and International Affairs (CCEIA). These collections deal primarily with Carnegie philanthropy and have very little personal material related to Carnegie. Carnegie Mellon University and the Carnegie Library of Pittsburgh jointly administer the Andrew Carnegie Collection of digitized archives on Carnegie's life.

</doc>
<doc id="1939" url="http://en.wikipedia.org/wiki?curid=1939" title="Approximant consonant">
Approximant consonant

Approximants are speech sounds that involve the articulators approaching each other but not narrowly enough nor with enough articulatory precision to create turbulent airflow. Therefore, approximants fall between fricatives, which do produce a turbulent airstream, and vowels, which produce no turbulence. This class of sounds includes lateral approximants like (as in "less"), non-lateral approximants like (as in "rest"), and semivowels like and (as in "yes" and "west", respectively).
Before Peter Ladefoged coined the term "approximant" in the 1960s the term "frictionless continuant" referred to non-lateral approximants.
Semivowels.
Some approximants resemble vowels in acoustic and articulatory properties and the terms "semivowel" and "glide" are often used for these non-syllabic vowel-like segments. The correlation between semivowels and vowels is strong enough that cross-language differences between semivowels correspond with the differences between their related vowels.
Vowels and their corresponding semivowels alternate in many languages depending on the phonological environment, or for grammatical reasons, as is the case with Indo-European ablaut. Similarly, languages often avoid configurations where a semivowel precedes its corresponding vowel. A number of phoneticians distinguish between semivowels and approximants by their location in a syllable. Although he uses the terms interchangeably, remarks that, for example, the final glides of English "par" and "buy" differ from French "par" ('through') and "baille" ('tub') in that, in the latter pair, the approximants appear in the syllable coda, whereas, in the former, they appear in the syllable nucleus. This means that opaque (if not minimal) contrasts can occur in languages like Italian (with the i-like sound of "piede" 'foot', appearing in the nucleus: , and that of "piano" 'slow', appearing in the syllable onset: ) and Spanish (with a near minimal pair being "abyecto" 'abject' and "abierto" 'opened').
In articulation and often diachronically, palatal approximants correspond to front vowels, velar approximants to back vowels, and labialized approximants to rounded vowels. In American English, the rhotic approximant corresponds to the rhotic vowel. This can create alternations (as shown in the above table).
In addition to alternations, glides can be inserted to the left or the right of their corresponding vowels when occurring next to a hiatus. For example, in Ukrainian, medial triggers the formation of an inserted that acts as a syllable onset so that when the affix is added to футбол ('football') to make футболіст 'football player', it's pronounced but маоїст ('Maoist'), with the same affix, is pronounced with a glide. Dutch has a similar process that extends to mid vowels:
Similarly, vowels can be inserted next to their corresponding glide in certain phonetic environments. Sievers' law describes this behaviour for Germanic.
Non-high semivowels also occur. In colloquial Nepali speech, a process of glide-formation occurs, wherein one of two adjacent vowels becomes non-syllabic; this process includes mid vowels so that ('cause to wish') features a non-syllabic mid vowel. Spanish features a similar process and even nonsyllabic can occur so that "ahorita" ('right away') is pronounced . It is not often clear, however, whether such sequences involve a semivowel (a consonant) or a diphthong (a vowel), and in many cases that may not be a meaningful distinction.
Although many languages have central vowels , which lie between back/velar and front/palatal , there are few cases of a corresponding approximant . One is in the Korean diphthong or , though this is more frequently analyzed as velar (as in the table above), and Mapudungun may be another: It has three high vowel sounds, , , and three corresponding consonants, , and , and a third one is often described as a voiced unrounded velar fricative; some texts note a correspondence between this approximant and that is parallel to – and –. An example is "liq" (?) ('white').
Approximants versus fricatives.
In addition to less turbulence, approximants also differ from fricatives in the precision required to produce them. 
When emphasized, approximants may be slightly fricated (that is, the airstream may become slightly turbulent), which is reminiscent of fricatives. For example, the Spanish word "ayuda" ('help') features a palatal approximant that is pronounced as a fricative in emphatic speech. However, such frication is generally slight and intermittent, unlike the strong turbulence of fricative consonants. 
Because voicelessness has comparatively reduced resistance to air flow from the lungs, the increased air flow creates more turbulence, making acoustic distinctions between voiceless approximants (which are extremely rare cross-linguistically) and voiceless fricatives difficult. This is why, for example, the voiceless labialized velar approximant (also transcribed with the special letter ) has traditionally been labeled a fricative, and no language is known to contrast it with a voiceless labialized velar fricative . Similarly, Standard Tibetan has a voiceless lateral approximant, , and Welsh has a voiceless lateral fricative , but the distinction is not always clear from descriptions of these languages. Again, no language is known to contrast the two. Iaai is reported to have an unusually large number of voiceless approximants, with .
For places of articulation further back in the mouth, languages do not contrast voiced fricatives and approximants. Therefore the IPA allows the symbols for the voiced fricatives to double for the approximants, with or without a lowering diacritic. 
Occasionally, the glottal "fricatives" are called approximants, since typically has no more frication than voiceless approximants, but they are often phonations of the glottis without any accompanying manner or place of articulation.
Lateral approximants.
In lateral approximants, the center of tongue makes solid contact with the roof of the mouth. However, the defining location is the side of the tongue, which only approaches the teeth. 
Voiceless approximants.
Voiceless approximants are rarely distinguished from voiceless fricatives. Some of them are:
Nasal approximants.
Examples are:
In Portuguese, the nasal glides and historically became and in some words. In Bini, the nasalized allophones of the approximants and are nasal occlusives, and .
What are transcribed as nasal approximants may include non-syllabic elements of nasal vowels/diphthongs.

</doc>
<doc id="1940" url="http://en.wikipedia.org/wiki?curid=1940" title="Astronomer Royal">
Astronomer Royal

Astronomer Royal is a senior post in the Royal Households of the United Kingdom. There are two officers, the senior being the Astronomer Royal dating from 22 June 1675; the second is the Astronomer Royal for Scotland dating from 1834.
King Charles II, who founded the Royal Observatory Greenwich in 1675 instructed the first Astronomer Royal John Flamsteed "."
From that time until 1972, the Astronomer Royal was Director of the Royal Observatory Greenwich. The Astronomer Royal receives a stipend of 100 GBP per year and is a member of the Royal Household, under the general authority of the Lord Chamberlain. After the separation of the two offices, the position of Astronomer Royal has been largely honorary, though he remains available to advise the Sovereign on astronomical and related scientific matters, and the office is of great prestige.
There was also formerly a Royal Astronomer of Ireland.

</doc>
<doc id="1941" url="http://en.wikipedia.org/wiki?curid=1941" title="Aeon">
Aeon

The word aeon , also spelled eon, originally means "life" or "being", though it then tended to mean "age", "forever" or "for eternity". It is a Latin transliteration from the koine Greek word ("ho aion"), from the archaic ("aiwon"). In Homer it typically refers to life or lifespan. Its latest meaning is more or less similar to the Sanskrit word "kalpa" and Hebrew word "olam". A cognate Latin word "aevum" or "aeuum" (cf. ) for "age" is present in words such as "longevity" and "mediaeval".
Although the term aeon may be used in reference to a period of a billion years (especially in geology, cosmology or astronomy), its more common usage is for any long, indefinite, period. Aeon can also refer to the four aeons on the Geologic Time Scale that make up the Earth's history, the Hadean, Archean, Proterozoic, and the current aeon Phanerozoic.
Astronomy and cosmology.
In astronomy an aeon is defined as a billion years (109).
Roger Penrose uses the word "aeon" to describe the period between successive and cyclic big bangs within the context of conformal cyclic cosmology.
Eternity or age.
The Bible translation is a treatment of the Hebrew word "olam" and the Greek word "aion". Both these words have similar meaning, and Young's Literal Translation renders them and their derivatives as “age” or “age-during”. Other English versions most often translate them to indicate eternity, being translated as eternal, everlasting, forever, etc. However, there are notable exceptions to this in all major translations, such as : “…I am with you always, to the end of the age” (NRSV), the word “age” being a translation of "aion". Rendering "aion" to indicate eternality in this verse would result in the contradictory phrase “end of eternity”, so the question arises whether it should ever be so. Proponents of Universal Reconciliation point out that this has significant implications for the problem of hell. Contrast in well-known English translations with its rendering in Young's Literal Translation:
And these shall go away to punishment age-during, but the righteous to life age-during. (YLT)
Then they will go away to eternal punishment, but the righteous to eternal life. (NIV)
These will go away into eternal punishment, but the righteous into eternal life. (NASB)
And these shall go away into everlasting punishment, but the righteous into eternal life. (KJV)
And these will depart into everlasting cutting-off, but the righteous ones into everlasting life. (NWT)
Philosophy and mysticism.
Plato used the word "aeon" to denote the eternal world of ideas, which he conceived was "behind" the perceived world, as demonstrated in his famous allegory of the cave.
Christianity's idea of "eternal life" comes from the word for life, "zoe", and a form of "aeon", which could mean life in the next aeon, the Kingdom of God, or Heaven, just as much as immortality, as in .
According to the Christian doctrine of Universal Reconciliation, the Greek New Testament scriptures use the word "eon" to mean a long period (perhaps 1000 years) and the word "eonian" to mean "during a long period"; Thus there was a time before the eons, and the eonian period is finite. After each man's mortal life ends, he is judged worthy of eonian life or eonian punishment. That is, after the period of the eons, all punishment will cease and death is overcome and then God becomes the all in each one (). This contrasts with the conventional Christian belief in eternal life and eternal punishment.
Occultists of the Thelema and O.T.O. traditions sometimes speak of a "magical Aeon" that may last for far less time, perhaps as little as 2,000 years.
Aeon may also be an archaic name for omnipotent beings, such as gods.
Gnosticism.
In many Gnostic systems, the various emanations of God, who is also known by such names as the One, the Monad, "Aion teleos" ( "The Broadest Aeon"), Bythos ("depth or profundity", Greek ), "Proarkhe" ("before the beginning", Greek ), the "Arkhe" ("the beginning", Greek ), "Sophia" (wisdom), Christos (the Anointed One) are called "Aeons". In the different systems these emanations are differently named, classified, and described, but the emanation theory itself is common to all forms of Gnosticism. 
In the Basilidian Gnosis they are called sonships (υἱότητες "huiotetes"; sing.: "huiotes"); according to Marcus, they are numbers and sounds; in Valentinianism they form male/female pairs called "syzygies" (Greek , from σύζυγοι "syzygoi").
Similarly, in the Greek Magical Papyri, the term "Aion" is often used to denote the All, or the supreme aspect of God. 

</doc>
<doc id="1942" url="http://en.wikipedia.org/wiki?curid=1942" title="Airline">
Airline

An airline is a company that provides air transport services for traveling passengers and freight. Airlines lease or own their aircraft with which to supply these services and may form partnerships or alliances with other airlines for mutual benefit. Generally, airline companies are recognized with an air operating certificate or license issued by a governmental aviation body.
Airlines vary from those with a single aircraft carrying mail or cargo, through full-service international airlines operating hundreds of aircraft. Airline services can be categorized as being intercontinental, intra-continental, domestic, regional, or international, and may be operated as scheduled services or charters.
History.
The first airlines.
DELAG, "Deutsche Luftschiffahrts-Aktiengesellschaft" was the world's first airline. It was founded on November 16, 1909 with government assistance, and operated airships manufactured by The Zeppelin Corporation. Its headquarters were in Frankfurt. The four oldest non-dirigible airlines that still exist are Netherlands' KLM, Colombia's Avianca, Australia's Qantas, and the Czech Republic's Czech Airlines. KLM first flew in May 1920, while Qantas (which stands for "Queensland and Northern Territory Aerial Services Limited") was founded in Queensland, Australia, in late 1920.
European airline industry.
Beginnings.
The earliest fixed wing airline was the Aircraft Transport and Travel, formed by George Holt Thomas in 1916. Using a fleet of former military Airco DH.4A biplanes that had been modified to carry two passengers in the fuselage, it operated relief flights between Folkestone and Ghent. On 15 July 1919, the company flew a proving flight across the English Channel, despite a lack of support from the British government. Flown by Lt. H Shaw in an Airco DH.9 between RAF Hendon and Paris - Le Bourget Airport, the flight took 2 hours and 30 minutes at £21 per passenger.
On 25 August 1919, the company used DH.16s to pioneer a regular service from Hounslow Heath Aerodrome to Le Bourget, the first regular international service in the world. The airline soon gained a reputation for reliability, despite problems with bad weather and began to attract European competition. In November 1919, it won the first British civil airmail contract. Six Royal Air Force Airco DH.9A aircraft were lent to the company, to operate the airmail service between Hawkinge and Cologne. In 1920, they were returned to the Royal Air Force.
Other British competitors were quick to follow - Handley Page Transport was established in 1919 and used the company's converted wartime Type O/400 bombers with a capacity for 19 passengers, to run a London-Paris passenger service.
The first French airlines were also established and began to offer competition for the same route. The Société Générale des Transports Aériens was created in late 1919, by the Farman brothers and the Farman F.60 Goliath plane flew scheduled services from Toussus-le-Noble to Kenley, near Croydon. Another early French airline was the Compagnie des Messageries Aériennes, established in 1919 by Louis-Charles Breguet, offering a mail and freight service between Le Bourget Airport, Paris and Lesquin Airport, Lille.
The Dutch airline KLM made its first flight in 1920, and is the oldest continuously operating airline in the world. Established by aviator Albert Plesman, it was immediately awarded a "Royal" predicate from Queen Wilhelmina Its first flight was from Croydon Airport, London to Amsterdam, using a leased Aircraft Transport and Travel DH-16, and carrying two British journalists and a number of newspapers. In 1921 KLM started scheduled services.
In Finland, the charter establishing Aero O/Y (now Finnair) was signed in the city of Helsinki on September 12, 1923. Junkers F.13 D-335 became the first aircraft of the company, when Aero took delivery of it on March 14, 1924. The first flight was between Helsinki and Tallinn, capital of Estonia, and it took place on March 20, 1924, one week later.
In the Soviet Union, the Chief Administration of the Civil Air Fleet was established in 1921. One of its first acts was to help found Deutsch-Russische Luftverkehrs A.G. (Deruluft), a German-Russian joint venture to provide air transport from Russia to the West. Domestic air service began around the same time, when Dobrolyot started operations on 15 July 1923 between Moscow and Nizhni Novgorod. Since 1932 all operations had been carried under the name Aeroflot.
Early European airlines tended to favour comfort - the passenger cabins were often spacious with luxury interiors - over speed and efficiency. The relatively basic navigational capabilities of pilots at the time also meant that delays due to the weather, especially during the winter in the south of England, were commonplace.
Rationalization.
By the early 1920s, small airlines were struggling to compete, and there was a movement towards increased rationalization and consolidation. In 1924, Imperial Airways was formed from the merger of Instone Air Line Company, British Marine Air Navigation, Daimler Airway and Handley Page Transport Co Ltd., to allow British airlines to compete with stiff competition from French and German airlines that were enjoying heavy government subsidies. The airline was a pioneer in surveying and opening up air routes across the world to serve far-flung parts of the British Empire and to enhance trade and integration.
The first new airliner ordered by Imperial Airways, was the Handley Page W8f "City of Washington", delivered on 3 November 1924. In the first year of operation the company carried 11,395 passengers and 212,380 letters. In April 1925, the film "The Lost World" became the first film to be screened for passengers on a scheduled airliner flight when it was shown on the London-Paris route.
Two French airlines also merged to form Air Union on 1 January 1923. This later merged with four other French airlines to become Air France, the country's flagship carrier to this day, on 7 October 1933.
Germany's Deutsche Luft Hansa was created in 1926 by merger of two airlines, one of them Junkers Luftverkehr. Luft Hansa, due to the Junkers heritage and unlike most other airlines at the time, became a major investor in airlines outside of Europe, providing capital to Varig and Avianca. German airliners built by Junkers, Dornier, and Fokker were among the most advanced in the world at the time.
Global expansion.
In 1926, Alan Cobham surveyed a flight route from the UK to Cape Town, South Africa, following this up with another proving flight to Melbourne, Australia. Other routes to British India and the Far East were also charted and demonstrated at this time. Regular services to Cairo and Basra began in 1927 and was extended to Karachi in 1929. The London-Australia service was inaugurated in 1932 with the Handley Page HP 42 airliners. Further services were opened up to Calcutta, Rangoon, Singapore, Brisbane and Hong Kong passengers departed London on 14 March 1936 following the establishment of a branch from Penang to Hong Kong.
Imperial's aircraft were small, most seating fewer than twenty passengers, and catered for the rich - only about 50,000 passengers used Imperial Airways in the 1930s. Most passengers on intercontinental routes or on services within and between British colonies were men doing colonial administration, business or research.
Like Imperial Airways, Air France and KLM's early growth depended heavily on the needs to service links with far-flung colonial possessions (North Africa and Indochina for the French and the East Indies for the Dutch). France began an air mail service to Morocco in 1919 that was bought out in 1927, renamed Aéropostale, and injected with capital to become a major international carrier. In 1933, Aéropostale went bankrupt, was nationalized and merged into Air France.
Although Germany lacked colonies, it also began expanding its services globally. In 1931, the airship Graf Zeppelin began offering regular scheduled passenger service between Germany and South America, usually every two weeks, which continued until 1937. In 1936, the airship Hindenburg entered passenger service and successfully crossed the Atlantic 36 times before crashing at Lakehurst, New Jersey on May 6, 1937.
By the end of the 1930s Aeroflot had become the world's largest airline, employing more than 4,000 pilots and 60,000 other service personnel and operating around 3,000 aircraft (of which 75% were considered obsolete by its own standards). During the Soviet era Aeroflot was synonymous with Russian civil aviation, as it was the only air carrier. It became the first airline in the world to operate sustained regular jet services on 15 September 1956 with the Tupolev Tu-104.
EU airline deregulation.
Deregulation of the European Union airspace in the early 1990s has had substantial effect on structure of the industry there. The shift towards 'budget' airlines on shorter routes has been significant. Airlines such as EasyJet and Ryanair have often grown at the expense of the traditional national airlines.
There has also been a trend for these national airlines themselves to be privatized such as has occurred for Aer Lingus and British Airways. Other national airlines, including Italy's Alitalia, have suffered - particularly with the rapid increase of oil prices in early 2008.
U.S. airline industry.
Early development.
Tony Jannus conducted the United States' first scheduled commercial airline flight on 1 January 1914 for the St. Petersburg-Tampa Airboat Line. The 23-minute flight traveled between St. Petersburg, Florida and Tampa, Florida, passing some above Tampa Bay in Jannus' Benoist XIV wood and muslin biplane flying boat. His passenger was a former mayor of St. Petersburg, who paid $400 for the privilege of sitting on a wooden bench in the open cockpit. The Airboat line operated for about four months, carrying more than 1,200 passengers who paid $5 each. Chalk's International Airlines began service between Miami and Bimini in the Bahamas in February 1919. Based in Ft. Lauderdale, Chalk's claimed to be the oldest continuously operating airline in the United States until its closure in 2008.
Following World War I, the United States found itself swamped with aviators. Many decided to take their war-surplus aircraft on barnstorming campaigns, performing aerobatic maneuvers to woo crowds. In 1918, the United States Postal Service won the financial backing of Congress to begin experimenting with air mail service, initially using Curtiss Jenny aircraft that had been procured by the United States Army Air Service. Private operators were the first to fly the mail but due to numerous accidents the US Army was tasked with mail delivery. During the Army's involvement they proved to be too unreliable and lost their air mail duties. By the mid-1920s, the Postal Service had developed its own air mail network, based on a transcontinental backbone between New York City and San Francisco. To supplant this service, they offered twelve contracts for spur routes to independent bidders. Some of the carriers that won these routes would, through time and mergers, evolve into Pan Am, Delta Air Lines, Braniff Airways, American Airlines, United Airlines (originally a division of Boeing), Trans World Airlines, Northwest Airlines, and Eastern Air Lines.
Service during the early 1920s was sporadic: most airlines at the time were focused on carrying bags of mail. In 1925, however, the Ford Motor Company bought out the Stout Aircraft Company and began construction of the all-metal Ford Trimotor, which became the first successful American airliner. With a 12-passenger capacity, the Trimotor made passenger service potentially profitable. Air service was seen as a supplement to rail service in the American transportation network.
At the same time, Juan Trippe began a crusade to create an air network that would link America to the world, and he achieved this goal through his airline, Pan American World Airways, with a fleet of flying boats that linked Los Angeles to Shanghai and Boston to London. Pan Am and Northwest Airways (which began flights to Canada in the 1920s) were the only U.S. airlines to go international before the 1940s.
With the introduction of the Boeing 247 and Douglas DC-3 in the 1930s, the U.S. airline industry was generally profitable, even during the Great Depression. This trend continued until the beginning of World War II.
Development since 1945.
As governments met to set the standards and scope for an emergent civil air industry toward the end of the war, the U.S. took a position of maximum operating freedom; U.S. airline companies were not as hard-hit as European and the few Asian ones had been. This preference for "open skies" operating regimes continues, with limitations, to this day.
World War II, like World War I, brought new life to the airline industry. Many airlines in the Allied countries were flush from lease contracts to the military, and foresaw a future explosive demand for civil air transport, for both passengers and cargo. They were eager to invest in the newly emerging flagships of air travel such as the Boeing Stratocruiser, Lockheed Constellation, and Douglas DC-6. Most of these new aircraft were based on American bombers such as the B-29, which had spearheaded research into new technologies such as pressurization. Most offered increased efficiency from both added speed and greater payload.
In the 1950s, the De Havilland Comet, Boeing 707, Douglas DC-8, and Sud Aviation Caravelle became the first flagships of the Jet Age in the West, while the Eastern bloc had Tupolev Tu-104 and Tupolev Tu-124 in the fleets of state-owned carriers such as Czechoslovak ČSA, Soviet Aeroflot and East-German Interflug. The Vickers Viscount and Lockheed L-188 Electra inaugurated turboprop transport.
The next big boost for the airlines would come in the 1970s, when the Boeing 747, McDonnell Douglas DC-10, and Lockheed L-1011 inaugurated widebody ("jumbo jet") service, which is still the standard in international travel. The Tupolev Tu-144 and its Western counterpart, Concorde, made supersonic travel a reality. Concorde first flew in 1969 and operated through 2003. In 1972, Airbus began producing Europe's most commercially successful line of airliners to date. The added efficiencies for these aircraft were often not in speed, but in passenger capacity, payload, and range. Airbus also features modern electronic cockpits that were common across their aircraft to enable pilots to fly multiple models with minimal cross-training.
US airline deregulation.
1970 U.S. airline industry deregulation lowered federally controlled barriers for new airlines just as a downturn in the nation's economy occurred. New start-ups entered during the downturn, during which time they found aircraft and funding, contracted hangar and maintenance services, trained new employees, and recruited laid off staff from other airlines.
Major airlines dominated their routes through aggressive pricing and additional capacity offerings, often swamping new start-ups. In the place of high barriers to entry imposed by regulation, the major airlines implemented an equally high barrier called loss leader pricing. In this strategy an already established and dominant airline stomps out its competition by lowering airfares on specific routes, below the cost of operating on it, choking out any chance a start-up airline may have. The industry side effect is an overall drop in revenue and service quality. Since deregulation in 1978 the average domestic ticket price has dropped by 40%. So has airline employee pay. By incurring massive losses, the airlines of the USA now rely upon a scourge of cyclical Chapter 11 bankruptcy proceedings to continue doing business. America West Airlines (which has since merged with US Airways) remained a significant survivor from this new entrant era, as dozens, even hundreds, have gone under.
In many ways, the biggest winner in the deregulated environment was the air passenger. Although not exclusively attributable to deregulation, indeed the U.S. witnessed an explosive growth in demand for air travel. Many millions who had never or rarely flown before became regular fliers, even joining frequent flyer loyalty programs and receiving free flights and other benefits from their flying. New services and higher frequencies meant that business fliers could fly to another city, do business, and return the same day, from almost any point in the country. Air travel's advantages put long distance intercity railroad travel and bus lines under pressure, with most of the latter having withered away, whilst the former is still protected under nationalization through the continuing existence of Amtrak.
By the 1980s, almost half of the total flying in the world took place in the U.S., and today the domestic industry operates over 10,000 daily departures nationwide.
Toward the end of the century, a new style of low cost airline emerged, offering a no-frills product at a lower price. Southwest Airlines, JetBlue, AirTran Airways, Skybus Airlines and other low-cost carriers began to represent a serious challenge to the so-called "legacy airlines", as did their low-cost counterparts in many other countries. Their commercial viability represented a serious competitive threat to the legacy carriers. However, of these, ATA and Skybus have since ceased operations.
Increasingly since 1978, US airlines have been reincorporated and spun off by newly created and internally led manangement companies, and thus becoming nothing more than operating units and subsidiaries with limited financially decisive control. Among some of these holding companies and parent companies which are relatively well known, are the UAL Corporation, along with the AMR Corporation, among a long list of airline holding companies sometime recognized worldwide. Less recognized are the private equity firms which often seize managerial, financial, and board of directors control of distressed airline companies by temporarily investing large sums of capital in air carriers, to rescheme an airlines assets into a profitable organization or liquidating an air carrier of their profitable and worthwhile routes and business operations.
Thus the last 50 years of the airline industry have varied from reasonably profitable, to devastatingly depressed. As the first major market to deregulate the industry in 1978, U.S. airlines have experienced more turbulence than almost any other country or region. In fact, no U.S. legacy carrier survived bankruptcy-free. Amongst the outspoken critics of deregulation, former CEO of American Airlines, Robert Crandall has publicly stated:
"Chapter 11 bankruptcy protection filing shows airline industry deregulation was a mistake."
The airline industry bailout.
Congress passed the Air Transportation Safety and System Stabilization Act (P.L. 107-42) in response to a severe liquidity crisis facing the already-troubled airline industry in the aftermath of the September 11th terrorist attacks. Congress sought to provide cash infusions to carriers for both the cost of the four-day federal shutdown of the airlines and the incremental losses incurred through December 31, 2001 as a result of the terrorist attacks. This resulted in the first government bailout of the 21st century. Between 2000 and 2005 US airlines lost $30 billion with wage cuts of over $15 billion and 100,000 employees laid off.
In recognition of the essential national economic role of a healthy aviation system, Congress authorized partial compensation of up to $5 billion in cash subject to review by the Department of Transportation and up to $10 billion in loan guarantees subject to review by a newly created Air Transportation Stabilization Board (ATSB). The applications to DOT for reimbursements were subjected to rigorous multi-year reviews not only by DOT program personnel but also by the Government Accountability Office and the DOT Inspector General.
Ultimately, the federal government provided $4.6 billion in one-time, subject-to-income-tax cash payments to 427 U.S. air carriers, with no provision for repayment, essentially a gift from the taxpayers. (Passenger carriers operating scheduled service received approximately $4 billion, subject to tax.) In addition, the ATSB approved loan guarantees to six airlines totaling approximately $1.6 billion. Data from the US Treasury Department show that the government recouped the $1.6 billion and a profit of $339 million from the fees, interest and purchase of discounted airline stock associated with loan guarantees.
Asian airline industry.
Although Philippine Airlines (PAL) was officially founded on February 26, 1941, its license to operate as an airliner was derived from merged Philippine Aerial Taxi Company (PATCO) established by mining magnate Emmanuel N. Bachrach on December 3, 1930, making it Asia's oldest scheduled carrier still in operation. Commercial air service commenced three weeks later from Manila to Baguio, making it Asia's first airline route. Bachrach's death in 1937 paved the way for its eventual merger with Philippine Airlines in March 1941 and made it Asia's oldest airline. It is also the oldest airline in Asia still operating under its current name. Bachrach's majority share in PATCO was bought by beer magnate Andres R. Soriano in 1939 upon the advice of General Douglas McArthur and later merged with newly formed Philippine Airlines with PAL as the surviving entity. Soriano has controlling interest in both airlines before the merger. PAL restarted service on March 15, 1941 with a single Beech Model 18 NPC-54 aircraft, which started its daily services between Manila (from Nielson Field) and Baguio, later to expand with larger aircraft such as the DC-3 and Vickers Viscount.
India was also one of the first countries to embrace civil aviation. One of the first West Asian airline companies was Air India, which had its beginning as Tata Airlines in 1932, a division of Tata Sons Ltd. (now Tata Group). The airline was founded by India's leading industrialist, JRD Tata. On October 15, 1932, J. R. D. Tata himself flew a single engined De Havilland Puss Moth carrying air mail (postal mail of Imperial Airways) from Karachi to Bombay via Ahmedabad. The aircraft continued to Madras via Bellary piloted by Royal Air Force pilot Nevill Vintcent. Tata Airlines was also one of the world's first major airlines which began its operations without any support from the Government.
With the outbreak of World War II, the airline presence in Asia came to a relative halt, with many new flag carriers donating their aircraft for military aid and other uses. Following the end of the war in 1945, regular commercial service was restored in India and Tata Airlines became a public limited company on July 29, 1946 under the name Air India. After the independence of India, 49% of the airline was acquired by the Government of India. In return, the airline was granted status to operate international services from India as the designated flag carrier under the name Air India International.
On July 31, 1946, a chartered Philippine Airlines (PAL) DC-4 ferried 40 American servicemen to Oakland, California, from Nielson Airport in Makati City with stops in Guam, Wake Island, Johnston Atoll and Honolulu, Hawaii, making PAL the first Asian airline to cross the Pacific Ocean. A regular service between Manila and San Francisco was started in December. It was during this year that the airline was designated as the flag carrier of Philippines.
During the era of decolonization, newly born Asian countries started to embrace air transport. Among the first Asian carriers during the era were Cathay Pacific of Hong Kong (founded in September 1946 ), Orient Airways (later Pakistan International Airlines; founded in October 1946), Malayan Airways Limited in 1947 (later Singapore and Malaysia Airlines), El Al in Israel in 1948, Garuda Indonesia in 1948, Japan Airlines in 1951, Thai Airways International in 1960, and Korean National Airlines in 1947.
Latin American airline industry.
Among the first countries to have regular airlines in Latin America were Bolivia with Lloyd Aéreo Boliviano, Cuba with Cubana de Aviación, Colombia with Avianca, Argentina with Aerolineas Argentinas, Chile with LAN Chile (today LAN Airlines), Brazil with Varig, Dominican Republic with Dominicana de Aviación, Mexico with Mexicana de Aviación, Trinidad and Tobago with BWIA West Indies Airways (today Caribbean Airlines), Venezuela with Aeropostal, and TACA based in El Salvador and representing several airlines of Central America (Costa Rica, Guatemala, Honduras and Nicaragua). All the previous airlines started regular operations well before World War II.
The air travel market has evolved rapidly over recent years in Latin America. Some industry estimates indicate that over 2,000 new aircraft will begin service over the next five years in this region.
These airlines serve domestic flights within their countries, as well as connections within Latin America and also overseas flights to North America, Europe, Australia, and Asia.
Only three airlines: LAN, OceanAir and TAM Airlines have international subsidiaries and cover many destinations within the Americas as well as major hubs in other continents. LAN with Chile as the central operation along with Peru, Ecuador, Colombia and Argentina and some operations in the Dominican Republic. The recently formed AviancaTACA group has control of Avianca Brazil, VIP Ecuador and a strategic alliance with AeroGal. And TAM with its Mercosur base in Asuncion, Paraguay. As of 2010, talks of uniting LAN and TAM have strongly developed to create a joint airline named LATAM.
Regulatory considerations.
National.
Many countries have national airlines that the government owns and operates. Fully private airlines are subject to a great deal of government regulation for economic, political, and safety concerns. For instance, governments often intervene to halt airline labor actions to protect the free flow of people, communications, and goods between different regions without compromising safety.
The United States, Australia, and to a lesser extent Brazil, Mexico, India, the United Kingdom, and Japan have "deregulated" their airlines. In the past, these governments dictated airfares, route networks, and other operational requirements for each airline. Since deregulation, airlines have been largely free to negotiate their own operating arrangements with different airports, enter and exit routes easily, and to levy airfares and supply flights according to market demand.
The entry barriers for new airlines are lower in a deregulated market, and so the U.S. has seen hundreds of airlines start up (sometimes for only a brief operating period). This has produced far greater competition than before deregulation in most markets, and average fares tend to drop 20% or more. The added competition, together with pricing freedom, means that new entrants often take market share with highly reduced rates that, to a limited degree, full service airlines must match. This is a major constraint on profitability for established carriers, which tend to have a higher cost base.
As a result, profitability in a deregulated market is uneven for most airlines. These forces have caused some major airlines to go out of business, in addition to most of the poorly established new entrants.
International.
Groups such as the International Civil Aviation Organization establish worldwide standards for safety and other vital concerns. Most international air traffic is regulated by bilateral agreements between countries, which designate specific carriers to operate on specific routes. The model of such an agreement was the Bermuda Agreement between the US and UK following World War II, which designated airports to be used for transatlantic flights and gave each government the authority to nominate carriers to operate routes.
Bilateral agreements are based on the "freedoms of the air", a group of generalized traffic rights ranging from the freedom to overfly a country to the freedom to provide domestic flights within a country (a very rarely granted right known as cabotage). Most agreements permit airlines to fly from their home country to designated airports in the other country: some also extend the freedom to provide continuing service to a third country, or to another destination in the other country while carrying passengers from overseas.
In the 1990s, "open skies" agreements became more common. These agreements take many of these regulatory powers from state governments and open up international routes to further competition. Open skies agreements have met some criticism, particularly within the European Union, whose airlines would be at a comparative disadvantage with the United States' because of cabotage restrictions.
Economic considerations.
Historically, air travel has survived largely through state support, whether in the form of equity or subsidies. The airline industry as a whole has made a cumulative loss during its 100-year history, once the costs include subsidies for aircraft development and airport construction.
One argument is that positive externalities, such as higher growth due to global mobility, outweigh the microeconomic losses and justify continuing government intervention. A historically high level of government intervention in the airline industry can be seen as part of a wider political consensus on strategic forms of transport, such as highways and railways, both of which receive public funding in most parts of the world. Profitability is likely to improve in the future as privatization continues and more competitive low-cost carriers proliferate.
Although many countries continue to operate state-owned or parastatal airlines, many large airlines today are privately owned and are therefore governed by microeconomic principles to maximize shareholder profit.
Top airline groups by revenue.
for 2010, source : Airline Business August 2011, Flightglobal Data Research
Ticket revenue.
Airlines assign prices to their services in an attempt to maximize profitability. The pricing of airline tickets has become increasingly complicated over the years and is now largely determined by computerized yield management systems.
Because of the complications in scheduling flights and maintaining profitability, airlines have many loopholes that can be used by the knowledgeable traveler. Many of these airfare secrets are becoming more and more known to the general public, so airlines are forced to make constant adjustments.
Most airlines use differentiated pricing, a form of price discrimination, to sell air services at varying prices simultaneously to different segments. Factors influencing the price include the days remaining until departure, the booked load factor, the forecast of total demand by price point, competitive pricing in force, and variations by day of week of departure and by time of day. Carriers often accomplish this by dividing each cabin of the aircraft (first, business and economy) into a number of travel classes for pricing purposes.
A complicating factor is that of origin-destination control ("O&D control"). Someone purchasing a ticket from Melbourne to Sydney (as an example) for A$200 is competing with someone else who wants to fly Melbourne to Los Angeles through Sydney on the same flight, and who is willing to pay A$1400. Should the airline prefer the $1400 passenger, or the $200 passenger plus a possible Sydney-Los Angeles passenger willing to pay $1300? Airlines have to make hundreds of thousands of similar pricing decisions daily.
The advent of advanced computerized reservations systems in the late 1970s, most notably Sabre, allowed airlines to easily perform cost-benefit analyses on different pricing structures, leading to almost perfect price discrimination in some cases (that is, filling each seat on an aircraft at the highest price that can be charged without driving the consumer elsewhere).
The intense nature of airfare pricing has led to the term "fare war" to describe efforts by airlines to undercut other airlines on competitive routes. Through computers, new airfares can be published quickly and efficiently to the airlines' sales channels. For this purpose the airlines use the Airline Tariff Publishing Company (ATPCO), who distribute latest fares for more than 500 airlines to Computer Reservation Systems across the world.
The extent of these pricing phenomena is strongest in "legacy" carriers. In contrast, low fare carriers usually offer preannounced and simplified price structure, and sometimes quote prices for each leg of a trip separately.
Computers also allow airlines to predict, with some accuracy, how many passengers will actually fly after making a reservation to fly. This allows airlines to overbook their flights enough to fill the aircraft while accounting for "no-shows," but not enough (in most cases) to force paying passengers off the aircraft for lack of seats, stimulative pricing for low demand flights coupled with overbooking on high demand flights can help reduce this figure. This is especially crucial during tough economic times as airlines undertake massive cuts to ticket prices to retain demand.
Operating costs.
Full-service airlines have a high level of fixed and operating costs to establish and maintain air services: labor, fuel, airplanes, engines, spares and parts, IT services and networks, airport equipment, airport handling services, sales distribution, catering, training, aviation insurance and other costs. Thus all but a small percentage of the income from ticket sales is paid out to a wide variety of external providers or internal cost centers.
Moreover, the industry is structured so that airlines often act as tax collectors. Airline fuel is untaxed because of a series of treaties existing between countries. Ticket prices include a number of fees, taxes and surcharges beyond the control of airlines. Airlines are also responsible for enforcing government regulations. If airlines carry passengers without proper documentation on an international flight, they are responsible for returning them back to the original country.
Analysis of the 1992–1996 period shows that every player in the air transport chain is far more profitable than the airlines, who collect and pass through fees and revenues to them from ticket sales. While airlines as a whole earned 6% return on capital employed (2-3.5% less than the cost of capital), airports earned 10%, catering companies 10-13%, handling companies 11-14%, aircraft lessors 15%, aircraft manufacturers 16%, and global distribution companies more than 30%. (Source: Spinetta, 2000, quoted in Doganis, 2002)
The widespread entrance of a new breed of low cost airlines beginning at the turn of the century has accelerated the demand that full service carriers control costs. Many of these low cost companies emulate Southwest Airlines in various respects, and like Southwest, they can eke out a consistent profit throughout all phases of the business cycle.
As a result, a shakeout of airlines is occurring in the U.S. and elsewhere. American Airlines, United Airlines, Continental Airlines (twice), US Airways (twice), Delta Air Lines, and Northwest Airlines have all declared Chapter 11 bankruptcy. Some argue that it would be far better for the industry as a whole if a wave of actual closures were to reduce the number of "undead" airlines competing with healthy airlines while being artificially protected from creditors via bankruptcy law. On the other hand, some have pointed out that the reduction in capacity would be short lived given that there would be large quantities of relatively new aircraft that bankruptcies would want to get rid of and would re-enter the market either as increased fleets for the survivors or the basis of cheap planes for new startups.
Where an airline has established an engineering base at an airport, then there may be considerable economic advantages in using that same airport as a preferred focus (or "hub") for its scheduled flights.
Assets and financing.
Airline financing is quite complex, since airlines are highly leveraged operations. Not only must they purchase (or lease) new airliner bodies and engines regularly, they must make major long-term fleet decisions with the goal of meeting the demands of their markets while producing a fleet that is relatively economical to operate and maintain. Compare Southwest Airlines and their reliance on a single airplane type (the Boeing 737 and derivatives), with the now defunct Eastern Air Lines which operated 17 different aircraft types, each with varying pilot, engine, maintenance, and support needs.
A second financial issue is that of hedging oil and fuel purchases, which are usually second only to labor in its relative cost to the company. However, with the current high fuel prices it has become the largest cost to an airline. Legacy airlines, compared with new entrants, have been hit harder by rising fuel prices partly due to the running of older, less fuel efficient aircraft. While hedging instruments can be expensive, they can easily pay for themselves many times over in periods of increasing fuel costs, such as in the 2000–2005 period.
In view of the congestion apparent at many international airports, the ownership of slots at certain airports (the right to take-off or land an aircraft at a particular time of day or night) has become a significant tradable asset for many airlines. Clearly take-off slots at popular times of the day can be critical in attracting the more profitable business traveler to a given airline's flight and in establishing a competitive advantage against a competing airline.
If a particular city has two or more airports, market forces will tend to attract the less profitable routes, or those on which competition is weakest, to the less congested airport, where slots are likely to be more available and therefore cheaper. For example, Reagan National Airport attracts profitable routes due partly to its congestion, leaving less-profitable routes to Baltimore-Washington International Airport and Dulles International Airport.
Other factors, such as surface transport facilities and onward connections, will also affect the relative appeal of different airports and some long distance flights may need to operate from the one with the longest runway. For example, LaGuardia Airport is the preferred airport for most of Manhattan due to its proximity, while long-distance routes must use John F. Kennedy International Airport's longer runways.
Airline partnerships.
Codesharing is the most common type of airline partnership; it involves one airline selling tickets for another airline's flights under its own airline code. An early example of this was Japan Airlines' (JAL) codesharing partnership with Aeroflot in the 1960s on Tokyo–Moscow flights; Aeroflot operated the flights using Aeroflot aircraft, but JAL sold tickets for the flights as if they were JAL flights. This practice allows airlines to expand their operations, at least on paper, into parts of the world where they cannot afford to establish bases or purchase aircraft. Another example was the Austrian– Sabena partnership on the Vienna–Brussels–New York/JFK route during the late '60s, using a Sabena Boeing 707 with Austrian livery.
Since airline reservation requests are often made by city-pair (such as "show me flights from Chicago to Düsseldorf"), an airline that can codeshare with another airline for a variety of routes might be able to be listed as indeed offering a Chicago–Düsseldorf flight. The passenger is advised however, that airline no. 1 operates the flight from say Chicago to Amsterdam, and airline no. 2 operates the continuing flight (on a different airplane, sometimes from another terminal) to Düsseldorf. Thus the primary rationale for code sharing is to expand one's service offerings in city-pair terms to increase sales.
A more recent development is the airline alliance, which became prevalent in late 1990s. These alliances can act as virtual mergers to get around government restrictions. Alliances of airlines such as Star Alliance, Oneworld, and SkyTeam coordinate their passenger service programs (such as lounges and frequent-flyer programs), offer special interline tickets, and often engage in extensive codesharing (sometimes systemwide). These are increasingly integrated business combinations—sometimes including cross-equity arrangements—in which products, service standards, schedules, and airport facilities are standardized and combined for higher efficiency. One of the first airlines to start an alliance with another airline was KLM, who partnered with Northwest Airlines. Both airlines later entered the SkyTeam alliance after the fusion of KLM and Air France in 2004.
Often the companies combine IT operations, or purchase fuel and aircraft as a bloc to achieve higher bargaining power. However, the alliances have been most successful at purchasing invisible supplies and services, such as fuel. Airlines usually prefer to purchase items visible to their passengers to differentiate themselves from local competitors. If an airline's main domestic competitor flies Boeing airliners, then the airline may prefer to use Airbus aircraft regardless of what the rest of the alliance chooses.
Fuel hedging.
Southwest is credited with maintaining strong business profits between 1999 and the early 2000s due to its fuel hedging policy. Looking at the annual reports, many other airlines are replicating Southwest's hedging policy to control their fuel costs.
Environmental impacts.
Aircraft engines emit noise pollution, gases and particulate emissions, and contribute to global dimming.
Growth of the industry in recent years raised a number of ecological questions.
Domestic air transport grew in China at 15.5 percent annually from 2001 to 2006. The rate of air travel globally increased at 3.7 percent per year over the same time. In the EU greenhouse gas emissions from aviation increased by 87% between 1990 and 2006. However it must be compared with the flights increase, only in UK, between 1990 and 2006 terminal passengers increased from 100 000 thousands to 250 000 thousands., according to AEA reports every year, 750 million passengers travel by European airlines, which also share 40% of merchandise value in and out of Europe. Without even pressure from "green activists", targeting lower ticket prices, generally, airlines do what is possible to cut the fuel consumption (and gas emissions connected therewith). Further, according to some reports, it can be concluded that the last piston-powered aircraft were as fuel-efficient as the average jet in 2005.
Despite continuing efficiency improvements from the major aircraft manufacturers, the expanding demand for global air travel has resulted in growing greenhouse gas (GHG) emissions. Currently, the aviation sector, including US domestic and global international travel, make approximately 1.6 percent of global anthropogenic GHG emissions per annum. North America accounts for nearly 40 percent of the world's GHG emissions from aviation fuel use.
CO2 emissions from the jet fuel burned per passenger on an average airline flight is about 353 kilograms (776 pounds). Loss of natural habitat potential associated with the jet fuel burned per passenger on a airline flight is estimated to be 250 square meters (2700 square feet).
In the context of purported climate change and peak oil, there is a debate about possible taxation of air travel and the inclusion of aviation in an emissions trading scheme, with a view to ensuring that the total external costs of aviation are taken into account.
The airline industry is responsible for about 11 percent of greenhouse gases emitted by the U.S. transportation sector. Boeing estimates that biofuels could reduce flight-related greenhouse-gas emissions by 60 to 80 percent. The solution would be blending algae fuels with existing jet fuel:
There are Electric aircraft projects, where some of them are fully operational planes as of 2013.
Call signs.
Each operator of a scheduled or charter flight uses an airline call sign when communicating with airports or air traffic control centres. Most of these call-signs are derived from the airline's trade name, but for reasons of history, marketing, or the need to reduce ambiguity in spoken English (so that pilots do not mistakenly make navigational decisions based on instructions issued to a different aircraft), some airlines and air forces use call-signs less obviously connected with their trading name. For example, British Airways uses a "Speedbird" call-sign, named after the logo of its predecessor, BOAC, while SkyEurope used "Relax".
Airline personnel.
The various types of airline personnel include:
Flight operations personnel including flight safety personnel.
Airlines follow a corporate structure where each broad area of operations (such as maintenance, flight operations(including flight safety),
and passenger service) is supervised by a vice president. Larger airlines often appoint vice presidents to oversee each of the
airline's hubs as well. Airlines employ lawyers to deal with regulatory procedures and other administrative tasks.
Industry trends.
The pattern of ownership has been privatized in the recent years, that is, the ownership has gradually changed from governments to private and individual sectors or organizations. This occurs as regulators permit greater freedom and non-government ownership, in steps that are usually decades apart. This pattern is not seen for all airlines in all regions. 
The overall trend of demand has been consistently increasing. In the 1950s and 1960s, annual growth rates of 15% or more were common. Annual growth of 5-6% persisted through the 1980s and 1990s. Growth rates are not consistent in all regions, but countries with a de-regulated airline industry have more competition and greater pricing freedom. This results in lower fares and sometimes dramatic spurts in traffic growth. The U.S., Australia, Canada, Japan, Brazil, India and other markets exhibit this trend. The industry has been observed to be cyclical in its financial performance. Four or five years of poor earnings precede five or six years of improvement. But profitability even in the good years is generally low, in the range of 2-3% net profit after interest and tax. In times of profit, airlines lease new generations of airplanes and upgrade services in response to higher demand. Since 1980, the industry has not earned back the cost of capital during the best of times. Conversely, in bad times losses can be dramatically worse. Warren Buffett once said that despite all the money that has been invested in all airlines, the net profit is less than zero. He believes it is one of the hardest businesses to manage.
As in many mature industries, consolidation is a trend. Airline groupings may consist of limited bilateral partnerships, long-term, multi-faceted alliances between carriers, equity arrangements, mergers, or takeovers. Since governments often restrict ownership and merger between companies in different countries, most consolidation takes place within a country. In the U.S., over 200 airlines have merged, been taken over, or gone out of business since deregulation in 1978. Many international airline managers are lobbying their governments to permit greater consolidation to achieve higher economy and efficiency.

</doc>
