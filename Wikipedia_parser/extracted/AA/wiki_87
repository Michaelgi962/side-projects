<doc id="6205" url="http://en.wikipedia.org/wiki?curid=6205" title="Chaitin's constant">
Chaitin's constant

In the computer science subfield of algorithmic information theory, a Chaitin constant (Chaitin omega number) or halting probability is a real number that informally represents the probability that a randomly constructed program will halt. These numbers are formed from a construction due to Gregory Chaitin. 
Although there are infinitely many halting probabilities, it is common to use the letter Ω to refer to them as if there were only one. Because Ω depends on the program encoding used, it is sometimes called Chaitin's construction instead of Chaitin's constant when not referring to any specific encoding.
Each halting probability is a normal and transcendental real number that is not computable, which means that there is no algorithm enumerating its digits.
Background.
The definition of a halting probability relies on the existence of prefix-free universal computable functions. Such a function, intuitively, represents a programming language with the property that no valid program can be obtained as a proper extension of another valid program.
Suppose that "F" is a partial function that takes one argument, a finite binary string, and possibly returns a single binary string as output. The function "F" is called computable if there is a Turing machine that computes it.
The function "F" is called universal if the following property holds: for every computable function "f" of a single variable there is a string "w" such that for all "x", "F"("w" "x") = "f"("x"); here "w" "x" represents the concatenation of the two strings "w" and "x". This means that "F" can be used to simulate any computable function of one variable. Informally, "w" represents a "script" for the computable function "f", and "F" represents an "interpreter" that parses the script as a prefix of its input and then executes it on the remainder of input.
Note that for any fixed "w" the function "f"("x") = "F"("w" "x") is computable; thus the universality property states that all computable functions of one variable can be obtained in this fashion.
The domain of "F" is the set of all inputs "p" on which it is defined. For "F" that are universal, such a "p" can generally be seen both as the concatenation of a program part and a data part, and as a single program for the function "F".
The function "F" is called prefix-free if there are no two elements "p", "p′" in its domain such that "p′" is a proper extension of "p". This can be rephrased as: the domain of "F" is a prefix-free code (instantaneous code) on the set of finite binary strings. A simple way to enforce prefix-free-ness is to use machines whose means of input is a binary stream from which bits can be read one at a time. There is no end-of-stream marker; the end of input is determined by when the universal machine decides to stop reading more bits. Here, the difference between the two notions of program mentioned in the last paragraph becomes clear; one is easily recognized by some grammar, while the other requires arbitrary computation to recognize.
The domain of any universal computable function is a computably enumerable set but never a computable set. The domain is always Turing equivalent to the halting problem.
Definition.
Let "P"F be the domain of a prefix-free universal computable function "F". The constant ΩF is then defined as
where formula_2 denotes the length of a string "p".
This is an infinite sum which has one summand for every "p" in the domain of "F". The requirement that the domain be prefix-free, together with Kraft's inequality, ensures that this sum converges to a real number between 0 and 1. If "F" is clear from context then ΩF may be denoted simply Ω, although different prefix-free universal computable functions lead to different values of Ω.
Relationship to the halting problem.
Knowing the first formula_3 bits of formula_4, one could calculate the halting problem for all programs of a size up to formula_3. Let the program formula_6 for which the halting problem is to be solved be "N" bits long. In dovetailing fashion, all programs of all lengths are run, until enough have halted to jointly contribute enough probability to match these first "N" bits. If the program formula_6 hasn't halted yet, then it never will, since its contribution to the halting probability would affect the first N bits.
Thus, the halting problem would be solved for formula_6.
Because many outstanding problems in number theory, such as Goldbach's conjecture are equivalent to solving the halting problem for special programs (which would basically search for counter-examples and halt if one is found), knowing enough bits of Chaitin's constant would also imply knowing the answer to these problems. But as the halting problem is not generally solvable, and therefore calculating any but the first few bits of Chaitin's constant is not possible, this just reduces hard problems to impossible ones, much like trying to build an oracle machine for the halting problem would be.
Interpretation as a probability.
The Cantor space is the collection of all infinite sequences of 0s and 1s. A halting probability can be interpreted as the measure of a certain subset of Cantor space under the usual probability measure on Cantor space. It is from this interpretation that halting probabilities take their name.
The probability measure on Cantor space, sometimes called the fair-coin measure, is defined so that for any binary string "x" the set of sequences that begin with "x" has measure 2-|"x"|. This implies that for each natural number "n", the set of sequences "f" in Cantor space such that "f"("n") = 1 has measure 1/2, and the set of sequences whose "n"th element is 0 also has measure 1/2.
Let "F" be a prefix-free universal computable function. The domain "P" of "F" consists of an infinite set of binary strings
Each of these strings "p""i" determines a subset "S""i" of Cantor space; the set "S""i" contains all sequences in cantor space that begin with "p""i". These sets are disjoint because "P" is a prefix-free set. The sum
represents the measure of the set 
In this way, Ω"F" represents the probability that a randomly selected infinite sequence of 0s and 1s begins with a bit string (of some finite length) that is in the domain of "F". It is for this reason that Ω"F" is called a halting probability.
Properties.
Each Chaitin constant Ω has the following properties:
Not every set that is Turing equivalent to the halting problem is a halting probability. A finer equivalence relation, Solovay equivalence, can be used to characterize the halting probabilities among the left-c.e. reals.
Uncomputability.
A real number is called computable if there is an algorithm which, given "n", returns the first "n" digits of the number. This is equivalent to the existence of a program that enumerates the digits of the real number.
No halting probability is computable. The proof of this fact relies on an algorithm which, given the first "n" digits of Ω, solves Turing's halting problem for programs of length up to "n". Since the halting problem is undecidable, Ω can not be computed.
The algorithm proceeds as follows. Given the first "n" digits of Ω and a "k"≤"n", the algorithm enumerates the domain of "F" until enough elements of the domain have been found so that the probability they represent is within 2-(k+1) of Ω. After this point, no additional program of length "k" can be in the domain, because each of these would add 2-"k" to the measure, which is impossible. Thus the set of strings of length "k" in the domain is exactly the set of such strings already enumerated.
Incompleteness theorem for halting probabilities.
For each specific consistent effectively represented axiomatic system for the natural numbers, such as Peano arithmetic, there exists a constant "N" such that no bit of Ω after the "N"th can be proven to be 1 or 0 within that system. The constant "N" depends on how the formal system is effectively represented, and thus does not directly reflect the complexity of the axiomatic system. This incompleteness result is similar to Gödel's incompleteness theorem in that it shows that no consistent formal theory for arithmetic can be complete.
Super Omega.
As mentioned above, the first n bits of Gregory Chaitin's constant Omega are random or incompressible in the sense that we cannot compute them by a halting algorithm with fewer than n-O(1) bits. However, consider the short but never halting algorithm which systematically lists and runs all possible programs; whenever one of them halts its probability gets added to the output (initialized by zero). After finite time the first n bits of the output will never change any more (it does not matter that this time itself is not computable by a halting program). So there is a short non-halting algorithm whose output converges (after finite time) onto the first n bits of Omega. In other words, the enumerable first n bits of Omega are highly compressible in the sense that they are limit-computable by a very short algorithm; they are not random with respect to the set of enumerating algorithms. Jürgen Schmidhuber (2000) constructed a limit-computable "Super Omega" which in a sense is much more random than the original limit-computable Omega, as one cannot significantly compress the Super Omega by any enumerating non-halting algorithm.

</doc>
<doc id="6206" url="http://en.wikipedia.org/wiki?curid=6206" title="Computable number">
Computable number

In mathematics, computable numbers are the real numbers that can be computed to within any desired precision by a finite, terminating algorithm. They are also known as the recursive numbers or the computable reals.
Equivalent definitions can be given using μ-recursive functions, Turing machines, or λ-calculus as the formal representation of algorithms. The computable numbers form a real closed field and can be used in the place of real numbers for many, but not all, mathematical purposes.
Informal definition using a Turing machine as example.
In the following, Marvin Minsky defines the numbers to be computed in a manner similar to those defined by Alan Turing in 1936; i.e., as "sequences of digits interpreted as decimal fractions" between 0 and 1:
The key notions in the definition are (1) that some "n" is specified at the start, (2) for any "n" the computation only takes a finite number of steps, after which the machine produces the desired output and terminates.
An alternate form of (2) – the machine successively prints all n of the digits on its tape, halting after printing the nth – emphasizes Minsky's observation: (3) That by use of a Turing machine, a "finite" definition – in the form of the machine's table – is being used to define what is a potentially-"infinite" string of decimal digits.
This is however not the modern definition which only requires the result be accurate to within any given accuracy. The informal definition above is subject to a rounding problem called the table-maker's dilemma whereas the modern definition is not.
Formal definition.
A real number "a" is computable if it can be approximated by some computable function formula_1 in the following manner: given any positive integer "n", the function produces an integer "f(n)" such that:
There are two similar definitions that are equivalent:
There is another equivalent definition of computable numbers via computable Dedekind cuts. A computable Dedekind cut is a computable function formula_8 which when provided with a rational number formula_9 as input returns formula_10 or formula_11, satisfying the following conditions:
An example is given by a program "D" that defines the cube root of 3. Assuming formula_15 this is defined by:
A real number is computable if and only if there is a computable Dedekind cut "D" converging to it. The function "D" is unique for each irrational computable number (although of course two different programs may provide the same function).
A complex number is called computable if its real and imaginary parts are computable.
Properties.
While the set of real numbers is uncountable, the set of computable numbers is only countable and thus almost all real numbers are not computable. The computable numbers can be counted by assigning a Gödel number to each Turing machine definition. This gives a function from the naturals to the computable reals. Although the computable numbers are an ordered field, the set of Gödel numbers corresponding to computable numbers is not itself computably enumerable, because it is not possible to effectively determine which Gödel numbers correspond to Turing machines that produce computable reals. In order to produce a computable real, a Turing machine must compute a total function, but the corresponding decision problem is in Turing degree 0′′. Thus Cantor's diagonal argument cannot be used to produce uncountably many computable reals; at best, the reals formed from this method will be uncomputable.
The arithmetical operations on computable numbers are themselves computable in the sense that whenever real numbers "a" and "b" are computable then the following real numbers are also computable: "a + b", "a - b", "ab", and "a/b" if "b" is nonzero.
These operations are actually "uniformly computable"; for example, there is a Turing machine which on input ("A","B",formula_18) produces output "r", where "A" is the description of a Turing machine approximating "a", "B" is the description of a Turing machine approximating "b", and "r" is an formula_18 approximation of "a"+"b".
The computable real numbers do not share all the properties of the real numbers used in analysis. For example, the least upper bound of a bounded increasing computable sequence of computable real numbers need not be a computable real number (Bridges and Richman, 1987:58). A sequence with this property is known as a Specker sequence, as the first construction is due to E. Specker (1949). Despite the existence of counterexamples such as these, parts of calculus and real analysis can be developed in the field of computable numbers, leading to the study of computable analysis. 
The order relation on the computable numbers is not computable. There is no Turing machine which on input "A" (the description of a Turing machine approximating the number formula_6) outputs "YES" if formula_21 and "NO" if formula_22. The reason: suppose the machine described by "A" keeps outputting 0 as formula_18 approximations. It is not clear how long to wait before deciding that the machine will "never" output an approximation which forces "a" to be positive. Thus the machine will eventually have to guess that the number will equal 0, in order to produce an output; the sequence may later become different from 0. This idea can be used to show that the machine is incorrect on some sequences if it computes a total function. A similar problem occurs when the computable reals are represented as Dedekind cuts. The same holds for the equality relation : the equality test is not computable.
While the full order relation is not computable, the restriction of it to pairs of unequal numbers is computable. That is, there is a program that takes an input two Turing machines "A" and "B" approximating numbers "a" and "b", where "a"≠"b", and outputs whether "a"<"b" or "a">"b". It is sufficient to use ε-approximations where ε<|b-a|/2; so by taking increasingly small ε (with a limit to 0), one eventually can decide whether "a"<"b" or "a">"b".
Every computable number is definable, but not vice versa. There are many definable, noncomputable real numbers, including:
Both of these examples in fact define an infinite set of definable, uncomputable numbers, one for each Universal Turing machine.
A real number is computable if and only if the set of natural numbers it represents (when written in binary and viewed as a characteristic function) is computable.
Every computable number is arithmetical.
The set of computable real numbers (as well as every countable, densely ordered subset of reals without ends) is order-isomorphic to the set of rational numbers.
Digit strings and the Cantor and Baire spaces.
Turing's original paper defined computable numbers as follows:
Turing was aware that this definition is equivalent to the formula_18-approximation definition given above. The argument proceeds as follows: if a number is computable in the Turing sense, then it is also computable in the formula_18 sense: if formula_29, then the first "n" digits of the decimal expansion for "a" provide an formula_18 approximation of "a". For the converse, we pick an formula_18 computable real number "a" and generate increasingly precisce approximations until the "n"th digit after the decimal point is certain. This always generates a decimal expansion equal to "a" but it may improperly end in an infinite sequence of 9's in which case it must have a finite (and thus computable) proper decimal expansion. 
Unless certain topological properties of the real numbers are relevant it is often more convenient to deal with elements of formula_32 (total 0,1 valued functions) instead of reals numbers in <math>The members of formula_32 can be identified with binary decimal expansions but since the decimal expansions formula_34 and formula_35 denote the same real number the interval formula_36 can only be bijectively (and homeomorphically under the subset topology) identified with the subset of formula_32 not ending in all 1's. 
Note that this property of decimal expansions means it's impossible to effectively identify computable real numbers defined in terms of a decimal expansion and those defined in the formula_18 approximation sense. Hirst has shown there is no algorithm which takes as input the description of a Turing machine which produces formula_18 approximations for the computable number "a", and produces as output a Turing machine which enumerates the digits of "a" in the sense of Turing's definition (see Hirst 2007). Similarly it means that the arithmetic operations on the computable reals are not effective on their decimal representations as when adding decimal numbers, in order to produce one digit it may be necessary to look arbitrarily far to the right to determine if there is a carry to the current location. This lack of uniformity is one reason that the contemporary definition of computable numbers uses formula_18 approximations rather than decimal expansions.
However, from a computational or measure theoretic perspective the two structures formula_32 and <math>are essentially identical. and computability theorists often refer to members of formula_32 as reals. While formula_36 formula_32 is totally disconnected for questions about formula_45 classes or randomness it's much less messy to work in formula_32.
Elements of formula_47 are sometimes called reals as well and though containing a homeomorphic image of formula_48 formula_47 in addition to being totally disconnected isn't even locally compact. This leads to genuine differences in the computational properties. For instance the formula_50 satisfying formula_51 with formula_52 quatifier free must be computable while the unique formula_53 satisfying a universal formula can be arbitrarily high in the hyperarithmetic hierarchy.
Can computable numbers be used instead of the reals?
The computable numbers include many of the specific real numbers which appear in practice, including all real algebraic numbers, as well as "e", formula_54, and many other transcendental numbers. Though the computable reals exhaust those reals we can calculate or approximate, the assumption that all reals are computable leads to substantially different conclusions about the real numbers. The question naturally arises of whether it is possible to dispose of the full set of reals and use computable numbers for all of mathematics. This idea is appealing from a constructivist point of view, and has been pursued by what Bishop and Richman call the "Russian school" of constructive mathematics. 
To actually develop analysis over computable numbers, some care must be taken. For example, if one uses the classical definition of a sequence, the set of computable numbers is not closed under the basic operation of taking the supremum of a bounded sequence (for example, consider a Specker sequence). This difficulty is addressed by considering only sequences which have a computable modulus of convergence. The resulting mathematical theory is called computable analysis.
Implementation.
There are some computer packages that work with computable real numbers,
representing the real numbers as programs computing approximations.
One example is the RealLib package (reallib home page).
References.
Computable numbers were defined independently by Turing, Post and Church. See "The Undecidable", ed. Martin Davis, for further original papers.

</doc>
<doc id="6207" url="http://en.wikipedia.org/wiki?curid=6207" title="Electric current">
Electric current

An electric current is a flow of electric charge. In electric circuits this charge is often carried by moving electrons in a wire. It can also be carried by ions in an electrolyte, or by both ions and electrons such as in a plasma.
The SI unit for measuring an electric current is the ampere, which is the flow of electric charge across a surface at the rate of one coulomb per second. Electric current is measured using a device called an ammeter.
Electric currents can have many effects, notably heating, but they also create magnetic fields, which are used in motors, inductors and generators.
Symbol.
The conventional symbol for current is formula_1, which originates from the French phrase "intensité de courant", or in English "current intensity". This phrase is frequently used when discussing the value of an electric current, but modern practice often shortens this to simply "current". The formula_1 symbol was used by André-Marie Ampère, after whom the unit of electric current is named, in formulating the eponymous Ampère's force law which he discovered in 1820. The notation travelled from France to Britain, where it became standard, although at least one journal did not change from using formula_3 to formula_1 until 1896.
Conventions.
A flow of positive charges gives the same electric current, and has the same effect in a circuit, as an equal flow of negative charges in the opposite direction. Since current can be the flow of either positive or negative charges, or both, a convention for the direction of current which is independent of the type of charge carriers is needed. The direction of "conventional current" is arbitrarily defined to be the same as the direction of the flow of positive charges.
In metals, which make up the wires and other conductors in most electrical circuits, the positive charges are immobile, and the charge carriers are electrons. Because the electrons carry negative charge, their motion in a metal conductor is in the direction opposite to that of conventional current.
Reference direction.
When analyzing electrical circuits, the actual direction of current through a specific circuit element is usually unknown. Consequently, each circuit element is assigned a current variable with an arbitrarily chosen "reference direction". This is usually indicated on the circuit diagram with an arrow next to the current variable. When the circuit is solved, the circuit element currents may have positive or negative values. A negative value means that the actual direction of current through that circuit element is opposite that of the chosen reference direction.
In electronic circuits, the reference current directions are often chosen so that all currents are toward ground. This often corresponds to conventional current direction, because in many circuits the power supply voltage is positive with respect to ground.
Ohm's law.
Ohm's law states that the current through a conductor between two points is directly proportional to the potential difference across the two points. Introducing the constant of proportionality, the resistance, one arrives at the usual mathematical equation that describes this relationship:
where "I" is the current through the conductor in units of amperes, "V" is the potential difference measured "across" the conductor in units of volts, and "R" is the resistance of the conductor in units of ohms. More specifically, Ohm's law states that the "R" in this relation is constant, independent of the current.
AC and DC.
The abbreviations "AC" and "DC" are often used to mean simply "alternating" and "direct", as when they modify "current" or "voltage".
Direct current.
Direct current (DC) is the unidirectional flow of electric charge. Direct current is produced by sources such as batteries, thermocouples, solar cells, and commutator-type electric machines of the dynamo type. Direct current may flow in a conductor such as a wire, but can also flow through semiconductors, insulators, or even through a vacuum as in electron or ion beams. The electric charge flows in a constant direction, distinguishing it from alternating current (AC). A term formerly used for "direct current" was galvanic current.
Alternating current.
In alternating current (AC, also ac), the movement of electric charge periodically reverses direction. In direct current (DC, also dc), the flow of electric charge is only in one direction. 
AC is the form in which electric power is delivered to businesses and residences. The usual waveform of an AC power circuit is a sine wave. In certain applications, different waveforms are used, such as triangular or square waves. Audio and radio signals carried on electrical wires are also examples of alternating current. In these applications, an important goal is often the recovery of information encoded (or modulated) onto the AC signal.
Occurrences.
Natural observable examples of electrical current include lightning, static electricity, and the solar wind, the source of the polar auroras.
Man-made occurrences of electric current include the flow of conduction electrons in metal wires such as the overhead power lines that deliver electrical energy across long distances and the smaller wires within electrical and electronic equipment. Eddy currents are electric currents that occur in conductors exposed to changing magnetic fields. Similarly, electric currents occur, particularly in the surface, of conductors exposed to electromagnetic waves. When oscillating electric currents flow at the correct voltages within radio antennas, radio waves are generated.
In electronics, other forms of electric current include the flow of electrons through resistors or through the vacuum in a vacuum tube, the flow of ions inside a battery or a neuron, and the flow of holes within a semiconductor.
Current measurement.
Current can be measured using an ammeter.
At the circuit level, there are various techniques that can be used to measure current:
Resistive heating.
Joule heating, also known as "ohmic heating" and "resistive heating", is the process by which the passage of an electric current through a conductor releases heat. It was first studied by James Prescott Joule in 1841. Joule immersed a length of wire in a fixed mass of water and measured the temperature rise due to a known current through the wire for a 30 minute period. By varying the current and the length of the wire he deduced that the heat produced was proportional to the square of the current multiplied by the electrical resistance of the wire.
This relationship is known as Joule's First Law. The SI unit of energy was subsequently named the joule and given the symbol "J". The commonly known unit of power, the watt, is equivalent to one joule per second.
Electromagnetism.
Electromagnet.
Electric current produces a magnetic field. The magnetic field can be visualized as a pattern of circular field lines surrounding the wire that persists as long as there is current.
Magnetism can also produce electric currents. When a changing magnetic field is applied to a conductor, an Electromotive force (EMF) is produced, and when there is a suitable path, this causes current.
Electric current can be directly measured with a galvanometer, but this method involves breaking the electrical circuit, which is sometimes inconvenient. Current can also be measured without breaking the circuit by detecting the magnetic field associated with the current. Devices used for this include Hall effect sensors, current clamps, current transformers, and Rogowski coils.
Radio waves.
When an electric current flows in a suitably shaped conductor at radio frequencies radio waves can be generated. These travel at the speed of light and can cause electric currents in distant conductors.
Conduction mechanisms in various media.
In metallic solids, electric charge flows by means of electrons, from lower to higher electrical potential. In other media, any stream of charged objects (ions, for example) may constitute an electric current. To provide a definition of current that is independent of the type of charge carriers flowing, "conventional current" is defined to be in the same direction as positive charges. So in metals where the charge carriers (electrons) are negative, conventional current is in the opposite direction as the electrons. In conductors where the charge carriers are positive, conventional current is in the same direction as the charge carriers.
In a vacuum, a beam of ions or electrons may be formed. In other conductive materials, the electric current is due to the flow of both positively and negatively charged particles at the same time. In still others, the current is entirely due to positive charge flow. For example, the electric currents in electrolytes are flows of positively and negatively charged ions. In a common lead-acid electrochemical cell, electric currents are composed of positive hydrogen ions (protons) flowing in one direction, and negative sulfate ions flowing in the other. Electric currents in sparks or plasma are flows of electrons as well as positive and negative ions. In ice and in certain solid electrolytes, the electric current is entirely composed of flowing ions.
Metals.
A solid conductive metal contains mobile, or free electrons, which function as conduction electrons. These electrons are bound to the metal lattice but no longer to an individual atom. Metals are particularly conductive because there are a large number of these free electrons, typically one per atom in the lattice. Even with no external electric field applied, these electrons move about randomly due to thermal energy but, on average, there is zero net current within the metal. At room temperature, the average speed of these random motions is 106 metres per second. Given a surface through which a metal wire passes, electrons move in both directions across the surface at an equal rate. As George Gamow wrote in his popular science book, "One, Two, Three...Infinity" (1947), "The metallic substances differ from all other materials by the fact that the outer shells of their atoms are bound rather loosely, and often let one of their electrons go free. Thus the interior of a metal is filled up with a large number of unattached electrons that travel aimlessly around like a crowd of displaced persons. When a metal wire is subjected to electric force applied on its opposite ends, these free electrons rush in the direction of the force, thus forming what we call an electric current."
When a metal wire is connected across the two terminals of a DC voltage source such as a battery, the source places an electric field across the conductor. The moment contact is made, the free electrons of the conductor are forced to drift toward the positive terminal under the influence of this field. The free electrons are therefore the charge carrier in a typical solid conductor.
For a steady flow of charge through a surface, the current "I" (in amperes) can be calculated with the following equation:
where "Q" is the electric charge transferred through the surface over a time "t". If "Q" and "t" are measured in coulombs and seconds respectively, "I" is in amperes.
More generally, electric current can be represented as the rate at which charge flows through a given surface as:
Electrolytes.
Electric currents in electrolytes are flows of electrically charged particles (ions). For example, if an electric field is placed across a solution of Na+ and Cl− (and conditions are right) the sodium ions move towards the negative electrode (cathode), while the chloride ions move towards the positive electrode (anode). Reactions take place at both electrode surfaces, absorbing each ion.
Water-ice and certain solid electrolytes called proton conductors contain positive hydrogen ions or "protons" which are mobile. In these materials, electric currents are composed of moving protons, as opposed to the moving electrons found in metals.
In certain electrolyte mixtures, brightly coloured ions are the moving electric charges. The slow progress of the colour makes the current visible.
Gases and plasmas.
In air and other ordinary gases below the breakdown field, the dominant source of electrical conduction is via relatively few mobile ions produced by radioactive gases, ultraviolet light, or cosmic rays. Since the electrical conductivity is low, gases are dielectrics or insulators. However, once the applied electric field approaches the breakdown value, free electrons become sufficiently accelerated by the electric field to create additional free electrons by colliding, and ionizing, neutral gas atoms or molecules in a process called avalanche breakdown. The breakdown process forms a plasma that contains enough mobile electrons and positive ions to make it an electrical conductor. In the process, it forms a light emitting conductive path, such as a spark, arc or lightning.
Plasma is the state of matter where some of the electrons in a gas are stripped or "ionized" from their molecules or atoms. A plasma can be formed by high temperature, or by application of a high electric or alternating magnetic field as noted above. Due to their lower mass, the electrons in a plasma accelerate more quickly in response to an electric field than the heavier positive ions, and hence carry the bulk of the current. The free ions recombine to create new chemical compounds (for example, breaking atmospheric oxygen into single oxygen → 2O, which then recombine creating ozone [O3]).
Vacuum.
Since a "perfect vacuum" contains no charged particles, it normally behaves as a perfect insulator. However, metal electrode surfaces can cause a region of the vacuum to become conductive by injecting free electrons or ions through either field electron emission or thermionic emission. Thermionic emission occurs when the thermal energy exceeds the metal's work function, while field electron emission occurs when the electric field at the surface of the metal is high enough to cause tunneling, which results in the ejection of free electrons from the metal into the vacuum. Externally heated electrodes are often used to generate an electron cloud as in the filament or indirectly heated cathode of vacuum tubes. Cold electrodes can also spontaneously produce electron clouds via thermionic emission when small incandescent regions (called cathode spots or anode spots) are formed. These are incandescent regions of the electrode surface that are created by a localized high current. These regions may be initiated by field electron emission, but are then sustained by localized thermionic emission once a vacuum arc forms. These small electron-emitting regions can form quite rapidly, even explosively, on a metal surface subjected to a high electrical field. Vacuum tubes and sprytrons are some of the electronic switching and amplifying devices based on vacuum conductivity.
Superconductivity.
Superconductivity is a phenomenon of exactly zero electrical resistance and expulsion of magnetic fields occurring in certain materials when cooled below a characteristic critical temperature. It was discovered by Heike Kamerlingh Onnes on April 8, 1911 in Leiden. Like ferromagnetism and atomic spectral lines, superconductivity is a quantum mechanical phenomenon. It is characterized by the Meissner effect, the complete ejection of magnetic field lines from the interior of the superconductor as it transitions into the superconducting state. The occurrence of the Meissner effect indicates that superconductivity cannot be understood simply as the idealization of "perfect conductivity" in classical physics.
Semiconductor.
In a semiconductor it is sometimes useful to think of the current as due to the flow of positive "holes" (the mobile positive charge carriers that are places where the semiconductor crystal is missing a valence electron). This is the case in a p-type semiconductor. A semiconductor has electrical conductivity intermediate in magnitude between that of a conductor and an insulator. This means a conductivity roughly in the range of 10−2 to 104 siemens per centimeter (S⋅cm−1).
In the classic crystalline semiconductors, electrons can have energies only within certain bands (i.e. ranges of levels of energy). Energetically, these bands are located between the energy of the ground state, the state in which electrons are tightly bound to the atomic nuclei of the material, and the free electron energy, the latter describing the energy required for an electron to escape entirely from the material. The energy bands each correspond to a large number of discrete quantum states of the electrons, and most of the states with low energy (closer to the nucleus) are occupied, up to a particular band called the "valence band". Semiconductors and insulators are distinguished from metals because the valence band in any given metal is nearly filled with electrons under usual operating conditions, while very few (semiconductor) or virtually none (insulator) of them are available in the "conduction band", the band immediately above the valence band.
The ease with which electrons in the semiconductor can be excited from the valence band to the conduction band depends on the band gap between the bands. The size of this energy bandgap serves as an arbitrary dividing line (roughly 4 eV) between semiconductors and insulators.
With covalent bonds, an electron moves by hopping to a neighboring bond. The Pauli exclusion principle requires the electron to be lifted into the higher anti-bonding state of that bond. For delocalized states, for example in one dimension – that is in a nanowire, for every energy there is a state with electrons flowing in one direction and another state with the electrons flowing in the other. For a net current to flow, more states for one direction than for the other direction must be occupied. For this to occur, energy is required, as in the semiconductor the next higher states lie above the band gap. Often this is stated as: full bands do not contribute to the electrical conductivity. However, as the temperature of a semiconductor rises above absolute zero, there is more energy in the semiconductor to spend on lattice vibration and on exciting electrons into the conduction band. The current-carrying electrons in the conduction band are known as "free electrons", although they are often simply called "electrons" if context allows this usage to be clear.
Current density and Ohm's law.
Current density is a measure of the density of an electric current. It is defined as a vector whose magnitude is the electric current per cross-sectional area. In SI units, the current density is measured in amperes per square metre.
where "formula_1" is current in the conductor, formula_11 is the current density, and formula_12 is the differential cross-sectional area vector.
The current density (current per unit area) "formula_11" in materials with finite resistance is directly proportional to the electric field formula_14 in the medium. The proportionality constant is call the conductivity "formula_15" of the material, whose value depends on the material concerned and, in general, is dependent on the temperature of the material:
The reciprocal of the conductivity "formula_15" of the material is called the resistivity "formula_18" of the material and the above equation, when written in terms of resistivity becomes:
Conduction in semiconductor devices may occur by a combination of drift and diffusion, which is proportional to diffusion constant formula_21 and charge density formula_22. The current density is then:
with formula_24 being the elementary charge and formula_25 the electron density. The carriers move in the direction of decreasing concentration, so for electrons a positive current results for a positive density gradient. If the carriers are holes, replace electron density formula_25 by the negative of the hole density formula_27.
In linear anisotropic materials, "σ", "ρ" and "D" are tensors.
In linear materials such as metals, and under low frequencies, the current density across the conductor surface is uniform. In such conditions, Ohm's law states that the current is directly proportional to the potential difference between two ends (across) of that metal (ideal) resistor (or other ohmic device):
where formula_1 is the current, measured in amperes; formula_30 is the potential difference, measured in volts; and formula_31 is the resistance, measured in ohms. For alternating currents, especially at higher frequencies, skin effect causes the current to spread unevenly across the conductor cross-section, with higher density near the surface, thus increasing the apparent resistance.
Drift speed.
The mobile charged particles within a conductor move constantly in random directions, like the particles of a gas. In order for there to be a net flow of charge, the particles must also move together with an average drift rate. Electrons are the charge carriers in metals and they follow an erratic path, bouncing from atom to atom, but generally drifting in the opposite direction of the electric field. The speed at which they drift can be calculated from the equation:
where
Typically, electric charges in solids flow slowly. For example, in a copper wire of cross-section 0.5 mm2, carrying a current of 5 A, the drift velocity of the electrons is on the order of a millimetre per second. To take a different example, in the near-vacuum inside a cathode ray tube, the electrons travel in near-straight lines at about a tenth of the speed of light.
Any accelerating electric charge, and therefore any changing electric current, gives rise to an electromagnetic wave that propagates at very high speed outside the surface of the conductor. This speed is usually a significant fraction of the speed of light, as can be deduced from Maxwell's Equations, and is therefore many times faster than the drift velocity of the electrons. For example, in AC power lines, the waves of electromagnetic energy propagate through the space between the wires, moving from a source to a distant load, even though the electrons in the wires only move back and forth over a tiny distance.
The ratio of the speed of the electromagnetic wave to the speed of light in free space is called the velocity factor, and depends on the electromagnetic properties of the conductor and the insulating materials surrounding it, and on their shape and size.
The magnitudes (but, not the natures) of these three velocities can be illustrated by an analogy with the three similar velocities associated with gases.

</doc>
<doc id="6208" url="http://en.wikipedia.org/wiki?curid=6208" title="Charles Ancillon">
Charles Ancillon

Charles Ancillon (28 July 1659 – 5 July 1715) was a French jurist and diplomat.
Ancillon was born in Metz into a distinguished family of Huguenots. His father, David Ancillon (1617–1692), was obliged to leave France on the revocation of the Edict of Nantes, and became pastor of the French Protestant community in Berlin.
Ancillon studied law at Marburg, Geneva, and Paris, where he was called to the bar. At the request of the Huguenots at Metz, he pleaded its cause at the court of King Louis XIV, urging that it should be excepted in the revocation of the Edict of Nantes, but his efforts were unsuccessful, and he joined his father in Berlin. He was at once appointed by Elector Frederick III ""juge et directeur de colonie de Berlin"." Before this, he had published several works on the revocation of the Edict of Nantes and its consequences, but his literary capacity was mediocre, his style stiff and cold, and it was his personal character rather than his reputation as a writer that earned him the confidence of the elector.
In 1687 Ancillon was appointed head of the so-called "Academie des nobles," the principal educational establishment of the state; later on, as councillor of embassy, he took part in the negotiations which led to the assumption of the title of "King in Prussia" by the elector. In 1699 he succeeded Samuel Pufendorf as historiographer to the elector, and the same year replaced his uncle Joseph Ancillon as judge of all the French refugees in the Margraviate of Brandenburg.
Ancillon is mainly remembered for what he did for education in Brandenburg-Prussia, and the share he took, in co-operation with Gottfried Leibniz, in founding the Academy of Berlin. Of his fairly numerous works the one of the most value is the ""Histoire de l'etablissement des Francais refugies dans les etats de Brandebourg"" published in Berlin in 1690.

</doc>
<doc id="6210" url="http://en.wikipedia.org/wiki?curid=6210" title="Clark Ashton Smith">
Clark Ashton Smith

Clark Ashton Smith (January 13, 1893 – August 14, 1961) was a self-educated American poet, sculptor, painter and author of fantasy, horror and science fiction short stories. He achieved early local recognition, largely through the enthusiasm of George Sterling, for traditional verse in the vein of Swinburne. As a poet, Smith is grouped with the West Coast Romantics alongside Ambrose Bierce, Joaquin Miller, Sterling, Nora May French, and remembered as "The Last of the Great Romantics" and "The Bard of Auburn".
Smith was one of "the big three of "Weird Tales", along with Robert E. Howard and H. P. Lovecraft", where some readers objected to his morbidness and violation of pulp traditions. It has been said of him that "nobody since Poe has so loved a well-rotted corpse." He was a member of the Lovecraft circle, and Smith's literary friendship with H. P. Lovecraft lasted from 1922 until Lovecraft's death in 1937. His work is marked chiefly by an extraordinarily wide and ornate vocabulary, a cosmic perspective and a vein of sardonic and sometimes ribald humor.
Biography.
Early life and education.
Smith was born January 13, 1893, in Long Valley, California, of English and New England parentage. He spent most of his life in the small town of Auburn, California, living in a small cabin built by his parents, Fanny and Timeus Smith. His formal education was limited: he suffered from psychological disorders including a fear of crowds, and although admitted to high school after attending eight years of grammar school (Long Valley School, whence dates the earliest known photo of him), he never went to high school. His parents decided it was better for him to be educated at home.
However, he was an insatiable reader, and continued to teach himself after he left school. His education began with the reading of "Robinson Crusoe" (unabridged), "Gulliver's Travels", the fairy tales of Hans Christian Andersen and Madame d'Aulnoy, the "Arabian Nights" and (at the age of 13) the poems of Edgar Allan Poe. He read an unabridged dictionary (the 13th edition of Webster's) through, word for word, studying not only the definitions of the words but also their derivations from ancient languages. Having an extraordinary eidetic memory, he seems to have retained most or all of it.
The other main course in Smith's self-education was to read the 11th edition of the "Encyclopædia Britannica" through at least twice. Smith later taught himself French and Spanish in order to translate verse out of those languages. Smith professed to hate the provinciality of the small town of Auburn but rarely left it until he married late in life.
Early writing.
His first literary efforts, at the age of 11, took the form of fairy tales and imitations of the Arabian Nights. Later, he wrote long adventure novels dealing with Oriental life. By 14 he had already written a short adventure novel called "The Black Diamonds" which was lost for years until published in 2002. Another juvenile novel was written in his teenaged years—"The Sword of Zagan" (unpublished until 2004). Like "The Black Diamonds", it uses a medieval, "Arabian Nights"-like setting, and the "Arabian Nights", like the fairy tales of the Brothers Grimm and the works of Edgar Allan Poe, are known to have strongly influenced Smith's early writing, as did William Beckford's "Vathek".
At age 17, he sold several tales to "The Black Cat", a magazine which specialized in unusual tales. He also published some tales in the "Overland Monthly" in this brief foray into fiction which preceded his poetic career.
However, it was primarily poetry that motivated the young Smith and he confined his efforts to poetry for more than a decade. In his later youth, Smith made the acquaintance of the San Francisco poet George Sterling through a member of the local Auburn Monday Night Club, where he read several of his poems with considerable success. On a month-long visit to Sterling in Carmel, California, Smith was introduced by Sterling to the poetry of Baudelaire.
He became Sterling's protégé and Sterling helped him to publish his first volume of poems, "The Star-Treader and Other Poems", at the age of 19. Smith received international acclaim for the collection "The Star-Treader" was received very favorably by American critics, one of whom named Smith "the Keats of the Pacific". Smith briefly moved among the circle that included Ambrose Bierce and Jack London, but his early fame soon faded away.
Health breakdown period.
A little later, Smith's health broke down and for eight years his literary production was intermittent, though he produced his best poetry during this period. A small volume, "Odes and Sonnets", was brought out in 1918. Smith came into contact with literary figures who would later form part of H.P. Lovecraft's circle of correspondents; Smith knew them far earlier than Lovecraft. These figures include poet Samuel Loveman and bookman George Kirk. It was Smith who in fact later introduced Donald Wandrei to Lovecraft. For this reason, it has been suggested that Lovecraft might as well be referred to as a member of a "Smith" circle as Smith was a member of a Lovecraft one.
In 1920 Smith composed a celebrated long poem in blank verse, "The Hashish Eater, or The Apocalypse of Evil" which was published in "Ebony and Crystal" (1922). This was followed by a fan letter from H. P. Lovecraft, which was the beginning of 15 years of friendship and correspondence. With studied playfulness, Smith and Lovecraft borrowed each other's coinages of place names and the names of strange gods for their stories, though so different is Smith's treatment of the Lovecraft theme that it has been dubbed the "Clark Ashton Smythos."
In 1925 Smith published "Sandalwood". He wrote little fiction in this period with the exception of some imaginative vignettes or prose poems. Smith was poor for most of his life and often did hard manual jobs such as fruit picking and woodcutting in order to support himself and his parents. He was an able cook and made many kinds of wine. He also did well digging, typing and journalism, as well as contributing a column to "The Auburn Journal" and sometimes worked as its night editor.
One of Smith's artistic patrons and frequent correspondents was San Francisco businessman Albert M. Bender.
Prolific fiction-writing period.
At the beginning of the Depression in 1929, with his aged parents' health weakening, Smith resumed fiction writing and turned out more than a hundred short stories, nearly all of which can be classed as weird horror or science fiction. Like Lovecraft, he drew upon the nightmares that had plagued him during youthful spells of sickness.
He published at his own expense a volume containing six of his best stories, "The Double Shadow and Other Fantasies", in an edition of 1000 copies printed by the "Auburn Journal". The theme of much of his work is egotism and its supernatural punishment; his weird fiction is generally macabre in subject matter, gloatingly preoccupied with images of death, decay and abnormality.
Most of Smith's weird fiction falls into four series set variously in Hyperborea, Poseidonis, Averoigne and Zothique. Hyperborea, which is a lost continent of the Miocene period, and Poseidonis, which is a remnant of Atlantis, are much the same, with a magical culture characterized by bizarreness, cruelty, death and postmortem horrors. Averoigne is Smith's version of pre-modern France, comparable to James Branch Cabell's Poictesme. Zothique exists millions of years in the future. It is "the last continent of earth, when the sun is dim and tarnished". These tales have been compared to the "Dying Earth" sequence of Jack Vance.
In 1933 Smith began corresponding with Robert E. Howard, the Texan creator of Conan the Barbarian. From 1933 to 1936, Smith, Howard and Lovecraft were the leaders of the Weird Tales school of fiction and corresponded frequently, although they never met. The writer of oriental fantasies E. Hoffmann Price is the only man known to have met all three in the flesh.
Critic Steve Behrends has suggested that the frequent theme of 'loss' in Smith's fiction (many of his characters attempt to recapture a long-vanished youth, early love, or picturesque past) may reflect Smith's own feeling that his career had suffered a "fall from grace":
Mid-late career: return to poetry and sculpture.
In Sept 1935, Smith's mother Fanny died. Smith spent the next two months nursing his father through his last illness. Timeus died in December 1937. Aged 44, Smith now virtually ceased writing fiction. He had been severely affected by several tragedies occurring in a short period of time: Robert E. Howard's death by suicide (1936), Lovecraft's death from cancer (1937) and the deaths of his parents, which left him exhausted. As a result, he withdrew from the scene, marking the end of "Weird Tales"' Golden Age. He began sculpting and resumed the writing of poetry. However, Smith was visited by many writers at his cabin, including Fritz Leiber, Rah Hoffman, Francis T. Laney and others.
In 1942, three years after August Derleth founded Arkham House for the purpose of preserving the work of H.P. Lovecraft, Derleth published the first of several major collections of Smith's fiction, "Out of Space and Time" (1942). This was followed by "Lost Worlds" (1944). The books sold slowly, went out of print and became costly rarities. Derleth published five more volumes of Smith's prose and two of his verse, and at his death in 1971 had a large volume of Smith's poems in press.
Later life, marriage and death.
In 1953 Smith suffered a coronary attack. Aged 61, he married Carol(yn) Jones Dorman on November 10, 1954. Dorman had much experience in Hollywood and radio public relations. After honeymooning at the Smith cabin, they moved to Pacific Grove, California, where he set up a household with their children. (Carol had been married before and had three children). For several years he alternated between the house on Indian Ridge and his wife's house in Pacific Grove. Having sold most of his father's tract, in 1957 the old house burned—the Smiths believed by arson, others said by accident.
Smith now reluctantly did gardening for other residents at Pacific Grove, and grew a goatee. He spent much time shopping and walking near the seafront but despite Derleth's badgering, resisted the writing of more fiction. In 1961 he suffered strokes. In August 1961 he quietly died in his sleep, aged 68. After Smith's death Carol remarried (becoming Carolyn Wakefield) and subsequently died of cancer.
The poet's ashes were buried beside, or beneath, a boulder to the immediate west of where his childhood home (destroyed by fire in 1957) stood; some were also scattered in a stand of blue oaks near the boulder. There was no marker. In more recent times a plaque to his memory has been erected at the Auburn, California Placer County Library.
Bookseller Roy A. Squires was appointed Smith's "west coast executor", with Jack L. Chalker as his "east coast executor". Squires published many letterpress editions of individual Smith poems.
Smith's literary estate is represented by his stepson, Prof William Dorman, director of CASiana Literary Enterprises. Arkham House owns the copyright to many Smith stories, though some are now in the public domain.
For 'posthumous collaborations' of Smith (stories completed by Lin Carter), see the entry on Lin Carter.
Artistic periods.
While Smith was always an artist who worked in several very different media, it is possible to identify three distinct periods in which one form of art had precedence over the others.
Poetry: until 1925.
Smith published most of his volumes of poetry in this period, including the aforementioned "The Star-Treader and Other Poems", as well as "Odes and Sonnets" (1918), "Ebony and Crystal" (1922) and "Sandalwood" (1925). His long poem "The Hashish-Eater; Or, the Apocalypse of Evil" was written in 1920.
Weird fiction: 1926–1935.
Smith wrote most of his weird fiction and Cthulhu Mythos stories, partially inspired by H. P. Lovecraft. Creatures of his invention include Aforgomon, Rlim-Shaikorth, Mordiggian, Tsathoggua, the wizard Eibon, and various others. In an homage to his friend, Lovecraft referred in some of his stories to a great dark wizard, "Klarkash-Ton."
Smith's weird stories form several cycles, called after the lands in which they are set: Averoigne, Hyperborea, Mars, Poseidonis, Zothique. To some extent Smith was influenced in his vision of such lost worlds by the teachings of Theosophy and the writings of Helena Blavatsky. Stories set in Zothique belong to the Dying Earth subgenre. Amongst Smith's science fiction tales are stories set on Mars and the invented planet of Xiccarph.
His short stories originally appeared in the magazines "Weird Tales", "Strange Tales", "Astounding Stories", "Stirring Science Stories" and "Wonder Stories".
Clark Ashton Smith was the third member of the great triumvirate of "Weird Tales", with Lovecraft and Robert E. Howard.
Many of Smith's stories were published in six hardcover volumes by August Derleth under his Arkham House imprint. For a full bibliography to 1978, see Sidney-Fryer, "Emperor of Dreams" (cited below). S.T. Joshi is working with other scholars to produce an updated bibliography of Smith's work.
A selection of Smith's best-known tales includes:
Visual art: 1935–1961.
By this time his interest in writing fiction began to lessen and he turned to creating sculptures from soft rock such as soapstone. Smith also made hundreds of fantastic paintings and drawings.

</doc>
<doc id="6211" url="http://en.wikipedia.org/wiki?curid=6211" title="Context-sensitive grammar">
Context-sensitive grammar

A context-sensitive grammar (CSG) is a formal grammar in which the left-hand sides and right-hand sides of any production rules may be surrounded by a context of terminal and nonterminal symbols. 
A formal language that can be described by a context-sensitive grammar, or, equivalently, by a noncontracting grammar or a linear bounded automaton, is called a context-sensitive language.
Context-sensitive grammars are more general than context-free grammars.
Noam Chomsky introduced context-sensitive grammars in the 1950s as a way to describe the syntax of natural language where it is indeed often the case that a word may or may not be appropriate in a certain place depending upon the context.
Formal definition.
A formal grammar "G" = ("N", Σ, "P", "S"), where "N" is a set of nonterminal symbols, Σ is a set of terminal symbols, "P" is a set of production rules, and "S" is the start symbol, is context-sensitive if all rules in "P" are of the form
where "A" ∈ "N" (i.e., "A" is a single nonterminal), α,β ∈ ("N" U Σ)* (i.e., α and β are strings of nonterminals and terminals) and γ ∈ ("N" U Σ)+ (i.e., γ is a nonempty string of nonterminals and terminals).
In addition, a rule of the form
where λ represents the empty string and S does not appear on the right-hand side of any rule is permitted. The addition of the empty string allows the statement that the context sensitive languages are a proper superset of the context free languages, rather than having to make the weaker statement that all context free grammars with no →λ productions are also context sensitive grammars.
The name "context-sensitive" is explained by the α and β that form the context of "A" and determine whether "A" can be replaced with γ or not. This is different from a context-free grammar where the context of a nonterminal is not taken into consideration. (Indeed, every production of a context free grammar is of the form V → w where V is a "single" nonterminal symbol, and w is a string of terminals and/or nonterminals (w can be empty)).
If the possibility of adding the empty string to a language is added to the strings recognized by the noncontracting grammars (which can never include the empty string) then the languages in these two definitions are identical.
A formal language can be described by a context-sensitive grammar if and only if it is accepted by some linear bounded automaton.
Examples.
Rules 1 to 4 allow for either generating the word "abc" or blowing-up to "a""n+1"("BC")"n""Bc". Rules 5 to 8 allow for successively exchanging each "CB" to "BC" (four rules are needed for that since a rule "CB" → "BC" wouldn't fit into the scheme α"A"β → αγβ). Their repeated application allows to sort the "B" and "C" nonterminals, resulting in a word of the form "a""n+1""B""n+1""C""n""c". Rules 9 to 11 allow for replacing the nonterminals "B" and "C" with terminals "b" and "c", respectively. The context ensures that they can only be applied to nonterminals that have been moved all the way to the right or left, using rules 5 to 8.
A generation chain for "aaabbbccc" is:
1 "aSBC"
More complicated grammars can be used to parse { "a""n""b""n""c""n""d""n": "n" ≥ 1 }, and other languages with even more letters.
A context-sensitive grammar for the language { "a"2i : i ≥ 1 } is constructed in Example 9.5 (p. 224) of (Hopcroft, Ullman, 1979).
Normal forms.
Every context-sensitive grammar which does not generate the empty string can be transformed into an equivalent one in Kuroda normal form. "Equivalent" here means that the two grammars generate the same language. The normal form will not in general be context-sensitive, but will be a noncontracting grammar.
Computational properties and uses.
The decision problem that asks whether a certain string "s" belongs to the language of a given context-sensitive grammar "G", is PSPACE-complete. Morever, there are context-sensitive grammars whose languages are PSPACE-complete. In other words, there is a context-sensitive grammar "G" such that deciding whether a certain string "s" belongs to the language of "G" is PSPACE-complete (so "G" is fixed and only "s" is part of the input of the problem).
The emptiness problem for context-sensitive grammars (given a context-sensitive grammar "G", is "L"("G")=∅ ?) is undecidable.
It has been shown that nearly all natural languages may in general be characterized by context-sensitive grammars, but the whole class of CSG's seems to be much bigger than natural languages. Worse yet, since the aforementioned decision problem for CSG's is PSPACE-complete, that makes them totally unworkable for practical use, as a polynomial-time algorithm for a PSPACE-complete problem would imply P=NP. Ongoing research on computational linguistics has focused on formulating other classes of languages that are "mildly context-sensitive" whose decision problems are feasible, such as tree-adjoining grammars, combinatory categorial grammars, coupled context-free languages, and linear context-free rewriting systems. The languages generated by these formalisms properly lie between the context-free and context-sensitive languages.

</doc>
<doc id="6212" url="http://en.wikipedia.org/wiki?curid=6212" title="Context-sensitive language">
Context-sensitive language

In theoretical computer science, a context-sensitive language is a formal language that can be defined by a context-sensitive grammar. That is one of the four types of grammars in the Chomsky hierarchy.
Computational properties.
Computationally, a context-sensitive language is equivalent with a linear bounded nondeterministic Turing machine, also called a linear bounded automaton. That is a non-deterministic Turing machine with a tape of only "kn" cells, where "n" is the size of the input and "k" is a constant associated with the machine. This means that every formal language that can be decided by such a machine is a context-sensitive language, and every context-sensitive language can be decided by such a machine.
This set of languages is also known as NLINSPACE or NSPACE("O"("n")), because they can be accepted using linear space on a non-deterministic Turing machine. The class LINSPACE (or DSPACE("O"("n"))) is defined the same, except using a deterministic Turing machine. Clearly LINSPACE is a subset of NLINSPACE, but it is not known whether LINSPACE=NLINSPACE.
Examples.
One of the simplest context-sensitive, but not context-free languages is formula_1: the language of all strings consisting of "n" occurrences of the symbol "a", then "n" "b"'s, then "n" "c"'s (abc, aabbcc, aaabbbccc, etc.). A superset of this language, called the Bach language, is defined as the set of all strings where "a", "b" and "c" (or any other set of three symbols) occurs equally often (aabccb, baabcaccb, etc.) and is also context-sensitive.
Another example of a context-sensitive language that is not context-free is "L" = { "ap" : "p" is a prime number }. "L" can be shown to be a context-sensitive language by constructing a linear bounded automaton which accepts "L". The language can easily be shown to be neither regular nor context free by applying the respective pumping lemmas for each of the language classes to "L".
An example of recursive language that is not context-sensitive is any recursive language whose decision is an EXPSPACE-hard problem, say, the set of pairs of equivalent regular expressions with exponentiation.

</doc>
<doc id="6216" url="http://en.wikipedia.org/wiki?curid=6216" title="Chinese room">
Chinese room

The Chinese room is a thought experiment presented by John Searle to challenge the claim that it is possible for a digital computer running a program to have a "mind" and "consciousness" in the same sense that people do, simply by virtue of running the right program. According to Searle, when referring to a hypothetical computer program which can be told a story then answer questions about it: Partisans of strong AI claim that in this question and answer sequence the machine is not only simulating a human ability but also (1) that the machine can literally be said to "understand" the story and provide the answers to questions, and (2) that what the machine and its program do "explains" the human ability to understand the story and answer questions about it.
To contest this view, Searle writes in his first description of the argument: "Suppose that I'm locked in a room and ... that I know no Chinese, either written or spoken". He further supposes that he has a set of rules in English that "enable me to correlate one set of formal symbols with another set of formal symbols", that is, the Chinese characters. These rules allow him to respond, in written Chinese, to questions, also written in Chinese, in such a way that the posers of the questions – who do understand Chinese – are convinced that Searle can actually understand the Chinese conversation too, even though he cannot. Similarly, he argues that if there is a computer program that allows a computer to carry on an intelligent conversation in written Chinese, the computer executing the program would not understand the conversation either.
The experiment is the centerpiece of Searle's Chinese room argument which holds that a program cannot give a computer a "mind", "understanding" or "consciousness", regardless of how intelligently it may make it behave. The argument is directed against the philosophical positions of functionalism and computationalism, which hold that the mind may be viewed as an information processing system operating on formal symbols. Although it was originally presented in reaction to the statements of artificial intelligence researchers, it is not an argument against the goals of AI research, because it does not limit the amount of intelligence a machine can display. The argument applies only to digital computers and does not apply to machines in general. This kind of argument against AI was described by John Haugeland as the "hollow shell" argument.
Searle's argument first appeared in his paper "Minds, Brains, and Programs", published in "Behavioral and Brain Sciences" in 1980. It has been widely discussed in the years since.
Chinese room thought experiment.
Searle's thought experiment begins with this hypothetical premise: suppose that artificial intelligence research has succeeded in constructing a computer that behaves as if it understands Chinese. It takes Chinese characters as input and, by following the instructions of a computer program, produces other Chinese characters, which it presents as output. Suppose, says Searle, that this computer performs its task so convincingly that it comfortably passes the Turing test: it convinces a human Chinese speaker that the program is itself a live Chinese speaker. To all of the questions that the person asks, it makes appropriate responses, such that any Chinese speaker would be convinced that he is talking to another Chinese-speaking human being.
The question Searle wants to answer is this: does the machine "literally" "understand" Chinese? Or is it merely "simulating" the ability to understand Chinese? Searle calls the first position "strong AI" and the latter "weak AI".
Searle then supposes that he is in a closed room and has a book with an English version of the computer program, along with sufficient paper, pencils, erasers, and filing cabinets. Searle could receive Chinese characters through a slot in the door, process them according to the program's instructions, and produce Chinese characters as output. If the computer had passed the Turing test this way, it follows, says Searle, that he would do so as well, simply by running the program manually.
Searle asserts that there is no essential difference between the roles of the computer and himself in the experiment. Each simply follows a program, step-by-step, producing a behavior which is then interpreted as demonstrating intelligent conversation. However, Searle would not be able to understand the conversation. ("I don't speak a word of Chinese," he points out.) Therefore, he argues, it follows that the computer would not be able to understand the conversation either.
Searle argues that without "understanding" (or "intentionality"), we cannot describe what the machine is doing as "thinking" and since it does not think, it does not have a "mind" in anything like the normal sense of the word. Therefore he concludes that "strong AI" is false.
History.
Gottfried Leibniz made a similar argument in 1714, using the thought experiment of expanding the brain until it was the size of a mill. Leibniz found it difficult to imagine that a "mind" capable of "perception" could be constructed using only mechanical processes. In 1974, Lawrence Davis imagined duplicating the brain using telephone lines and offices staffed by people, and in 1978 Ned Block envisioned the entire population of China involved in such a brain simulation. This thought experiment is called the China brain, also the "Chinese Nation" or the "Chinese Gym".
The Chinese room was introduced in Searle's 1980 paper "Minds, Brains, and Programs", published in "Behavioral and Brain Sciences". It eventually became the journal's "most influential target article", generating an enormous number of commentaries and responses in the ensuing decades. David Cole writes that "the Chinese Room argument has probably been the most widely discussed philosophical argument in cognitive science to appear in the past 25 years".
Most of the discussion consists of attempts to refute it. "The overwhelming majority," notes "BBS" editor Stevan Harnad, "still think that the Chinese Room Argument is dead wrong." The sheer volume of the literature that has grown up around it inspired Pat Hayes to quip that the field of cognitive science ought to be redefined as "the ongoing research program of showing Searle's Chinese Room Argument to be false".
Searle's paper has become "something of a classic in cognitive science," according to Harnad. Varol Akman agrees, and has described his paper as "an exemplar of philosophical clarity and purity".
Philosophy.
Although the Chinese Room argument was originally presented in reaction to the statements of AI researchers, philosophers have come to view it as an important part of the philosophy of mind. It is a challenge to functionalism and the computational theory of mind, and is related to such questions as the mind–body problem, the problem of other minds, the symbol-grounding problem, and the hard problem of consciousness.
Strong AI.
Searle identified a philosophical position he calls "strong AI":
The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.
The definition hinges on the distinction between "simulating" a mind and "actually having" a mind. Searle writes that "according to Strong AI, the correct simulation really is a mind. According to Weak AI, the correct simulation is a model of the mind."
The position is implicit in some of the statements of early AI researchers and analysts. For example, in 1955, AI founder Herbert A. Simon declared that "there are now in the world machines that think, that learn and create" and claimed that they had "solved the venerable mind–body problem, explaining how a system composed of matter can have the properties of mind." John Haugeland wrote that "AI wants only the genuine article: "machines with minds", in the full and literal sense. This is not science fiction, but real science, based on a theoretical conception as deep as it is daring: namely, we are, at root, "computers ourselves"."
Searle also ascribes the following positions to advocates of strong AI:
Strong AI as computationalism or functionalism.
In more recent presentations of the Chinese room argument, Searle has identified "strong AI" as "computer functionalism" (a term he attributes to Daniel Dennett). Functionalism is a position in modern philosophy of mind that holds that we can define mental phenomena (such as beliefs, desires, and perceptions) by describing their functions in relation to each other and to the outside world. Because a computer program can accurately represent functional relationships as relationships between symbols, a computer can have mental phenomena if it runs the right program, according to functionalism.
Stevan Harnad argues that Searle's depictions of strong AI can be reformulated as "recognizable tenets of "computationalism", a position (unlike "strong AI") that is actually held by many thinkers, and hence one worth refuting." Computationalism is the position in the philosophy of mind which argues that the mind can be accurately described as an information-processing system.
Each of the following, according to Harnad, is a "tenet" of computationalism:
Strong AI vs. biological naturalism.
Searle holds a philosophical position he calls "biological naturalism": that consciousness and understanding require specific biological machinery that is found in brains. He writes "brains cause minds" and that "actual human mental phenomena dependent on actual physical–chemical properties of actual human brains". Searle argues that this machinery (known to neuroscience as the "neural correlates of consciousness") must have some (unspecified) "causal powers" that permit the human experience of consciousness. Searle's faith in the existence of these powers has been criticized.
Searle does not disagree that machines can have consciousness and understanding, because, as he writes, "we are precisely such machines". Searle holds that the brain is, in fact, a machine, but the brain gives rise to consciousness and understanding using machinery that is non-computational. If neuroscience is able to isolate the mechanical process that gives rise to consciousness, then Searle grants that it may be possible to create machines that have consciousness and understanding. However, without the specific machinery required, Searle does not believe that consciousness can occur.
Biological naturalism implies that one cannot determine if the experience of consciousness is occurring merely by examining how a system functions, because the specific machinery of the brain is essential. Thus, biological naturalism is directly opposed to both behaviorism and functionalism (including "computer functionalism" or "strong AI"). Biological naturalism is similar to identity theory (the position that mental states are "identical to" or "composed of" neurological events), however, Searle has specific technical objections to identity theory. Searle's biological naturalism and strong AI are both opposed to Cartesian dualism, the classical idea that the brain and mind are made of different "substances". Indeed, Searle accuses strong AI of dualism, writing that "strong AI only makes sense given the dualistic assumption that, where the mind is concerned, the brain doesn't matter."
Consciousness.
Searle's original presentation emphasized "understanding"—that is, mental states with what philosophers call "intentionality"—and did not directly address other closely related ideas such as "consciousness". However, in more recent presentations Searle has included consciousness as the real target of the argument.
David Chalmers writes "it is fairly clear that consciousness is at the root of the matter" of the Chinese room. 
Colin McGinn argues that that the Chinese room provides strong evidence that the hard problem of consciousness is fundamentally insoluble. The argument, to be clear, is not about whether a machine can be conscious, but about whether it (or anything else for that matter) can be shown to be conscious. It is plain that any other method of probing the occupant of a Chinese room has the same difficulties in principle as exchanging questions and answers in Chinese. It is simply not possible to divine whether a conscious agency inhabits the room or some clever simulation. 
Searle argues that this only true for an observer "outside" of the room. The whole point of the thought experiment is to put someone "inside" the room, where they can directly observe the operations of consciousness. Searle claims that from his vantage point within the room there is nothing he can see that could imaginably give rise to consciousness, other than himself, and clearly he does not have a mind that can speak Chinese.
Computer science.
The Chinese room argument is primarily an argument in the philosophy of mind, and both major computer scientists and artificial intelligence researchers consider it irrelevant to their fields. However, several concepts developed by computer scientists are essential to understanding the argument, including symbol processing, Turing machines, Turing completeness, and the Turing test.
Strong AI vs. AI research.
Searle's arguments are not usually considered an issue for AI research. Stuart Russell and Peter Norvig observe that most AI researchers "don't care about the strong AI hypothesis—as long as the program works, they don't care whether you call it a simulation of intelligence or real intelligence." The primary mission of artificial intelligence research is only to create useful systems that "act" intelligently, and it does not matter if the intelligence is "merely" a simulation. 
Searle does not disagree that AI research can create machines that are capable of highly intelligent behavior. The Chinese room argument leaves open the possibility that a digital machine could be built that "acts" more intelligent than a person, but does not have a mind or intentionality in the same way that brains do. Indeed, Searle writes that "the Chinese room argument ... assumes complete success on the part of artificial intelligence in simulating human cognition."
Searle's "strong AI" should not be confused with "strong AI" as defined by Ray Kurzweil and other futurists, who use the term to describe machine intelligence that rivals or exceeds human intelligence. Kurzweil is concerned primarily with the "amount" of intelligence displayed by the machine, whereas Searle's argument sets no limit on this. Searle argues that even a super-intelligent machine would not necessarily have a mind and consciousness.
Symbol processing.
The Chinese room (and all modern computers) manipulate physical objects in order to carry out calculations and do simulations. AI researchers Allen Newell and Herbert A. Simon called this kind of machine a physical symbol system. It is also equivalent to the formal systems used in the field of mathematical logic. Searle emphasizes the fact that this kind of symbol manipulation is syntactic (borrowing a term from the study of grammar). The computer manipulates the symbols using a form of syntax rules, without any knowledge of the symbol's semantics (that is, their meaning).
Chinese room as a Turing machine.
The Chinese room has a design analogous to that of a modern computer. It has a Von Neumann architecture, which consists of a program (the book of instructions), some memory (the papers and file cabinets), a CPU which follows the instructions (the man), and a means to write symbols in memory (the pencil and eraser). A machine with this design is known in theoretical computer science as "Turing complete", because it has the necessary machinery to carry out any computation that a Turing machine can do, and therefore it is capable of doing a step-by-step simulation of any other digital machine, given enough memory and time. Alan Turing writes, "all digital computers are in a sense equivalent." The widely accepted Church-Turing thesis holds that any function computable by an effective procedure is computable by a Turing machine. In other words, the Chinese room can do whatever any other digital computer can do (albeit much, much more slowly).
There are some critics, such as Hanoch Ben-Yami, who argue that the Chinese room can not simulate all the abilities of a digital computer, such as being able to determine the current time.
Turing test.
The Turing test is a test of a machine's ability to exhibit intelligent behaviour. In Alan Turing's original illustrative example, a human judge engages in a natural language conversation with a human and a machine designed to generate performance indistinguishable from that of a human being. All participants are separated from one another. If the judge cannot reliably tell the machine from the human, the machine is said to have passed the test.
Complete argument.
Searle has produced a more formal version of the argument of which the Chinese Room forms a part. He presented the first version in 1984. The version given below is from 1990. The only part of the argument which should be controversial is A3 and it is this point which the Chinese room thought experiment is intended to prove.
He begins with three axioms:
Searle posits that these lead directly to this conclusion:
This much of the argument is intended to show that artificial intelligence can never produce a machine with a mind by writing programs that manipulate symbols. The remainder of the argument addresses a different issue. Is the human brain running a program? In other words, is the computational theory of mind correct? He begins with an axiom that is intended to express the basic modern scientific consensus about brains and minds:
Searle claims that we can derive "immediately" and "trivially" that:
And from this he derives the further conclusions:
Replies.
Replies to Searle's argument may be classified according to what they claim to show:
Some of the arguments (robot and brain simulation, for example) fall into multiple categories.
System and virtual mind replies: finding the mind.
These replies attempt to answer the question: since the man in the room doesn't speak Chinese, "where" is the "mind" that does? These replies address the key ontological issues of mind vs. body and simulation vs. reality. All of the replies that identify the mind in the room are versions of "the system reply".
More sophisticated versions of the system reply try to identify more precisely what "the system" is and they differ in exactly how they describe it. According to these replies, the "mind that speaks Chinese" could be such things as: the "software", a "program", a "running program", a simulation of the "neural correlates of consciousness", the "functional system", a "simulated mind", an "emergent property", or "a virtual mind" (Marvin Minsky's version of the system reply, described below).
These replies provide an explanation of exactly who it is that understands Chinese. If there is something "besides" the man in the room that can understand Chinese, Searle can't argue that (1) the man doesn't understand Chinese, therefore (2) nothing in the room understands Chinese. This, according to those who make this reply, shows that Searle's argument fails to prove that "strong AI" is false.
However, the replies, by themselves, do not prove that strong AI is "true", either: they provide no evidence that the system (or the virtual mind) understands Chinese, other than the hypothetical premise that it passes the Turing Test. As Searle writes "the systems reply simply begs the question by insisting that system must understand Chinese."
Robot and semantics replies: finding the meaning.
As far as the person in the room is concerned, the symbols are just meaningless "squiggles." But if the Chinese room really "understands" what it is saying, then the symbols must get their meaning from somewhere. These arguments attempt to connect the symbols to the things they symbolize. These replies address Searle's concerns about intentionality, symbol grounding and syntax vs. semantics.
To each of these suggestions, Searle's response is the same: no matter how much knowledge is written into the program and no matter how the program is connected to the world, he is still in the room manipulating symbols according to rules. His actions are syntactic and this can never explain to him what the symbols stand for. Searle writes "syntax is insufficient for semantics."
However, for those who accept that Searle's actions simulate a mind, separate from his own, the important question is not what the symbols mean "to Searle", what is important is what they mean "to the virtual mind." While Searle is trapped in the room, the virtual mind is not: it is connected to the outside world through the Chinese speakers it speaks to, through the programmers who gave it world knowledge, and through the cameras and other sensors that roboticists can supply.
Brain simulation and connectionist replies: redesigning the room.
These arguments are all versions of the system reply that identify a particular "kind" of system as being important; they identify some special technology that would create conscious understanding in a machine. (Note that the "robot" and "commonsense knowledge" replies above also specify a certain kind of system as being important.)
magine that instead of a monolingual man in a room shuffling symbols we have the man operate an elaborate set of water pipes with valves connecting them. When the man receives the Chinese symbols, he looks up in the program, written in English, which valves he has to turn on and off. Each water connection corresponds to a synapse in the Chinese brain, and the whole system is rigged up so that after doing all the right firings, that is after turning on all the right faucets, the Chinese answers pop out at the output end of the series of pipes.
Now where is the understanding in this system? It takes Chinese as input, it simulates the formal structure of the synapses of the Chinese brain, and it gives Chinese as output. But the man certainly doesn't understand Chinese, and neither do the water pipes, and if we are tempted to adopt what I think is the absurd view that somehow the conjunction of man and water pipes understands, remember that in principle the man can internalize the formal structure of the water pipes and do all the "neuron firings" in his imagination.
These arguments (and the robot or commonsense knowledge replies) identify some special technology that would help create conscious understanding in a machine. They may be interpreted in two ways: either they claim (1) this technology is required for consciousness, the Chinese room does not or can not implement this technology, and therefore the Chinese room can not pass the Turing test or (even if it did) it would not have conscious understanding. Or they may be claiming that (2) it is easier to see that the Chinese room has a mind if we visualize this technology is being used to create it.
In the first case, where features like a robot body or a connectionist architecture are required, Searle claims that strong AI (as he understands it) has been abandoned. The Chinese room has all the elements of a Turing complete machine, and thus is capable of simulating any digital computation whatsoever. If Searle's room can't pass the Turing test then there is no other digital technology that could pass the Turing test. If Searle's room "could" pass the Turing test, but still does not have a mind, then the Turing test is not sufficient to determine if the room has a "mind". Either way, it denies one or the other of the positions Searle thinks of as "strong AI", proving his argument.
The brain arguments in particular deny strong AI if they assume that there is no simpler way to describe the mind than to create a program that is just as mysterious as the brain was. He writes "I thought the whole idea of strong AI was that we don't need to know how the brain works to know how the mind works." If computation does not provide an "explanation" of the human mind, then strong AI has failed, according to Searle.
Other critics hold that the room as Searle described it does, in fact, have a mind, however they argue that it is difficult to see—Searle's description is correct, but "misleading." By redesigning the room more realistically they hope to make this more obvious. In this case, these arguments are being used as appeals to intuition (see next section).
In fact, the room can just as easily be redesigned to "weaken" our intuitions. Ned Block's "blockhead" argument suggests that the program could, in theory, be rewritten into a simple lookup table of rules of the form "if the user writes "S", reply with "P" and goto X". At least in principle, any program can be rewritten (or "refactored") into this form, even a brain simulation. In the blockhead scenario, the entire mental state is hidden in the letter X, which represents a memory address—a number associated with the next rule. It is hard to visualize that an instant of one's conscious experience can be captured in a single large number, yet this is exactly what "strong AI" claims. On the other hand, such a lookup table would be ridiculously large (probably to the point of being impossible in practice), and the states could therefore be "extremely" specific.
Searle's argues that however the program is written or however the machine is connected to the world, the mind is being "simulated" by a simple step by step digital machine (or machines). These machines are always just like the man in the room: they understand nothing and don't speak Chinese. They are merely manipulating symbols without knowing what they mean. Searle writes: "I can have any formal program you like, but I still understand nothing."
Speed and complexity: appeals to intuition.
The following arguments (and the intuitive interpretations of the arguments above) do not directly explain how a Chinese speaking mind could exist in Searle's room, or how the symbols he manipulates could become meaningful. However, by raising doubts about Searle's intuitions they support other positions, such as the system and robot replies. These arguments, if accepted, prevent Searle from claiming that his conclusion is obvious by undermining the intuitions that his certainty requires.
Several critics believe that Searle's argument relies entirely on intuitions. Ned Block writes "Searle's argument depends for its force on intuitions that certain entities do not think." Daniel Dennett describes the Chinese room argument as a misleading "intuition pump" and writes "Searle's thought experiment depends, illicitly, on your imagining too simple a case, an irrelevant case, and drawing the 'obvious' conclusion from it."
Some of the arguments above also function as appeals to intuition, especially those that are intended to make it seem more plausible that the Chinese room contains a mind, which can include the robot, commonsense knowledge, brain simulation and connectionist replies. Several of the replies above also address the specific issue of complexity. The connectionist reply emphasizes that a working artificial intelligence system would have to be as complex and as interconnected as the human brain. The commonsense knowledge reply emphasizes that any program that passed a Turing test would have to be "an extraordinarily supple, sophisticated, and multilayered system, brimming with 'world knowledge' and meta-knowledge and meta-meta-knowledge", as Daniel Dennett explains.
An especially vivid version of the speed and complexity reply is from Paul and Patricia Churchland. They propose this analogous thought experiment:
Stevan Harnad is critical of speed and complexity replies when they stray beyond addressing our intuitions. He writes "Some have made a cult of speed and timing, holding that, when accelerated to the right speed, the computational may make a phase transition into the mental. It should be clear that is not a counterargument but merely an "ad hoc" speculation (as is the view that it is all just a matter of ratcheting up to the right degree of 'complexity.')"
Searle accuses his critics of placing too much faith in their own intuitions. Searle argues that anyone who is willing to accept the "system reply" (which asserts that a mind can emerge from "a system" without saying what the system is or how such a thing might give rise to a mind) has been completely misled by their own intuitions, writing that they are "under the grip of an ideology".
Other minds and zombies: meaninglessness.
Several replies argue that Searle's argument is irrelevant because his assumptions about the mind and consciousness are faulty. Searle believes that human beings directly experience their consciousness, intentionality and the nature of the mind every day, and that this experience of consciousness is not open to question. He writes that we must "presuppose the reality and knowability of the mental." These replies question whether Searle is justified in using his own experience of consciousness to determine that it is more than mechanical symbol processing. In particular, the other minds reply argues that we cannot use our experience of consciousness to answer questions about other minds (even the mind of a computer), and the epiphenomena reply argues that Searle's consciousness does not "exist" in the sense that Searle thinks it does.
Searle disagrees with this analysis and argues that "the study of the mind starts with such facts as that humans have beliefs, while thermostats, telephones, and adding machines don't ... what we wanted to know is what distinguishes the mind from thermostats and livers." He takes it as obvious that we can detect the presence of consciousness and dismisses these replies as being off the point.
Daniel Dennett provides this extension to the "epiphenomena" argument.

</doc>
<doc id="6217" url="http://en.wikipedia.org/wiki?curid=6217" title="Charon">
Charon

Charon may refer to:

</doc>
<doc id="6220" url="http://en.wikipedia.org/wiki?curid=6220" title="Circle">
Circle

A circle is a simple shape of Euclidean geometry that is the set of all points in a plane that are at a given distance from a given point, the centre. The distance between any of the points and the centre is called the radius. It can also be defined as the locus of a point equidistant from a fixed point.
A circle is a simple closed curve which divides the plane into two regions: an interior and an exterior. In everyday use, the term "circle" may be used interchangeably to refer to either the boundary of the figure, or to the whole figure including its interior; in strict technical usage, the circle is the former and the latter is called a disk. 
A circle can be defined as the curve traced out by a point that moves so that its distance from a given point is constant. 
A circle may also be defined as a special ellipse in which the two foci are coincident and the eccentricity is 0, or the two-dimensional shape enclosing the most area per unit perimeter, using calculus of variations. 
History.
[[Image:God the Geometer.jpg|thumb|right|200px|
The compass in this 13th-century manuscript is a symbol of God's act of Creation. Notice also the circular shape of the halo]]
The word "circle" derives from the Greek "κίρκος" ("kirkos"), itself a metathesis of the Homeric Greek "κρίκος" ("krikos"), meaning "hoop" or "ring". The origins of the words "circus" and "circuit" are closely related.
The circle has been known since before the beginning of recorded history. Natural circles would have been observed, such as the Moon, Sun, and a short plant stalk blowing in the wind on sand, which forms a circle shape in the sand. The circle is the basis for the wheel, which, with related inventions such as gears, makes much of modern machinery possible. In mathematics, the study of the circle has helped inspire the development of geometry, astronomy, and calculus.
Early science, particularly geometry and astrology and astronomy, was connected to the divine for most medieval scholars, and many believed that there was something intrinsically "divine" or "perfect" that could be found in circles.
Some highlights in the history of the circle are:
Analytic results.
Length of circumference.
The ratio of a circle's circumference to its diameter is Pi (pi), an irrational constant approximately equal to 3.141592654. Thus the length of the circumference "C" is related to the radius "r" and diameter "d" by:
Area enclosed.
As proved by Archimedes, the area enclosed by a circle is equal to that of a triangle whose base has the length of the circle's circumference and whose height equals the circle's radius, which comes to multiplied by the radius squared:
Equivalently, denoting diameter by "d",
that is, approximately 79 percent of the circumscribing square (whose side is of length "d").
The circle is the plane curve enclosing the maximum area for a given arc length. This relates the circle to a problem in the calculus of variations, namely the isoperimetric inequality.
Equations.
Cartesian coordinates.
In an "x"–"y" Cartesian coordinate system, the circle with centre coordinates ("a", "b") and radius "r" is the set of all points ("x", "y") such that
This equation, also known as Equation of the Circle, follows from the Pythagorean theorem applied to any point on the circle: as shown in the diagram to the right, the radius is the hypotenuse of a right-angled triangle whose other sides are of length and . If the circle is centred at the origin (0, 0), then the equation simplifies to
The equation can be written in parametric form using the trigonometric functions sine and cosine as
where "t" is a parametric variable in the range 0 to 2, interpreted geometrically as the angle that the ray from ("a", "b") to ("x", "y") makes with the "x"-axis. An alternative parametrisation of the circle is:
In this parametrisation, the ratio of "t" to "r" can be interpreted geometrically as the stereographic projection of the circle onto the line passing through the centre parallel to the "x"-axis.
In homogeneous coordinates each conic section with equation of a circle is of the form
It can be proven that a conic section is a circle exactly when it contains (when extended to the complex projective plane) the points "I"(1: "i": 0) and "J"(1: −"i": 0). These points are called the circular points at infinity.
Polar coordinates.
In polar coordinates the equation of a circle is:
where "a" is the radius of the circle, formula_12 is the polar coordinate of a generic point on the circle, and formula_13 is the polar coordinate of the centre of the circle (i.e., "r"0 is the distance from the origin to the centre of the circle, and "φ" is the anticlockwise angle from the positive "x"-axis to the line connecting the origin to the centre of the circle). For a circle centred at the origin, i.e. "r"0 = 0, this reduces to simply . When , or when the origin lies on the circle, the equation becomes
In the general case, the equation can be solved for "r", giving
the solution with a minus sign in front of the square root giving the same curve.
Complex plane.
In the complex plane, a circle with a centre at "c" and radius ("r") has the equation formula_16. In parametric form this can be written formula_17.
The slightly generalised equation formula_18 for real "p", "q" and complex "g" is sometimes called a generalised circle. This becomes the above equation for a circle with formula_19, since formula_20. Not all generalised circles are actually circles: a generalised circle is either a (true) circle or a line.
Tangent lines.
The tangent line through a point "P" on the circle is perpendicular to the diameter passing through "P". If and the circle has centre ("a", "b") and radius "r", then the tangent line is perpendicular to the line from ("a", "b") to ("x"1, "y"1), so it has the form . Evaluating at ("x"1, "y"1) determines the value of "c" and the result is that the equation of the tangent is
or
If then the slope of this line is
This can also be found using implicit differentiation.
When the centre of the circle is at the origin then the equation of the tangent line becomes
and its slope is
Properties.
Sagitta.
Another proof of this result which relies only on two chord properties given above is as follows. Given a chord of length "y" and with sagitta of length "x", since the sagitta intersects the midpoint of the chord, we know it is part of a diameter of the circle. Since the diameter is twice the radius, the "missing" part of the diameter is () in length. Using the fact that one part of one chord times the other part is equal to the same product taken along a chord intersecting the first chord, we find that (. Solving for "r", we find the required result.
Inscribed angles.
An inscribed angle (examples are the blue and green angles in the figure) is exactly half the corresponding central angle (red). Hence, all inscribed angles that subtend the same arc (pink) are equal. Angles inscribed on the arc (brown) are supplementary. In particular, every inscribed angle that subtends a diameter is a right angle (since the central angle is 180 degrees).
Circle of Apollonius.
Apollonius of Perga showed that a circle may also be defined as the set of points in a plane having a constant "ratio" (other than 1) of distances to two fixed foci, "A" and "B". (The set of points where the distances are equal is the perpendicular bisector of "A" and "B", a line.) That circle is sometimes said to be drawn "about" two points.
The proof is in two parts. First, one must prove that, given two foci "A" and "B" and a ratio of distances, any point "P" satisfying the ratio of distances must fall on a particular circle. Let "C" be another point, also satisfying the ratio and lying on segment "AB". By the angle bisector theorem the line segment "PC" will bisect the interior angle "APB", since the segments are similar:
Analogously, a line segment "PD" through some point "D" on "AB" extended bisects the corresponding exterior angle "BPQ" where "Q" is on "AP" extended. Since the interior and exterior angles sum to 180 degrees, the angle "CPD" is exactly 90 degrees, i.e., a right angle. The set of points "P" such that angle "CPD" is a right angle forms a circle, of which "CD" is a diameter.
Second, see for a proof that every point on the indicated circle satisfies the given ratio.
Cross-ratios.
A closely related property of circles involves the geometry of the cross-ratio of points in the complex plane. If "A", "B", and "C" are as above, then the circle of Apollonius for these three points is the collection of points "P" for which the absolute value of the cross-ratio is equal to one:
Stated another way, "P" is a point on the circle of Apollonius if and only if the cross-ratio ["A","B";"C","P"] is on the unit circle in the complex plane.
Generalised circles.
If "C" is the midpoint of the segment "AB", then the collection of points "P" satisfying the Apollonius condition
is not a circle, but rather a line.
Thus, if "A", "B", and "C" are given distinct points in the plane, then the locus of points "P" satisfying the above equation is called a "generalised circle." It may either be a true circle or a line. In this sense a line is a generalised circle of infinite radius.
Circles inscribed in or circumscribed about other figures.
In every triangle a unique circle, called the incircle, can be inscribed such that it is tangent to each of the three sides of the triangle.
About every triangle a unique circle, called the circumcircle, can be circumscribed such that it goes through each of the triangle's three vertices.
A tangential polygon, such as a tangential quadrilateral, is any convex polygon within which a circle can be inscribed that is tangent to each side of the polygon.
A cyclic polygon is any convex polygon about which a circle can be circumscribed, passing through each vertex. A well-studied example is the cyclic quadrilateral.
A hypocycloid is a curve that is inscribed in a given circle by tracing a fixed point on a smaller circle that rolls within and tangent to the given circle.
Circle as limiting case of other figures.
The circle can be viewed as a limiting case of each of various other figures:

</doc>
<doc id="6221" url="http://en.wikipedia.org/wiki?curid=6221" title="Cardinal (Catholicism)">
Cardinal (Catholicism)

A cardinal (Latin: "sanctae romanae ecclesiae cardinalis") is a senior ecclesiastical official, an ecclesiastical prince, and usually an ordained bishop of the Roman Catholic Church. The cardinals of the Church are collectively known as the College of Cardinals. The duties of the cardinals include attending the meetings of the College and making themselves available individually or in groups to the pope as requested. Most have additional duties, such as leading a diocese or archdiocese or managing a department of the Roman Curia. A cardinal's other main function is electing the pope when the see becomes vacant. During the "sede vacante", the period between a pope's death or resignation and the election of his successor, the day-to-day governance of the Church as a whole is in the hands of the College of Cardinals. The right to enter the conclave of cardinals where the pope is elected is limited to those who have not reached the age of 80 years by the day the vacancy occurs.
In 1059, the right of electing the pope was reserved to the principal clergy of Rome and the bishops of the seven suburbicarian sees. In the 12th century the practice of appointing ecclesiastics from outside Rome as cardinals began, with each of them assigned a church in Rome as his titular church or linked with one of the suburbicarian dioceses, while still being incardinated in a diocese other than that of Rome.
The term "cardinal" at one time applied to any priest permanently assigned or incardinated to a church, or specifically to the senior priest of an important church, based on the Latin "cardo" (hinge), meaning "principal" or "chief". The term was applied in this sense as early as the ninth century to the priests of the "tituli" (parishes) of the diocese of Rome. The Church of England retains an instance of this origin of the title, which is held by the two senior members of the College of Minor Canons of St Paul's Cathedral.
History.
The pope was originally elected by the clergy and the people of the diocese of Rome. In medieval times, Roman nobility gained influence. The Holy Roman Emperors had a hand in choosing the pontiff. As the pope gained greater political independence, the right of election was, as stated in the bull "In nomine Domini", reserved to cardinals in 1059, leaving the emperor only with a vague right of approbation.
The earlier influence of temporal rulers, notably the French kings, reasserted itself through the influence of cardinals of certain nationalities or politically significant movements. Traditions even developed entitling certain monarchs, including those of Austria, Spain, and Portugal, to nominate one of their trusted clerical subjects to be created cardinal, a so-called "crown-cardinal".
In theory, the pope could substitute another body of electors for the College of Cardinals. Some have proposed that the Synod of Bishops should perform this function, a proposal that was not accepted, because, among other reasons, the Synod of Bishops can meet only when called by the pope.
In early modern times, cardinals often had important roles in secular affairs. In some cases, they took on powerful positions in government. In Henry VIII's England, his chief minister was Cardinal Wolsey. Cardinal Richelieu's power was so great that he was for many years effectively the ruler of France. Richelieu successor was also a cardinal, Jules Mazarin. Guillaume Dubois and André-Hercule de Fleury complete the list of the "four great" cardinals to have ruled France. In Portugal, due to a succession crisis, one cardinal, Henry, King of Portugal, was crowned king, the only example of a cardinal-king.
College and orders of cardinalate.
Pope Sixtus V limited the number of cardinals to 70, comprising six cardinal bishops, 50 cardinal priests, and 14 cardinal deacons. Pope John XXIII exceeded that limit. At the start of 1971, Pope Paul VI set the number of cardinal electors at a maximum of 120, but set no limit on the number of cardinals generally. He also established a maximum age of eighty years for electors. His action deprived twenty-five living cardinals, including the three elevated by Pope Pius XI, of the right to participate in a conclave. Popes can dispense from church laws and have sometimes brought the number of cardinals under the age of 80 to more than 120. Pope Paul VI also increased the number of cardinal bishops by giving that rank to patriarchs of the Eastern Catholic Churches.
Titular church.
Each cardinal takes on a titular church, either a church in the city of Rome or one of the suburbicarian sees. The only exception is for patriarchs of Eastern Catholic Churches. Nevertheless, cardinals possess no power of governance nor are they to intervene in any way in matters which pertain to the administration of goods, discipline, or the service of their titular churches.
The Dean of the College of Cardinals in addition to such a titular church also receives the titular bishopric of Ostia, the primary suburbicarian see. Cardinals governing a particular Church retain that church.
Title and reference style.
In 1630, Pope Urban VIII decreed their title to be "Eminence" (previously, it had been "illustrissimo" and "reverendissimo") and decreed that their secular rank would equate to Prince, making them secondary only to the Pope and crowned monarchs. 
In accordance with tradition, they sign by placing the title "Cardinal" (abbreviated "Card.") after their personal name and before their surname as, for instance, "John Card(inal) Doe" or, in Latin, "Ioannes Card(inalis) Cognomen". Similarly, the traditional official signature of popes inserts the Latin title "Papa" (abbreviated "PP.") immediately after the personal name, as "Benedictus PP. XVI" for "Pope Benedict XVI". Pope Francis signs some documents simply as "Franciscus", but others in the traditional way as "Franciscus PP." Some writers, such as James-Charles Noonan, hold that, in the case of cardinals, the form used for signatures should be used also when referring to them, even in English; and this is the usual but not the only way of referring to cardinals in Latin. Several influential stylebooks, both secular and religious, however, indicate that the correct form for referring to a cardinal in English is as "Cardinal <Name> <Surname>". This style is also generally followed on the websites of the Holy See and episcopal conferences. Oriental Patriarchs who are created Cardinals customarily use "Sanctae Ecclesiae Cardinalis" as their full title, probably because they do not belong to the Roman clergy.
A well-known instance of the "John Cardinal Doe" style is that in the proclamation, in Latin, of the election of a new pope by the cardinal protodeacon: ""Annuntio vobis gaudium magnum; habemus Papam: Eminentissimum ac Reverendissimum Dominum, Dominum (first name) Sanctae Romanae Ecclesiae Cardinalem (last name), ..."" (Meaning: ""I announce to you a great joy; we have a Pope: The Most Eminent and Most Reverend Lord, Lord (first name) Cardinal of the Holy Roman Church (last name), ..."")
Orders.
Cardinal bishop.
Cardinal bishops (cardinals of the episcopal order) are among the most senior prelates of the Catholic Church. Though in modern times most cardinals are also bishops, the term "cardinal bishop" only refers to the cardinals who are titular bishops of one of the "suburbicarian" sees.
In early times, the privilege of papal election was not reserved to the cardinals, and for centuries the person elected was customarily a Roman priest and never a bishop from elsewhere. To preserve apostolic succession the rite of consecrating him a bishop had to be performed by someone who was already a bishop. The rule remains that, if the person elected Pope is not yet a bishop, he is consecrated by the Dean of the College of Cardinals, the Cardinal Bishop of Ostia.
The Dean of the College of Cardinals, the "primus inter pares" of the College of Cardinals, is elected by the cardinal bishops holding suburbicarian sees from among their own number, an election, however, that must be approved by the Pope. Formerly the position of dean belonged to the longest-serving of the cardinal bishops.
The suburbicarian sees are seven: Ostia, Albano, Porto and Santa Rufina, Palestrina, Sabina and Mentana, Frascati, and Velletri. Velletri was united with Ostia from 1150 until 1914, when Pope Pius X separated them again, but decreed that whatever cardinal bishop became Dean of the College of Cardinals would keep the suburbicarian see he already held, adding to it that of Ostia, with the result that there continued to be only six cardinal bishops.
Since 1962, the cardinal bishops have only a titular relationship with the suburbicarian sees, with no powers of governance over them. Each see has its own bishop, with the exception of Ostia, in which the Cardinal Vicar of the see of Rome is apostolic administrator.
The current cardinal bishops of the suburbicarian dioceses are:
For a period ending in the mid-20th century, long-serving cardinal priests were entitled to fill vacancies that arose among the cardinal bishops, just as cardinal deacons of ten years' standing are still entitled to become cardinal priests. Since then, cardinals have been advanced to cardinal bishop exclusively by papal appointment.
In 1965, Pope Paul VI decreed in his motu proprio "Ad Purpuratorum Patrum" that patriarchs of the Eastern Catholic Churches who were named cardinals would also be part of the episcopal order, ranked after the six cardinal bishops of the suburbicarian sees (who had been relieved of direct responsibilities for those sees by Pope John XXIII three years earlier). Not holding a suburbicarian see, they cannot elect the dean or become dean. There are currently three Eastern Patriarchs who are cardinal bishops:
If a Latin Rite patriarch is made a cardinal, he ranks as a cardinal priest, not as a cardinal bishop. While the incumbents of some sees are regularly made cardinals, no see carries an actual right to the cardinalate, even if its bishop is a patriarch.
Cardinal priest.
Cardinal priests are the most numerous of the three orders of cardinals in the Catholic Church, ranking above the cardinal deacons and below the cardinal bishops. Those who are named cardinal priests today are generally bishops of important dioceses throughout the world, though some hold Curial positions.
In modern times, the name "cardinal priest" is interpreted as meaning a cardinal who is of the order of priests. Originally, however, this referred to certain key priests of important churches of the Diocese of Rome, who were recognized as the "cardinal" priests, the important priests chosen by the pope to advise him in his duties as Bishop of Rome (the Latin "cardo" means "hinge"). Certain clerics in many dioceses at the time, not just that of Rome, were said to be the key personnel — the term gradually became exclusive to Rome to indicate those entrusted with electing the bishop of Rome, the pope.
While the cardinalate has long been expanded beyond the Roman pastoral clergy and Roman Curia, every cardinal priest has a titular church in Rome, though they may be bishops or archbishops elsewhere, just as cardinal bishops are given one of the suburbicarian dioceses around Rome. Pope Paul VI abolished all administrative rights cardinals had with regard to their titular churches, though the cardinal's name and coat of arms are still posted in the church, and they are expected to preach there if convenient when they are in Rome.
While the number of cardinals was small from the times of the Roman Empire to the Renaissance, and frequently smaller than the number of recognized churches entitled to a cardinal priest, in the 16th century the College expanded markedly. In 1587, Pope Sixtus V sought to arrest this growth by fixing the maximum size of the College at 70, including 50 cardinal priests, about twice the historical number. This limit was respected until 1958, and the list of titular churches modified only on rare occasions, generally when a building fell into disrepair. When Pope John XXIII abolished the limit, he began to add new churches to the list, which Popes Paul VI and John Paul II continued to do. Today there are close to 150 titular churches, out of over 300 churches in Rome.
The cardinal who is the longest-serving member of the order of cardinal priests is titled "cardinal protopriest". He had certain ceremonial duties in the conclave that have effectively ceased because he would generally have already reached age 80, at which cardinals are barred from the conclave. The current cardinal protopriest is Paulo Evaristo Arns of Brazil.
Cardinal deacon.
The cardinal deacons are the lowest-ranking cardinals. Cardinals elevated to the diaconal order are either officials of the Roman Curia or priests elevated after their 80th birthday. Bishops with diocesan responsibilities, however, are created cardinal priests.
Cardinal deacons derive originally from the seven deacons in the Papal Household and the seven deacons who supervised the Church's works in the districts of Rome during the early Middle Ages, when church administration was effectively the government of Rome and provided all social services. Cardinal deacons are given title to one of these deaconries.
Cardinals elevated to the diaconal order are mainly officials of the Roman Curia holding various posts in the church administration. Their number and influence has varied through the years. While historically predominantly Italian the group has become much more internationally diverse in later years. While in 1939 about half were Italian by 1994 the number was reduced to one third. Their influence in the election of the Pope has been considered important, they are better informed and connected than the dislocated cardinals but their level of unity has been varied. Under the 1587 decree of Pope Sixtus V, which fixed the maximum size of the College of Cardinals, there were 14 cardinal deacons. Later the number increased. As late as 1939 almost half of the cardinals were members of the curia. Pius XII reduced this percentage to 24 percent. John XXIII brought it back up to 37 percent but Paul VI brought it down to 27 percent where John Paul II has maintained this ratio.
As of 2005, there were over 50 churches recognized as cardinalatial deaconries, though there were only 30 cardinals of the order of deacons. Cardinal deacons have long enjoyed the right to "opt for the order of cardinal priests" ("optazione") after they have been cardinal deacons for 10 years. They may on such elevation take a vacant "title" (a church allotted to a cardinal priest as the church in Rome with which he is associated) or their diaconal church may be temporarily elevated to a cardinal priest's "title" for that occasion. When elevated to cardinal priests, they take their precedence according to the day they were first made cardinal deacons (thus ranking above cardinal priests who were elevated to the college after them, regardless of order).
When not celebrating Mass but still serving a liturgical function, such as the semiannual "Urbi et Orbi" papal blessing, some Papal Masses and some events at Ecumenical Councils, cardinal deacons can be recognized by the dalmatics they would don with the simple white mitre (so called "mitra simplex").
Cardinal protodeacon.
The Cardinal protodeacon, the senior cardinal deacon in order of appointment to the College of Cardinals, has the privilege of announcing a new pope's election and name (once he has been ordained to the Episcopate) from the central balcony at the Basilica of Saint Peter in Vatican City State. In the past, during papal coronations, the Proto-Deacon also had the honor of bestowing the pallium on the new pope and crowning him with the papal tiara. However, in 1978 Pope John Paul I chose not to be crowned and opted for a simpler papal inauguration ceremony, and his three successors followed that example. As a result, the Cardinal protodeacon's privilege of crowning a new pope has effectively ceased although it could be revived if a future Pope were to restore a coronation ceremony. However, the Proto-Deacon still has the privilege of bestowing the pallium on a new pope at his papal inauguration. “Acting in the place of the Roman Pontiff, he also confers the pallium upon metropolitan bishops or gives the pallium to their proxies.” The current Cardinal Proto-Deacon is Renato Raffaele Martino.
Cardinal Protodeacons since 1911.
<br>†Was protodeacon at time of death
Special types of cardinals.
Camerlengo.
The Cardinal Camerlengo of the Holy Roman Church, assisted by the Vice-Camerlengo and the other prelates of the office known as the Apostolic Camera, has functions that in essence are limited to a period of sede vacante of the papacy. He is to collate information about the financial situation of all administrations dependent on the Holy See and present the results to the College of Cardinals, as they gather for the papal conclave.
Cardinals who are not bishops.
Until 1917, it was possible for someone who was not a priest, but only in minor orders, to become a cardinal (see "lay cardinals", below), but they were enrolled only in the order of cardinal deacons. For example, in the 16th century, Reginald Pole was a cardinal for 18 years before he was ordained a priest. In 1917 it was established that all cardinals, even cardinal deacons, had to be priests, and, in 1962, Pope John XXIII set the norm that all cardinals be ordained as bishops, even if they are only priests at the time of appointment. As a consequence of these two changes, canon 351 of the 1983 Code of Canon Law requires that a cardinal be at least in the order of priesthood at his appointment, and that those who are not already bishops must receive episcopal consecration. Several cardinals aged over 80 or close to it when appointed have obtained dispensation from the rule of having to be a bishop. These were all appointed cardinal-deacons, but one of them, Roberto Tucci, lived long enough to exercise the right of option and be promoted to the rank of cardinal-priest.
A cardinal who is not a bishop is still entitled to wear and use the episcopal vestments and other pontificalia (episcopal regalia: mitre, crozier, zucchetto, pectoral cross and ring). Even if not a bishop, any cardinal has honorary precedence over bishops who are not cardinals, but he cannot perform the functions reserved solely to bishops, such as ordination. The prominent priests who since 1962 were not ordained bishops on their elevation to the cardinalate were over the age of 80 or near to it, and so no cardinal who was not a bishop has participated in recent papal conclaves.
"Lay cardinals".
At various times, there have been cardinals who had only received first tonsure and minor orders but not yet been ordained as deacons or priests. Though clerics, they were inaccurately called "lay cardinals" and were permitted to marry. Teodolfo Mertel was among the last of the lay cardinals. When he died in 1899 he was the last surviving cardinal who was not at least ordained a priest. With the revision of the Code of Canon Law promulgated in 1917 by Pope Benedict XV, only those who are already priests or bishops may be appointed cardinals. Since the time of Pope John XXIII a priest who is appointed a cardinal must be consecrated a bishop, unless he obtains a dispensation.
Cardinals "in pectore" or secret cardinals.
In addition to the named cardinals, the pope may name secret cardinals or cardinals "in pectore" (Latin for "in the breast").
During the Western Schism, many cardinals were created by the contending popes. Beginning with the reign of Pope Martin V, cardinals were created without publishing their names until later, termed "creati et reservati in pectore".
A cardinal named "in pectore" is known only to the pope; not even the cardinal so named is necessarily aware of his elevation, and in any event cannot function as a cardinal while his appointment is "in pectore". Today, cardinals are named "in pectore" to protect them or their congregations from reprisals if their identities were known.
If conditions change, so that the pope judges it safe to make the appointment public, he may do so at any time. The cardinal in question then ranks in precedence with those raised to the cardinalate at the time of his "in pectore" appointment. If a pope dies before revealing the identity of an "in pectore" cardinal, the cardinalate expires.
Of the 232 cardinals that Pope John Paul II elevated, four were named "in pectore". The identities of three of these were subsequently revealed:
Vesture and privileges.
When in choir dress, a Latin-rite cardinal wears scarlet garments — the blood-like red symbolizes a cardinal's willingness to die for his faith. Excluding the rochet — which is always white — the scarlet garments include the cassock, mozzetta, and biretta (over the usual scarlet zucchetto). The biretta of a cardinal is distinctive not merely for its scarlet color, but also for the fact that it does not have a pompon or tassel on the top as do the birettas of other prelates. Until the 1460s, it was customary for cardinals to wear a violet or blue cape unless granted the privilege of wearing red when acting on papal business. His normal-wear cassock is black but has scarlet piping and a scarlet fascia (sash). Occasionally, a cardinal wears a scarlet "ferraiolo" which is a cape worn over the shoulders, tied at the neck in a bow by narrow strips of cloth in the front, without any 'trim' or piping on it. It is because of the scarlet color of cardinals' vesture that the bird of the same name has become known as such.
Eastern Catholic cardinals continue to wear the normal dress appropriate to their liturgical tradition, though some may line their cassocks with scarlet and wear scarlet fascias, or in some cases, wear Eastern-style cassocks entirely of scarlet.
In previous times, at the consistory at which the pope named a new cardinal, he would bestow upon him a distinctive wide-brimmed hat called a galero. This custom was discontinued in 1969 and the investiture now takes place with the scarlet biretta. In ecclesiastical heraldry, however, the scarlet galero is still displayed on the cardinal's coat of arms. Cardinals had the right to display the galero in their cathedral, and when a cardinal died, it would be suspended from the ceiling above his tomb. Some cardinals will still have a galero made, even though it is not officially part of their apparel.
To symbolize their bond with the papacy, the pope gives each newly appointed cardinal a gold ring, which is traditionally kissed by Catholics when greeting a cardinal (as with a bishop's episcopal ring). The pope chooses the image on the outside: under Pope Benedict XVI it was a modern depiction of the crucifixion of Jesus, with Mary and John to each side. The ring includes the pope's coat of arms on the inside.
Cardinals have in canon law a "privilege of forum" (i.e., exemption from being judged by ecclesiastical tribunals of ordinary rank): only the pope is competent to judge them in matters subject to ecclesiastical jurisdiction (cases that refer to matters that are spiritual or linked with the spiritual, or with regard to infringement of ecclesiastical laws and whatever contains an element of sin, where culpability must be determined and the appropriate ecclesiastical penalty imposed). This does not exempt them from being judged for alleged violations of civil law. The pope either decides the case himself or delegates the decision to another tribunal, usually one of the tribunals or congregations of the Roman Curia. Absent such delegation, other ecclesiastical courts, even the Roman Rota, are incompetent to judge a case against a cardinal.

</doc>
<doc id="6225" url="http://en.wikipedia.org/wiki?curid=6225" title="Cantigas de Santa Maria">
Cantigas de Santa Maria

The Cantigas de Santa Maria ("Canticles of Holy Mary"; , ) are 420 poems with musical notation, written in the Galician variant of Galician-Portuguese during the reign of Alfonso X "El Sabio" (1221–1284) and often attributed to him.
It is one of the largest collections of monophonic (solo) songs from the Middle Ages and is characterized by the mention of the Virgin Mary in every song, while every tenth song is a hymn.
The "Cantigas" have survived in four manuscript codices: two at El Escorial, one at Madrid's National Library, and one in Florence, Italy. The E codex from El Escorial is illuminated with colored miniatures showing pairs of musicians playing a wide variety of instruments. The "Códice Rico" (T) from El Escorial and the one in the Biblioteca Nazionale Centrale of Florence (F) are richly illuminated with narrative vignettes.
Description.
The Cantigas are written in Galician-Portuguese, fashionable as a lyrical language in Castile at the time. The Cantigas are composed of 420 poems, 356 of which are in a narrative format relating to Marian miracles; the rest of them, except an introduction and two prologues, are of "lore" or involve Marian festivities. The Cantigas depict the Virgin Mary in a very humanized way, often having her play a role in earthly episodes.
The authors are unknown, even if several studies indicate that Galician poet Airas Nunes might well have been the author of a large part of them. King Alfonso X — named as Affonso in the Cantigas — is also believed to be an author of some of them as he refers himself in first person. Support for this theory can be found in the prologue of the Cantigas. Also, many sources credit Alfonso owing to his influence on other works within the poetic tradition, including his introduction on religious song. Although King Alfonso X's authorship is debatable, his influence is not. While the other major works that came out of Alfonso's workshops, including histories and other prose texts, were in Castilian, the Cantigas are in Galician-Portuguese, and reflect the popularity in the Castilian court of other poetic corpuses such as the cantigas d'amigo and cantigas d'amor.
The metrics are extraordinarily diverse: 280 different formats for the 420 Cantigas. The most common are the "virelai" and the "rondeau". The length of the lines varies between two and 24 syllables. The narrative voice in many of the songs describes an erotic relationship, in the troubadour fashion, with the Divine. According to 2000 publishings by scholar Manuel Pedro Ferreira the models for the Cantigas might actually be something different than a traditional French rondeau. He calls the format for some of the Cantigas the "Andalusian rondeau" which has a structure of AB/BB/AB.
The music is written in notation which is similar to that used for chant, but also contains some information about the length of the notes. Several transcriptions exist. The Cantigas are frequently recorded and performed by Early Music groups, and quite a few CDs featuring music from the Cantigas are available.
Codices.
The Cantigas are preserved in four manuscripts: "To" ("códice de Toledo," Biblioteca Nacional de España, MS 10069), "T" (Biblioteca de El Escorial, MS T.I.1), "F" ("códice de Florencia," Florence, Biblioteca Nazionale, MS b.r. 20) and "E" ("códice de los músicos," Biblioteca de El Escorial MS B.I.2). "E" contains the largest number of songs (406 Cantigas, plus the Introduction and the Prologue); it contains 41 carefully detailed miniatures and many illuminated letters. "To" is the earliest collection and contains 129 songs. Although not illustrated, it is richly decorated with pen flourished initials, and great care has been taken over its construction. The "T" and "F" manuscripts are sister volumes. "T" contains 195 surviving cantigas (8 are missing due to loss of folios) which roughly correspond in order to the first two hundred in "E", each song being illustrated with either 6 or 12 miniatures that depict scenes from the cantiga. "F" follows the same format but has only 111 cantigas, of which 7 have no text, only miniatures. These are basically a subset of those found in the second half of "E", but are presented here in a radically different order. "F" was never finished, and so no music was ever added. Only the empty staves display the intention to add musical notation to the codex at a later date. It is generally thought that the codices were constructed during Alfonso's lifetime, "To" perhaps in the 1270s, and "T"/"F" and "E" in the early 1280s up until the time of his death in 1284.
The music.
The musical forms within the Cantigas, and there are many, are still being studied. There have been many false leads, and there is little beyond pitch value that is very reliable. Mensuration is a particular problem in the Cantigas, and most attempts at determining meaningful rhythmic schemes have tended, with some exceptions, to be unsatisfactory. This remains a lively topic of debate and study. Progress, while on-going, has nevertheless been significant over the course of the last 20 years. According to Manuel Pedro Ferreira, a scholar on the Cantigas, the music found in the Cantigas cannot be classified as Arabic, but instead must be classified as "Moorish-Andalusian". This musical influence is unusual in European music of the Middle Ages, and lends special qualities to the collection, although it does not predominate. The more obvious and demonstrable influences that come directly from the Latin and Byzantine ecclesiastical traditions must be taken into account.

</doc>
<doc id="6226" url="http://en.wikipedia.org/wiki?curid=6226" title="Claudio Monteverdi">
Claudio Monteverdi

Claudio Giovanni Antonio Monteverdi (; 15 May 1567 (baptized) – 29 November 1643) was an Italian composer, gambist, singer and Roman Catholic priest.
Monteverdi's work, often regarded as revolutionary, marked the transition from the Renaissance style of music to that of the Baroque period. He developed two individual styles of composition – the heritage of Renaissance polyphony and the new basso continuo technique of the Baroque. Monteverdi wrote one of the earliest operas, "L'Orfeo", an innovative work that is the earliest surviving opera that is still regularly performed. He was recognized as an innovative composer and enjoyed considerable fame in his lifetime.
Life.
Claudio Monteverdi was born in 1567 in Cremona, Lombardy. His father was Baldassare Monteverdi, a doctor, apothecary and amateur surgeon. He was the oldest of five children. During his childhood, he was taught by Marc'Antonio Ingegneri, the "maestro di cappella" at the Cathedral of Cremona. The Maestro’s job was to conduct important worship services in accordance with the liturgy of the Catholic Church. Monteverdi learned about music as a member of the cathedral choir. He also studied at the University of Cremona. His first music was written for publication, including some motets and sacred madrigals, in 1582 and 1583. His first five publications were: "Sacrae cantiunculae", 1582 (a collection of miniature motets); "Madrigali Spirituali", 1583 (a volume of which only the bass partbook is extant); "Canzonette a tre voci", 1584 (a collection of three-voice canzonettes); and the five-part madrigals Book I, 1587, and Book II, 1590. Monteverdi worked for the court of Mantua first as a singer and violist, then as music director. He worked at the court of Vincenzo I of Gonzaga in Mantua as a vocalist and viol player. In 1602, he was working as the court conductor.
In 1599 Monteverdi married the court singer Claudia Cattaneo, who died in September 1607. They had two sons (Francesco and Massimilino) and a daughter (Leonora). Another daughter died shortly after birth.
By 1613, he had moved to San Marco in Venice where, as conductor, he quickly restored the musical standard of both the choir and the instrumentalists. The musical standard had declined due to the financial mismanagement of his predecessor, Giulio Cesare Martinengo. The managers of the basilica were relieved to have such a distinguished musician in charge, as the music had been declining since the death of Giovanni Croce in 1609.
In 1632, he became a priest. During the last years of his life, when he was often ill, he composed his two last masterpieces: "Il ritorno d'Ulisse in patria" ("The Return of Ulysses", 1641), and the historic opera "L'incoronazione di Poppea" ("The Coronation of Poppea", 1642), based on the life of the Roman emperor Nero. "L'incoronazione" especially is considered a culminating point of Monteverdi's work. It contains tragic, romantic, and comic scenes (a new development in opera), a more realistic portrayal of the characters, and warmer melodies than previously heard. It requires a smaller orchestra, and has a less prominent role for the choir. For a long period of time, Monteverdi's operas were merely regarded as a historical or musical interest. Since the 1960s, "The Coronation of Poppea" has re-entered the repertoire of major opera companies worldwide.
Monteverdi died, aged 76, in Venice on 29 November 1643 and was buried at the church of the Frari.
Works.
Monteverdi's works are split into three categories: madrigals, operas, and church-music.
Madrigals.
Until the age of forty, Monteverdi worked primarily on madrigals, composing a total of nine books. It took Monteverdi about four years to finish his first book of twenty-one madrigals for five voices. As a whole, the first eight books of madrigals show the enormous development from Renaissance polyphonic music to the monodic style typical of Baroque music.
The titles of his Madrigal books are:
The Fifth Madrigal Book.
The "Fifth Book of Madrigals" shows the shift from the late Renaissance style of music to the early Baroque. The "Quinto Libro" (Fifth Book), published in 1605, was at the heart of the controversy between Monteverdi and Giovanni Artusi. Artusi attacked the "crudities" and "license" of the modern style of composing, centering his attacks on madrigals (including "Cruda Amarilli", composed around 1600) (See Fabbri, "Monteverdi", p. 60) from the fourth book. Monteverdi made his reply in the introduction to the fifth book, with a proposal of the division of musical practice into two streams, which he called "prima pratica", and "seconda pratica". "Prima pratica" was described as the previous polyphonic ideal of the sixteenth century, with flowing strict counterpoint, prepared dissonance, and equality of voices. "Seconda pratica" used much freer counterpoint with an increasing hierarchy of voices, emphasizing soprano and bass. In "Prima pratica" the harmony controls the words. In "Seconda pratica" the words should be in control of the harmonies. This represented a move towards the new style of monody. The introduction of continuo in many of the madrigals was a further self-consciously modern feature. In addition, the fifth book showed the beginnings of conscious functional tonality.
The Eighth Madrigal Book.
While in Venice, Monteverdi also finished his sixth (1614), seventh (1619), and eighth (1638) books of madrigals. The eighth is the largest, containing works written over a thirty-year period. Originally the work was to be dedicated to Ferdinand II, but because of his ill health, his son was made king in December 1636. When the work was first published in 1638 Monteverdi rededicated it to the new King Ferdinand III. The eighth book includes the so-called "Madrigali dei guerrieri et amorosi" ("Madrigals of War and Love").
The important preface of Monteverdi’s eighth madrigal book seems to be connected with his "seconda pratica." He claims to have invented a new "agitated" style ("Genere concitato", later called "Stile concitato").
The book is divided into sections of War and Love each containing madrigals, a piece in dramatic form ("genere rappresentativo"), and a ballet. In the "Madrigals of War," Monteverdi has organized poetry that describes the pursuits of love through the allegory of war; the hunt for love, and the battle to find love. In the second half of the book, the "Madrigals of Love," Monteverdi organized poetry that describes the unhappiness of being in love, unfaithfulness, and ungrateful lovers who feel no shame. In his previous madrigal collections, Monteverdi usually set poetry from one or two poets he was in contact with through the court where he was employed. The "Madrigals of War and Love" represent an overview of the poets he has dealt with throughout his life; the classical poetry of Petrarch, poetry by his contemporaries (Tasso, Guarini, Marino, Rinuccini, Testi and Strozzi), or anonymous poets who Monteverdi found and adapted to his needs.
Madrigals of War
Madrigals of Love
The Ninth Madrigal Book.
The ninth book of madrigals, published posthumously in 1651, contains lighter pieces such as canzonettas which were probably composed throughout Monteverdi's lifetime representing both styles.
Operas.
Monteverdi was often ill during the last years of his life. During this time, he composed his two last masterpieces: "Il ritorno d'Ulisse in patria" ("The Return of Ulysses", 1640), and the historic opera, "L'incoronazione di Poppea "("The Coronation of Poppea", 1642), based on an episode in the life of the Roman emperor Nero. The libretto for "Il ritorno d'Ulisse" was written by Giacomo Badoarro and for "L'incoronazione di Poppea" by Giovanni Busenello.
L'Orfeo.
Monteverdi composed at least eighteen operas, but only "L'Orfeo", "Il ritorno d'Ulisse in patria", "L'incoronazione di Poppea", and the famous aria, "Lamento", from his second opera "L'Arianna" have survived. From monody (with melodic lines, intelligible text and placid accompanying music), it was a logical step for Monteverdi to begin composing opera. In 1607, his first opera, "L'Orfeo", premiered in Mantua. "L'Orfeo" was not the first opera, but it was the first mature opera, or one that realized all of its potential. It was normal at that time for composers to create works on demand for special occasions, and this piece was part of the ducal celebrations of carnival. (Monteverdi was later to write for the first opera houses supported by ticket sales which opened in Venice). "L'Orfeo" has dramatic power and lively orchestration. "L'Orfeo" is arguably the first example of a composer assigning specific instruments to parts in operas. It is also one of the first large compositions for which the exact instrumentation of the premiere is still known. The plot is described in vivid musical pictures and the melodies are linear and clear. With this opera, Monteverdi created an entirely new style of music, the "dramma per la musica" or musical drama.
L'Arianna.
"L'Arianna" was the second opera written by Monteverdi. It is one of the most influential and famous specimens of early Baroque opera. It was first performed in Mantua in 1608. Its subject matter was the ancient Greek legend of Ariadne and Theseus. Italian composer Ottorino Respighi famously orchestrated the "Lamento di Arianna" in 1908, and the work was premiered by the Berlin Philharmonic the same year under conductor Arthur Nikisch. The manuscript was restored and published as a critical edition in 2013 by Italian composer/conductor Salvatore Di Vittorio under publisher Edizioni Panastudio. A later completion of the "Lamento" from "L'Arianna" by Scottish composer Gareth Wilson (b. 1976) was performed at King's College, London University on 29 November 2013, the 370th anniversary of Monteverdi's death.
Sacred music.
Vespro della Beata Vergine.
Monteverdi's first church music publication was the archaic Mass "In illo tempore" to which the "Vesper Psalms" of 1610 were added. The "Vesper Psalms" of 1610 are also one of the best examples of early repetition and contrast, with many of the parts having a clear "ritornello". The published work is on a very grand scale and there has been some controversy as to whether all the movements were intended to be performed in a single service. However, there are various indications of internal unity. In its scope, it foreshadows such summits of Baroque music as Handel's "Messiah", and J.S. Bach's "St Matthew Passion". Each part (there are twenty-five in total) is fully developed in both a musical and dramatic sense – the instrumental textures are used to precise dramatic and emotional effect, in a way that had not been seen before.
Sacred "contrafacta".
In 1607, Aquilino Coppini published in Milan his ""Musica tolta da i Madrigali di Claudio Monteverde, e d'altri autori ... e fatta spirituale"" for 5 and 6 voices, in which many of Monteverdi's madrigals (especially from the third, fourth and fifth books) are presented with the original secular texts replaced with sacred Latin "contrafacta" carefully prepared by Coppini in order to fit the music in every aspect.
References.
Notes
Cited sources
Other sources

</doc>
<doc id="6229" url="http://en.wikipedia.org/wiki?curid=6229" title="Colossus computer">
Colossus computer

Colossus was the world's first electronic digital computer that was at all programmable. The Colossus computers were developed for British codebreakers during World War II to help in the cryptanalysis of the Lorenz cipher. Without them, the Allies would have been deprived of the very valuable military intelligence that was obtained from reading the vast quantity of encrypted high-level telegraphic messages between the German High Command (OKW) and their army commands throughout occupied Europe. Colossus used thermionic valves (vacuum tubes) to perform Boolean operations and calculations.
Colossus was designed by the engineer Tommy Flowers to solve a problem posed by mathematician Max Newman at the Government Code and Cypher School (GC&CS) at Bletchley Park. Alan Turing's use of probability in cryptanalysis contributed to its design. It has sometimes been erroneously stated that Turing designed Colossus to aid the Cryptanalysis of the Enigma. Turing's machine that helped decode Enigma was the electromechanical Bombe, not Colossus.
The prototype, Colossus Mark 1, was shown to be working in December 1943 and was operational at Bletchley Park by 5 February 1944. An improved Colossus Mark 2 that used shift registers to quintuple the speed, first worked on 1 June 1944, just in time for the Normandy Landings. Ten Colossi were in use by the end of the war.
The destruction of most of the Colossus hardware and blueprints, as part of the effort to maintain a project secrecy that was kept up into the 1970s, deprived most of those involved with Colossus of credit for their pioneering advancements in electronic digital computing during their lifetimes. A functioning replica of a Colossus computer was completed in 2007 and is on display at The National Museum of Computing at Bletchley Park.
Purpose and origins.
The Colossus computers were used to help decrypt radio teleprinter messages that had been encrypted using the electromechanical Lorenz SZ40/42 in-line cipher machine. To encipher a message with the Lorenz machine, the 5-bit plaintext characters were combined with a stream of key ciphertext characters using the XOR Boolean function. This is a Vernam cipher and the deciphering process involved an identically setup Lorenz SZ machine generating the same key sequence and XOR-ing it with the received ciphertext to reproduce the plaintext. The keystream was generated using twelve pinwheels.
British codebreakers referred to encrypted German teleprinter traffic as "Fish" and called the SZ40/42 machine and the intercepted messages "Tunny". Colossus was used for finding possible Lorenz key settings – not completely decrypting the message. It compared two character streams, counting a statistic based on a programmable Boolean function. The ciphertext was read at high speed from a paper tape. The other stream was generated internally, and was an electronic simulation of part of the Lorenz machine. If the count for a setting was above a certain threshold, it would be sent as output to an electric typewriter.
The logical structure of the Lorenz machine was diagnosed at Bletchley Park without a machine being seen – something that did not happen until almost the end of the war. First, John Tiltman, a very talented GC&CS cryptanalyst derived a key stream of almost 4000 characters from a German operating blunder in August 1941. Then Bill Tutte, a newly arrived member of the Research Section used this key stream to work out the logical structure of the Lorenz machine. He correctly deduced that it had twelve wheels in two groups of five, which he named the χ ("chi") and ψ ("psi") wheels, and the remaining two the μ "mu" or "motor" wheels. The "chi" wheels stepped regularly with each letter that was encrypted, while the "psi" wheels stepped irregularly, under the control of the motor wheels.
In order to decrypt the ciphertext of the transmitted messages, there were two tasks that had to be performed. The first was "wheel breaking", which was the discovery of the pin patterns for all the wheels. These patterns were set up once on the Lorenz machine and then used for a fixed period of time and for a number of different messages. The second task was "wheel setting", which could be attempted once the pin patterns were known. Each message encrypted using Lorenz was enciphered at a different start position for the wheels, and it was this start position of the "chi" wheels that Colossus was initially designed to discover.
The XOR function used in the Vernam cipher for both enciphering and deciphering could also be used to upset the cipher's obscuring of the characteristics of the plaintext in the ciphertext. This was discovered by Alan Turing in July 1942 when he was on loan from the German Naval Enigma section to the Research Section at Bletchley Park. He was studying Tunny and invented a method of wheel-breaking that became known as Turingery. With a truly random key, the Vernam cipher removes the natural language property of a plaintext message of having an uneven frequency distribution of the different characters, to produce a uniform distribution in the ciphertext. Turing worked out that if, instead of examining the frequency distribution of the characters in the ciphertext, examining the character-to-character changes of character streams showed a departure from uniformity which provided a way into the system. Providing the character-to-character changes was achieved by "differencing" in which each bit or character was XOR-ed with its successor.
By using differencing and knowing that the "psi" wheels did not advance with each character, Tutte worked out that trying just two differenced bits (impulses) of the "chi"-stream against the differenced ciphertext would produce a statistic that was non-random. This became known as Tutte's "1+2 break in". The process of wheel setting found the start position of the key wheels in relation to the start of the message. Initially Colossus was used only to work out the start positions of the "chi" wheels, but later, methods were devised for the other wheels. Later still an additional electronic unit was designed for wheel breaking, which was added to some Mark 2 Colossi.
The manual processes in decrypting messages were undertaken in a section at Bletchley Park led by Major Ralph Tester which was known as the "Testery". Colossus was developed for the "Newmanry", the section headed by the mathematician Max Newman at Bletchley Park responsible for machine methods against the Lorenz machine. The Colossus design arose out of a prior project that produced a counting machine dubbed "Heath Robinson". The main problems with Heath Robinson were the relative slowness of electro-mechanical parts and the difficulty of synchronising two paper tapes, one punched with the enciphered message, the other representing the patterns produced by the wheels of the Lorenz machine. The tapes tended to stretch when being read, at some 2000 characters per second, resulting in unreliable counts.
Design and construction.
Tommy Flowers was a senior electrical engineer at the Post Office Research Station at Dollis Hill who had been appointed MBE in June 1943. Prior to his work on Colossus, he had been involved with GC&CS at Bletchley Park from February 1941 in an attempt to improve the Bombes that were used in the Cryptanalysis of the German Enigma cipher machine. He was recommended to Max Newman by Alan Turing who had been impressed by his work on the Bombes. The main components of Colossus's predecessor, Heath Robinson were as follows.
Flowers had been brought in to design the Heath Robinson's combining unit. He was not impressed by the system of a key tape that had to be kept synchronised with the message tape and, on his own initiative, he designed an electronic machine which eliminated the need for the key tape by having an electronic analogue of the Lorenz (Tunny) machine. He presented this design to Max Newman in February 1943, but the idea that the one to two thousand thermionic valves (vacuum tubes and thyratrons) proposed, could work together reliably, was greeted with great scepticism, so more Robinsons were ordered from Dollis Hill. Flowers, however, knew from his pre-war work that most thermionic valve failures occurred as a result of the thermal stresses at power up, so not powering a machine down, reduced failure rates very substantially. Flowers persisted with the idea and obtained support from the Director of the Research Station, W Gordon Radley. Flowers and his team of some 50 people in the switching group, spent eleven months from early February 1943 designing and building a machine that dispensed with the second tape of the Heath Robinson, by generating the wheel patterns electronically.
This prototype, Mark 1 Colossus performed satisfactorily at Dollis Hill on 8 December 1943, and was taken apart and shipped to Bletchley Park, where it was delivered on 18 January and re-assembled by Harry Fensom and Don Horwood. It attacked its first message on 5 February 1944. As it was a large structure it was quickly dubbed Colossus by the WRNS operators. This machine contained 1600 thermionic valves (tubes). and was soon followed by an improved production Mark 2 machine. Nine of this version of the machine were constructed, the first being commissioned on 1 June 1944, after which Allen Coombs took over leadership of Colossus production. The original Mark 1 machine was converted into a Mark 2 and an eleventh Colossus was essentially finished when the war in Europe ended.
The main units of Flowers' design were as follows.
Most of the design of the electronics was the work of Tommy Flowers, assisted by William Chandler, with Sidney Broadhurst working on the auxiliary electromechanical parts. The Mark 2 Colossus was designed while Mark 1 was being constructed. It contained 2400 valves and was both 5 times faster and simpler to operate than the original version.
Flowers overcame the problem of synchronizing the electronics with the message tape by generating a clock signal from the reading of the sprocket holes of the message tape. The speed of operation was thus limited by the mechanics of reading the tape. The tape reader was tested up to 9700 characters per second (53 mph) before the tape disintegrated. So 5000 characters/second was settled on as the speed for regular use.
The Mark 2 Colossus included the first ever use of what would now be called shift registers and systolic arrays, enabling five simultaneous tests, each involving up to 100 Boolean operations, on each of the five channels of the punched tape (although in normal operation fewer channels were examined in most runs). This gave an effective processing speed of 25,000 characters per second.
Operation.
Colossus used state-of-the-art vacuum tubes (thermionic valves), thyratrons and photomultipliers to optically read a paper tape and then applied programmable logical functions to the bits of the key and ciphertext characters, counting how often the function returned "true".
Colossus was designed to perform the task of "Wheel Setting", that is determining the start point of the stream of key characters in relation to the characters of the enciphered message on the paper tape loop. Initially it was only the χ ("chi") wheels that were examined. To keep the size of the task manageable, only two bits of the "chi"-stream were examined in the first run, then progressively the other bits. Success at this stage allowed the production of a version of the ciphertext from which the "chi" component of the key had been removed, the so-called "de-"chi"". This transformation allowed manual methods to be used to work out the settings of the ψ ("psi") and μ "mu" "motor" wheels.
Later, Colossus was used for determining the settings of the "psi" wheels. All of this required that "wheel breaking", the discovery of the pin patterns for all the wheels, had been successfully achieved. Later Mark 2 Colossi were equipped with a special unit to achieve this as well. Programming Colossus was by setting switches and plugging appropriate units together. Sometimes, two or more Colossus computers tried different possibilities simultaneously in what now is called parallel computing, speeding the decoding process by perhaps as much as double the rate of comparison.
Influence and fate.
Colossus was the first of the electronic digital machines with programmability, albeit limited by modern standards.
A Colossus computer was thus not a fully general Turing complete machine. However, Professor Benjamin Wells of the Departments of Computer Science and Mathematics, University of San Francisco, has shown that a Universal Turing Machine could have been run on the set of ten Colossus computers. This means that Colossus satisfies the definition of 'Turing Complete' given in the Wikipedia article Turing completeness. Most of the other computing machines of this era were also not Turing complete (e.g. the Atanasoff–Berry Computer, the Bell Labs relay machines (by George Stibitz et al.), or the first designs of Konrad Zuse). The notion of a computer as a general purpose machine — that is, as more than a calculator devoted to solving difficult but specific problems — did not become prominent until after World War II.
Colossus was preceded by several computers, many of them first in some category. Zuse's Z3 was the first functional fully program-controlled computer, and was based on electromechanical relays, as were the (less advanced) Bell Labs machines of the late 1930s (George Stibitz, et al.). The Atanasoff–Berry Computer was electronic and binary (digital) but not programmable. Assorted analog computers were semiprogrammable; some of these much predated the 1930s (e.g., Vannevar Bush). Babbage's Analytical engine design predated all these (in the mid-19th century), it was a decimal, programmable, entirely mechanical construction—but was only partially built and never functioned during Babbage's lifetime (the first complete mechanical Difference engine No. 2, built in 1991, does work however). Colossus was the first combining "digital", (partially) "programmable", and "electronic". The first fully programmable digital electronic computer was the ENIAC which was completed in 1946.
The use to which the Colossus computers were put was of the highest secrecy, and the Colossus itself was highly secret, and remained so for many years after the War. Thus, it could not be included in the history of computing hardware for many years, and Flowers and his associates were deprived of the recognition they were due.
Being not widely known, Colossus had little direct influence on the development of later computers; it was EDVAC that was the early design which had the most influence on subsequent computer architecture. However, the technology of Colossus, and the knowledge that reliable high-speed electronic digital computing devices were feasible, did have a significant influence on the development of some early computers in the United Kingdom and probably in the US. A number of people who were associated with the project and knew all about Colossus played significant roles in early computer work in the UK. In 1972, Herman Goldstine wrote that:
In writing that, Goldstine was unaware of Colossus, and its legacy to those projects of people such as Alan Turing (with the Pilot ACE and ACE), and Max Newman and I. J. Good (with the Manchester Mark 1 and other early Manchester computers). Brian Randell later wrote that:
Colossus documentation and hardware were classified from the moment of their creation and remained so after the War, when Winston Churchill specifically ordered the destruction of most of the Colossus machines into "pieces no bigger than a man's hand"; Tommy Flowers was ordered to destroy all documentation and burnt them in a furnace at Dollis Hill. He later said of that order: Some parts, sanitised as to their original use, were taken to Newman's Royal Society Computing Machine Laboratory at Manchester University. The Colossus Mark 1 was dismantled and parts returned to the Post Office. Two Colossus computers, along with two replica Tunny machines, were retained, moving to GCHQ's new headquarters at Eastcote in April 1946, and moving again with GCHQ to Cheltenham between 1952 and 1954. One of the Colossi, known as "Colossus Blue", was dismantled in 1959; the other in 1960. In their later years, the Colossi were used for training, but before that, there had been attempts to adapt them, with varying success, to other purposes. Jack Good relates how he was the first to use it after the war, persuading NSA that Colossus could be used to perform a function for which they were planning to build a special purpose machine. Colossus was also used to perform character counts on one-time pad tape to test for non-randomness.
Throughout this period the Colossus remained secret, long after any of its technical details were of any importance. This was due to the UK's intelligence agencies use of Enigma-like machines which they promoted and sold to other governments, and then broke the codes using a variety of methods. Had the knowledge of the codebreaking machines been widely known, no one would have accepted these machines; rather, they would have developed their own methods for encryption, methods that the UK services might not have been able to break. The need for such secrecy ebbed away as communications moved to digital transmission and all-digital encryption systems became common in the 1960s.
Information about Colossus began to emerge publicly in the late 1970s, after the secrecy imposed was broken when Group Captain Winterbotham published his book "The Ultra Secret". More recently, a 500-page technical report on the Tunny cipher and its cryptanalysis – entitled "General Report on Tunny" – was released by GCHQ to the national Public Record Office in October 2000; the complete report is available online, and it contains a fascinating paean to Colossus by the cryptographers who worked with it:
Reconstruction.
Construction of a fully functional replica of a Colossus Mark 2 was undertaken by a team led by Tony Sale. In spite of the blueprints and hardware being destroyed, a surprising amount of material survived, mainly in engineers' notebooks, but a considerable amount of it in the U.S. The optical tape reader might have posed the biggest problem, but Dr. Arnold Lynch, its original designer, was able to redesign it to his own original specification. The reconstruction is on display, in the historically correct place for Colossus No. 9, at The National Museum of Computing, in H Block Bletchley Park in Milton Keynes, Buckinghamshire.
In November 2007, to celebrate the project completion and to mark the start of a fundraising initiative for The National Museum of Computing, a Cipher Challenge pitted the rebuilt Colossus against radio amateurs worldwide in being first to receive and decode three messages enciphered using the Lorenz SZ42 and transmitted from radio station DL0HNF in the "Heinz Nixdorf MuseumsForum" computer museum. The challenge was easily won by radio amateur Joachim Schüth, who had carefully prepared for the event and developed his own signal processing and code-breaking code using Ada. The Colossus team were hampered by their wish to use World War II radio equipment, delaying them by a day because of poor reception conditions. Nevertheless, the victor's 1.4 GHz laptop, running his own code, took less than a minute to find the settings for all 12 wheels. The German codebreaker said: "My laptop digested ciphertext at a speed of 1.2 million characters per second—240 times faster than Colossus. If you scale the CPU frequency by that factor, you get an equivalent clock of 5.8 MHz for Colossus. That is a remarkable speed for a computer built in 1944."
The Cipher Challenge verified the successful completion of the rebuild project. "On the strength of today's performance Colossus is as good as it was six decades ago", commented Tony Sale. "We are delighted to have produced a fitting tribute to the people who worked at Bletchley Park and whose brainpower devised these fantastic machines which broke these ciphers and shortened the war by many months."
Other meanings.
There was a fictional computer named "Colossus" in the movie "". Also see List of fictional computers. Neal Stephenson's novel "Cryptonomicon" (1999) also contains a fictional treatment of the historical role played by Turing and Bletchley Park.

</doc>
<doc id="6230" url="http://en.wikipedia.org/wiki?curid=6230" title="Canadian Shield">
Canadian Shield

The Canadian Shield, also called the Laurentian Plateau, or Bouclier Canadien (French), is a large area of exposed Precambrian igneous and high-grade metamorphic rocks (geological shield) that forms the ancient geological core of the North American continent (North American or Laurentia craton), covered by a thin layer of soil. It is an area mostly composed of igneous rock which relates to its long volcanic history. It has a deep, common, joined bedrock region in Eastern and central Canada and stretches North from the Great Lakes to the Arctic Ocean, covering over half of Canada; it also extends South into the Northern reaches of the United States. Human population is sparse, and industrial development is minimal, while mining is very prevalent.
Geographical extent.
The Canadian Shield is a physiographic division, consisting of five smaller physiographic provinces, the Laurentian Upland, Kazan Region, Davis, Hudson, and James. The shield extends into the United States as the Adirondack Mountains (connected by the Frontenac Axis) and the Superior Upland. The Canadian Shield is U-shaped, but almost semi-circular, which yields an appearance of a warrior's shield, and is a subsection of the Laurentia craton signifying the area of greatest glacial impact (scraping down to bare rock) creating the thin soils. The Canadian Shield is more than 3.96 billion years old. The Canadian Shield once had jagged peaks, higher than any of today's mountains, but millions of years of erosion have changed these mountains to rolling hills.
The Canadian Shield is a collage of Archean plates and accreted juvenile arc terranes and sedimentary basins of the Proterozoic Eon that were progressively amalgamated during the interval 2.45 to 1.24 Ga, with the most substantial growth period occurring during the Trans-Hudson orogeny, between ca. 1.90 to 1.80 Ga. The Canadian Shield was the first part of North America to be permanently elevated above sea level and has remained almost wholly untouched by successive encroachments of the sea upon the continent. It is the Earth's greatest area of exposed Archean rock. The metamorphic base rocks are mostly from the Precambrian Era (between 4.5 billion and 540 million years ago), and have been repeatedly uplifted and eroded. Today it consists largely of an area of low relief above sea level with a few monadnocks and low mountain ranges (including the Torngat and Laurentian Mountains) probably eroded from the plateau during the Cenozoic Era. During the Pleistocene Epoch, continental ice sheets depressed the land surface (see Hudson Bay), scooped out thousands of lake basins, and carried away much of the region's soil.
When the Greenland section is included, the Shield is approximately circular, bounded on the northeast by the northeast edge of Greenland, with Hudson Bay in the middle. It covers much of Greenland, Labrador, most of Quebec north of the St. Lawrence River, much of Ontario including northern sections of the southern peninsula between the Great Lakes, the Adirondack Mountains of northern New York, the northernmost part of Lower Michigan and all of Upper Michigan, northern Wisconsin, northeastern Minnesota, the central/northern portions of Manitoba away from Hudson Bay, northern Saskatchewan, a small portion of northeastern Alberta, and the mainland northern Canadian territories to the east of a line extended north from the Saskatchewan/Alberta border (Northwest Territories and Nunavut). In total, it covers approximately . It covers even more area and stretches to the Western Cordillera in the west and Appalachians in the east, but the formations are still underground. The underlying rock structure does include Hudson Bay and the submerged area between North America and Greenland.
Geology.
The multitude of rivers and lakes in the entire region is caused by the watersheds of the area being so young and in a state of sorting themselves out with the added effect of post-glacial rebound. The Shield was originally an area of very large mountains (about ) with much volcanic activity, but over hundreds of million of years, the area has been eroded to its current topographic appearance of relatively low relief. It has some of the oldest (extinct) volcanoes on the planet. It has over 150 volcanic belts (now deformed and eroded down to nearly flat plains) whose bedrock ranges from 600 to 1200 million years old.
Each belt probably grew by the coalescence of accumulations erupted from numerous vents, making the tally of volcanoes reach the hundreds. Many of Canada's major ore deposits are associated with Precambrian volcanoes.
The Sturgeon Lake Caldera in Kenora District, Ontario, is one of the world's best preserved mineralized Neoarchean caldera complexes, which is some 2.7 billion years old. The Canadian Shield also contains the Mackenzie dike swarm, which is the largest dike swarm known on Earth.
Mountains have deep roots and float on the denser mantle much like an iceberg at sea. As mountains erode, their roots rise and are eroded in turn. The rocks that now form the surface of the Shield were once far below the Earth's surface.
The high pressures and temperatures at those depths provided ideal conditions for mineralization. Although these mountains are now heavily eroded, many large mountains still exist in Canada's far north called the Arctic Cordillera. This is a vast deeply dissected mountain range, stretching from northernmost Ellesmere Island to the northernmost tip of Labrador. The range's highest peak is Nunavut's Barbeau Peak at above sea level. Precambrian rock is the major component of the bedrock.
The North American craton is the bedrock forming the heart of the North American continent and the Canadian Shield is the largest exposed part of the craton's bedrock.
The Canadian Shield is part of an ancient continent called Arctica, which was formed about 2.5 billion years ago during the Neoarchean era. It was split into Greenland, Laurentia, Scotland, Siberia, East Antarctica and is now roughly situated in the Arctic around the current North Pole.
Ecology.
The current surface expression of the Shield is one of very thin soil lying on top of the bedrock, with many bare outcrops. This arrangement was caused by severe glaciation during the ice age, which covered the Shield and scraped the rock clean.
The lowlands of the Canadian Shield have a very dense soil that is not suitable for forestation, but it also contains many marshes and bogs. The rest of the region has coarse soil that does not retain moisture well and is frozen as permafrost year round. Forests are not as dense in the north.
The Shield is covered in parts by vast boreal forests in the south that support natural ecosystems as well as a major logging industry. This boreal forest area includes ecoregions such as the Eastern Canadian Shield taiga that covers northern Quebec and most of Labrador, and the Midwestern Canadian Shield forests that run westwards from Northwestern Ontario. Hydrographical drainage is generally poor, the effects of glaciation being one of the many reasons. Tundra typically prevails in the northern regions. Many mammals such as caribou, white-tailed deer, moose, wolves, wolverines, weasels, mink, otters, grizzly bear, polar bears and black bears are present. In the case of polar bears ("Ursus maritimus") the Shield area contains many of the denning locations such as the Wapusk National Park.
Mining and economics.
The Shield is one of the world's richest areas in terms of mineral ores. It is filled with substantial deposits of nickel, gold, silver, and copper. Throughout the Shield there are many mining towns extracting these minerals. The largest, and one of the best known, is Sudbury, Ontario. Sudbury is an exception to the normal process of forming minerals in the Shield since there is significant evidence that the Sudbury Basin is an ancient meteorite impact crater. Ejecta from the meteorite impact was found in the Rove Formation in May 2007. The nearby, but less known Temagami Magnetic Anomaly, has striking similarities to the Sudbury Basin. This suggests it could be a second metal-rich impact crater.
In northeastern Quebec, the giant Manicouagan Reservoir is the site of an extensive hydroelectric project (Manic-cinq, or Manic-5). This is one of the largest-known meteorite impact craters on Earth.
The Flin Flon greenstone belt in central Manitoba and east-central Saskatchewan is one of the largest Paleoproterozoic volcanic-hosted massive sulfide (VMS) districts in the world, containing 27 copper-zinc-(gold) deposits from which more than 183 million tons of sulfide have been mined.
The Shield, particularly the portion in the Northwest Territories, has recently been the site of several major diamond discoveries. The kimberlite pipes in which the diamonds are found are closely associated with cratons, which provide the deep lithospheric mantle required to stabilize diamond as a mineral. The kimberlite eruptions then bring the diamonds from over depth to the surface. Currently the Ekati and Diavik mines are actively mining kimberlite diamonds.

</doc>
<doc id="6231" url="http://en.wikipedia.org/wiki?curid=6231" title="Comic book">
Comic book

A comic book or comicbook, also called comic magazine or simply comic, is a publication, first popularized in the United States. It consists of comics art in the form of sequential juxtaposed panels that represent individual scenes. Panels are often accompanied by brief descriptive prose and written narrative, usually dialog contained in word balloons emblematic of the comics art form. The first comic book appeared in the United States in 1933 and was a reprinting of earlier newspaper comic strips which had established many of the story-telling devices used in comics. The term "comic book" arose because the first book sold as a comic book reprinted humor comic strips. Despite their name, comic books are not necessarily humorous in tone, and feature stories in all genres. 
American comic books.
Since the introduction of the comic book format in 1933 with the publication of "Famous Funnies", the United States has produced the most titles, along with British comics and Japanese manga, in terms of quantity of titles.
Cultural historians divide the career of the comic book in the U.S. into several ages or historical eras:
Comic book historians continue to debate the exact boundaries of these eras, but they have come to an agreement, the terms for which originated in the fan press. Comics as a print medium have existed in America since the printing of "The Adventures of Obadiah Oldbuck" in 1842 in hardcover, making it the first known American prototype comic book. The introduction of Jerry Siegel and Joe Shuster's Superman in 1938 turned comic books into a major industry, and is the start of the Golden Age of comics. Historians have proposed several names for the Age before Superman, most commonly dubbing it the Platinum Age.
During that time, the G. W. Dillingham Company published the first known proto-comic-book magazine in the U.S., "The Yellow Kid in McFadden's Flats", in 1897. It reprinted material – primarily the October 18, 1896 to January 10, 1897 sequence titled "McFadden's Row of Flats" – from cartoonist Richard F. Outcault's newspaper comic strip "Hogan's Alley", starring the Yellow Kid, the lead character. The 196-page, square-bound, black-and-white publication, which also includes introductory text by E. W. Townsend, measured 5×7 inches and sold for 50 cents. The neologism "comic book" appears on the back cover. Despite the publication of a series of related Hearst comics soon afterward, the first monthly comic book, Embee Distributing Company's "Comic Monthly", did not appear until 1922. Produced in an 8½-by-9-inch format, it reprinted black-and-white newspaper comic strips and lasted a year.
It was not until the Golden Age that the archetype of the superhero would originate.
The Silver Age of comic books is generally considered to date from the first successful revival of the dormant superhero form—the debut of Robert Kanigher and Carmine Infantino's Flash in "Showcase" #4 (Oct. 1956). The Silver Age lasted through the late 1960s or early 1970s, during which time Marvel Comics revolutionized the medium with such naturalistic superheroes as Stan Lee and Jack Kirby's Fantastic Four and Stan Lee and Steve Ditko's Spider-Man.
The precise beginning and end of the Bronze Age remain less well-defined. Suggested starting points for the Bronze Age of comics include Roy Thomas and Barry Windsor-Smith's "Conan" #1 (October 1970), Denny O'Neil and Neal Adams' "Green Lantern/Green Arrow" #76 (April 1970), or Stan Lee and Gil Kane's "The Amazing Spider-Man" #96 (May 1971; the non-Comics Code issue). The Bronze Age ends with the publications of DC's "Crisis on Infinite Earths" (1984-1985), "", and "Watchmen" (1986-1987).
A notable event in the history of the American comic book came with the psychiatrist Fredric Wertham's criticisms of the medium in his book "Seduction of the Innocent" (1954), which prompted the American Senate Subcommittee on Juvenile Delinquency to investigate comic books. In response to attention from the government and from the media, the U.S. comic book industry set up the Comics Code Authority in 1954 and drafted the "Comics Code" in the same year.
Underground comic books.
In the early 1970s a surge of creativity emerged in what became known as underground comics. Published and distributed independently of the established comics industry, most of such comics reflected the youth counterculture and drug culture of the time. Many had an uninhibited, often irreverent style; their frank depictions of nudity, sex, profanity, and politics had no parallel outside their precursors, the pornographic and even more obscure "Tijuana bibles". Underground comics were almost never sold at news stands, but rather in such youth-oriented outlets as head shops and record stores, as well as by mail order.
Frank Stack's "The Adventures of Jesus", published under the name Foolbert Sturgeon, has been credited as the first underground comic.
Alternative comics.
The rise of comic book specialty stores in the late 1970s created/paralleled a dedicated market for "independent" or "alternative comics" in the U.S. The first such comics included the anthology series "Star Reach", published by comic-book writer Mike Friedrich from 1974 to 1979, and Harvey Pekar's "American Splendor", which continued sporadic publication into the 21st century and which Shari Springer Berman and Robert Pulcini adapted into a 2003 film. Some independent comics continued in the tradition of underground comics, though their content generally remained less explicit; others resembled the output of mainstream publishers in format and genre but were published by smaller artist-owned companies or by single artists. A few (notably "RAW") represented experimental attempts to bring comics closer to the status of fine art.
During the 1970s the "small press" culture grew and diversified. By the 1980s, several independent publishers - such as Pacific, Eclipse, First, Comico, and Fantagraphics - had started releasing a wide range of styles and formats—from color-superhero, detective, and science-fiction comic books to black-and-white magazine-format stories of Latin American magical realism.
A number of small publishers in the 1990s changed the format and distribution of their comics to more closely resemble non-comics publishing. The "minicomics" form, an extremely informal version of self-publishing, arose in the 1980s and became increasingly popular among artists in the 1990s, despite reaching an even more limited audience than the small press.
Small publishers regularly releasing titles include Avatar Comics, Hyperwerks, Raytoons, and Terminal Press, buoyed by such advances in printing technology as digital print-on-demand.
Graphic novels.
In 1964, Richard Kyle coined the term "graphic novel" to distinguish newly translated European works from genre-driven subject matter common in American comics. Precursors of the form existed by the 1920s, which saw a revival of the medieval woodcut tradition by Belgian Frans Masereel, American Lynd Ward and others. 
In 1950, St. John Publications produced the digest-sized, adult-oriented "picture novel" "It Rhymes with Lust", a 128-page digest by pseudonymous writer "Drake Waller" (Arnold Drake and Leslie Waller), penciler Matt Baker and inker Ray Osrin, touted as "an original full-length novel" on its cover. In 1971, writer-artist Gil Kane and collaborators devised the paperback "comics novel" "Blackmark". Will Eisner popularized the term "graphic novel" when he used it on the cover of the paperback edition of his work "A Contract with God, and Other Tenement Stories" in 1978.
Comic book collecting.
Comic book collectors are often lifelong enthusiasts of the comic book stories and they usually focus on particular heroes and attempt to assemble the entire run of a title. Comics are published with a sequential number. The very first issue of the Marvel magazine 'The Amazing Spider-Man' was number 1 and that was followed by number 2 until the end of the run which ran to the hundreds. Number 1 is commonly the rarest and most desirable to collectors. 
However, the first appearance of a character might be in an existing title. For example, Spider-Man's first appearance was in Amazing Fantasy number 15. New characters were often introduced this way, and did not receive their own titles until there was a proven audience for the hero. As a result, comics that feature the first appearance of an important character will sometimes be even harder to find than the number 1 issue of a character's own title.
Some rare comic books include copies of the unreleased "Motion Picture Funnies Weekly" #1 from 1939. Eight copies, plus one without a cover, emerged in the estate of the deceased publisher in 1974. The "Pay Copy" of this book sold for $43,125 in a 2005 Heritage auction.
The most valuable American comics have combined rarity and quality with the first appearances of popular and enduring characters. Four comic books have sold for over $1 million USD as of December 2010, including two examples of Action Comics #1, the first appearance of Superman, both sold privately through online dealer ComicConnect.com in 2010, and Detective Comics #27, the first appearance of Batman, via public auction.
Updating the above price obtained for Action Comics #1, the first appearance of Superman, the highest sale on record for this book is $2.16 million, for a 9.0 copy. 
Misprints, promotional comic-dealer incentive printings, and issues with extremely low distribution also generally have scarcity value. The rarest modern comic books include the original press run of "The League of Extraordinary Gentlemen" #5, which DC executive Paul Levitz recalled and pulped due to the appearance of a vintage Victorian era advertisement for "Marvel Douche", which the publisher considered offensive; only 100 copies exist, most of which have been CGC graded. (See Recalled comics for more pulped, recalled, and erroneous comics.)
In 2000, a company named CGC began to "slab" comics, encasing them in a thick plastic and giving them a numeric grade. As of 2014, there are two companies that provide third party grading of comic book condition. Because condition is so important to the value of rare comics, the idea of grading by a company that does not buy or sell comics seems like a good one. However, there is some controversy about whether this grading service is worth the high cost, and whether it is a positive development for collectors, or if it primarily services speculators who wish to make a quick profit trading in comics as one might trade in stocks or fine art.
The original artwork pages from comic books are also collected, and these are perhaps the rarest of all comic book collector's items, as there is only one unique page of artwork for each page that was printed and published. These were created by a writer who created the story, a pencil artist, who laid out the sequential panels on the page, an ink artist, who went over the pencil with pen and black ink, a letterer, who provided the dialogue and narration of the story by hand lettering each word and finally a colorist, who added color as the last step before the finished pages went to the printer.
When the original pages of artwork are returned by the printer, they are typically given back to the artists, who sometimes sell them at comic book conventions, or in galleries and art shows related to comic book art. The original pages of the first appearances of such legendary characters as Superman, Batman, Wonder Woman and Spider-man are priceless national treasures, and they belong in a museum such as the Smithsonian.
European comics.
Franco-Belgian comics.
France and Belgium have a long tradition in comics and comic books, called "BDs" (an abbreviation of "bande dessinées") in French and "strips" in Dutch. Belgian comic books originally written in Dutch show the influence of the Francophone "Franco-Belgian" comics, but have their own distinct style.
The name "la bande dessinée" derives from the original description of the art form as drawn strips (the phrase literally translates as "the drawn strip"), analogous to the sequence of images in a film strip. As in its English equivalent, the word "bande" can be applied to both film and comics. Significantly, the French-language term contains no indication of subject-matter, unlike the American terms "comics" and "funnies", which imply an art form not to be taken seriously. The distinction of comics as "le neuvième art" (literally, "the ninth art") is prevalent in French scholarship on the form, as is the concept of comics criticism and scholarship itself. Relative to the respective size of their populations, the innumerable authors in France and Belgium publish a high volume of comic books. In North America, the more serious Franco-Belgian comics are often seen as equivalent to graphic novels, but whether they are long or short, bound or in magazine format, in Europe there is no need for a more sophisticated term, as the art's name does not itself imply something frivolous.
In France, authors control the publication of most comics. The author works within a self-appointed time-frame, and it is common for readers to wait six months or as long as two years between installments. Most books first appear in print as a hardcover book, typically with 48, 56, or 64 pages.
British comics.
Originally the same size as a usual comic book in the U.S. (although lacking the glossy cover), the British comic has adopted a magazine size, with "The Beano" and "The Dandy" the last to adopt this size (in the 1980s). Although the British generally speak of "a comic" or of "a comic magazine", and they also historically spoke of "a comic paper". Some comics, such as "Judge Dredd" and other "2000 AD" titles, have been published in a tabloid form.
Although "Ally Sloper's Half Holiday" (1884), the first comic published in Britain, aimed at an adult market, publishers quickly targeted a younger demographic, which has led to most publications being for children and has created an association in the public's mind of comics as somewhat juvenile.
Popular titles within the UK have included "The Beano", "The Dandy", "The Eagle", "2000 AD", and "Viz". Underground comics and "small press" titles have also appeared in the UK, notably "Oz" and "Escape Magazine".
The content of "Action", another title aimed at children and launched in the mid-1970s, became the subject of discussion in the House of Commons. Although on a smaller scale than similar investigations in the U.S., such concerns led to a moderation of content published within British comics. Such moderation never became formalized to the extent of promulgating a code, nor did it last long.
The UK has also established a healthy market in the reprinting and repackaging of material, notably material originating in the U.S. The lack of reliable supplies of American comic books led to a variety of black-and-white reprints, including Marvel's monster comics of the 1950s, Fawcett's Captain Marvel, and other characters such as Sheena, Mandrake the Magician, and the Phantom. Several reprint companies became involved in repackaging American material for the British market, notably the importer and distributor Thorpe & Porter.
Marvel Comics established a UK office in 1972. DC Comics and Dark Horse Comics also opened offices in the 1990s. The repackaging of European material has occurred less frequently, although "The Adventures of Tintin" and "Asterix" serials have been successfully translated and repackaged in softcover books.
At Christmas time, publishers repackage and commission material for comic annuals, printed and bound as hardcover A4-size books; "Rupert" supplies a famous example of the British comic annual. DC Thomson also repackages "The Broons" and "Oor Wullie" strips in softcover A4-size books for the holiday season.
On 19 March 2012, the British postal service, the Royal Mail, released a set of stamps depicting British comic-book characters and series. The collection featured The Beano, The Dandy, Eagle, The Topper, Roy of the Rovers, Bunty, Buster, Valiant, Twinkle and 2000 AD.
Italian comics.
In Italy, comics (known in Italian as "fumetti") made their debut as humor strips at the end of the 19th century, and later evolved into adventure stories. After World War II, however, artists like Hugo Pratt and Guido Crepax exposed Italian comics to an international audience. Popular comic books such as "Diabolik" or the "Bonelli" line—namely "Tex Willer" or "Dylan Dog"—remain best-sellers.
Mainstream comics are usually published on a monthly basis, in a black-and-white digest size format, with approximately 100 to 132 pages. Collections of classic material for the most famous characters, usually with more than 200 pages, are also common. Author comics are published in the French BD format, with an example being Pratt's "Corto Maltese".
Italian cartoonists show the influence of comics from other countries, including France, Belgium, Spain, and Argentina. Italy is also famous for being one of the foremost producers of Walt Disney comic stories outside the U.S. Donald Duck's superhero alter ego, Paperinik, known in English as Superduck, was created in Italy.
Czech comics.
"Čtyřlístek" (in English translated as Lucky Four or Four-Leaf Clover) is one of the most well-known comics for children published Czech Republic.
Japanese comics (manga).
The first comic books in Japan appeared during the 18th century in the form of woodblock-printed booklets containing short stories drawn from folk tales, legends, and historical accounts, told in a simple visual-verbal idiom. Known as , , and , these were written primarily for less literate readers. However, with the publication in 1775 of Koikawa Harumachi's comic book , an evolved form of comic book originated, which required greater literacy and cultural sophistication. This was known as the . Published in thousands of copies, the "kibyōshi" may have been the earliest fully realized comic book for adults in world literary history. Approximately 2,000 titles remain extant.
Modern comic books in Japan developed from a mixture of these earlier comic books and of woodblock prints with Western styles of drawing. They took their current form shortly after World War II. They are usually published in black-and-white, except for the covers, which are usually printed in four colors, although occasionally, the first few pages may also be printed in full color. The term "manga" means "random (or whimsical) pictures", and first came into common usage in the late 18th century with the publication of such works as Santō Kyōden's picturebook (1798) and Aikawa Minwa's "Comic Sketches of a Hundred Women" (1798). During the Meiji period, the term "Akahon" was also common.
Western artists were brought over to teach their students such concepts as line, form, and color; things which had not been regarded as conceptually important in "ukiyo-e", as the idea behind the picture was of paramount importance. Manga at this time was referred to as "Ponchi-e" (Punch-picture) and, like its British counterpart "Punch" magazine, mainly depicted humor and political satire in short one- or four-picture format.
Dr. Osamu Tezuka (1928–1989) further developed this form. Seeing an animated war propaganda film titled inspired Tezuka to become a comic artist. He introduced episodic storytelling and character development in comic format, in which each story is part of larger story arc. The only text in Tezuka's comics was the characters' dialogue and this further lent his comics a cinematic quality. Inspired by the work of Walt Disney, Tezuka also adopted a style of drawing facial features in which a character's eyes, nose, and mouth are drawn in an extremely exaggerated manner. This style created immediately recognizable expressions using very few lines, and the simplicity of this style allowed Tezuka to be prolific. Tezuka's work generated new interest in the "ukiyo-e" tradition, in which the image is a representation of an idea, rather than a depiction of reality.
Though a close equivalent to the American comic book, manga has historically held a more important place in Japanese culture than comics have in American culture. Japanese society shows a wide respect for manga, both as an art form and as a form of popular literature. Many manga become television shows or short films. As with its American counterpart, some manga has been criticized for its sexuality and violence, although in the absence of official or even industry restrictions on content, artists have freely created manga for every age group and for every topic.
Manga magazines—also known as "anthologies"—often run several series concurrently, with approximately 20 to 40 pages allocated to each series per issue. These magazines range from 200 to more than 850 pages each. Manga magazines also contain one-shot comics and a variety of four-panel yonkoma (equivalent to comic strips). Manga series may continue for many years if they are successful, with stories often collected and reprinted in book-sized volumes called , the equivalent of the American trade paperbacks. These volumes use higher-quality paper and are useful to readers who want to be brought up to date with a series, or to readers who find the cost of the weekly or monthly publications to be prohibitive. Deluxe versions are printed as commemorative or collectible editions. Conversely, old manga titles are also reprinted using lower-quality paper and sold for 120 ¥ (approximately $1 USD) each.
Doujinshi.
, fan-made Japanese comics operate in a far larger market in Japan than the American "underground comics" market; the largest doujinshi fair, Comiket, attracts 500,000 visitors twice a year.
Distribution.
Distribution has historically been a problem for the comic book industry with many mainstream retailers declining to carry extensive stocks of the most interesting and popular comics. The smart phone and the tablet computer, however, have turned out to be an ideal medium for online distribution.
Digital distribution.
Marvel Comics launched Marvel Digital Comics Unlimited, a subscription service allowing readers to read many comics from Marvel's history online, on November 13, 2007. The service also includes periodic release new comics not available elsewhere. With the release of "Avenging Spider-Man" Marvel also became the first publisher to provide free digital copies as part of the print copy of the comic book.
With the growing popularity of smart phones and tablet computers, particularly Apple's iPhone and iPad, many major publishers have begun releasing titles in digital form. Some of the most popular platforms for such release are Graphicly and comiXology.

</doc>
<doc id="6233" url="http://en.wikipedia.org/wiki?curid=6233" title="Connected space">
Connected space

In topology and related branches of mathematics, a connected space is a topological space that cannot be represented as the union of two or more disjoint nonempty open subsets. Connectedness is one of the principal topological properties that is used to distinguish topological spaces. A stronger notion is that of a path-connected space, which is a space where any two points can be joined by a path.
A subset of a topological space "X" is a connected set if it is a connected space when viewed as a subspace of "X".
An example of a space that is not connected is a plane with an infinite line deleted from it. Other examples of disconnected spaces (that is, spaces which are not connected) include the plane with a closed annulus removed, as well as the union of two disjoint open disks in two-dimensional Euclidean space.
Formal definition.
A topological space "X" is said to be disconnected if it is the union of two disjoint nonempty open sets. Otherwise, "X" is said to be connected. A subset of a topological space is said to be connected if it is connected under its subspace topology. Some authors exclude the empty set (with its unique topology) as a connected space, but this article does not follow that practice.
For a topological space "X" the following conditions are equivalent:
Connected components.
The maximal connected subsets (ordered by inclusion) of a nonempty topological space are called the connected components of the space.
The components of any topological space "X" form a partition of "X": they are disjoint, nonempty, and their union is the whole space.
Every component is a closed subset of the original space. It follows that, in the case where their number is finite, each component is also an open subset. However, if their number is infinite, this might not be the case; for instance, the connected components of the set of the rational numbers are the one-point sets, which are not open.
Let formula_1 be the connected component of "x" in a topological space "X", and formula_2 be the intersection of all open-closed sets containing "x" (called quasi-component of "x".) Then formula_3 where the equality holds if "X" is compact Hausdorff or locally connected.
Disconnected spaces.
A space in which all components are one-point sets is called totally disconnected. Related to this property, a space "X" is called totally separated if, for any two distinct elements "x" and "y" of "X", there exist disjoint open neighborhoods "U" of "x" and "V" of "y" such that "X" is the union of "U" and "V". Clearly any totally separated space is totally disconnected, but the converse does not hold. For example take two copies of the rational numbers Q, and identify them at every point except zero. The resulting space, with the quotient topology, is totally disconnected. However, by considering the two copies of zero, one sees that the space is not totally separated. In fact, it is not even Hausdorff, and the condition of being totally separated is strictly stronger than the condition of being Hausdorff.
Path connectedness.
A path from a point "x" to a point "y" in a topological space "X" is a continuous function "f" from the unit interval [0,1] to "X" with "f"(0) = "x" and "f"(1) = "y". A path-component of "X" is an equivalence class of "X" under the equivalence relation which makes "x" equivalent to "y" if there is a path from "x" to "y". The space "X" is said to be path-connected (or pathwise connected or 0-connected) if there is at most one path-component, i.e. if there is a path joining any two points in "X". Again, many authors exclude the empty space.
Every path-connected space is connected. The converse is not always true: examples of connected spaces that are not path-connected include the extended long line "L"* and the "topologist's sine curve".
However, subsets of the real line R are connected if and only if they are path-connected; these subsets are the intervals of R.
Also, open subsets of R"n" or C"n" are connected if and only if they are path-connected.
Additionally, connectedness and path-connectedness are the same for finite topological spaces.
Arc connectedness.
A space "X" is said to be arc-connected or arcwise connected if any two distinct points can be joined by an "arc", that is a path "f" which is a homeomorphism between the unit interval and its image "f"([0, 1). It can be shown any Hausdorff space which is path-connected is also arc-connected. An example of a space which is path-connected but not arc-connected is provided by adding a second copy 0' of 0 to the nonnegative real numbers [0, ∞). One endows this set with a partial order by specifying that 0'<"a" for any positive number "a", but leaving 0 and 0' incomparable. One then endows this set with the "order topology", that is one takes the open intervals
("a", "b") = {"x" | "a" < "x" < "b"} and the half-open intervals [0, "a") = {"x" | 0 ≤ x < "a"}, [0', "a") = {"x" | 0' ≤ "x" < "a"} as a base for the topology. The resulting space is a T1 space but not a Hausdorff space. Clearly 0 and 0' can be connected by a path but not by an arc in this space.
Local connectedness.
A topological space is said to be locally connected at a point "x" if every neighbourhood of "x" contains a connected open neighbourhood. It is locally connected if it has a base of connected sets. It can be shown that a space "X" is locally connected if and only if every component of every open set of "X" is open. The topologist's sine curve is an example of a connected space that is not locally connected.
Similarly, a topological space is said to be if it has a base of path-connected sets.
An open subset of a locally path-connected space is connected if and only if it is path-connected.
This generalizes the earlier statement about R"n" and C"n", each of which is locally path-connected. More generally, any topological manifold is locally path-connected.
Graphs.
Graphs have path connected subsets, namely those subsets for which every pair of points has a path of edges joining them.
But it is not always possible to find a topology on the set of points which induces the same connected sets. The 5-cycle graph (and any "n"-cycle with "n">3 odd) is one such example.
As a consequence, a notion of connectedness can be formulated independently of the topology on a space. To wit, there is a category of connective spaces consisting of sets with collections of connected subsets satisfying connectivity axioms; their morphisms are those functions which map connected sets to connected sets . Topological spaces and graphs are special cases of connective spaces; indeed, the finite connective spaces are precisely the finite graphs.
However, every graph can be canonically made into a topological space, by treating vertices as points and edges as copies of the unit interval (see topological graph theory#Graphs as topological spaces). Then one can show that the graph is connected (in the graph theoretical sense) if and only if it is connected as a topological space.
Stronger forms of connectedness.
There are stronger forms of connectedness for topological spaces, for instance: 
In general, note that any path connected space must be connected but there exist connected spaces that are not path connected. The deleted comb space furnishes such an example, as does the above mentioned topologist's sine curve.

</doc>
<doc id="6235" url="http://en.wikipedia.org/wiki?curid=6235" title="Cell nucleus">
Cell nucleus

In cell biology, the nucleus (pl. "nuclei"; from Latin or , meaning kernel) is a membrane-enclosed organelle found in eukaryotic cells. It contains most of the cell's genetic material, organized as multiple long linear DNA molecules in complex with a large variety of proteins, such as histones, to form chromosomes. The genes within these chromosomes are the cell's nuclear genome. The function of the nucleus is to maintain the integrity of these genes and to control the activities of the cell by regulating gene expression — the nucleus is, therefore, the control center of the cell. The main structures making up the nucleus are the nuclear envelope, a double membrane that encloses the entire organelle and isolates its contents from the cellular cytoplasm, and the nucleoskeleton (which includes nuclear lamina), a network within the nucleus that adds mechanical support, much like the cytoskeleton, which supports the cell as a whole.
Because the nuclear membrane is impermeable to large molecules, nuclear pores are required that regulate nuclear transport of molecules across the envelope. The pores cross both nuclear membranes, providing a channel through which larger molecules must be actively transported by carrier proteins while allowing free movement of small molecules and ions. Movement of large molecules such as proteins and RNA through the pores is required for both gene expression and the maintenance of chromosomes. The interior of the nucleus does not contain any membrane-bound sub compartments, its contents are not uniform, and a number of "sub-nuclear bodies" exist, made up of unique proteins, RNA molecules, and particular parts of the chromosomes. The best-known of these is the nucleolus, which is mainly involved in the assembly of ribosomes. After being produced in the nucleolus, ribosomes are exported to the cytoplasm where they translate mRNA.
History.
[[Image:Flemming1882Tafel1Fig14.jpg|thumb|Drawing of a "Chironomus" salivary gland cell published by Walther Flemming in 1882. The nucleus contains Polytene chromosomes.
The nucleus was the first organelle to be discovered. What is most likely the oldest preserved drawing dates back to the early microscopist Antonie van Leeuwenhoek (1632 – 1723). He observed a "Lumen", the nucleus, in the red blood cells of salmon. Unlike mammalian red blood cells, those of other vertebrates still possess nuclei.
The nucleus was also described by Franz Bauer in 1804 and in more detail in 1831 by Scottish botanist Robert Brown in a talk at the Linnean Society of London. Brown was studying orchids under microscope when he observed an opaque area, which he called the areola or nucleus, in the cells of the flower's outer layer.
He did not suggest a potential function. In 1838, Matthias Schleiden proposed that the nucleus plays a role in generating cells, thus he introduced the name "Cytoblast" (cell builder). He believed that he had observed new cells assembling around "cytoblasts". Franz Meyen was a strong opponent of this view, having already described cells multiplying by division and believing that many cells would have no nuclei. The idea that cells can be generated de novo, by the "cytoblast" or otherwise, contradicted work by Robert Remak (1852) and Rudolf Virchow (1855) who decisively propagated the new paradigm that cells are generated solely by cells ("Omnis cellula e cellula"). The function of the nucleus remained unclear.
Between 1877 and 1878, Oscar Hertwig published several studies on the fertilization of sea urchin eggs, showing that the nucleus of the sperm enters the oocyte and fuses with its nucleus. This was the first time it was suggested that an individual develops from a (single) nucleated cell. This was in contradiction to Ernst Haeckel's theory that the complete phylogeny of a species would be repeated during embryonic development, including generation of the first nucleated cell from a "Monerula", a structureless mass of primordial mucus ("Urschleim"). Therefore, the necessity of the sperm nucleus for fertilization was discussed for quite some time. However, Hertwig confirmed his observation in other animal groups, e.g., amphibians and molluscs. Eduard Strasburger produced the same results for plants (1884). This paved the way to assign the nucleus an important role in heredity. In 1873, August Weismann postulated the equivalence of the maternal and paternal germ "cells" for heredity. The function of the nucleus as carrier of genetic information became clear only later, after mitosis was discovered and the Mendelian rules were rediscovered at the beginning of the 20th century; the chromosome theory of heredity was therefore developed.
Structures.
The nucleus is the largest cellular organelle in animals.
In mammalian cells, the average diameter of the nucleus is approximately 6 micrometers (μm), which occupies about 10% of the total cell volume. The viscous liquid within it is called nucleoplasm, and is similar in composition to the cytosol found outside the nucleus. It appears as a dense, roughly spherical organelle.
Nuclear envelope and pores.
The nuclear envelope, otherwise known as nuclear membrane, consists of two cellular membranes, an inner and an outer membrane, arranged parallel to one another and separated by 10 to 50 nanometers (nm). The nuclear envelope completely encloses the nucleus and separates the cell's genetic material from the surrounding cytoplasm, serving as a barrier to prevent macromolecules from diffusing freely between the nucleoplasm and the cytoplasm. The outer nuclear membrane is continuous with the membrane of the rough endoplasmic reticulum (RER), and is similarly studded with ribosomes. The space between the membranes is called the perinuclear space and is continuous with the RER lumen.
Nuclear pores, which provide aqueous channels through the envelope, are composed of multiple proteins, collectively referred to as nucleoporins. The pores are about 125 million daltons in molecular weight and consist of around 50 (in yeast) to several hundred proteins (in vertebrates). The pores are 100 nm in total diameter; however, the gap through which molecules freely diffuse is only about 9 nm wide, due to the presence of regulatory systems within the center of the pore. This size selectively allows the passage of small water-soluble molecules while preventing larger molecules, such as nucleic acids and larger proteins, from inappropriately entering or exiting the nucleus. These large molecules must be actively transported into the nucleus instead. The nucleus of a typical mammalian cell will have about 3000 to 4000 pores throughout its envelope, each of which contains an eightfold-symmetric ring-shaped structure at a position where the inner and outer membranes fuse. Attached to the ring is a structure called the "nuclear basket" that extends into the nucleoplasm, and a series of filamentous extensions that reach into the cytoplasm. Both structures serve to mediate binding to nuclear transport proteins.
Most proteins, ribosomal subunits, and some DNAs are transported through the pore complexes in a process mediated by a family of transport factors known as karyopherins. Those karyopherins that mediate movement into the nucleus are also called importins, whereas those that mediate movement out of the nucleus are called exportins. Most karyopherins interact directly with their cargo, although some use adaptor proteins. Steroid hormones such as cortisol and aldosterone, as well as other small lipid-soluble molecules involved in intercellular signaling, can diffuse through the cell membrane and into the cytoplasm, where they bind nuclear receptor proteins that are trafficked into the nucleus. There they serve as transcription factors when bound to their ligand; in the absence of ligand, many such receptors function as histone deacetylases that repress gene expression.
Nuclear lamina.
In animal cells, two networks of intermediate filaments provide the nucleus with mechanical support: The nuclear lamina forms an organized meshwork on the internal face of the envelope, while less organized support is provided on the cytosolic face of the envelope. Both systems provide structural support for the nuclear envelope and anchoring sites for chromosomes and nuclear pores.
The nuclear lamina is composed mostly of lamin proteins. Like all proteins, lamins are synthesized in the cytoplasm and later transported to the nucleus interior, where they are assembled before being incorporated into the existing network of nuclear lamina. Lamins found on the cytosolic face of the membrane, such as emerin and nesprin, bind to the cytoskeleton to provide structural support. Lamins are also found inside the nucleoplasm where they form another regular structure, known as the "nucleoplasmic veil", that is visible using fluorescence microscopy. The actual function of the veil is not clear, although it is excluded from the nucleolus and is present during interphase. Lamin structures that make up the veil, such as LEM3, bind chromatin and disrupting their structure inhibits transcription of protein-coding genes.
Like the components of other intermediate filaments, the lamin monomer contains an alpha-helical domain used by two monomers to coil around each other, forming a dimer structure called a coiled coil. Two of these dimer structures then join side by side, in an antiparallel arrangement, to form a tetramer called a "protofilament". Eight of these protofilaments form a lateral arrangement that is twisted to form a ropelike "filament". These filaments can be assembled or disassembled in a dynamic manner, meaning that changes in the length of the filament depend on the competing rates of filament addition and removal.
Mutations in lamin genes leading to defects in filament assembly are known as "laminopathies". The most notable laminopathy is the family of diseases known as progeria, which causes the appearance of premature aging in its sufferers. The exact mechanism by which the associated biochemical changes give rise to the aged phenotype is not well understood.
Chromosomes.
The cell nucleus contains the majority of the cell's genetic material in the form of multiple linear DNA molecules organized into structures called chromosomes. Each human cell contains roughly two meters of DNA. During most of the cell cycle these are organized in a DNA-protein complex known as chromatin, and during cell division the chromatin can be seen to form the well-defined chromosomes familiar from a karyotype. A small fraction of the cell's genes are located instead in the mitochondria.
There are two types of chromatin. Euchromatin is the less compact DNA form, and contains genes that are frequently expressed by the cell. The other type, heterochromatin, is the more compact form, and contains DNA that is infrequently transcribed. This structure is further categorized into "facultative" heterochromatin, consisting of genes that are organized as heterochromatin only in certain cell types or at certain stages of development, and "constitutive" heterochromatin that consists of chromosome structural components such as telomeres and centromeres. During interphase the chromatin organizes itself into discrete individual patches, called "chromosome territories". Active genes, which are generally found in the euchromatic region of the chromosome, tend to be located towards the chromosome's territory boundary.
Antibodies to certain types of chromatin organization, in particular, nucleosomes, have been associated with a number of autoimmune diseases, such as systemic lupus erythematosus. These are known as anti-nuclear antibodies (ANA) and have also been observed in concert with multiple sclerosis as part of general immune system dysfunction. As in the case of progeria, the role played by the antibodies in inducing the symptoms of autoimmune diseases is not obvious.
Nucleolus.
The nucleolus is a discrete densely stained structure found in the nucleus. It is not surrounded by a membrane, and is sometimes called a "suborganelle". It forms around tandem repeats of rDNA, DNA coding for ribosomal RNA (rRNA). These regions are called nucleolar organizer regions (NOR). The main roles of the nucleolus are to synthesize rRNA and assemble ribosomes. The structural cohesion of the nucleolus depends on its activity, as ribosomal assembly in the nucleolus results in the transient association of nucleolar components, facilitating further ribosomal assembly, and hence further association. This model is supported by observations that inactivation of rDNA results in intermingling of nucleolar structures.
In the first step of ribosome assembly, a protein called RNA polymerase I transcribes rDNA, which forms a large pre-rRNA precursor. This is cleaved into the subunits 5.8S, 18S, and 28S rRNA. The transcription, post-transcriptional processing, and assembly of rRNA occurs in the nucleolus, aided by small nucleolar RNA (snoRNA) molecules, some of which are derived from spliced introns from messenger RNAs encoding genes related to ribosomal function. The assembled ribosomal subunits are the largest structures passed through the nuclear pores.
When observed under the electron microscope, the nucleolus can be seen to consist of three distinguishable regions: the innermost "fibrillar centers" (FCs), surrounded by the "dense fibrillar component" (DFC), which in turn is bordered by the "granular component" (GC). Transcription of the rDNA occurs either in the FC or at the FC-DFC boundary, and, therefore, when rDNA transcription in the cell is increased, more FCs are detected. Most of the cleavage and modification of rRNAs occurs in the DFC, while the latter steps involving protein assembly onto the ribosomal subunits occur in the GC.
Other subnuclear bodies.
Besides the nucleolus, the nucleus contains a number of other non-membrane-delineated bodies. These include Cajal bodies, Gemini of coiled bodies, polymorphic interphase karyosomal association (PIKA), promyelocytic leukaemia (PML) bodies, paraspeckles, and splicing speckles. Although little is known about a number of these domains, they are significant in that they show that the nucleoplasm is not uniform mixture, but rather contains organized functional subdomains.
Other subnuclear structures appear as part of abnormal disease processes. For example, the presence of small intranuclear rods has been reported in some cases of nemaline myopathy. This condition typically results from mutations in actin, and the rods themselves consist of mutant actin as well as other cytoskeletal proteins.
Cajal bodies and gems.
A nucleus typically contains between 1 and 10 compact structures called Cajal bodies or coiled bodies (CB), whose diameter measures between 0.2 µm and 2.0 µm depending on the cell type and species. When seen under an electron microscope, they resemble balls of tangled thread and are dense foci of distribution for the protein coilin. CBs are involved in a number of different roles relating to RNA processing, specifically small nucleolar RNA (snoRNA) and small nuclear RNA (snRNA) maturation, and histone mRNA modification.
Similar to Cajal bodies are Gemini of coiled bodies, or gems, whose name is derived from the Gemini constellation in reference to their close "twin" relationship with CBs. Gems are similar in size and shape to CBs, and in fact are virtually indistinguishable under the microscope. Unlike CBs, gems do not contain small nuclear ribonucleoproteins (snRNPs), but do contain a protein called "survivor of motor neurons" (SMN) whose function relates to snRNP biogenesis. Gems are believed to assist CBs in snRNP biogenesis, though it has also been suggested from microscopy evidence that CBs and gems are different manifestations of the same structure.
RAFA and PTF domains.
RAFA domains, or polymorphic interphase karyosomal associations, were first described in microscopy studies in 1991. Their function was and remains unclear, though they were not thought to be associated with active DNA replication, transcription, or RNA processing. They have been found to often associate with discrete domains defined by dense localization of the transcription factor PTF, which promotes transcription of snRNA.
PML bodies.
Promyelocytic leukaemia bodies (PML bodies) are spherical bodies found scattered throughout the nucleoplasm, measuring around 0.1–1.0 µm. They are known by a number of other names, including nuclear domain 10 (ND10), Kremer bodies, and PML oncogenic domains. PML bodies are named after one of their major components, the Promyelocytic leukemia protein. They are often seen in the nucleus in association with Cajal bodies and cleavage bodies. PML bodies belong to the nuclear matrix, an ill-defined super-structure of the nucleus proposed to anchor and regulate many nuclear functions, including DNA replication, transcription, or epigenetic silencing. The PML protein is the key organizer of these domains that recruits an ever-growing number of proteins, whose only common known feature to date is their ability to be sumoylated. Yet, pml-/- mice (which have their PML gene deleted) cannot assemble nuclear bodies, develop normally and live well, demonstrating that PML bodies are dispensable for most basic biological functions.
Splicing speckles.
Speckles are subnuclear structures that are enriched in pre-messenger RNA splicing factors and are located in the interchromatin regions of the nucleoplasm of mammalian cells. At the fluorescence-microscope level they appear as irregular, punctate structures, which vary in size and shape, and when examined by electron microscopy they are seen as clusters of interchromatin granules. Speckles are dynamic structures, and both their protein and RNA-protein components can cycle continuously between speckles and other nuclear locations, including active transcription sites. Studies on the composition, structure and behaviour of speckles have provided a model for understanding the functional compartmentalization of the nucleus and the organization of the gene-expression machinery.
splicing snRNPs and other splicing proteins necessary for pre-mRNA processing. Because of a cell's changing requirements, the composition and location of these bodies changes according to mRNA transcription and regulation via phosphorylation of specific proteins.
The splicing speckles are also known as nuclear speckles (nuclear specks), splicing factor compartments (SF compartments), interchromatin granule clusters (IGCs), B snurposomes.
B snurposomes are found in the amphibian oocyte nuclei and in "Drosophila melanogaster" embryos. B snurposomes appear alone or attached to the Cajal bodies in the electron micrographs of the amphibian nuclei.
IGCs function as storage sites for the splicing factors.
Paraspeckles.
Discovered by Fox et al. in 2002, paraspeckles are irregularly shaped compartments in the nucleus' interchromatin space. First documented in HeLa cells, where there are generally 10–30 per nucleus, paraspeckles are now known to also exist in all human primary cells, transformed cell lines, and tissue sections. Their name is derived from their distribution in the nucleus; the "para" is short for parallel and the "speckles" refers to the splicing speckles to which they are always in close proximity.
Paraspeckles are dynamic structures that are altered in response to changes in cellular metabolic activity. They are transcription dependent and in the absence of RNA Pol II transcription, the paraspeckle disappears and all of its associated protein components (PSP1, p54nrb, PSP2, CFI(m)68, and PSF) form a crescent shaped perinucleolar cap in the nucleolus. This phenomenon is demonstrated during the cell cycle. In the cell cycle, paraspeckles are present during interphase and during all of mitosis except for telophase. During telophase, when the two daughter nuclei are formed, there is no RNA Pol II transcription so the protein components instead form a perinucleolar cap.
Perichromatin fibrils.
Perichromatin fibrils are visible only under electron microscope. They are located next to the transcriptionally active chromatin and is hypothesized to be the site of active pre-mRNA processing.
Function.
The nucleus provides a site for genetic transcription that is segregated from the location of translation in the cytoplasm, allowing levels of gene regulation that are not available to prokaryotes. The main function of the cell nucleus is to control gene expression and mediate the replication of DNA during the cell cycle.
Cell compartmentalization.
The nuclear envelope allows the nucleus to control its contents, and separate them from the rest of the cytoplasm where necessary. This is important for controlling processes on either side of the nuclear membrane. In most cases where a cytoplasmic process needs to be restricted, a key participant is removed to the nucleus, where it interacts with transcription factors to downregulate the production of certain enzymes in the pathway. This regulatory mechanism occurs in the case of glycolysis, a cellular pathway for breaking down glucose to produce energy. Hexokinase is an enzyme responsible for the first the step of glycolysis, forming glucose-6-phosphate from glucose. At high concentrations of fructose-6-phosphate, a molecule made later from glucose-6-phosphate, a regulator protein removes hexokinase to the nucleus, where it forms a transcriptional repressor complex with nuclear proteins to reduce the expression of genes involved in glycolysis.
In order to control which genes are being transcribed, the cell separates some transcription factor proteins responsible for regulating gene expression from physical access to the DNA until they are activated by other signaling pathways. This prevents even low levels of inappropriate gene expression. For example, in the case of NF-κB-controlled genes, which are involved in most inflammatory responses, transcription is induced in response to a signal pathway such as that initiated by the signaling molecule TNF-α, binds to a cell membrane receptor, resulting in the recruitment of signalling proteins, and eventually activating the transcription factor NF-κB. A nuclear localisation signal on the NF-κB protein allows it to be transported through the nuclear pore and into the nucleus, where it stimulates the transcription of the target genes.
The compartmentalization allows the cell to prevent translation of unspliced mRNA. Eukaryotic mRNA contains introns that must be removed before being translated to produce functional proteins. The splicing is done inside the nucleus before the mRNA can be accessed by ribosomes for translation. Without the nucleus, ribosomes would translate newly transcribed (unprocessed) mRNA, resulting in misformed and nonfunctional proteins.
Gene expression.
Gene expression first involves transcription, in which DNA is used as a template to produce RNA. In the case of genes encoding proteins, that RNA produced from this process is messenger RNA (mRNA), which then needs to be translated by ribosomes to form a protein. As ribosomes are located outside the nucleus, mRNA produced needs to be exported.
Since the nucleus is the site of transcription, it also contains a variety of proteins that either directly mediate transcription or are involved in regulating the process. These proteins include helicases, which unwind the double-stranded DNA molecule to facilitate access to it, RNA polymerases, which synthesize the growing RNA molecule, topoisomerases, which change the amount of supercoiling in DNA, helping it wind and unwind, as well as a large variety of transcription factors that regulate expression.
Processing of pre-mRNA.
Newly synthesized mRNA molecules are known as primary transcripts or pre-mRNA. They must undergo post-transcriptional modification in the nucleus before being exported to the cytoplasm; mRNA that appears in the cytoplasm without these modifications is degraded rather than used for protein translation. The three main modifications are 5' capping, 3' polyadenylation, and RNA splicing. While in the nucleus, pre-mRNA is associated with a variety of proteins in complexes known as heterogeneous ribonucleoprotein particles (hnRNPs). Addition of the 5' cap occurs co-transcriptionally and is the first step in post-transcriptional modification. The 3' poly-adenine tail is only added after transcription is complete.
RNA splicing, carried out by a complex called the spliceosome, is the process by which introns, or regions of DNA that do not code for protein, are removed from the pre-mRNA and the remaining exons connected to re-form a single continuous molecule. This process normally occurs after 5' capping and 3' polyadenylation but can begin before synthesis is complete in transcripts with many exons. Many pre-mRNAs, including those encoding antibodies, can be spliced in multiple ways to produce different mature mRNAs that encode different protein sequences. This process is known as alternative splicing, and allows production of a large variety of proteins from a limited amount of DNA.
Dynamics and regulation.
Nuclear transport.
The entry and exit of large molecules from the nucleus is tightly controlled by the nuclear pore complexes. Although small molecules can enter the nucleus without regulation, macromolecules such as RNA and proteins require association karyopherins called importins to enter the nucleus and exportins to exit. "Cargo" proteins that must be translocated from the cytoplasm to the nucleus contain short amino acid sequences known as nuclear localization signals, which are bound by importins, while those transported from the nucleus to the cytoplasm carry nuclear export signals bound by exportins. The ability of importins and exportins to transport their cargo is regulated by GTPases, enzymes that hydrolyze the molecule guanosine triphosphate to release energy. The key GTPase in nuclear transport is Ran, which can bind either GTP or GDP (guanosine diphosphate), depending on whether it is located in the nucleus or the cytoplasm. Whereas importins depend on RanGTP to dissociate from their cargo, exportins require RanGTP in order to bind to their cargo.
Nuclear import depends on the importin binding its cargo in the cytoplasm and carrying it through the nuclear pore into the nucleus. Inside the nucleus, RanGTP acts to separate the cargo from the importin, allowing the importin to exit the nucleus and be reused. Nuclear export is similar, as the exportin binds the cargo inside the nucleus in a process facilitated by RanGTP, exits through the nuclear pore, and separates from its cargo in the cytoplasm.
Specialized export proteins exist for translocation of mature mRNA and tRNA to the cytoplasm after post-transcriptional modification is complete. This quality-control mechanism is important due to these molecules' central role in protein translation; mis-expression of a protein due to incomplete excision of exons or mis-incorporation of amino acids could have negative consequences for the cell; thus, incompletely modified RNA that reaches the cytoplasm is degraded rather than used in translation.
Assembly and disassembly.
During its lifetime, a nucleus may be broken down, either in the process of cell division or as a consequence of apoptosis (the process of programmed cell death). During these events, the structural components of the nucleus — the envelope and lamina — can be systematically degraded.
In most cells, the disassembly of the nuclear envelope marks the end of the prophase of mitosis. However, this disassembly of the nucleus is not a universal feature of mitosis and does not occur in all cells. Some unicellular eukaryotes (e.g., yeasts) undergo so-called closed mitosis, in which the nuclear envelope remains intact. In closed mitosis, the daughter chromosomes migrate to opposite poles of the nucleus, which then divides in two. The cells of higher eukaryotes, however, usually undergo open mitosis, which is characterized by breakdown of the nuclear envelope. The daughter chromosomes then migrate to opposite poles of the mitotic spindle, and new nuclei reassemble around them.
At a certain point during the cell cycle in open mitosis, the cell divides to form two cells. In order for this process to be possible, each of the new daughter cells must have a full set of genes, a process requiring replication of the chromosomes as well as segregation of the separate sets. This occurs by the replicated chromosomes, the sister chromatids, attaching to microtubules, which in turn are attached to different centrosomes. The sister chromatids can then be pulled to separate locations in the cell. In many cells, the centrosome is located in the cytoplasm, outside the nucleus; the microtubules would be unable to attach to the chromatids in the presence of the nuclear envelope. Therefore the early stages in the cell cycle, beginning in prophase and until around prometaphase, the nuclear membrane is dismantled. Likewise, during the same period, the nuclear lamina is also disassembled, a process regulated by phosphorylation of the lamins by protein kinases such as the CDC2 protein kinase. Towards the end of the cell cycle, the nuclear membrane is reformed, and around the same time, the nuclear lamina are reassembled by dephosphorylating the lamins.
However, in dinoflagellates, the nuclear envelope remains intact, the centrosomes are located in the cytoplasm, and the microtubules come in contact with chromosomes, whose centromeric regions are incorporated into the nuclear envelope (the so-called closed mitosis with extranuclear spindle). In many other protists (e.g., ciliates, sporozoans) and fungi, the centrosomes are intranuclear, and their nuclear envelope also does not disassemle during cell division.
Apoptosis is a controlled process in which the cell's structural components are destroyed, resulting in death of the cell. Changes associated with apoptosis directly affect the nucleus and its contents, for example, in the condensation of chromatin and the disintegration of the nuclear envelope and lamina. The destruction of the lamin networks is controlled by specialized apoptotic proteases called caspases, which cleave the lamin proteins and, thus, degrade the nucleus' structural integrity. Lamin cleavage is sometimes used as a laboratory indicator of caspase activity in assays for early apoptotic activity. Cells that express mutant caspase-resistant lamins are deficient in nuclear changes related to apoptosis, suggesting that lamins play a role in initiating the events that lead to apoptotic degradation of the nucleus. Inhibition of lamin assembly itself is an inducer of apoptosis.
The nuclear envelope acts as a barrier that prevents both DNA and RNA viruses from entering the nucleus. Some viruses require access to proteins inside the nucleus in order to replicate and/or assemble. DNA viruses, such as herpesvirus replicate and assemble in the cell nucleus, and exit by budding through the inner nuclear membrane. This process is accompanied by disassembly of the lamina on the nuclear face of the inner membrane.
Disease-related dynamics.
Initially, it has been suspected that immunoglobulins in general and autoantibodies in particular do not enter the nucleus. Now there is a body of evidence that under pathological conditions (e.g. lupus erythematosus) IgG can enter the nucleus.
Anucleated and multinucleated cells.
Anucleated cells contain no nucleus and are, therefore, incapable of dividing to produce daughter cells. The best-known anucleated cell is the mammalian red blood cell, or erythrocyte, which also lacks other organelles such as mitochondria, and serves primarily as a transport vessel to ferry oxygen from the lungs to the body's tissues. Erythrocytes mature through erythropoiesis in the bone marrow, where they lose their nuclei, organelles, and ribosomes. The nucleus is expelled during the process of differentiation from an erythroblast to a reticulocyte, which is the immediate precursor of the mature erythrocyte. The presence of mutagens may induce the release of some immature "micronucleated" erythrocytes into the bloodstream. Anucleated cells can also arise from flawed cell division in which one daughter lacks a nucleus and the other has two nuclei.
Multinucleated cells contain multiple nuclei. Most acantharean species of protozoa and some fungi in mycorrhizae have naturally multinucleated cells. Other examples include the intestinal parasites in the genus "Giardia", which have two nuclei per cell. In humans, skeletal muscle cells, called myocytes and syncytium, become multinucleated during development; the resulting arrangement of nuclei near the periphery of the cells allows maximal intracellular space for myofibrils. Multinucleated and binucleated cells can also be abnormal in humans; for example, cells arising from the fusion of monocytes and macrophages, known as giant multinucleated cells, sometimes accompany inflammation and are also implicated in tumor formation.
A number of dinoflagelates are known to have two nuclei. Unlike other multinucleated cells these nuclei contain two distinct lineages of DNA: one from the dinoflagelate and the other from a symbiotic diatom. Curiously the mitochondrion and the plastid of the diatom remain functional.
Evolution.
As the major defining characteristic of the eukaryotic cell, the nucleus' evolutionary origin has been the subject of much speculation. Four major hypotheses have been proposed to explain the existence of the nucleus, although none have yet earned widespread support.
The first model known as the "syntrophic model" proposes that a symbiotic relationship between the archaea and bacteria created the nucleus-containing eukaryotic cell. (Organisms of the Archaea and Bacteria domain have no cell nucleus.) It is hypothesized that the symbiosis originated when ancient archaea, similar to modern methanogenic archaea, invaded and lived within bacteria similar to modern myxobacteria, eventually forming the early nucleus. This theory is analogous to the accepted theory for the origin of eukaryotic mitochondria and chloroplasts, which are thought to have developed from a similar endosymbiotic relationship between proto-eukaryotes and aerobic bacteria. The archaeal origin of the nucleus is supported by observations that archaea and eukarya have similar genes for certain proteins, including histones. Observations that myxobacteria are motile, can form multicellular complexes, and possess kinases and G proteins similar to eukarya, support a bacterial origin for the eukaryotic cell.
A second model proposes that proto-eukaryotic cells evolved from bacteria without an endosymbiotic stage. This model is based on the existence of modern planctomycetes bacteria that possess a nuclear structure with primitive pores and other compartmentalized membrane structures. A similar proposal states that a eukaryote-like cell, the chronocyte, evolved first and phagocytosed archaea and bacteria to generate the nucleus and the eukaryotic cell.
The most controversial model, known as "viral eukaryogenesis", posits that the membrane-bound nucleus, along with other eukaryotic features, originated from the infection of a prokaryote by a virus. The suggestion is based on similarities between eukaryotes and viruses such as linear DNA strands, mRNA capping, and tight binding to proteins (analogizing histones to viral envelopes). One version of the proposal suggests that the nucleus evolved in concert with phagocytosis to form an early cellular "predator". Another variant proposes that eukaryotes originated from early archaea infected by poxviruses, on the basis of observed similarity between the DNA polymerases in modern poxviruses and eukaryotes. It has been suggested that the unresolved question of the evolution of sex could be related to the viral eukaryogenesis hypothesis.
A more recent proposal, the "exomembrane hypothesis", suggests that the nucleus instead originated from a single ancestral cell that evolved a second exterior cell membrane; the interior membrane enclosing the original cell then became the nuclear membrane and evolved increasingly elaborate pore structures for passage of internally synthesized cellular components such as ribosomal subunits.

</doc>
<doc id="6236" url="http://en.wikipedia.org/wiki?curid=6236" title="Chardonnay socialist">
Chardonnay socialist

Chardonnay socialist is a derogatory Australasian term for those on the political left with comfortable middle or upper-class incomes, tertiary education, and a penchant for the finer things in life, Chardonnay being a form of white wine for example.
It is similar in thrust to the North American term limousine liberal, though without quite the same taint of great wealth attached to it. The term was modelled on the British term: Champagne socialist. When the term was coined around 1989, Chardonnay was seen as a drink of affluent people. It became a popular drink during the next decade and hence the term has lost some of its sting.
The term "chardonnay socialist" is regularly used by people from throughout the political spectrum to criticise opponents. For example, Australian left-wing "true believers" levelled it at supporters of the failed republic referendum of 1999 (where the vote was split not along conventional party lines but very much along socio-economic divides, with the rich overwhelmingly supporting the change while the less well-off were opposed – a superficially bizarre pattern for a non-economic issue). Staunch Australian right-wingers, on the other hand, level it at those who support such things as government funding for the arts, free tertiary education, and the ABC – all causes which are described by critics as "middle-class welfare."
The older term for this or a similar kind of person was "salon communist."
Other similar terms are the "chattering classes" (coined in England in the 1980s) and "latte liberal".

</doc>
<doc id="6237" url="http://en.wikipedia.org/wiki?curid=6237" title="Christmas">
Christmas

Christmas (, meaning "Christ's Mass") is an annual commemoration of the birth of Jesus ChristArchived 2009-10-31.
</ref> and a widely observed cultural holiday, celebrated generally on December 25 by billions of people around the world. A feast central to the Christian liturgical year, it closes the Advent season and initiates the twelve days of Christmastide, which ends after the twelfth night. Christmas is a civil holiday in many of the world's nations, is celebrated by an increasing number of non-Christians, and is an integral part of the Christmas and holiday season.
While the birth year of Jesus is estimated among modern historians to have been between 7 and 2 BC, the exact month and day of his birth are unknown. His birth is mentioned in two of the four canonical gospels. By the early-to-mid 4th century, the Western Christian Church had placed Christmas on December 25, a date later adopted in the East, although some churches celebrate on the December 25 of the older Julian calendar, which corresponds to January in the modern-day Gregorian calendar. The date of Christmas may have initially been chosen to correspond with the day exactly nine months after early Christians believed Jesus to have been conceived, or with one or more ancient polytheistic festivals that occurred near southern solstice (i.e., the Roman winter solstice); a further solar connection has been suggested because of a biblical verse identifying Jesus as the "Sun of righteousness".Tighe, William J., "Calculating Christmas". Archived 2009-10-31.</ref>
The celebratory customs associated in various countries with Christmas have a mix of pagan, pre-Christian, Christian, and secular themes and origins. Popular modern customs of the holiday include gift giving, Christmas music and caroling, an exchange of Christmas cards, church celebrations, a special meal, and the display of various Christmas decorations, including Christmas trees, Christmas lights, nativity scenes, garlands, wreaths, mistletoe, and holly. In addition, several closely related and often interchangeable figures, known as Santa Claus, Father Christmas, Saint Nicholas, and Christkind, are associated with bringing gifts to children during the Christmas season and have their own body of traditions and lore. Because gift-giving and many other aspects of the Christmas festival involve heightened economic activity among both Christians and non-Christians, the holiday has become a significant event and a key sales period for retailers and businesses. The economic impact of Christmas is a factor that has grown steadily over the past few centuries in many regions of the world.
Etymology.
"Christmas" is a compound word originating in the term "Christ's Mass". It is derived from the Middle English "Cristemasse", which is from Old English "Crīstesmæsse", a phrase first recorded in 1038 followed by the word Cristes-messe in 1131. "Crīst" (genitive "Crīstes") is from Greek "Khrīstos" (Χριστός), a translation of Hebrew "Māšîaḥ" (מָשִׁיחַ), "Messiah", meaning "anointed"; and "mæsse" is from Latin "missa", the celebration of the Eucharist. The form "Christenmas" was also historically used, but is now considered archaic and dialectal; it derives from Middle English "Cristenmasse", literally "Christian mass". "Xmas" is an abbreviation of "Christmas" found particularly in print, based on the initial letter chi (Χ) in Greek "Khrīstos" (Χριστός), "Christ", though numerous style guides discourage its use; it has precedent in Middle English "Χρ̄es masse" (where "Χρ̄" is an abbreviation for Χριστός).
Other names.
In addition to "Christmas", the holiday has been known by various other names throughout its history. The Anglo-Saxons referred to the feast as "midwinter", or, more rarely, as "Nātiuiteð" (from Latin "nātīvitās" below). "Nativity", meaning "birth", is from Latin "nātīvitās". In Old English, "Gēola" ("Yule") referred to the period corresponding to January and December, which was eventually equated with Christian Christmas. "Noel" (or "Nowell") entered English in the late 14th century and is from the Old French "noël" or "naël", itself ultimately from the Latin "nātālis (diēs)", "(day) of birth".
History.
The Chronography of 354 AD contains early evidence of the celebration on December 25 of a Christian liturgical feast of the birth of Jesus. This was in Rome, while in Eastern Christianity the birth of Jesus was already celebrated in connection with the Epiphany on January 6. The December 25 celebration was imported into the East later: in Antioch by John Chrysostom towards the end of the 4th century, probably in 388, and in Alexandria only in the following century. Even in the West, the January 6 celebration of the nativity of Jesus seems to have continued until after 380. In 245, Origen of Alexandria, writing about , commented that Scripture mentions only sinners as "celebrating" their birthdays, namely Pharaoh, who then had his chief baker hanged (), and Herod, who then had John the Baptist beheaded (), and mentions saints as "cursing" the day of their birth, namely Jeremiah () and Job ().partially quoted in "Natal Day", "The Catholic Encyclopedia", 1911.</ref> In 303, Arnobius ridiculed the idea of celebrating the birthdays of gods, a passage cited as evidence that Arnobius was unaware of any nativity celebration.G. Brunner, "Arnobius eine Zeuge gegen das Weihnachtsfest? " JLW 13 (1936) pp. 178–181.</ref> Since Christmas does not celebrate Christ's birth "as God" but "as man", this is not evidence against Christmas being a feast at this time. The fact the Donatists of North Africa celebrated Christmas may indicate that the feast was established by the time that church was created in 311.
Many popular customs associated with Christmas developed independently of the commemoration of Jesus' birth, with certain elements having origins in pre-Christian festivals that were celebrated around the winter solstice by pagan populations who were later converted to Christianity. These elements, including the Yule log from Yule and gift giving from Saturnalia, became syncretized into Christmas over the centuries. The prevailing atmosphere of Christmas has also continually evolved since the holiday's inception, ranging from a sometimes raucous, drunken, carnival-like state in the Middle Ages, to a tamer family-oriented and children-centered theme introduced in a 19th-century reformation. Additionally, the celebration of Christmas was banned on more than one occasion within certain Protestant groups, such as the Puritans, due to concerns that it was too pagan or unbiblical.
Relation to concurrent celebrations.
Prior to and through the early Christian centuries, winter festivals—especially those centered on the winter solstice—were the most popular of the year in many European pagan cultures. Reasons included the fact that less agricultural work needs to be done during the winter, as well as an expectation of better weather as spring approached. Many modern Christmas customs have been directly influenced by such festivals, including gift-giving and merrymaking from the Roman Saturnalia, greenery, lights, and charity from the Roman New Year, and Yule logs and various foods from Germanic feasts.
Pagan Scandinavia celebrated a winter festival called Yule, held in the late December to early January period. As northern Europe was the last part to Christianize, its pagan traditions had a major influence on Christmas there, an example being the Koleda, which was incorporated into the Christmas carol. Scandinavians still call Christmas "Jul". In English, the word Yule is synonymous with Christmas, a usage first recorded in 900.
"Dies Natalis Solis Invicti".
"Dies Natalis Solis Invicti" means "the birthday of the Unconquered Sun", a festival inaugurated by the Roman emperor Aurelian to celebrate the sun god and celebrated at the winter solstice, 25 December. During the reign of the emperor Constantine, Christian writers assimilated this feast as the birthday of Jesus, associating him with the 'sun of righteousness' mentioned in ("Sol Iustitiae"). In his work "Adversus Haereses", Irenaeus (c. 130–202) identified the conception of Jesus as March 25 and linked it to the crucifixion, with the birth of Jesus nine months after on December 25. Celebration of the conception of Jesus, known as the Annunciation, became associated with the spring equinox, thus led to Christmas coinciding with the winter solstice. An anonymous work known as "De Pascha Computus" (243) linked the idea that creation began at the spring equinox, on 25 March with the conception or birth (the word "nascor" can mean either) of Jesus on 28 March, the day of the creation of the sun in the Genesis account. One translation reads: "O the splendid and divine providence of the Lord, that on that day, the very day, on which the sun was made, the 28 March, a Wednesday, Christ should be born. For this reason Malachi the prophet, speaking about him to the people, fittingly said, 'Unto you shall the sun of righteousness arise, and healing is in his wings.'"
In the fourth century, John Chrysostom, who promoted the celebration on 25 December, commented on the connection: "But Our Lord, too, is born in the month of December ... the eight before the calends of January December ..., But they call it the 'Birthday of the Unconquered'. Who indeed is so unconquered as Our Lord ...? Or, if they say that it is the birthday of the Sun, He is the Sun of Justice." With regard to a December religious feast of the sun as a god (Sol), as distinct from a solstice feast of the (re)birth of the astronomical sun, one scholar has commented that, "while the winter solstice on or around December 25 was well established in the Roman imperial calendar, there is no evidence that a religious celebration of Sol on that day antedated the celebration of Christmas". "Thomas Talley has shown that, although the Emperor Aurelian's dedication of a temple to the sun god in the Campus Martius (C.E. 274) probably took place on the 'Birthday of the Invincible Sun' on December 25, the cult of the sun in pagan Rome ironically did not celebrate the winter solstice nor any of the other quarter-tense days, as one might expect." The "Oxford Companion to Christian Thought" remarks on the uncertainty about the order of precedence between the religious celebrations of the Birthday of the Unconquered Sun and of the birthday of Jesus, stating that the hypothesis that 25 December was chosen for celebrating the birth of Jesus on the basis of the belief that his conception occurred on 25 March "potentially establishes 25 December as a Christian festival before Aurelian's decree, which, when promulgated, might have provided for the Christian feast both opportunity and challenge".
Feast established.
The Chronography of 354, an illuminated manuscript compiled in Rome, is an early reference to the date of the nativity as December 25.
Christmas was promoted in the Christian East as part of the revival of Catholicism following the death of the pro-Arian Emperor Valens at the Battle of Adrianople in 378. The feast was introduced to Constantinople in 379, and to Antioch in about 380. The feast disappeared after Gregory of Nazianzus resigned as bishop in 381, although it was reintroduced by John Chrysostom in about 400.
Middle Ages.
In the Early Middle Ages, Christmas Day was overshadowed by Epiphany, which in western Christianity focused on the visit of the magi. But the medieval calendar was dominated by Christmas-related holidays. The forty days before Christmas became the "forty days of St. Martin" (which began on November 11, the feast of St. Martin of Tours), now known as Advent. In Italy, former Saturnalian traditions were attached to Advent. Around the 12th century, these traditions transferred again to the Twelve Days of Christmas (December 25 – January 5); a time that appears in the liturgical calendars as Christmastide or Twelve Holy Days.
The prominence of Christmas Day increased gradually after Charlemagne was crowned Emperor on Christmas Day in 800. King Edmund the Martyr was anointed on Christmas in 855 and King William I of England was crowned on Christmas Day 1066.
By the High Middle Ages, the holiday had become so prominent that chroniclers routinely noted where various magnates celebrated Christmas. King Richard II of England hosted a Christmas feast in 1377 at which twenty-eight oxen and three hundred sheep were eaten. The Yule boar was a common feature of medieval Christmas feasts. Caroling also became popular, and was originally a group of dancers who sang. The group was composed of a lead singer and a ring of dancers that provided the chorus. Various writers of the time condemned caroling as lewd, indicating that the unruly traditions of Saturnalia and Yule may have continued in this form. "Misrule"—drunkenness, promiscuity, gambling—was also an important aspect of the festival. In England, gifts were exchanged on New Year's Day, and there was special Christmas ale.
Christmas during the Middle Ages was a public festival that incorporated ivy, holly, and other evergreens. Christmas gift-giving during the Middle Ages was usually between people with legal relationships, such as tenant and landlord. The annual indulgence in eating, dancing, singing, sporting, and card playing escalated in England, and by the 17th century the Christmas season featured lavish dinners, elaborate masques, and pageants. In 1607, King James I insisted that a play be acted on Christmas night and that the court indulge in games. It was during the Reformation in 16th–17th-century Europe that many Protestants changed the gift bringer to the Christ Child or "Christkindl", and the date of giving gifts changed from December 6 to Christmas Eve.
Reformation to the 18th century.
Following the Protestant Reformation, groups such as the Puritans strongly condemned the celebration of Christmas, considering it a Catholic invention and the "trappings of popery" or the "rags of the Beast".
The Catholic Church responded by promoting the festival in a more religiously oriented form. King Charles I of England directed his noblemen and gentry to return to their landed estates in midwinter to keep up their old style Christmas generosity. Following the Parliamentarian victory over Charles I during the English Civil War, England's Puritan rulers banned Christmas in 1647.
Protests followed as pro-Christmas rioting broke out in several cities and for weeks Canterbury was controlled by the rioters, who decorated doorways with holly and shouted royalist slogans. The book, "The Vindication of Christmas" (London, 1652), argued against the Puritans, and makes note of Old English Christmas traditions, dinner, roast apples on the fire, card playing, dances with "plow-boys" and "maidservants", and carol singing.
The Restoration of King Charles II in 1660 ended the ban, but many clergymen still disapproved of Christmas celebration. In Scotland, the Presbyterian Church of Scotland also discouraged the observance of Christmas, and though James VI commanded its celebration in 1618, attendance at church was scant. The Parliament of Scotland officially abolished the observance of Christmas in 1640, claiming that the church had been "purged of all superstitious observation of days". It was not until 1958 that Christmas again became a Scottish public holiday.
Despite the disapproval of many people in Britain, others continued to celebrate the Christmas season. Following the Restoration, Poor Robins Almanack contained the lines:
The diary of James Woodforde, from the latter half of the 18th century, details the observance of Christmas and celebrations associated with the season over a number of years.
In Colonial America, the Puritans of New England shared radical Protestant disapproval of Christmas. Celebration was outlawed in Boston from 1659 to 1681. The ban by the Pilgrims was revoked in 1681 by English governor Sir Edmund Andros, however it was not until the mid-19th century that celebrating Christmas became fashionable in the Boston region.
At the same time, Christian residents of Virginia and New York observed the holiday freely. Pennsylvania German Settlers, pre-eminently the Moravian settlers of Bethlehem, Nazareth and Lititz in Pennsylvania and the Wachovia Settlements in North Carolina, were enthusiastic celebrators of Christmas. The Moravians in Bethlehem had the first Christmas trees in America as well as the first Nativity Scenes. Christmas fell out of favor in the United States after the American Revolution, when it was considered an English custom.
George Washington attacked Hessian (German) mercenaries on the day after Christmas during the Battle of Trenton on December 26, 1776, Christmas being much more popular in Germany than in America at this time.
19th century.
In the early 19th century, writers imagined Tudor Christmas as a time of heartfelt celebration. In 1843, Charles Dickens wrote the novel "A Christmas Carol" that helped revive the "spirit" of Christmas and seasonal merriment. Its instant popularity played a major role in portraying Christmas as a holiday emphasizing family, goodwill, and compassion.
Dickens sought to construct Christmas as a family-centered festival of generosity, in contrast to the community-based and church-centered observations, the observance of which had dwindled during the late 18th century and early 19th century. Superimposing his secular vision of the holiday, Dickens influenced many aspects of Christmas that are celebrated today in Western culture, such as family gatherings, seasonal food and drink, dancing, games, and a festive generosity of spirit. A prominent phrase from the tale, "Merry Christmas", was popularized following the appearance of the story. This coincided with the appearance of the Oxford Movement and the growth of Anglo-Catholicism, which led a revival in traditional rituals and religious observances.
The term Scrooge became a synonym for miser, with "Bah! Humbug!" dismissive of the festive spirit. In 1843, the first commercial Christmas card was produced by Sir Henry Cole. The revival of the Christmas Carol began with William Sandys "Christmas Carols Ancient and Modern" (1833), with the first appearance in print of ""The First Noel", "I Saw Three Ships", "Hark the Herald Angels Sing" and "God Rest Ye Merry, Gentlemen", popularized in Dickens' "A Christmas Carol".
In Britain, the Christmas tree was introduced in the early 19th century following the personal union with the Kingdom of Hanover by Charlotte of Mecklenburg-Strelitz, wife of King George III. In 1832, the future Queen Victoria wrote about her delight at having a Christmas tree, hung with lights, ornaments, and presents placed round it. After her marriage to her German cousin Prince Albert, by 1841 the custom became more widespread throughout Britain.
An image of the British royal family with their Christmas tree at Windsor Castle created a sensation when it was published in the "Illustrated London News" in 1848. A modified version of this image was published in the United States in 1850. By the 1870s, putting up a Christmas tree had become common in America.
In America, interest in Christmas had been revived in the 1820s by several short stories by Washington Irving which appear in his "The Sketch Book of Geoffrey Crayon, Gent." and "Old Christmas". Irving's stories depicted harmonious warm-hearted English Christmas festivities he experienced while staying in Aston Hall, Birmingham, England, that had largely been abandoned, and he used the tract "Vindication of Christmas" (1652) of Old English Christmas traditions, that he had transcribed into his journal as a format for his stories.
In 1822, Clement Clarke Moore wrote the poem "A Visit From St. Nicholas" (popularly known by its first line: "Twas the Night Before Christmas").
The poem helped popularize the tradition of exchanging gifts, and seasonal Christmas shopping began to assume economic importance.
This also started the cultural conflict between the holiday's spiritual significance and its associated commercialism that some see as corrupting the holiday. In her 1850 book "The First Christmas in New England", Harriet Beecher Stowe includes a character who complains that the true meaning of Christmas was lost in a shopping spree.
While the celebration of Christmas was not yet customary in some regions in the U.S., Henry Wadsworth Longfellow detected "a transition state about Christmas here in New England" in 1856. "The old puritan feeling prevents it from being a cheerful, hearty holiday; though every year makes it more so."
In Reading, Pennsylvania, a newspaper remarked in 1861, "Even our presbyterian friends who have hitherto steadfastly ignored Christmas—threw open their church doors and assembled in force to celebrate the anniversary of the Savior's birth."
The First Congregational Church of Rockford, Illinois, "although of genuine Puritan stock", was 'preparing for a grand Christmas jubilee', a news correspondent reported in 1864. By 1860, fourteen states including several from New England had adopted Christmas as a legal holiday. In 1875, Louis Prang introduced the Christmas card to Americans. He has been called the "father of the American Christmas card". In 1885, Christmas was formally declared a United States federal holiday.
Traditions.
Christmas Day is celebrated as a major festival and public holiday in countries around the world, including many whose populations are mostly non-Christian. In some non-Christian countries, periods of former colonial rule introduced the celebration (e.g. Hong Kong); in others, Christian minorities or foreign cultural influences have led populations to observe the holiday. Countries such as Japan, where Christmas is popular despite there being only a small number of Christians, have adopted many of the secular aspects of Christmas, such as gift-giving, decorations, and Christmas trees.
Countries in which Christmas is not a formal public holiday include Afghanistan, Algeria, Azerbaijan, Bahrain, Bhutan, Cambodia, China (excepting Hong Kong and Macao), Comoros, Iran, Israel, Japan, Kuwait, Laos, Libya, Maldives, Mauritania, Mongolia, Morocco, North Korea, Oman, Pakistan, Qatar, Sahrawi Arab Democratic Republic, Saudi Arabia, Somalia, Tajikistan, Thailand, Tunisia, Turkey, Turkmenistan, United Arab Emirates, Uzbekistan, Vietnam, and Yemen. Christmas celebrations around the world can vary markedly in form, reflecting differing cultural and national traditions.
Among countries with a strong Christian tradition, a variety of Christmas celebrations have developed that incorporate regional and local cultures. For Christians, participating in a religious service plays an important part in the recognition of the season. Christmas, along with Easter, is the period of highest annual church attendance. In Catholic countries, people hold religious processions or parades in the days preceding Christmas. In other countries, secular processions or parades featuring Santa Claus and other seasonal figures are often held. Family reunions and the exchange of gifts are a widespread feature of the season. Gift giving takes place on Christmas Day in most countries. Others practice gift giving on December 6, Saint Nicholas Day, and January 6, Epiphany.
Commemorating Jesus' birth.
Christians celebrate the birth of Jesus to the Virgin Mary as a fulfillment of the Old Testament's Messianic prophecy. The Bible contains two accounts which describe the events surrounding Jesus' birth. Depending on one's perspective, these accounts either differ from each other or tell two versions of the same story. These biblical accounts are found in the Gospel of Matthew, namely Matthew 1:18, and the Gospel of Luke, specifically Luke 1:26 and 2:40. According to these accounts, Jesus was born to Mary, assisted by her husband Joseph, in the city of Bethlehem.
According to popular tradition, the birth took place in a stable, surrounded by farm animals. A manger (that is, a feeding trough) is mentioned in , where it states Mary "wrapped him in swaddling clothes and laid him in a manger, because there was no room for them in the inn" (KJV); and "She wrapped him in cloths and placed him in a manger, because there was no guest room available for them" (NIV).
Shepherds from the fields surrounding Bethlehem were told of the birth by an angel, and were the first to see the child. Popular tradition also holds that three kings or wise men (named Melchior, Caspar, and Balthazar) visited the infant Jesus in the manger, though this does not strictly follow the biblical account. The Gospel of Matthew instead describes a visit by an unspecified number of magi, or astrologers, sometime after Jesus was born while the family was living in a house (), who brought gifts of gold, frankincense, and myrrh to the young child Jesus. The visitors were said to be following a mysterious star, commonly known as the Star of Bethlehem, believing it to announce the birth of a king of the Jews. The commemoration of this visit, the Feast of Epiphany celebrated on January 6, is the formal end of the Christmas season in some churches.
Christians celebrate Christmas in various ways. In addition to this day being one of the most important and popular for the attendance of church services, there are other devotions and popular traditions. In some Christian denominations, children re-enact the events of the Nativity with animals to portray the event with more realism or sing carols that reference the event. A long artistic tradition has grown of producing painted depictions of the nativity in art. Nativity scenes are traditionally set in a stable with livestock and include Mary, Joseph, the infant Jesus in the manger, the three wise men, the shepherds and their sheep, the angels, and the Star of Bethlehem. Some Christians also display a small re-creation of the Nativity, known as a Nativity scene or crèche, in their homes, using figurines to portray the key characters of the event. Prior to Christmas Day, the Eastern Orthodox Church practices the 40-day Nativity Fast in anticipation of the birth of Jesus, while much of Western Christianity celebrates four weeks of Advent. The final preparations for Christmas are made on Christmas Eve, and many families' major observation of Christmas actually falls in the evening of this day.
Decorations.
The practice of putting up special decorations at Christmas has a long history. In the 15th century, it was recorded that in London it was the custom at Christmas for every house and all the parish churches to be "decked with holm, ivy, bays, and whatsoever the season of the year afforded to be green". The heart-shaped leaves of ivy were said to symbolize the coming to earth of Jesus, while holly was seen as protection against pagans and witches, its thorns and red berries held to represent the Crown of Thorns worn by Jesus at the crucifixion and the blood he shed.
Nativity scenes are known from 10th-century Rome. They were popularised by Saint Francis of Asissi from 1223, quickly spreading across Europe. Different types of decorations developed across the Christian world, dependent on local tradition and available resources, and can vary from simple representations of the crib to far more elaborate sets - renowned manger scene traditions include the colourful "Kraków szopka" in Poland, which imitate Kraków's historical buildings as settings, the elaborate Italian "presepi" (Neapolitan, Genoese and Bolognese), or the Provençal crèches in southern France, using hand-painted terracotta figurines called "santons". In certain parts of the world, notably Sicily, living nativity scenes following the tradition of Saint Francis are a popular alternative to static crèches. The first commercially produced decorations appeared in Germany in the 1860s, inspired by paper chains made by children. In countries where a representation of the Nativity scene is very popular, people are encouraged to compete and create the most original or realistic ones. Within some families, the pieces used to make the representation are considered a valuable family heirloom.
The traditional colors of Christmas decorations are red, green, and gold. Red symbolizes the blood of Jesus, which was shed in his crucifixion, while green symbolizes eternal life, and in particular the evergreen tree, which does not lose its leaves in the winter, and gold is the first color associated with Christmas, as one of the three gifts of the Magi, symbolizing royalty.
The Christmas tree is considered by some as Christianisation of pagan tradition and ritual surrounding the Winter Solstice, which included the use of evergreen boughs, and an adaptation of pagan tree worship; according to eighth-century biographer Æddi Stephanus, Saint Boniface (634–709), who was a missionary in Germany, took an axe to an oak tree dedicated to Thor and pointed out a fir tree, which he stated was a more fitting object of reverence because it pointed to heaven and it had a triangular shape, which he said was symbolic of the Trinity. The English language phrase "Christmas tree" is first recorded in 1835 and represents an importation from the German language. The modern Christmas tree tradition is believed to have begun in Germany in the 18th century though many argue that Martin Luther began the tradition in the 16th century.
From Germany the custom was introduced to Britain, first via Queen Charlotte, wife of George III, and then more successfully by Prince Albert during the reign of Queen Victoria. By 1841 the Christmas tree had become even more widespread throughout Britain. By the 1870s, people in the United States had adopted the custom of putting up a Christmas tree. Christmas trees may be decorated with lights and ornaments.
Since the 19th century, the poinsettia, a native plant from Mexico, has been associated with Christmas. Other popular holiday plants include holly, mistletoe, red amaryllis, and Christmas cactus. Along with a Christmas tree, the interior of a home may be decorated with these plants, along with garlands and evergreen foliage. The display of Christmas villages has also become a tradition in many homes during this season. The outside of houses may be decorated with lights and sometimes with illuminated sleighs, snowmen, and other Christmas figures.
Other traditional decorations include bells, candles, candy canes, stockings, wreaths, and angels. Both the displaying of wreaths and candles in each window are a more traditional Christmas display. The concentric assortment of leaves, usually from an evergreen, make up Christmas wreaths and are designed to prepare Christians for the Advent season. Candles in each window are meant to demonstrate the fact that Christians believe that Jesus Christ is the ultimate light of the world.
Christmas lights and banners may be hung along streets, music played from speakers, and Christmas trees placed in prominent places. It is common in many parts of the world for town squares and consumer shopping areas to sponsor and display decorations. Rolls of brightly colored paper with secular or religious Christmas motifs are manufactured for the purpose of wrapping gifts. In some countries, Christmas decorations are traditionally taken down on Twelfth Night, the evening of January 5.
Music and carols.
The earliest extant specifically Christmas hymns appear in 4th-century Rome. Latin hymns such as "Veni redemptor gentium", written by Ambrose, Archbishop of Milan, were austere statements of the theological doctrine of the Incarnation in opposition to Arianism. "Corde natus ex Parentis" ("Of the Father's love begotten") by the Spanish poet Prudentius (d. 413) is still sung in some churches today.
In the 9th and 10th centuries, the Christmas "Sequence" or "Prose" was introduced in North European monasteries, developing under Bernard of Clairvaux into a sequence of rhymed stanzas. In the 12th century the Parisian monk Adam of St. Victor began to derive music from popular songs, introducing something closer to the traditional Christmas carol.
By the 13th century, in France, Germany, and particularly, Italy, under the influence of Francis of Asissi, a strong tradition of popular Christmas songs in the native language developed. Christmas carols in English first appear in a 1426 work of John Awdlay, a Shropshire chaplain, who lists twenty-five "caroles of Cristemas", probably sung by groups of wassailers, who went from house to house.
The songs we know specifically as carols were originally communal folk songs sung during celebrations such as "harvest tide" as well as Christmas. It was only later that carols began to be sung in church. Traditionally, carols have often been based on medieval chord patterns, and it is this that gives them their uniquely characteristic musical sound. Some carols like "Personent hodie", "Good King Wenceslas", and "The Holly and the Ivy" can be traced directly back to the Middle Ages. They are among the oldest musical compositions still regularly sung. "Adeste Fideles" (O Come all ye faithful) appears in its current form in the mid-18th century, although the words may have originated in the 13th century.
Singing of carols initially suffered a decline in popularity after the Protestant Reformation in northern Europe, although some Reformers, like Martin Luther, wrote carols and encouraged their use in worship. Carols largely survived in rural communities until the revival of interest in popular songs in the 19th century. The 18th-century English reformer Charles Wesley understood the importance of music to worship. In addition to setting many psalms to melodies, which were influential in the Great Awakening in the United States, he wrote texts for at least three Christmas carols. The best known was originally entitled "Hark! How All the Welkin Rings", later renamed "Hark! the Herald Angels Sing".
Felix Mendelssohn wrote a melody adapted to fit Wesley's words. In Austria in 1818 Mohr and Gruber made a major addition to the genre when they composed "Silent Night" for the St. Nicholas Church, Oberndorf. William Sandys' "Christmas Carols Ancient and Modern" (1833) contained the first appearance in print of many now-classic English carols, and contributed to the mid-Victorian revival of the festival.
Completely secular Christmas seasonal songs emerged in the late 18th century. "Deck The Halls" dates from 1784, and the American "Jingle Bells" was copyrighted in 1857. In the 19th and 20th century, African American spirituals and songs about Christmas, based in their tradition of spirituals, became more widely known. An increasing number of seasonal holidays songs were commercially produced in the 20th century, including jazz and blues variations. In addition, there was a revival of interest in early music, from groups singing folk music, such as The Revels, to performers of early medieval and classical music.
Traditional cuisine.
A special Christmas family meal is traditionally an important part of the holiday's celebration, and the food that is served varies greatly from country to country. Some regions, such as Sicily, have special meals for Christmas Eve, when 12 kinds of fish are served. In the United Kingdom and countries influenced by its traditions, a standard Christmas meal includes turkey or goose, meat, gravy, potatoes, vegetables, sometimes bread and cider. Special desserts are also prepared, such as Christmas pudding, mince pies, and fruit cake.
In Poland and other parts of eastern Europe and Scandinavia, fish often is used for the traditional main course, but richer meat such as lamb is increasingly served. In Germany, France, and Austria, goose and pork are favored. Beef, ham, and chicken in various recipes are popular throughout the world. The Maltese traditionally serve "Imbuljuta tal-Qastan", a chocolate and chestnuts beverage, after Midnight Mass and throughout the Christmas season. Slovaks prepare the traditional Christmas bread potica, "bûche de Noël" in France, "panettone" in Italy, and elaborate tarts and cakes. The eating of sweets and chocolates has become popular worldwide, and sweeter Christmas delicacies include the German "stollen", marzipan cake or candy, and Jamaican rum fruit cake. As one of the few fruits traditionally available to northern countries in winter, oranges have been long associated with special Christmas foods.
Cards.
Christmas cards are illustrated messages of greeting exchanged between friends and family members during the weeks preceding Christmas Day. The traditional greeting reads "wishing you a Merry Christmas and a Happy New Year", much like that of the first commercial Christmas card, produced by Sir Henry Cole in London in 1843. The custom of sending them has become popular among a wide cross-section of people with the emergence of the modern trend towards exchanging E-cards.
Christmas cards are purchased in considerable quantities, and feature artwork, commercially designed and relevant to the season. The content of the design might relate directly to the Christmas narrative with depictions of the Nativity of Jesus, or Christian symbols such as the Star of Bethlehem, or a white dove which can represent both the Holy Spirit and Peace on Earth. Other Christmas cards are more secular and can depict Christmas traditions, mythical figures such as Santa Claus, objects directly associated with Christmas such as candles, holly and baubles, or a variety of images associated with the season, such as Christmastide activities, snow scenes and the wildlife of the northern winter. There are even humorous cards and genres depicting nostalgic scenes of the past such as crinolined shoppers in idealized 19th century streetscapes.
Some prefer cards with a poem, prayer, or Biblical verse; while others distance themselves from religion with an all-inclusive "Season's greetings".
Commemorative stamps.
A number of nations have issued commemorative stamps at Christmastide. Postal customers will often use these stamps to mail Christmas cards, and they are popular with philatelists. These stamps are regular postage stamps, unlike Christmas seals, and are valid for postage year-round. They usually go on sale some time between early October and early December, and are printed in considerable quantities.
In 1898 a Canadian stamp was issued to mark the inauguration of the Imperial Penny Postage rate. The stamp features a map of the globe and bears an inscription "XMAS 1898" at the bottom. In 1937, Austria issued two "Christmas greeting stamps" featuring a rose and the signs of the zodiac. In 1939, Brazil issued four semi-postal stamps with designs featuring the three kings and a star of Bethlehem, an angel and child, the Southern Cross and a child, and a mother and child.
Both the US Postal Service and the Royal Mail regularly issue Christmas-themed stamps each year.
Gift giving.
The exchanging of gifts is one of the core aspects of the modern Christmas celebration, making it the most profitable time of year for retailers and businesses throughout the world. Gift giving was common in the Roman celebration of Saturnalia, an ancient festival which took place in late December and may have influenced Christmas customs. On Christmas, people exchange gifts based on the tradition associated with St. Nicholas, and the gifts of gold, frankincense, and myrrh which were given to the baby Jesus by the Magi.
Gift-bearing figures.
A number of figures are associated with Christmas and the seasonal giving of gifts. Among these are Father Christmas, also known as Santa Claus (derived from the Dutch for Saint Nicholas), Père Noël, and the Weihnachtsmann; Saint Nicholas or Sinterklaas; the Christkind; Kris Kringle; Joulupukki; Babbo Natale; Saint Basil; and Father Frost.
The best known of these figures today is red-dressed Santa Claus, of diverse origins. The name Santa Claus can be traced back to the Dutch "Sinterklaas", which means simply Saint Nicholas. Nicholas was Bishop of Myra, in modern-day Turkey, during the 4th century. Among other saintly attributes, he was noted for the care of children, generosity, and the giving of gifts. His feast on December 6 came to be celebrated in many countries with the giving of gifts.
Saint Nicholas traditionally appeared in bishop's attire, accompanied by helpers, inquiring about the behaviour of children during the past year before deciding whether they deserved a gift or not. By the 13th century, Saint Nicholas was well known in the Netherlands, and the practice of gift-giving in his name spread to other parts of central and southern Europe. At the Reformation in 16th–17th-century Europe, many Protestants changed the gift bringer to the Christ Child or "Christkindl", corrupted in English to Kris Kringle, and the date of giving gifts changed from December 6 to Christmas Eve.
The modern popular image of Santa Claus, however, was created in the United States, and in particular in New York. The transformation was accomplished with the aid of notable contributors including Washington Irving and the German-American cartoonist Thomas Nast (1840–1902). Following the American Revolutionary War, some of the inhabitants of New York City sought out symbols of the city's non-English past. New York had originally been established as the Dutch colonial town of New Amsterdam and the Dutch Sinterklaas tradition was reinvented as Saint Nicholas.
In 1809, the New-York Historical Society convened and retroactively named "Sancte Claus" the patron saint of Nieuw Amsterdam, the Dutch name for New York City. At his first American appearance in 1810, Santa Claus was drawn in bishops' robes. However as new artists took over, Santa Claus developed more secular attire. Nast drew a new image of "Santa Claus" annually, beginning in 1863. By the 1880s, Nast's Santa had evolved into the robed, fur clad, form we now recognize, perhaps based on the English figure of Father Christmas. The image was standardized by advertisers in the 1920s and continues through the present day: indeed, some have made a career out of portraying Santa Claus, particularly if they are slightly overweight middle aged men with beards and a jolly disposition.
Father Christmas, a jolly, well nourished, bearded man who typified the spirit of good cheer at Christmas, predates the Santa Claus character. He is first recorded in early 17th century England, but was associated with holiday merrymaking and drunkenness rather than the bringing of gifts. In Victorian Britain, his image was remade to match that of Santa. The French Père Noël evolved along similar lines, eventually adopting the Santa image. In Italy, Babbo Natale acts as Santa Claus, while La Befana is the bringer of gifts and arrives on the eve of the Epiphany. It is said that La Befana set out to bring the baby Jesus gifts, but got lost along the way. Now, she brings gifts to all children. In some cultures Santa Claus is accompanied by Knecht Ruprecht, or Black Peter. In other versions, elves make the toys. His wife is referred to as Mrs. Claus.
There has been some opposition to the narrative of the American evolution of Saint Nicholas into the modern Santa. It has been claimed that the Saint Nicholas Society was not founded until 1835, almost half a century after the end of the American War of Independence. Moreover, a study of the "children's books, periodicals and journals" of New Amsterdam by Charles Jones revealed no references to Saint Nicholas or Sinterklaas. However, not all scholars agree with Jones's findings, which he reiterated in a book-length study in 1978; Howard G. Hageman, of New Brunswick Theological Seminary, maintains that the tradition of celebrating Sinterklaas in New York was alive and well from the early settlement of the Hudson Valley on.
Current tradition in several Latin American countries (such as Venezuela and Colombia) holds that while Santa makes the toys, he then gives them to the Baby Jesus, who is the one who actually delivers them to the children's homes, a reconciliation between traditional religious beliefs and the iconography of Santa Claus imported from the United States.
In South Tyrol (Italy), Austria, Czech Republic, Southern Germany, Hungary, Liechtenstein, Slovakia, and Switzerland, the Christkind (Ježíšek in Czech, Jézuska in Hungarian and Ježiško in Slovak) brings the presents. Greek children get their presents from Saint Basil on New Year's Eve, the eve of that saint's liturgical feast. The German St. Nikolaus is not identical with the Weihnachtsmann (who is the German version of Santa Claus / Father Christmas). St. Nikolaus wears a bishop's dress and still brings small gifts (usually candies, nuts, and fruits) on December 6 and is accompanied by Knecht Ruprecht. Although many parents around the world routinely teach their children about Santa Claus and other gift bringers, some have come to reject this practice, considering it deceptive.
Date.
Irenaeus (c. 130–202) viewed Christ's conception as March 25 in association with the Passion, with the nativity nine months after on December 25. Hippolytus of Rome (170–235) may also have identified December 25 for the birth of Jesus and March 25 for the conception. Sextus Julius Africanus (c. 160–c. 240) identified December 25, later to become the most widely accepted date of celebration, as the date Jesus' birth in 221. The precise origin of assigning December 25 to the birth of Jesus is unclear. Various dates were speculated: May 20, April 18 or 19, March 25, January 2, November 17 or 20. When celebration on a particular date began, January 6 prevailed at least in the East; but, except among Armenians (the Armenian Apostolic Church and the Armenian Catholic Church), who continue to celebrate the birth on January 6, December 25 eventually won acceptance everywhere.
The New Testament Gospel of Luke may indirectly give the date as December for the birth of Jesus, with the sixth month of Elizabeth's pregnancy with John the Baptist cited by John Chrysostom (c. 386) as a date for the Annunciation. Tertullian (d. 220) did not mention Christmas as a major feast day in the Church of Roman Africa. In "Chronographai", a reference work published in 221, Sextus Julius Africanus suggested that Jesus was conceived on the spring equinox. The equinox was March 25 on the Roman calendar, so this implied a birth in December.Roll p. 87.</ref>
The birth of Jesus was announced in , "For unto you is born this day in the city of David a Saviour, which is Christ the Lord." Moreover, the belief that God came into the world in the form of man to atone for the sins of humanity is considered to be the primary purpose in celebrating Christmas.
In the early 4th century, the church calendar in Rome contained Christmas on December 25 and other holidays placed on solar dates. According to Hijmans "It is cosmic symbolism ... which inspired the Church leadership in Rome to elect the southern solstice, December 25, as the birthday of Christ, and the northern solstice as that of John the Baptist, supplemented by the equinoxes as their respective dates of conception." Usener and others proposed that the Christians chose this day because it was the Roman feast celebrating the birthday of Sol Invictus. Modern scholar S. E. Hijmans, however, states that "While they were aware that pagans called this day the 'birthday' of Sol Invictus, this did not concern them and it did not play any role in their choice of date for Christmas."
Around the year 386 John Chrysostom delivered a sermon in Antioch in favour of adopting the 25 December celebration also in the East, since, he said, the conception of Jesus () had been announced during the sixth month of Elisabeth's pregnancy with John the Baptist (), which he dated from the duties Zacharias performed on the Day of Atonement during the seventh month of the Hebrew calendar Ethanim or Tishri (, ) which falls from late September to early October. That shepherds watched the flocks by night in the fields in the winter time is supported by the phrase "frost by night" in . A special group known as the shepherds of Migdal Eder (, ) watched the flocks by night year round pastured for Temple Sacrifice near Bethlehem.
In the early 18th century, some scholars proposed alternative explanations. Isaac Newton argued that the date of Christmas, celebrating the birth of him whom Christians consider to be the "Sun of righteousness" prophesied in , was selected to correspond with the southern solstice, which the Romans called "bruma", celebrated on December 25.Pliny the Elder, Natural History, 18:59</ref> In 1743, German Protestant Paul Ernst Jablonski argued Christmas was placed on December 25 to correspond with the Roman solar holiday "Dies Natalis Solis Invicti" and was therefore a "paganization" that debased the true church. It has been argued that, on the contrary, the Emperor Aurelian, who in 274 instituted the holiday of the "Dies Natalis Solis Invicti", did so partly as an attempt to give a pagan significance to a date already important for Christians in Rome. In 1889, Louis Duchesne proposed that the date of Christmas was calculated as nine months after the Annunciation, the traditional date of the conception of Jesus.Duchesne, Louis, "Les Origines du Culte Chrétien," Paris, 1902, 262 ff.</ref>
Using the Julian calendars.
Eastern Orthodox national churches, including those of Russia, Georgia, Ukraine, Macedonia, Montenegro, Serbia, and the Greek Patriarchate of Jerusalem mark feasts using the older Julian calendar. December 25 on the Julian calendar currently corresponds to January 7 on the internationally used Gregorian calendar. However, other Orthodox Christians, such as the churches of Bulgaria, Greece, Romania, Antioch, Alexandria, Albania, Finland, and the Orthodox Church in America, among others, began using the Revised Julian calendar in the early 20th century, which corresponds exactly to the Gregorian calendar.
The original date of the celebration in Eastern Christianity was January 6, in connection with Epiphany, and that is still the date of the celebration for the Armenian Apostolic Church and in Armenia, where it is a public holiday. As of , there is a difference of 13 days between the modern Gregorian calendar and the older Julian calendar. Those who continue to use the Julian calendar or its equivalents thus celebrate December 25 and January 6, which on the Gregorian calendar translate as January 7 and January 19. For this reason, Egypt, Ethiopia, Eritrea, Russia, Georgia, Ukraine, Serbia, Montenegro, the Republic of Macedonia, and the Republic of Moldova celebrate Christmas on what in the Gregorian calendar is January 7. Eastern Orthodox Churches in Bulgaria, Greece, Romania, Antioch, Alexandria, Albania, Finland, and the Orthodox Church in America celebrate Christmas on December 25 in the revised Julian calendar, corresponding to December 25 also in the Gregorian calendar.
Economy.
Christmas is typically a peak selling season for retailers in many nations around the world. Sales increase dramatically as people purchase gifts, decorations, and supplies to celebrate. In the U.S., the "Christmas shopping season" starts as early as October. In Canada, merchants begin advertising campaigns just before Halloween (October 31), and step up their marketing following Remembrance Day on November 11. In the UK and Ireland, the Christmas shopping season starts from mid November, around the time when high street Christmas lights are turned on. In the United States, it has been calculated that a quarter of all personal spending takes place during the Christmas/holiday shopping season. Figures from the U.S. Census Bureau reveal that expenditure in department stores nationwide rose from $20.8 billion in November 2004 to $31.9 billion in December 2004, an increase of 54 percent. In other sectors, the pre-Christmas increase in spending was even greater, there being a November–December buying surge of 100 percent in bookstores and 170 percent in jewelry stores. In the same year employment in American retail stores rose from 1.6 million to 1.8 million in the two months leading up to Christmas. Industries completely dependent on Christmas include Christmas cards, of which 1.9 billion are sent in the United States each year, and live Christmas Trees, of which 20.8 million were cut in the U.S. in 2002. In the UK in 2010, up to £8 billion was expected to be spent online at Christmas, approximately a quarter of total retail festive sales.
In most Western nations, Christmas Day is the least active day of the year for business and commerce; almost all retail, commercial and institutional businesses are closed, and almost all industries cease activity (more than any other day of the year), whether laws require such or not. In England and Wales, the Christmas Day (Trading) Act 2004 prevents all large shops from trading on Christmas Day. Scotland is currently planning similar legislation. Film studios release many high-budget movies during the holiday season, including Christmas films, fantasy movies or high-tone dramas with high production values to hopes of maximizing the chance of nominations for the Academy Awards.
One economist's analysis calculates that, despite increased overall spending, Christmas is a deadweight loss under orthodox microeconomic theory, because of the effect of gift-giving. This loss is calculated as the difference between what the gift giver spent on the item and what the gift receiver would have paid for the item. It is estimated that in 2001, Christmas resulted in a $4 billion deadweight loss in the U.S. alone. Because of complicating factors, this analysis is sometimes used to discuss possible flaws in current microeconomic theory. Other deadweight losses include the effects of Christmas on the environment and the fact that material gifts are often perceived as white elephants, imposing cost for upkeep and storage and contributing to clutter.
Controversies.
Christmas has at times been the subject of controversy and attacks from various sources. A Puritan-led controversy began during the English Interregnum, when England was ruled by a Puritan Parliament. Puritans sought to remove the remaining pagan elements of Christmas. During this brief period, the Puritan-led English Parliament banned the celebration of Christmas entirely, considering it "a popish festival with no biblical justification", and a time of wasteful and immoral behavior. In Colonial America, the Puritans outlawed celebration of Christmas in 1659.
Some Christians and organizations such as Pat Robertson's American Center for Law and Justice cite alleged attacks on Christmas (dubbed a "war on Christmas"). One controversy is the occurrence of Christmas trees being renamed Holiday trees. In the United States there has been a tendency, in some contexts, to replace the greeting "Merry Christmas" with "Happy Holidays". Groups such as the American Civil Liberties Union have initiated court cases to bar the display of images and other material referring to Christmas from public property, including schools. Such groups argue that government-funded displays of Christmas imagery and traditions violate the First Amendment to the United States Constitution, which prohibits the establishment by Congress of a national religion. In 1984, the U.S. Supreme Court ruled in "Lynch vs. Donnelly" that a Christmas display (which included a Nativity scene) owned and displayed by the city of Pawtucket, Rhode Island, did not violate the First Amendment.
In November 2009, the Federal appeals court in Philadelphia endorsed a school district's ban on the singing of Christmas carols. The US Supreme Court declined to hear an appeal. In the private sphere also, it has been alleged that any specific mention of the term "Christmas" or its religious aspects was being increasingly censored, avoided, or discouraged by a number of advertisers and retailers. In response, the American Family Association and other groups have organized boycotts of individual retailers.
In the United Kingdom there have been some minor controversies, one of the most famous being Birmingham City Council's temporary promotion of a Christmas-period festival, not Christmas itself, as "Winterval" in 1998. Critics attacked the use of the word "Winterval" as political correctness gone mad, accusing council officials of trying to take the Christ out of Christmas. The council responded to the criticism by stating that Christmas-related words and symbols were prominent in its publicity material. There were also protests in November 2009 when the city council of Dundee promoted its celebrations as the "Winter Night Light festival", initially with no specific Christmas references.

</doc>
<doc id="6239" url="http://en.wikipedia.org/wiki?curid=6239" title="Contraction mapping">
Contraction mapping

In mathematics, a contraction mapping, or contraction or contractor, on a metric space "(M,d)" is a function "f" from "M" to itself, with the property that there is some nonnegative real number formula_1 such that for all "x" and "y" in "M",
The smallest such value of "k" is called the Lipschitz constant of "f". Contractive maps are sometimes called Lipschitzian maps. If the above condition is instead satisfied for
"k" ≤ 1, then the mapping is said to be a non-expansive map.
More generally, the idea of a contractive mapping can be defined for maps between metric spaces. Thus, if ("M","d") and ("N","d"') are two metric spaces, and formula_3, then there is a constant formula_4 such that
for all "x" and "y" in "M".
Every contraction mapping is Lipschitz continuous and hence uniformly continuous (for a Lipschitz continuous function, the constant "k" is no longer necessarily less than 1).
A contraction mapping has at most one fixed point. Moreover, the Banach fixed point theorem states that every contraction mapping on a nonempty complete metric space has a unique fixed point, and that for any "x" in "M" the iterated function sequence "x", "f" ("x"), "f" ("f" ("x")), "f" ("f" ("f" ("x"))), ... converges to the fixed point. This concept is very useful for iterated function systems where contraction mappings are often used. Banach's fixed point theorem is also applied in proving the existence of solutions of ordinary differential equations, and is used in one proof of the inverse function theorem.
Firmly non-expansive mapping.
A non-expansive mapping with formula_6 can be strengthened to a firmly non-expansive mapping in a Hilbert space "H" if the following holds for all "x" and "y" in "H":
where
This is a special case of formula_9 averaged nonexpansive operators with formula_10. A firmly non-expansive mapping is always non-expansive, via the Cauchy–Schwarz inequality.
Subcontraction map.
A subcontraction map or subcontractor is a map "f" on a metric space ("M","d") such that
If the image of a subcontractor "f" is compact, then "f" has a fixed point.

</doc>
<doc id="6246" url="http://en.wikipedia.org/wiki?curid=6246" title="Covalent bond">
Covalent bond

A covalent bond is a chemical bond that involves the sharing of electron pairs between atoms. The stable balance of attractive and repulsive forces between atoms when they share electrons is known as covalent bonding. For many molecules, the sharing of electrons allows each atom to attain the equivalent of a full outer shell, corresponding to a stable electronic configuration. 
Covalent bonding includes many kinds of interactions, including σ-bonding, π-bonding, metal-to-metal bonding, agostic interactions, and three-center two-electron bonds. The term "covalent bond" dates from 1939. The prefix "co-" means "jointly, associated in action, partnered to a lesser degree, " etc.; thus a "co-valent bond", in essence, means that the atoms share "valence", such as is discussed in valence bond theory. In the molecule , the hydrogen atoms share the two electrons via covalent bonding. Covalency is greatest between atoms of similar electronegativities. Thus, covalent bonding does not necessarily require that the two atoms be of the same elements, only that they be of comparable electronegativity. Covalent bonding that entails sharing of electrons over more than two atoms is said to be delocalized.
History.
The term "covalence" in regard to bonding was first used in 1919 by Irving Langmuir in a "Journal of the American Chemical Society" article entitled "The Arrangement of Electrons in Atoms and Molecules". Langmuir wrote that "we shall denote by the term covalence the number of pairs of electrons that a given atom shares with its neighbors."
The idea of covalent bonding can be traced several years before 1919 to Gilbert N. Lewis, who in 1916 described the sharing of electron pairs between atoms. He introduced the "Lewis notation" or "electron dot notation" or "Lewis dot structure", in which valence electrons (those in the outer shell) are represented as dots around the atomic symbols. Pairs of electrons located between atoms represent covalent bonds. Multiple pairs represent multiple bonds, such as double bonds and triple bonds. An alternative form of representation, not shown here, has bond-forming electron pairs represented as solid lines.
Lewis proposed that an atom forms enough covalent bonds to form a full (or closed) outer electron shell. In the methane diagram shown here, the carbon atom has a valence of four and is, therefore, surrounded by eight electrons (the octet rule), four from the carbon itself and four from the hydrogens bonded to it. Each hydrogen has a valence of one and is surrounded by two electrons (a duet rule) - its own one electron plus one from the carbon. The numbers of electrons correspond to full shells in the quantum theory of the atom; the outer shell of a carbon atom is the n=2 shell, which can hold eight electrons, whereas the outer (and only) shell of a hydrogen atom is the n=1 shell, which can hold only two.
While the idea of shared electron pairs provides an effective qualitative picture of covalent bonding, quantum mechanics is needed to understand the nature of these bonds and predict the structures and properties of simple molecules. Walter Heitler and Fritz London are credited with the first successful quantum mechanical explanation of a chemical bond (molecular hydrogen) in 1927. Their work was based on the valence bond model, which assumes that a chemical bond is formed when there is good overlap between the atomic orbitals of participating atoms. These atomic orbitals are known to have specific angular relationships between each other, and thus the valence bond model can successfully predict the bond angles observed in simple molecules.
Polarity of covalent bonds.
Covalent bonds are affected by the electronegativity of the connected atoms. Two atoms with equal electronegativity will make nonpolar covalent bonds such as H–H. An unequal relationship creates a polar covalent bond such as with H−Cl.
Subdivision of covalent bonds.
There are three types of covalent substances: individual molecules, molecular structures, and macromolecular structures. Individual molecules have strong bonds that hold the atoms together, but there are negligible forces of attraction between molecules. Such covalent substances are usually gases, for example, HCl, SO2, CO2, and CH4. In molecular structures, there are weak forces of attraction. Such covalent substances are low-boiling-temperature liquids (such as ethanol), and low-melting-temperature solids (such as iodine and solid CO2). Macromolecular structures have large numbers of atoms linked in chains or sheets (such as graphite), or in 3-dimensional structures (such as diamond and quartz). These substances have high melting and boiling points, are frequently brittle, and tend to have high electrical resistivity. Elements that have high electronegativity, and the ability to form three or four electron pair bonds, often form such large macromolecular structures.

</doc>
<doc id="6247" url="http://en.wikipedia.org/wiki?curid=6247" title="Condensation polymer">
Condensation polymer

Condensation polymers are any kind of polymers formed through a condensation reaction—where molecules join together--"losing" small molecules as by-products such as water or methanol, as opposed to addition polymers which involve the reaction of unsaturated monomers. Types of condensation polymers include polyamides, polyacetals and polyesters.
Condensation polymerization, a form of step-growth polymerization, is a process by which two molecules join together, resulting loss of small molecules which is often water. The type of end product resulting from a condensation polymerization is dependent on the number of functional end groups of the monomer which can react.
Monomers with only one reactive group terminate a growing chain, and thus give end products with a lower molecular weight. Linear polymers are created using monomers with two reactive end groups and monomers with more than two end groups give three-dimensional polymers which are crosslinked.
Dehydration synthesis often involves joining monomers with an -OH (hydroxyl) group and a freely ionized -H on either end (such as a hydrogen from the -NH2 in nylon or proteins). Normally, two or more different monomers are used in the reaction. The bonds between the hydroxyl group, the hydrogen atom and their respective atoms break forming water from the hydroxyl and hydrogen, and the polymer.
Polyester is created through ester linkages between monomers, which involve the functional groups carboxyl and hydroxyl (an organic acid and an alcohol monomer).
Nylon is another common condensation polymer. It can be manufactured by reacting di-amines with carboxyl derivatives. In this example the derivative is a di-carboxylic acid, but di-acyl chlorides are also used. Another approach used is the reaction of di-functional monomers, with one amine and one carboxylic acid group on the same molecule:
The carboxylic acids and amines link to form peptide bonds, also known as amide groups. Proteins are condensation polymers made from amino acid monomers. Carbohydrates are also condensation polymers made from sugar monomers such as glucose and galactose.
Condensation polymerization is occasionally used to form simple hydrocarbons. This method, however, is expensive and inefficient, so the addition polymer of ethene (polyethylene) is generally used.
Condensation polymers, unlike addition polymers, may be biodegradable. The peptide or ester bonds between monomers can be hydrolysed by acid catalysts or bacterial enzymes breaking the polymer chain into smaller pieces.
The most commonly known condensation polymers are proteins, fabrics such as nylon, silk, or polyester.

</doc>
<doc id="6249" url="http://en.wikipedia.org/wiki?curid=6249" title="Timeline of computing">
Timeline of computing

Timeline of computing presents events in the history of computing organized by year and grouped into six topic areas: predictions and concepts, first use and inventions, hardware systems and processors, operating systems, programming languages, and new application areas. More detailed timelines are listed toward the end of the article.
__TOC__

</doc>
<doc id="6250" url="http://en.wikipedia.org/wiki?curid=6250" title="Colorado Springs, Colorado">
Colorado Springs, Colorado

Colorado Springs is a Home Rule Municipality that is the county seat and most populous city of El Paso County, Colorado, United States. Colorado Springs is located in the east central portion of the state. It is situated on Fountain Creek and is located south of the Colorado State Capitol in Denver. At the city stands over one mile (1.6 km) above sea level, though some areas of the city are significantly higher and lower. Colorado Springs is situated near the base of one of the most famous American mountains, Pikes Peak, rising over 8,000 feet above the city on the eastern edge of the Southern Rocky Mountains. The United States Air Force Academy is located in Colorado Springs. The city is often referred to as "The Springs."
The city had an estimated population of 439,886 in 2013., making it the second most populous city in the state of Colorado, behind Denver, and the 41st most populous city in the United States, while the Colorado Springs Metropolitan Statistical Area had an estimated population of 678,319 in 2013. The city covers , making it Colorado's largest city in area. Colorado Springs was selected as the No. 1 Best Big City in "Best Places to Live" by "Money" magazine in 2006, and placed number one in "Outside"'s 2009 list of America's Best Cities.
History.
Ute, Arapahoe and Cheyenne peoples were the first to use the area which would become Colorado Springs. Part of the territory included in the United States' 1803 Louisiana Purchase, the current city area was designated part of the 1854 Kansas Territory. In 1859 after the first local settlement was established, it became part of the Jefferson Territory on October 24 and of El Paso County on November 28. Colorado City at the Front Range confluence of Fountain and Camp creeks was "formally organized on August 13, 1859" during the Pikes Peak Gold Rush. It served as the capital of the Colorado Territory from November 5, 1861, until August 14, 1862 when it was moved to the Denver City.
In 1871 the Colorado Springs Company laid out the towns of La Font (later called Manitou Springs) and Fountain Colony, upstream and downstream respectively, of Colorado City. Within a year, Fountain Colony would be renamed Colorado Springs, and was officially incorporated. The El Paso County seat shifted from Colorado City in 1873 to the Town of Colorado Springs. On December 1, 1880, Colorado Springs expanded northward with 2 annexations. 
The 2nd period of annexations was during 1889-90, and included Seavey's Addition, West Colorado Springs, East End, and another North End addition. In 1891 the Broadmoor Land Company built the Broadmoor suburb, which included the Broadmoor Casino, and by December 12, 1895, the city had "four Mining Exchanges and 275 mining brokers." By 1898, the city was designated into quadrants by the north-south Cascade Avenue and the east-west Washington/Pike's Peak avenues.
From 1899 to 1901 Tesla Experimental Station operated on Knob Hill, and aircraft flights to the Broadmoor's neighboring fields began in 1919. The Alexander Airport north of the city opened in 1925 and in 1927, the Original Colorado Springs Municipal Airport land was purchased east of the city.
In World War II the United States Army Air Forces leased land adjacent to the municipal airfield, naming it "Peterson Field" in December 1942. This was only one of several military presences in and around Colorado Springs during the war. "fashionable suburb".
In November 1950, the Headquarters Area, Colorado Springs, was selected as the Cold War headquarters for Air Defense Command (ADC)--later renamed Ent Air Force Base. The former WWII Army Air Base, Peterson Field, which had been inactivated at the end of the war, was re-opened in 1951 as a USAF base. The 1950s through 1970s saw a continued expansion of the military presence in the area with the establishment of NORAD’s headquarters in the city, as well as the ADCOM Headquarters.
Between 1965 and 1968 the University of Colorado at Colorado Springs, Pikes Peak Community College and the Colorado Technical University were established in or near the city In 1977, most of the former Ent AFB became a US Olympic Training Center.
On October 1, 1981, "the Broadmoor Addition", Cheyenne Canon, Ivywild, Skyway, and Stratton Meadows were annexed after the Colorado Supreme Court "overturned a district court decision that voided the annexation". Further annexations expanding the city include the "Nielson Addition" and "Vineyard Commerce Park Annexation" in September 2008.
Geography.
The city lies in a high desert with the Southern Rocky Mountains to the west, the Palmer Divide to the north, high plains further east, and high desert lands to the south when leaving Fountain and approaching Pueblo.
According to the United States Census Bureau, the city has a total area of , of which is land and (0.21%) is water.
Metropolitan area.
Colorado Springs has many features of a modern urban area, such as parks, bike trails, and urban open-area spaces. However, it is not exempt from problems that typically plague cities that experience tremendous growth, such as overcrowded roads and highways, crime, sprawl, and government budget issues. Many of the problems are indirectly or directly caused by the city's difficulty in coping with the large population growth experienced in the last twenty years, and the annexation of the Banning Lewis Ranch area to accommodate further population growth of 175,000 future residents.
Climate.
Colorado Springs has a semi-arid climate (Köppen "BSk"), and its location just east of the Southern Rocky Mountains affords it the rapid warming influence from chinook winds during winter but also subjects it to drastic day-to-day variability in weather conditions. The city has abundant sunshine throughout the year, averaging over 300 days of sun per year, and receives approximately of annual precipitation. Due to unusually low precipitation for several years after flooding in 1999, Colorado Springs enacted lawn water restrictions in 2002. These were lifted in 2005.
Colorado Springs is also one of the most active lightning strike areas in the United States. This natural phenomenon led Nikola Tesla to select Colorado Springs as the preferred location to build his lab and study electricity.
Seasonal climate.
Winters are moderately cold, with December, the coldest month, averaging ; historically January has been the coldest month, but, in recent years, December has had both lower daily maxima and minima. Typically, there are 5.2 nights with sub- lows and 23.6 days where the high does not rise above freezing, and extended sub-zero (°F) cold is rare. Snowfall is usually moderate and remains on the ground briefly because of direct sun, with the city receiving per season, although the mountains to the west often receive in excess of triple that amount; March is the snowiest month in the region, both by total accumulation and number of days with measurable snowfall. In addition, 8 of the top 10 heaviest 24-hour snowfalls have occurred from March to May. Summers are warm, with July, the warmest month, averaging , and 18 days of + highs annually. Due to the high elevation and aridity, nights are usually relatively cool and rarely does the low remain above . Dry weather generally prevails, but brief afternoon thunderstorms are common, especially in July and August when the city receives the majority of its annual rainfall, due to the North American Monsoon.
The first freeze in the autumn and the last freeze in the spring on average occur on October 2 and May 6, respectively; the average window for measurable snowfall (≥) is October 21 thru April 25. Extreme temperatures range from on June 26, 2012 down to on February 1, 1951 and December 9, 1919.
Demographics.
As of the 2010 census, the population of Colorado Springs was 416,427 (41st most populous U.S. city), and the population of the Colorado Springs Metropolitan Statistical Area was 645,613 in 2010 (84th most populous MSA), and the population of the Front Range Urban Corridor in Colorado was an estimated 4,166,855.
As of the April 2010 census: 78.8% White, 16.1% Hispanic or Latino (of any race), 6.3% Black or African American, 3.0% Asian, 1.0% Native American, 0.3% Native Hawaiian and Other Pacific Islander, 
5.5% Some other race, 5.1% Two or more races. Mexican Americans made up 14.6% of the city's population. Non-Hispanic Whites were 70.7% of the population, compared to 86.6% in 1970.
Economy.
Colorado Springs' economy is driven primarily by the military, the high-tech industry, and tourism, in that order. The city is currently experiencing some growth mainly in the service sectors. The current unemployment rate, as of November 2013, in Colorado Springs is 7.3% compared to 6.5% for the State and 7.0% for the Nation.
Defense industry.
The defense industry plays a major role in the Colorado Springs economy, with some of the city's largest employers coming from the sector. A large segment of this industry is dedicated to the development and operation of various projects for missile defense. With its close ties to defense, the aerospace industry has also influenced the Colorado Springs economy. Although some defense corporations have left or downsized city campuses, a slight growth trend is still recorded. Significant defense corporations in the city include Boeing, General Dynamics, Harris Corporation, SAIC, ITT, L-3 Communications, Lockheed Martin, and Northrop Grumman. The Space Foundation is based in Colorado Springs.
High-tech industry.
A large percentage of Colorado Springs' economy is still based on manufacturing high tech and complex electronic equipment. The high tech sector in the Colorado Springs area has decreased its overall presence from 2000 to 2006 (from around 21,000 down to around 8,000), with notable reductions in information technology and complex electronic equipment. Due to a slowing in tourism, the high tech sector still remains second to the military in terms of total revenue generated and employment. Current trends project the high tech employment ratio will continue to decrease in the near future.
High tech corporations with connections to the city include:
Verizon Business, a telecommunications firm, had nearly 1300 employees in 2008. Hewlett-Packard has a large sales, support, and SAN storage engineering center for the computer industry.
Storage Networking Industry Association is the home of the SNIA Technology Center. Agilent, spun off from HP in 1999 as an independent, publicly traded company. Intel had 250 employees in 2009. The facility is now used for the centralized unemployment and social services complex.
LSI Corporation designs semiconductors and software that accelerate storage and networking in datacenters and mobile networks. Atmel (formerly Honeywell), is a chip fabrication organization. Cypress Semiconductor Colorado Design Center is a chip fabrication research and development site. The Apple Inc. facility was sold to Sanmina-SCI in 1996.
Top employers.
According to the City's 2011 Comprehensive Annual Financial Report, the top employers in the city are:
Military installations.
Colorado Springs is home to both Army and Air Force bases. These military installations border the city, to the north, south and east, aside from Schriever Air Force Base, which is located farther east of the city, still in El Paso County.
Fort Carson.
Fort Carson is the city's largest military base, and until mid-2006 was home to the 3d Armored Cavalry Regiment, which relocated to Fort Hood, Texas. In 2009, Fort Carson became the home station of the 4th Infantry Division, which nearly doubled the base's population. Fort Carson is host to various training grounds for infantry, armor, and aviation units. Fort Carson is also the headquarters of the second and third battalions of the 10th Special Forces Group.
Peterson Air Force Base.
The Air Force has critical aspects of their service based at Colorado Springs which carry on missile defense operations and development. The Air Force bases a large section of the national missile defense operations here, with Peterson Air Force Base set to operate large sections of the program. Peterson AFB is currently the headquarters of the majority of Air Force Space Command and the operations half of Army Space and Missile Defense Command/Army Strategic Command (SMDC/ARSTRAT).
Peterson is also headquarters for the United States Northern Command (USNORTHCOM), one of the Unified Combatant Commands. USNORTHCOM directs all branches of the U.S. military operations in their area of responsibility which includes the continental United States, Alaska, Canada, and Mexico. In the event of national emergencies the President or Secretary of Defense can call upon USNORTHCOM for any required military assistance. Service members from every branch of the US Military are stationed at the command.
Schriever Air Force Base (formerly Falcon AFB).
Schriever Air Force Base is home to the 50th Space Wing, which controls warning, navigational, communications and spy satellites. It is also the home of the Space Warfare Center and the home for the 576th Flight Test Squadron. It is the location of the Global Positioning System (GPS) master control station and GPS Operations Center and the US Naval Observatory Alternate Master Clock, used to synchronize GPS satellite time. Schriever is also developing parts of national missile defense and runs parts of the annual wargames used by the nation's military.
United States Air Force Academy.
Bordering the north-western side of the city lie the vast grounds of the United States Air Force Academy, where cadets train to become officers in the Air Force. The campus is famous for its unique chapel and draws visitors year round. Most of the Air Force Academy's sports programs belong to the Mountain West Conference.
NORAD and Cheyenne Mountain Air Station.
The North American Aerospace Defense Command (NORAD), a component of America's missile defense system, is located in Cheyenne Mountain Air Station. When it was built at the height of the Cold War, NORAD caused some anxiety for the residents in and around Colorado Springs, who believed the installation would be a primary target during a nuclear attack. Although NORAD still operates today, it is primarily tasked with the tracking of ICBMs, and the military has recently decided to place Cheyenne Mountain's NORAD/NORTHCOM operations on warm standby and move operations to nearby Peterson Air Force Base.
Culture.
Tourism.
The city's location at the base of Pikes Peak and the Rocky Mountains makes it a popular tourism destination. Tourism is the third largest employer in the Pikes Peak region, accounting for more than 13,000 jobs. Nearly 5 million visitors come to the area annually, contributing $1.35 billion in revenue.
Colorado Springs has more than 55 attractions and activities in the area, including Garden of the Gods, United States Air Force Academy, the ANA Money Museum, Cheyenne Mountain Zoo, Colorado Springs Fine Arts Center, Old Colorado City and the U.S. Olympic Training Center.
The downtown Colorado Springs Visitor Information Center offers free area information to leisure and business travelers. The Cultural Office of the Pikes Peak Region (COPPeR), also located downtown, supports and advocates for the arts throughout the Pikes Peak Region. It operates the PeakRadar website to communicate city events.
Religious institutions.
Although houses of worship of almost every major world religion can be found in the city, Colorado Springs has in particular attracted a large influx of Evangelical Christians and Christian organizations in recent years. At one time Colorado Springs was counted to be the national headquarters for 81 different religious organizations, earning the city the tongue-in-cheek nicknames "the Evangelical Vatican" and "The Christian Mecca." Religious groups with regional or international headquarters in Colorado Springs include:
In popular culture.
Colorado Springs has been the subject or setting for many books, movies and television shows, and is especially a frequent backdrop for political thrillers and military-themed stories because of its many military installations and vital importance to the United States' continental defense. Notable television series using the city as a setting include "Dr. Quinn, Medicine Woman" and the "Stargate" series "Stargate SG-1", as well as the films "WarGames" and "The Prestige".
In a North Korean propaganda video released in April 2013, Colorado Springs was inexplicably singled out as one of four targets for a missile strike. The video failed to pinpoint Colorado Springs on the map, instead showing a spot somewhere in Louisiana.
Sports.
Olympic sports.
Colorado Springs is home to the United States Olympic Training Center and the headquarters of the United States Olympic Committee. In addition, a number of United States national federations for individual Olympic sports have their headquarters in Colorado Springs, including: United States or USA bobsled, fencing, skating, basketball, boxing, cycling, judo, field hockey, hockey, swimming, shooting, table tennis, taekwondo, triathlon, volleyball, and
wrestling associations and organizations and the United States Anti-Doping Agency.
The city has a particularly long association with the sport of figure skating, having hosted the U.S. Figure Skating Championships 6 times and the World Figure Skating Championships 5 times. It is home to the World Figure Skating Museum and Hall of Fame and the Broadmoor Skating Club, a notable training center for the sport. In recent years, the World Arena has hosted skating events such as Skate America and the Four Continents Figure Skating Championships.
Local teams.
College teams.
The local colleges feature many sports teams. Notable among them are the following nationally competitive NCAA Division I teams: United States Air Force Academy (Fighting Falcons) Football, Basketball and Hockey, Colorado College (Tigers) Hockey, and Women's Soccer.
Colorado Springs and Denver hosted the 1962 International Ice Hockey Federation World Championships.
The Mountain West Conference is based in Colorado Springs.
Rodeo.
Colorado Springs is home to the Pro Rodeo Hall of Fame and the headquarters of the Professional Rodeo Cowboys Association.
Colorado Springs was the original headquarters of the Professional Bull Riders (PBR) from its founding in 1992 until 2005, when the organization was moved to Pueblo; the PBR used to hold an annual Built Ford Tough Series event at the World Arena from 2001 until 2005 when the organization made the move to Pueblo.
Parks, trails and open space.
There are 136 neighborhood, 8 community, 7 regional parks and 5 sports complexes totally 9,000 acres managed by the city's Parks, Recreation and Cultural Services. They also manage 500 acres of trails, which are 160 miles of park trails and 105 miles of urban trails. There are 5,000 acres of open spaces in 48 open space areas.
Parks.
One of the most popular areas in Colorado Springs is Garden of the Gods. It is a National Natural Landmark with 300 foot sandstone rock formations against a backdrop of the snow-capped mountains of Pikes Peak. The park offers a variety of annual events. One of the most popular events is the Starlight Spectacular. It is a recreational bike ride held every summer to benefit the Trails and Open Space Coalition of Colorado Springs.
Colorado Springs has several major parks, such as Palmer Park, America the Beautiful Park (Confluence Park), Memorial Park, and Monument Valley Park. The Austin Bluffs Park also affords a place of recreation in eastern Colorado Springs.
Trails.
Three trails, the New Santa Fe Regional Trail, Pikes Peak Greenway and Fountain Creek Regional Trail, form a continuous path from Palmer Lake, through Colorado Springs, to Fountain, Colorado. The Urban Trails system has more than 100 miles of multi-use trails for biking, jogging, roller blading and walking. The trails, except Monument Valley Park trails, may be used for equestrian traffic. Motorized vehicles are not allowed on the trails. Many of the trails are interconnected, having main "spine" trails, like the Pikes Peak Greenway, that lead to secondary trails.
Government.
On November 2, 2010 Colorado Springs voters adopted a council-strong mayor form of government. The City of Colorado Springs transitioned to the new system of government in 2011. Under the council-strong mayor system of government, the mayor is the chief executive and the city council is the legislative branch. The mayor is a full-time elected position and not a member of the city council. The city council has nine members total, four of which represent one of four equally populated districts each. Districts 5 and 6 do not have a direct representative. The remaining five members are elected "at-large". The mayor has veto authority, with the city council having the ability to override a mayoral veto by a two-thirds majority vote (6 out of 9).
Colorado Springs City Hall was built from 1902 to 1904 on land donated by W. S. Stratton.
Education.
Elementary and middle schools.
Public schools
The city's public schools are divided into several districts: 
Private schools
Higher education.
Bachelors and graduate degree programs are offered at these colleges and universities in the city:
The United States Air Force Academy is a military school for officer candidates.
IntelliTec College is a technical training school. Pikes Peak Community College offers a two year degree program.
Transportation.
Colorado Springs is served by a bus system called Mountain Metro (short for Mountain Metropolitan Transit). Although the transit system serves much of the city and its nearest suburbs (Manitou Springs and Security/Widefield), it lacks service to many important areas (Powers Blvd, Northgate, the Airport) and has only limited hours of operation.
Colorado Springs is served by the Colorado Springs Municipal Airport. In the state of Colorado, only Denver International has more passenger traffic. The airport has experienced a higher recovery rate in the post-9/11 era than the rest of the country and is in the process of expanding its maintenance facilities, taxiways, and runways to accommodate future growth. In 2005 it served approximately two million passengers.
Colorado Springs is part of a consortium of cities trying to build the Front Range Commuter Rail.
Major highways and roads.
Interstate highways.
Colorado Springs is primarily served by two interstate highways. I-25 runs north and south through Colorado, and is in the city for nearly 18 miles, entering the city south of Circle Drive and exiting north of North Gate Blvd. In El Paso County it is known as Ronald Reagan Highway. US 24 runs across the central mountains, through the city, and onto the plains. From west to east in Colorado Springs, US 24 follows the western portion of Cimarron Street and the Midland Expressway, a 2-mile concurrent section with I-25/US 87 between exits 139 and 141, part of Fountain Blvd, an expressway called the Martin Luther King Bypass, part of South Powers Blvd (where it is concurrent with Colorado 21), and the easternmost portion of Platte Avenue out of the city.
State highways.
A number of state highways serve the city. State Highway 21 is a major east side semi-expressway from Black Forest to Fountain. It is widely known as Powers Boulevard. State Highway 83 runs north-south from Denver to northern Colorado Springs. State Highway 94 runs east-west from western Cheyenne County to eastern Colorado Springs. State Highway 115 begins in Cañon City and runs up Nevada Avenue. US 85 and SH 115 are concurrent between Lake Avenue and I-25. US 85 enters the city at Fountain and was signed at Venetucci Blvd, Lake Avenue, and Nevada Avenue.
County and city roads.
In 2004, the voters of Colorado Springs and El Paso County established the Pikes Peak Rural Transportation Authority and adopted a 1% sales tax dedicated to improving the region's transportation infrastructure. Together with state funding for the Colorado Springs Metro Interstate Expansion (COSMIX) (2007 completion) and the I-25 interchange with Highway 16 (2008 completion), significant progress has been made since 2003 in addressing the transportation needs of the area. Currently the City is trying to overcome a $23.3 million budget gap created by falling sales taxes and rising expenses.
Several suggestions have been made to create a loop around the city though none have been implemented. To manage congestion, the city implemented two graded separated intersections at Powers and Woodmen and at Austin Bluffs and Union. A third interchange was completed in 2011 at the Woodmen Road/Academy Boulevard intersection.
In early 2010, the city of Colorado Springs approved an expansion of the northernmost part of Powers Boulevard in order to create an Interstate 25 bypass commonly referred to as the Copper Ridge Expansion.
Walkability.
A 2011 study by Walk Score ranked Colorado Springs 34th most walkable of fifty largest U.S. cities.
Sister cities.
Sister cities of Colorado Springs include:
Colorado Springs' sister city organization began when Colorado Springs became partners with Fujiyoshida. The "torii" gate erected to commemorate the relationship stands at the corner of Bijou Street and Nevada Avenue, and is one of the city's most recognizable landmarks. The "torii" gate, crisscrossed bridge and shrine, located in the median between Platte and Bijou Streets in downtown Colorado Springs, were a gift to Colorado Springs, erected in 1966 by the Rotary Club of Colorado Springs to celebrate the friendship between the two communities. A plaque near the "torii" gate states that "the purpose of the sister city relationship is to promote understanding between the people of our two countries and cities". The Fujiyoshida Student exchange program has become an annual event.
To strengthen relations between the two cities, the Colorado Springs Youth Symphony regularly invites the Taiko drummers from the city to participate in a joint concert in the Pikes Peak Center. The orchestra played in Bankstown, Australia, in 2002 and again in June 2006 as part of their tours to Australia and New Zealand.
Also, in 2006 and 2010, the Bankstown TAP (Talent Advancement Program), performed with the Youth Symphony, and the Colorado Springs Children's Chorale, as a part of the annual "In Harmony" program.
A notable similarity between Colorado Springs and its sister cities are their geographic positions: three of the seven cities are also located near the base of a major mountain or mountain range.
References.
Explanatory notes
 
Citations

</doc>
<doc id="6251" url="http://en.wikipedia.org/wiki?curid=6251" title="Professional certification">
Professional certification

Professional certification, trade certification, or professional designation, often called simply "certification" or "qualification", is a designation earned by a person to assure qualification to perform a job or task. Not all certifications that use post-nominal letters are an acknowledgement of educational achievement, or an agency appointed to safeguard the public interest.
Overview.
Certifications are earned from a professional society, university, or from a private certifier, for some specific certifications (e.g., Microsoft, Cisco, etc.). Some certifications must be renewed periodically, or may be valid for a specific period of time (e.g., the lifetime of the product upon which the individual is certified). As a part of a complete renewal of an individual's certification, it is common for the individual to show evidence of continued learning—often termed continuing education—or earning continuing education units (CEU).
Many certification programs are created, sponsored, or affiliated with professional associations, trade organizations, or private vendors interested in raising standards. Many of those programs completely independent from membership organizations enjoy association support and endorsement.
It is important to note that certifications are usually earned from a professional society or educational institute, not the government. However, a government agency can decree a certification is required "by law" in order to being allowed to perform a task or job. Certification is different of professional licensure. In the United States, professional licenses are usually issued by state agencies, having as a requirement the university title for that profession. In other countries, licensing is granted by the professional society or college, but you need to certificate after some years (usually three to five) and so on thereafter. The certification assessment process, for some organizations, is very similar or even the same as licensure and may differ only in terms of legal status, while in other organizations, can be quite different and more comprehensive than that of licensure.
The American National Standards Institute (ANSI), Standard 1100, defines the requirements of meeting the ANSI standard for being a certifying organization. According to ANSI Standard 1100, a professional certifying organization must meet two requirements:
Certifications are very common in aviation, construction, technology, environment, and other industrial sectors, as well as health care, business, Real estate broker and finance. In the United States, the Federal Aviation Administration regulates aviator certifications.
The Institute for Credentialing Excellence (ICE) is a U.S.-based organization that sets rigorous standards for accreditation of certification programs based on the Standards for Educational and Psychological Testing (APA, AERA, NCME). Many members of the Association of Test Publishers (ATP) are also certification organizations.k
Types of certifications.
There are three general types of certification. Listed in order of development level and portability, they are: corporate (internal), product-specific, and profession-wide.
Corporate, or "internal" certifications, are made by a corporation or low-stakes organization for internal purposes. For example, a corporation might require a one-day training course for all sales personnel, after which they receive a certificate. While this certificate has limited portability – to other corporations, for example – it is the most simple to develop.
Product-specific certifications are more involved, and are intended to be referenced to a product across all applications. This approach is very prevalent in the information technology (IT) industry, where personnel are certified on a version of software or hardware. This type of certification is portable across locations (for example, different corporations that use that software), but not across other products. Another example could be the certifications issued for shipping personnel, which are under international standards even for the recognition of the certification body, under an International Maritime Organization (IMO).
The most general type of certification is profession-wide. Certification in the medical profession is often offered by particular specialties. In order to apply professional standards, increase the level of practice, and protect the public, a professional organization might establish a certification. This is intended to be portable to all places a certified professional might work. Of course, this generalization increases the cost of such a program; the process to establish a legally defensible assessment of an entire profession is very extensive. An example of this is a Certified Public Accountant (CPA), which would not be certified for just one corporation or one piece of accountancy software but for general work in the profession.
Professional certificates awarded by universities.
Many universities grant professional certificates as an award for the completion of an educational program. The curriculum of a professional certificate is most often in a focused subject matter. The typical professional certificate program is between 200-300 class-hours in size. It is uncommon for a program to be larger or smaller than that. Most professional certificate programs are open enrollment, but some have admissions processes. A few universities put some of their professional certificates into a subclass they refer to as advanced professional certificates.
Some universities that offer extensive ranges of professional certificates include Duke, Georgetown, Harvard, UC Berkeley, and UC San Diego.
Areas of certification.
Accountancy, auditing and finance.
There are many professional bodies for accountants and auditors throughout the world; some of them are legally recognized in their jurisdictions.
Public Accountants are the accountancy and control experts that are legally certified in different jurisdictions to work in public practices, certifying accounts as statutory auditors, eventually selling advice and services to other individuals and businesses. Today, however, many work within private corporations, financial industry, and government bodies.
Accounting and external auditing.
Cf. Accountancy qualifications and regulation
Administration.
(CPS) Certified Professional Secretary
(CAP-OM) Certified Administrative Professional - Organizational Management 
(CAP-TA) Certified Administrative Professional - Technology Applications
In 1951, IAAP administered the first Certified Professional Secretary (CPS) exam, which has evolved through the years into a four-part certification test called the Certified Administrative Professional - Organizational Management (CAP-OM). IAAP also offers the Certified Administrative Professional - Technology Applications (CAP-TA) exam, focusing on the Microsoft Office suite of products. Depending upon an individual's level of higher education, an applicant needs between two and fours years of verifiable working experience as an administrative professional to sit for the exams.
Aerospace.
Aerospace Technicians assemble, service, test, operate, and repair systems associated with both expendable and reusable space launch vehicles, payloads, related laboratories, and ground support equipment. Because space-related activities are evolving, professional certifications vary by country and sometimes by company within a country. Credentials are not yet closely regulated but have evolved over time. Most are performance-based due to the hands-on nature of the work and the importance of assuring the quality and safety of activities and operations in this demanding field.
Currently performance based certifications for aerospace technicians in the United States of America are patterned after the Federal Aviation Administration's (FAA) Airframe and Powerplant certificates for aircraft mechanics. The regulatory arm of the FAA for space is the Office of Commercial Space Transportation (FAA-AST).
Presently the only national credentials meeting the ISO-17024 standards for certification and holding an FAA Safety Approval for the credentialing process are those conferred by the SpaceTEC® - a National Science Foundation National Resource Center. These certifications employ a proctored computer-based written examination followed by an oral examination (if required) and a practical performance-based skills examination administered by a trained and certified SpaceTEC® Examiner (STE).
The existing certifications for aerospace technicians include a Core certification for entry level employees and Concentrations in specialized disciplines for journeymen technicians:
Usually candidates for aerospace certification must qualify through a combination of experience and education, meeting at least one of the following criteria:
Specialized Concentration exams for journeyman credentials require a Core Certification.
Aviation.
Aviators are certified through theoretical and in-flight examinations. Requirements for certifications are quite equal in most countries and are regulated by each National Aviation Authority. The existing certificates or pilot licenses are:
Licensing in these categories require not only examinations but also a minimum of flight hours. All categories are available for Fixed-Wing Aircraft (airplanes) and Rotatory-Wing Aircraft (helicopters). Within each category, aviators may also obtain certifications in:
Usually, aviators must be certified also in their log books for the type and model of aircraft they are allowed to fly. Currency checks as well as regular medical check-ups with a frequency of 6 months, 12 months, or 36 months, depending on the type of flying permitted, are obligatory. An aviator can fly only if holding:
In Europe, the ANSP, ATCO & ANSP technicians are certified according to ESARRs [http://www.eurocontrol.int/src/public/standard_page/src_deliverables.html] (according to EU regulation 2096/2005 "Common Requirements").
Computer technology.
Certification is often used in the professions of software engineering and information technology.
Economic development.
The Council of Development Finance Agencies (CDFA) Training Institute has been the nation’s most comprehensive education initiative dedicated to the economic development finance industry. Through the Institute’s vast course offerings and professional interactions, economic developers and finance professionals from both the public and private sectors have gained valuable knowledge and access to the entire development finance industry. Training Institute courses cover a wide range of topics. CDFA’s Development Finance Certified Professional (DFCP) Program is an intense training course learning experience and is the industry's only comprehensive development finance professional certification program. The DFCP Program is designed to produce graduates with a comprehensive knowledge of development finance concepts, tools and applicability as well as a deep understanding of the entire development finance spectrum. To be considered for graduation from the DFCP Program, individuals must attend a total of six CDFA Training Institute courses and complete a single comprehensive exam.
The International Economic Development Council (IEDC), based in Washington, D.C., recognizes economic developers around the world who have achieved a level of excellence in their understanding of the tools and programs of economic development. In order to become a Certified Economic Developer (CEcD), one must sit through the exam and fulfill a number of requirements.
The Business District Executive Management Certificate Program (BDM), based at Rutgers University (Newark, NJ, USA), School of Public Affairs & Administration, provides professionals and students with six (6) CEU credits in a user-friendly, four module, online learning platform that offers an exploration and training into the theories and practices that identify the multisectoral profession of business district management as a form of public-private partnership. [http://spaa.newark.rutgers.edu/bdem]
Explosive Atmospheres.
IECEx IEC System for Certification to Standards Relating to Equipment for Use in Explosive Atmospheres covers the highly specialized field of explosion protection associated with the use of equipment in areas where flammable gases, liquids and combustible dusts may be present. This System provides the assurance that equipment is manufactured to meet safety standards, and that services such as installation, repair and overhaul also comply with IEC International Standards on safety. The United Nations, via UNECE (United Nations Economic Commission for Europe), recommends the IEC and IECEx as the world’s best practice model for the verification of conformity to International Standards. It published a “Common Regulatory Framework” encompassing the use of IEC International Standards developed by IEC TC (Technical Committee) 31: Equipment for explosive atmospheres, with proof of compliance demonstrated by IECEx.
Genealogy.
AG (Accredited Genealogist) conferred by the International Commission for the Accreditation of Professional Genealogists (ICAPGen).
CG (Certified Genealogist) conferred by the Board for Certification of Genealogists (BCG).
CGL (Certified Genealogical Lecturer) conferred by the Board for Certification of Genealogists (BCG).
Insurance and risk management.
In the United States, insurance professionals are licensed separately by each state. Many individuals seek one or more certifications to distinguish themselves from their peers. The most recognizable certifications are issued by four organizations:
American Institute For Chartered Property Casualty Underwriters (AICPCU)
American College of Financial Services
National Alliance for Insurance Education & Research
National Association of Mutual Insurance Companies
National Registry of Workers' Compensation Specialists
Professional Liability Underwriting Society (PLUS)
Language education.
TESOL is a large field of employment with widely varying degrees of regulation. Most provision worldwide is through the state school system of each individual country, and as such, the instructors tend to be trained primary- or secondary school teachers who are native speakers of the language of their pupils, and not of English. Though native speakers of English have been working in non-English speaking countries in this capacity for years, it was not until the last twenty-five years or so that there was any widespread focus on training particularly for this field. Previously, workers in this sort of job were anyone from backpackers hoping to earn some extra travel money to well-educated professionals in other fields doing volunteer work, or retired people. These sort of people are certainly still to be found, but there are many who consider TESOL their main profession.
One of the problems facing these full-time teachers is the absence of international governing body for the certification or licenture of English language teachers. However, Cambridge University and its subsidiary body UCLES are pioneers in trying to get some degree of accountability and quality control to consumers of English courses, through their CELTA and DELTA programs. Trinity College, London has equivalent programs, the CertTESOL and the LTCL DipTESOL. They offer initial certificates in teaching, in which candidates are trained in language awareness and classroom techniques, and given a chance to practice teaching, after which feedback is reported. Both institutions have as a follow-up a professional diploma, usually taken after a year or two in the field. Although the initial certificate is available to anyone with a high school education, the diploma is meant to be a post-graduate qualification and in fact can be incorporated into a Master's degree program.
Legal affairs.
An increasing number of lawyers are choosing to be recognized as having special knowledge and experience by becoming certified specialists in certain fields of law. According to the American Bar Association, a lawyer that is a certified specialist has been recognized by an independent professional certifying organization as having an enhanced level of skill and expertise, as well as substantial involvement in an established legal specialty. These organizations require a lawyer to demonstrate special training, experience and knowledge to ensure that the lawyer's recognition as a certified specialist is meaningful and reliable. Lawyer conduct with regard to specialty certification is regulated by the states.
NBLSC is an American Bar Association (ABA) accredited organization providing Board Certification for US Lawyers. Board Certification is a rigorous testing and approval process that officially recognizes the extensive education and courtroom experience of attorneys. NBLSC provides Board Certification for Trial Lawyers & Trial Attorneys, Civil Lawyers, Criminal Lawyers, Family Lawyers and Social Security Disability Lawyers.
Logistics and transport.
Logistician is the Profession in the logistics & transport sectors, including sea, air, land and rail modes. Professional qualification for logisticians usually carries post-nominal letters. Common examples include:
Ministers.
Churches have their own process of who may use various religious titles. Protestant churches typically require a Masters of Divinity, accreditation by the denomination and ordination by the local church in order for a minister to become a "Reverend". Those qualifications may or may not also give government authorization to solemnize marriages
Medicine.
Board certification is the process by which a physician in the United States documents by written, practical and/or computer based testing, illustrating a mastery of knowledge and skills that define a particular area of medical specialization. The American Board of Medical Specialties, a not-for-profit organization, assists 24 approved medical specialty boards in the development and use of standards in the ongoing evaluation and certification of physicians.
Medical specialty certification in the United States is a voluntary process. While medical licensure sets the minimum competency requirements to diagnose and treat patients, it is not specialty specific. Board certification demonstrates a physician’s exceptional expertise in a particular specialty and/or sub-specialty of medical practice.
Patients, physicians, health care providers, insurers and quality organizations regard certification as an important measure of a physician’s knowledge, experience and skills to provide quality health care within a given specialty.
Other professional certifications include certifications such as medical licenses, Membership of the Royal College of Physicians, nursing board certification, diplomas in social work. The Commission for Certification in Geriatric Pharmacy certifies pharmacists that are knowledgeable about principles of geriatric pharmacotherapy and the provision of pharmaceutical care to the elderly. The Commission on Accreditation for Law Enforcement Agencies administers a voluntary accreditation program for law enforcement agencies.
See also.
Project management.
Certification is of significant importance in the project management (PM) industry. Certification refers to the evaluation and recognition of the skills, knowledge, and/or competence of a practitioner in the field.
Project management certifications come in a variety of flavors:
Combination of Competence-based, Knowledge-based, and Experience-based
Knowledge-based
Public relations.
There are 15 professional associations from around the world offering the ' Accredited in Public Relations (APR) designation and one offering the 'Accredited Business Communicator (ABC) designation. In 2008, the http://www.globalalliancepr.org, after examining the more prevalent APR and ABC examination processes, determined that a core set of competencies should be part of a world standard to establish competency in the public relations profession.
Speaking.
Conferred by the National Speakers Association, the Certified Speaking Professional® (CSP) is the speaking profession's international measure of professional platform competence. This certification is awarded by the National Speakers Association Only about 10% of the speakers who belong to the Global Speakers Federation (GSF) hold this designation. Those who have earned their certification have done so by demonstrating a track record of experience and expertise.
Training.
Australian Institute of Certified Practising Trainers administers the Certified Practising Trainer (CPT).
Conferred by the Australian Institute of Certified Practising Trainers, [http://www.aicpt.org.au] this certification is the hallmark for professional trainers.
Criticisms.
Many political commentators, often criticize professional or occupational licensing, especially medical and legal licensing, for restricting the supply of services and therefore making them more expensive, often putting them out of reach of poor people.
The current proliferation of IT certifications (both offered and attained), like the FSI's IT baseline protection certification, has led some technologists to question their value. Proprietary content that has been distributed on the Internet allows some to gain credentials without the implied depth or breadth of expertise. Certifying agencies have responded in various ways: Some now incorporate hands-on elements, anti-cheating methodologies or have expanded their content. Others have expired and restructured their certificate programs, and/or raised their fees to deter abuse.
Certification programs that take into account length of service, and demonstrated experience, via industry peer and/or employer recommendation avoid some of the issues associated with purely passing an examination; however, certification remains a contentious issue.
Also, some professional certifications require a criminal record check for the certification to be approved. The presence of a criminal history when applying for certification may be grounds for denial of certification.

</doc>
<doc id="6255" url="http://en.wikipedia.org/wiki?curid=6255" title="Carl Menger">
Carl Menger

Carl Menger (; February 23, 1840 – February 26, 1921) was the founder of the Austrian School of economics. Menger contributed to the development of the theory of marginal utility, which contested the cost-of-production theories of value, developed by the classical economists such as Adam Smith and David Ricardo.
Biography.
Menger was born in Nowy Sącz in Austrian Galicia, now in Poland. He was the son of a wealthy family of minor nobility; his father, Anton, was a lawyer. His mother, Caroline, was the daughter of a wealthy Bohemian merchant. He had two brothers, Anton and Max, both prominent as lawyers.
After attending "Gymnasium" he studied law at the Universities of Prague and Vienna and later received a doctorate in jurisprudence from the Jagiellonian University in Kraków. In the 1860s Menger left school and enjoyed a stint as a journalist reporting and analyzing market news, first at the "Lemberger Zeitung" in Lwów, Ukraine and later at the "Wiener Zeitung" in Vienna.
During the course of his newspaper work he noticed a discrepancy between what the classical economics he was taught in school said about price determination and what real world market participants believed. In 1867 Menger began a study of political economy which culminated in 1871 with the publication of his "Principles of Economics" "(Grundsätze der Volkswirtschaftslehre)," thus becoming the father of the Austrian School of economic thought. It was in this work that he challenged classical cost-based theories of value with his theory of marginality – that price is determined at the margin.
In 1872 Menger was enrolled into the law faculty at the University of Vienna and spent the next several years teaching finance and political economy both in seminars and lectures to a growing number of students. In 1873 he received the university's chair of economic theory at the very young age of 33.
In 1876 Menger began tutoring Archduke Rudolf von Habsburg, the Crown Prince of Austria in political economy and statistics. For two years Menger accompanied the prince in his travels, first through continental Europe and then later through the British Isles. He is also thought to have assisted the crown prince in the composition of a pamphlet, published anonymously in 1878, which was highly critical of the higher Austrian aristocracy. His association with the prince would last until Rudolf's suicide in 1889 (see the Mayerling Affair).
In 1878 Rudolf's father, Emperor Franz Josef, appointed Menger to the chair of political economy at Vienna. The title of "Hofrat" was conferred on him, and he was appointed to the Austrian "Herrenhaus" in 1900.
Ensconced in his professorship he set about refining and defending the positions he took and methods he utilized in "Principles," the result of which was the 1883 publication of "Investigations into the Method of the Social Sciences with Special Reference to Economics (Untersuchungen über die Methode der Socialwissenschaften und der politischen Oekonomie insbesondere)." The book caused a firestorm of debate, during which members of the Historical school of economics began to derisively call Menger and his students the "Austrian School" to emphasize their departure from mainstream economic thought in Germany – the term was specifically used in an unfavorable review by Gustav von Schmoller.
In 1884 Menger responded with the pamphlet "The Errors of Historicism in German Economics" and launched the infamous "Methodenstreit," or methodological debate, between the Historical School and the Austrian School. During this time Menger began to attract like-minded disciples who would go on to make their own mark on the field of economics, most notably Eugen von Böhm-Bawerk, and Friedrich von Wieser.
In the late 1880s Menger was appointed to head a commission to reform the Austrian monetary system. Over the course of the next decade he authored a plethora of articles which would revolutionize monetary theory, including "The Theory of Capital" (1888) and "Money" (1892). Largely due to his pessimism about the state of German scholarship, Menger resigned his professorship in 1903 to concentrate on study.
Economics.
Menger used his “Subjective Theory of Value” to arrive at one of the most powerful insights in economics: both sides gain from exchange.
Unlike William Jevons, Menger did not believe that goods provide “utils,” or units of utility. Rather, he wrote, goods are valuable because they serve various uses whose importance differs. Menger also came up with an explanation of how money develops that is still accepted today.
If people barter, he pointed out, then they can rarely get what they want in one or two transactions. If they have lamps and want chairs, for example, they will not necessarily be able to trade lamps for chairs but may instead have to make a few intermediate trades. This is a hassle. But people notice that the hassle is much less when they trade what they have for some good that is widely accepted, and then use this good to buy what they want. The good that is widely accepted eventually becomes money.

</doc>
<doc id="6256" url="http://en.wikipedia.org/wiki?curid=6256" title="List of cartoonists">
List of cartoonists

This List of cartoonists is a list of notable artists of every nationality who specialize in drawing cartoons and/or comic strips. The list contains only the names of notable cartoonists who currently have a Wikipedia article.
Notable cartoonists.
Notable cartoonists include:

</doc>
<doc id="6258" url="http://en.wikipedia.org/wiki?curid=6258" title="Civilization">
Civilization

Civilization or civilisation (in British English) generally refers to state polities which combine these basic institutions, having one or more of each: a ceremonial centre (a formal gathering place for social and cultural activities), a system of writing, and a city. The term is used to contrast with other types of communities including hunter-gatherers, nomadic pastoralists and tribal villages. Civilizations have more densely populated settlements divided into hierarchical social classes with a ruling elite and subordinate urban and rural populations, which, by the division of labour, engage in intensive agriculture, mining, small-scale manufacture and trade. Civilization concentrates power, extending human control over both nature, and over other human beings.
The emergence of civilization is generally associated with the final stages of the Neolithic Revolution, a slow cumulative process occurring independently over many locations between 10,000 and 3,000 BCE, culminating in the relatively rapid process of state formation, a political development associated with the appearance of a governing elite. This neolithic technology and lifestyle was established first in the Middle East (for example at Göbekli Tepe, from about 9,130 BCE), and Yangtze and later in the Yellow river basin in China (for example the Pengtoushan culture from 7,500 BCE), and later spread. But similar "revolutions" also began independently from 9,000 years ago in such places as the Norte Chico civilization in Peru and Mesoamerica at the Balsas River. These were among the six civilizations worldwide that arose independently. This revolution consisted in the development of the domestication of plants and animals and the development of new sedentary lifestyles which allowed economies of scale and productive surpluses.
Towards the end of the Neolithic period, various Bronze Age civilizations began to rise in various "cradles" from around 3300 BCE. Civilizations, as defined above, also developed in Pre-Columbian Americas and much later in Africa. The Bronze Age collapse was followed by the Iron Age around 1200 BCE, during which a number of new civilizations emerged, culminating in the Axial Age transition to Classical civilization. A major technological and cultural transition to modernity began approximately 1500 CE in western Europe, and from this beginning new approaches to science and law spread rapidly around the world.
History of the concept.
The word "civilization" comes from the Latin "civilis", meaning "civil", related to the Latin "civis", meaning "citizen", and "civitas", meaning "city" or "city-state". Adjectives such as English "civility" developed from this root, but during the 18th century Enlightenment a verb "civilize" came to be commonly used, leading to a new word "civilization" to describe the result. This was used first by authors writing about national and personal improvement such as Victor Riqueti, marquis de Mirabeau in France, and Adam Ferguson in Scotland who in his 1767 "Essay on the History of Civil Society" wrote that, "Not only the individual advances from infancy to manhood, but the species itself from rudeness to civilisation."" The word was therefore opposed to barbarism or rudeness, but the thinking behind the new word was connected to modernism's active pursuit of progress and enlightenment. As such it has always been coloured by Social Darwinist assumptions about superiority and inferiority.
In the late 1700s and early 1800s, both during the French revolution, and in English, "civilization" was referred to in the singular, never the plural, because it referred to the progress of humanity as a whole. This is still the case in French. More recently however, "civilizations" (the plural) is sometimes used as a synonym for the broader term "cultures" (defined as "the arts, customs, habits... beliefs, values, behavior and material habits that constitute a people's way of life") in both popular and academic circles. Using the terms "civilization" and "culture" as equivalents is controversial and not generally accepted, so that for example some types of culture are not normally described as civilizations.
Already in the 18th century civilization was not always seen as an improvement. One historically important distinction between culture and civilization stems from the writings of Rousseau, and particularly his work concerning education, "". In this perspective, civilization, being more rational and socially driven, is not fully in accordance with human nature, and "human wholeness is achievable only through the recovery of or approximation to an original prediscursive or prerational natural unity". (See noble savage.) From this notion, a new approach was developed especially in Germany, first by Johann Gottfried Herder, and later by philosophers such as Kierkegaard and Nietzsche. This sees cultures (plural) as natural organisms which are not defined by "conscious, rational, deliberative acts" but rather a kind of pre-rational "folk spirit". Civilization, in contrast, though more rational and more successful concerning material progress, is seen as un-natural, and leads to "vices of social life" such as guile, hypocrisy, envy, and avarice. During World War II, Leo Strauss, having fled Germany, argued in New York that this approach to civilization was behind Nazism and German militarism and nihilism.
In his book "The Philosophy of Civilization", Albert Schweitzer outlined the idea that there are dual opinions within society: one regarding civilization as purely material and another regarding civilization as both ethical and material. He stated that the current world crisis was, then in 1923, due to a humanity having lost the ethical conception of civilization. In this same work, he defined civilization, saying that it "is the sum total of all progress made by man in every sphere of action and from every point of view in so far as the progress helps towards the spiritual perfecting of individuals as the progress of all progress."
Characteristics.
Social scientists such as V. Gordon Childe have named a number of traits that distinguish a civilization from other kinds of society. Civilizations have been distinguished by their means of subsistence, types of livelihood, settlement patterns, forms of government, social stratification, economic systems, literacy, and other cultural traits.
All civilizations have depended on agriculture for subsistence. Growing food on farms can result in a surplus of food, particularly when people use intensive agricultural techniques such as artificial fertilisation, irrigation and crop rotation. Grain surpluses have been especially important because they can be stored for a long time. A surplus of food permits some people to do things besides produce food for a living: early civilizations included soldiers, artisans, priests and priestesses, and other people with specialized careers. A surplus of food results in a division of labor and a more diverse range of human activity, a defining trait of civilizations. However, in some places hunter-gatherers have had access to food surpluses, such as among some of the indigenous peoples of the Pacific Northwest and perhaps during the Mesolithic Natufian culture. It is possible that food surpluses and relatively large scale social organization and division of labor predates plant and animal domestication.
Civilizations have distinctly different settlement patterns from other societies. The word "civilization" is sometimes simply defined as "'living in cities'". Non-farmers tend to gather in cities to work and to trade.
Compared with other societies, civilizations have a more complex political structure, namely the state. State societies are more stratified than other societies; there is a greater difference among the social classes. The ruling class, normally concentrated in the cities, has control over much of the surplus and exercises its will through the actions of a government or bureaucracy. Morton Fried, a conflict theorist, and Elman Service, an integration theorist, have classified human cultures based on political systems and social inequality. This system of classification contains four categories:
Economically, civilizations display more complex patterns of ownership and exchange than less organized societies. Living in one place allows people to accumulate more personal possessions than nomadic people. Some people also acquire landed property, or private ownership of the land. Because a percentage of people in civilizations do not grow their own food, they must trade their goods and services for food in a market system, or receive food through the levy of tribute, redistributive taxation, tariffs or tithes from the food producing segment of the population. Early human cultures functioned through a gift economy supplemented by limited barter systems. By the early Iron Age contemporary civilizations developed money as a medium of exchange for increasingly complex transactions. To oversimplify, in a village the potter makes a pot for the brewer and the brewer compensates the potter by giving him a certain amount of beer. In a city, the potter may need a new roof, the roofer may need new shoes, the cobbler may need new horseshoes, the blacksmith may need a new coat, and the tanner may need a new pot. These people may not be personally acquainted with one another and their needs may not occur all at the same time. A monetary system is a way of organizing these obligations to ensure that they are fulfilled fairly.
Writing, developed first by people in Sumer, is considered a hallmark of civilization and "appears to accompany the rise of complex administrative bureaucracies or the conquest state." Traders and bureaucrats relied on writing to keep accurate records. Like money, writing was necessitated by the size of the population of a city and the complexity of its commerce among people who are not all personally acquainted with each other. However, writing is not always necessary for civilization. The Inca civilization of the Andes did not use writing at all, yet it still functioned as a society.
Aided by their division of labor and central government planning, civilizations have developed many other diverse cultural traits. These include organized religion, development in the arts, and countless new advances in science and technology.
Through history, successful civilizations have spread, taking over more and more territory, and assimilating more and more previously-uncivilized people. Nevertheless, some tribes or people remain uncivilized even to this day. These cultures are called by some "primitive," a term that is regarded by others as pejorative. "Primitive" implies in some way that a culture is "first" (Latin = primus), that it has not changed since the dawn of humanity, though this has been demonstrated not to be true. Specifically, as all of today's cultures are contemporaries, today's so-called primitive cultures are in no way antecedent to those we consider civilized. Anthropologists today use the term "non-literate" to describe these peoples.
Civilization has been spread by colonization, invasion, religious conversion, the extension of bureaucratic control and trade, and by introducing agriculture and writing to non-literate peoples. Some non-civilized people may willingly adapt to civilized behaviour. But civilization is also spread by the technical, material and social dominance that civilization engenders.
Assessments of what level of civilization a polity has reached are based on comparisons of the relative importance of agricultural as opposed to trade or manufacturing capacities, the territorial extensions of its power, the complexity of its division of labor, and the carrying capacity of its urban centres. Secondary elements include a developed transportation system, writing, standardized measurement, currency, contractual and tort-based legal systems, art, architecture, mathematics, scientific understanding, metallurgy, political structures, and organized religion.
Traditionally, polities that managed to achieve notable military, ideological and economic power defined themselves as "civilized" as opposed to other societies or human grouping which lay outside their sphere of influence, calling the latter barbarians, savages, and primitives, while in a modern-day context, "civilized people" have been contrasted with indigenous people or tribal societies.
Cultural identity.
"Civilization" can also refer to the culture of a complex society, not just the society itself. Every society, civilization or not, has a specific set of ideas and customs, and a certain set of manufactures and arts that make it unique. Civilizations tend to develop intricate cultures, including literature, professional art, architecture, organized religion, and complex customs associated with the elite.
The intricate culture associated with civilization has a tendency to spread to and influence other cultures, sometimes assimilating them into the civilization (a classic example being Chinese civilization and its influence on nearby civilizations such as Korea, Japan and Vietnam). Many civilizations are actually large cultural spheres containing many nations and regions. The civilization in which someone lives is that person's broadest cultural identity.
Many historians have focused on these broad cultural spheres and have treated civilizations as discrete units. Early twentieth-century philosopher Oswald Spengler, uses the German word "Kultur," "culture," for what many call a "civilization". Spengler believes a civilization's coherence is based on a single primary cultural symbol. Cultures experience cycles of birth, life, decline, and death, often supplanted by a potent new culture, formed around a compelling new cultural symbol. Spengler states civilization is the beginning of the decline of a culture as, "...the most external and artificial states of which a species of developed humanity is capable."
This "unified culture" concept of civilization also influenced the theories of historian Arnold J. Toynbee in the mid-twentieth century. Toynbee explored civilization processes in his multi-volume "A Study of History," which traced the rise and, in most cases, the decline of 21 civilizations and five "arrested civilizations." Civilizations generally declined and fell, according to Toynbee, because of the failure of a "creative minority", through moral or religious decline, to meet some important challenge, rather than mere economic or environmental causes.
Samuel P. Huntington defines civilization as "the highest cultural grouping of people and the broadest level of cultural identity people have short of that which distinguishes humans from other species." Huntington's theories about civilizations are discussed below.
Complex systems.
Another group of theorists, making use of systems theory, looks at a civilization as a complex system, i.e., a framework by which a group of objects can be analyzed that work in concert to produce some result. Civilizations can be seen as networks of cities that emerge from pre-urban cultures, and are defined by the economic, political, military, diplomatic, social, and cultural interactions among them. Any organization is a complex social system, and a civilization is a large organization. Systems theory helps guard against superficial but misleading analogies in the study and description of civilizations.
Systems theorists look at many types of relations between cities, including economic relations, cultural exchanges, and political/diplomatic/military relations. These spheres often occur on different scales. For example, trade networks were, until the nineteenth century, much larger than either cultural spheres or political spheres. Extensive trade routes, including the Silk Road through Central Asia and Indian Ocean sea routes linking the Roman Empire, Persian Empire, India, and China, were well established 2000 years ago, when these civilizations scarcely shared any political, diplomatic, military, or cultural relations. The first evidence of such long distance trade is in the ancient world. During the Uruk period Guillermo Algaze has argued that trade relations connected Egypt, Mesopotamia, Iran and Afghanistan. Resin found later in the Royal Tombs of Ur it is suggested was traded northwards from Mozambique.
Many theorists argue that the entire world has already become integrated into a single "world system", a process known as globalization. Different civilizations and societies all over the globe are economically, politically, and even culturally interdependent in many ways. There is debate over when this integration began, and what sort of integration – cultural, technological, economic, political, or military-diplomatic – is the key indicator in determining the extent of a civilization. David Wilkinson has proposed that economic and military-diplomatic integration of the Mesopotamian and Egyptian civilizations resulted in the creation of what he calls the "Central Civilization" around 1500 BCE. Central Civilization later expanded to include the entire Middle East and Europe, and then expanded to a global scale with European colonization, integrating the Americas, Australia, China and Japan by the nineteenth century. According to Wilkinson, civilizations can be culturally heterogeneous, like the Central Civilization, or homogeneous, like the Japanese civilization. What Huntington calls the "clash of civilizations" might be characterized by Wilkinson as a clash of cultural spheres within a single global civilization. Others point to the Crusades as the first step in globalization. The more conventional viewpoint is that networks of societies have expanded and shrunk since ancient times, and that the current globalized economy and culture is a product of recent European colonialism.
History.
Early civilizations.
The Neolithic Era.
The process of sedentarization is first thought to have occurred around 12,000 BCE in the Levant region of southwest Asia though other regions around the world soon followed. The emergence of civilization is generally associated with the Neolithic, or Agricultural Revolution, which occurred in various locations between 8,000 and 5,000 BCE, specifically in southwestern/southern Asia, northern/central Africa and Central America. At first the Neolithic was associated with shifting subsistence cultivation, where continuous farming led to the depletion of soil fertility resulting in the requirement to cultivate fields further and further removed from the settlement, eventually compelling the settlement itself to move. In major semi-arid river valleys, annual flooding renewed soil fertility to be renewed yearly, with the result that population densities could rise significantly. This encouraged a "secondary products revolution" where domesticated animals became useful for more than meat production; being used also for milk, wool, and animal traction of ploughs and carts. The 8.2 Kiloyear Arid Event and the 5.9 Kiloyear Interpluvial saw the drying out of semiarid regions and a major spread of deserts. This climate change shifted the cost-benefit ratio of endemic violence between communities, which saw the abandonment of unwalled village communities and the appearance of walled cities, associated with the first civilisations. This "urban revolution" marked the beginning of stable agriculture and animal domestication which enabled economies and cities to develop. It was associated with the state monopoly and violence, the appearance of a soldier class and endemic warfare, rapid development of hierarchies and a fall in the status of women.
The Iron Age.
The Iron Age is the period generally occurring after the Bronze Age, marked by the prevalent use of iron. The early period of the age is characterized by the widespread use of iron or steel. The adoption of such material coincided with other changes in society, including differing agricultural practices, religious beliefs and artistic styles. The "Iron Age" as an archaeological term indicates the condition as to civilization and culture of a people using iron as the material for their cutting tools and weapons. The "Iron Age" is the third principal period of the three-age system created by Christian Thomsen (1788–1865) for classifying ancient societies and prehistoric stages of progress.
Karl Jaspers, the German historical philosopher, proposed that the ancient civilizations were affected greatly by an Axial Age in the period between 800 BCE–200 BCE during which a series of male sages, prophets, religious reformers and philosophers, from China, India, Iran, Israel and Greece, changed the direction of civilizations.
William Hardy McNeill proposed that this period of history was one in which culture contact between previously separate civilizations saw the "closure of the oecumene", and led to accelerated social change from China to the Mediterranean, associated with the spread of coinage, larger empires and new religions. This view has recently been championed by Christopher Chase-Dunn and other world systems theorists.
Fall of civilizations.
There have been many explanations put forward for the collapse of civilization. Some focus on historical examples, and others on general theory.
Future.
Political scientist Samuel Huntington has argued that the defining characteristic of the 21st century will be a clash of civilizations. According to Huntington, conflicts between civilizations will supplant the conflicts between nation-states and ideologies that characterized the 19th and 20th centuries. These views have been strongly challenged by others like Edward Said, Muhammed Asadi and Amartya Sen. Ronald Inglehart and Pippa Norris have argued that the "true clash of civilizations" between the Muslim world and the West is caused by the Muslim rejection of the West's more liberal sexual values, rather than a difference in political ideology, although they note that this lack of tolerance is likely to lead to an eventual rejection of (true) democracy. In "Identity and Violence" Sen questions if people should be divided along the lines of a supposed 'civilization', defined by religion and culture only. He argues that this ignores the many others identities that make up people and leads to a focus on differences.
Some environmental scientists see the world entering a Planetary Phase of Civilization, characterized by a shift away from independent, disconnected nation-states to a world of increased global connectivity with worldwide institutions, environmental challenges, economic systems, and consciousness. In an attempt to better understand what a Planetary Phase of Civilization might look like in the current context of declining natural resources and increasing consumption, the Global scenario group used scenario analysis to arrive at three archetypal futures: Barbarization, in which increasing conflicts result in either a fortress world or complete societal breakdown; Conventional Worlds, in which market forces or Policy reform slowly precipitate more sustainable practices; and a Great Transition, in which either the sum of fragmented Eco-Communalism movements add up to a sustainable world or globally coordinated efforts and initiatives result in a new sustainability paradigm.
Author Derrick Jensen argues that modern civilization is intrinsically directed towards the domination of the environment and humanity itself in a harmful and destructive fashion.
The Kardashev scale classifies civilizations based on their level of technological advancement, specifically measured by the amount of energy a civilization is able to harness. The Kardashev scale makes provisions for civilizations far more technologically advanced than any currently known to exist "(see also: Civilizations and the Future, Space civilization)".

</doc>
<doc id="6259" url="http://en.wikipedia.org/wiki?curid=6259" title="Civilization (video game)">
Civilization (video game)

"Sid Meier's Civilization" is a turn-based strategy "4X"-type strategy video game created by Sid Meier and Bruce Shelley for MicroProse in 1991. The game's objective is to "Build an empire to stand the test of time": it begins in 4000 BC and the players attempt to expand and develop their empires through the ages from the ancient era until modern and near-future times. It is also known simply as "Civilization", or abbreviated to "Civ" or "Civ I".
"Civilization" was originally developed for DOS running on a PC. It has undergone numerous revisions for various platforms (including Windows, Macintosh, Amiga, Atari ST, PlayStation, N-Gage and Super Nintendo) and now exists in several versions. A multiplayer remake, "Sid Meier's CivNet" was released for the PC in 1995. The N-Gage version was the last game released for the system in North America.
Gameplay.
"Civilization" is a turn-based single- or multiplayer strategy game. The player takes on the role of the ruler of a civilization, starting with only one settler unit and one warrior, and attempts to build an empire in competition with one to eleven other civilizations. The game requires a fair amount of micromanagement (although less than any of the simulation games). Along with the larger tasks of exploration, warfare and diplomacy, the player has to make decisions about where to build new cities, which improvements or units to build in each city, which advances in knowledge should be sought (and at what rate), and how to transform the land surrounding the cities for maximum benefit. From time to time the player's towns may be harassed by barbarians, units with no specific nationality and no named leader. These threats only come from unclaimed land or sea, so that over time there are fewer and fewer places from which barbarians will emanate.
Before the game begins, the player chooses which historical or current civilization to play. In contrast to later games in the "Civilization" series, this is largely a cosmetic choice, affecting titles, city names, musical heralds, and color. The choice does affect their starting position on the "Play on Earth" map, and thus different resources in one's initial cities, but has no effect on starting position when starting a random world game or a customized world game. The player's choice of civilization also prevents the computer from being able to play as that civilization or the other civilization of the same color, and since computer-controlled opponents display certain traits of their civilizations this affects gameplay as well. The Aztecs are both fiercely expansionist and generally extremely wealthy, for example. Other civilizations include the Americans, the Mongols, and Romans. Each civilization is led by a famous historical figure, such as Mohandas K. Gandhi for India.
The scope of "Civilization" is larger than most other games. The game begins in 4000 BC, before the Bronze Age, and can last through to AD 2050 (on the easiest setting) with Space Age and "future technologies". At the start of the game there are no cities anywhere in the world: the player controls one or two settler units, which can be used to found new cities in appropriate sites (and those cities may build other settler units, which can go out and found new cities, thus expanding the empire). Settlers can also alter terrain, build improvements such as mines and irrigation, build roads to connect cities, and later in the game they can construct railroads which offer unlimited movement.
As time advances, new technologies are developed; these technologies are the primary way in which the game changes and grows. At the start, players choose from advances such as pottery, the wheel, and the alphabet to, near the end of the game, nuclear fission and spaceflight. Players can gain a large advantage if their civilization is the first to learn a particular technology (the secrets of flight, for example) and put it to use in a military or other context. Most advances give access to new units, city improvements or derivative technologies: for example, the chariot unit becomes available after the wheel is developed, and the granary building becomes available to build after pottery is developed. The whole system of advancements from beginning to end is called the technology tree, or simply the Tech tree; this concept has been adopted in many other strategy games. Since only one tech may be "researched" at any given time, the order in which technologies are chosen makes a considerable difference in the outcome of the game and generally reflects the player's preferred style of gameplay.
Players can also build "Wonders of the World" in each of the epochs of the game, subject only to obtaining the prerequisite knowledge. These wonders are important achievements of society, science, culture and defense, ranging from the Pyramids and the Great Wall in the Ancient age, to Copernicus' Observatory and Magellan's Expedition in the middle period, up to the Apollo program, the United Nations, and the Manhattan Project in the modern era. Each wonder can only be built once in the world, and requires a lot of resources to build, far more than most other city buildings or units. Wonders provide unique benefits to the controlling civilization. For example, Magellan's Expedition increases the movement rate of naval units. Wonders typically affect either the city in which they are built (for example, the Colossus), every city on the continent (for example, the Hanging Gardens), or the civilization as a whole (for example, Darwin's Voyage). Some wonders are made obsolete by new technologies.
A good strategy for players who are new to Civilization is to be friendly to the strong, and destroy the weaker civilizations. Diplomats are extremely useful, for making peace, sabotaging enemy's cities, even "buying" enemy's cities (inciting a revolt). To start the game, choose about five other countries to compete against. Next, found a city as soon as possible in an appropriate area. Bear in mind that although coastal cities may be attacked by pirates (build barracks to prevent this) they are the only ones that can launch ships. Try to build ships as soon as possible. Then put settlers and a strong attack and/or defense unit on the ship, and you are ready to found some new cities. You can look at the top five cities in the world on the World tab. These countries should be left alone unless your civilization is quite powerful. Triremes are flimsy little ships that must stay next to shore, lest they sink. Sails, (sailing ships) are much more useful, but require more technological advancements.
The game can be won by conquering all other civilizations or by winning the space race by reaching the star system of Alpha Centauri.
Development.
Meier admits to "borrowing" many of the technology tree ideas from the board game "Civilization", published in the United Kingdom in 1980 by Hartland Trefoil (later by Gibson Games), and in the United States in 1981 by Avalon Hill.
The early versions of the game even included a flier of information and ordering materials for the board game. There is also a based on Sid Meier's computer game version of "Civilization" that was published in 2002.
Meier was the third major designer to plan a computer version of "Civilization", but the first to actually carry out that plan. Danielle Bunten Berry planned to start work on the game after completing "M.U.L.E." in 1983, and again in 1985, after completing "The Seven Cities of Gold" at Electronic Arts. In 1983 Bunten and producer Joe Ybarra opted to first do "Seven Cities of Gold". The success of "Seven Cities" in 1985 in turn led to a sequel, "Heart of Africa". Bunten never returned to the idea of "Civilization". Meier's designs of "Pirates!" and "Colonization" both contain elements of Bunten's "The Seven Cities of Gold". Don Daglow, designer of "Utopia", the first simulation game, began work programming a version of "Civilization" in 1987. He dropped the project, however, when he was offered an executive position at Brøderbund, and never returned to the game.
"Civilization" originally started off as a real-time game, however Meier found it too similar to other real-time strategy games such as "SimCity", and instead opted for a system where each turn takes a predetermined amount of time, and will automatically execute. This plan was abandoned after wide dislike over the mechanic.
When the first version of "Civilization" was being developed, it was designed to run on a PC, which at the time was transitioning from 16 color EGA to VGA, which could use 256 different colors. The decision to limit the number of different civilizations to 16 was made to make "Civilization" compatible with both display standards: 16 civilizations for the 16 colors available to EGA.
Reception.
"Civilization" has been called one of the most important strategy games of all time, and has a loyal following of fans. This high level of interest has led to the creation of a number of free and open source versions and inspired similar games by other commercial developers.
The game was reviewed in 1992 in "Dragon" #183 by Hartley, Patricia, and Kirk Lesser in "The Role of Computers" column. The reviewers gave the game 5 out of 5 stars. They commented: ""Civilization" is one of the highest dollar-to-play-ratio entertainments we've enjoyed. The scope is enormous, the strategies border on being limitless, the excitement is genuinely high, and the
experience is worth every dime of the game's purchase price."
"Civilization" won the Origins Award in the category Best Military or Strategy Computer Game of 1991. A 1992 "Computer Gaming World" survey of wargames with modern settings gave the game five stars out of five, describing it as "more addictive than crack ... so rich and textured that the documentation is incomplete". In 1992 the magazine named it the Overall Game of the Year, and in 1996 it chose "Civilization" as the best game of all time:
In 2000, GameSpot rated "Civilization" as the seventh most influential video game of all time. In 2004, readers of "Retro Gamer" voted it as the 29th top retro game. In 2007, it was named one of the 16 most influential games in history at a German technology and games trade show Telespiele. In Poland, it was included in the retrospective lists of the best Amiga games by Wirtualna Polska (ranked ninth) and "CHIP" (ranked fifth). In 2012, "Time" named it one of the 100 greatest video games of all time.
On March 12, 2007, "The New York Times" reported on a list of the ten most important video games of all time, the so-called game canon, which included "Civilization".
"Sid Meier's CivNet".
"Sid Meier's CivNet" is a remake of the original game with added multiplayer, improved graphics and sound, and Windows 3.1/95 support. Gameplay is almost identical to the original game. There are several methods of multiplayer, including LAN, primitive Internet play, hotseat, modem, and direct serial link.
Legacy.
There have been several sequels to "Civilization", including "Civilization II" (1996), "Civilization III" (2001), "Civilization IV" (2005), "Civilization Revolution" (2008), and "Civilization V" (2010). In 1994, Meier produced a similar game titled "Colonization". The 1999 game "Sid Meier's Alpha Centauri" was also created by Meier and is in the same genre, but with a futuristic/space theme; many of the interface and gameplay innovations in this game eventually made their way into "Civilization III" and "IV". "Alpha Centauri" is not actually a sequel to "Civilization", despite beginning with the same event that ends "Civilization" and "Civilization II": a manned spacecraft from Earth arrives in the Alpha Centauri star system.
In 1994, MicroProse published "Master of Magic", a similar game but embedded in a medieval-fantasy setting where instead of technologies the player (a powerful wizard) develops spells, among other things. The game also shared many things with the popular fantasy card-trading game '. In 1999, Activision released ', a sequel of sorts to "Civilization II" but created by a completely different design team. "Call to Power" spawned a sequel in 2000, but by then Activision had has lost the rights to the "Civilization" name and could only call it "Call to Power II".
An open source clone of "Civilization" has been developed under the name of "Freeciv", with the slogan "'Cause civilization should be free" (currently it can be configured to match the rules of either "Civilization" or "Civilization II"). Another game that partially clones it is a public domain game called "C-evo".

</doc>
<doc id="6260" url="http://en.wikipedia.org/wiki?curid=6260" title="Claude Debussy">
Claude Debussy

Achille-Claude Debussy (; 22 August 1862 – 25 March 1918) was a French composer. Along with Maurice Ravel, he was one of the most prominent figures associated with Impressionist music, though he himself intensely disliked the term when applied to his compositions. In France, he was made Chevalier of the Legion of Honour in 1903. Debussy was among the most influential composers of the late 19th and early 20th centuries, and his use of non-traditional scales and chromaticism influenced many composers who followed.
Debussy's music is noted for its sensory content and frequent eschewing of tonality. The French literary style of his period was known as Symbolism, and this movement directly inspired Debussy both as a composer and as an active cultural participant.
Early life.
Claude Debussy was born on 22 August 1862 in Saint-Germain-en-Laye, France, the eldest of five children. His father, Manuel-Achille Debussy, owned a china shop there; his mother, Victorine Manoury Debussy, was a seamstress. The family moved to Paris in 1867, but in 1870 Debussy's pregnant mother fled with Claude to his paternal aunt's home in Cannes to escape the Franco-Prussian war. Debussy began piano lessons there at the age of seven with an Italian violinist in his early 40s named Cerutti; his aunt paid for his lessons. In 1871 he drew the attention of Marie Mauté de Fleurville, who claimed to have been a pupil of Frédéric Chopin. Debussy always believed her, although there is no independent evidence to support her claim. His talents soon became evident, and in 1872, at age ten, Debussy entered the Paris Conservatoire, where he spent the next 11 years. During his time there he studied composition with Ernest Guiraud, music history/theory with Louis-Albert Bourgault-Ducoudray, harmony with Émile Durand, piano with Antoine François Marmontel, organ with César Franck, and solfège with Albert Lavignac, as well as other significant figures of the era. He also became a lifelong friend of fellow student and noted pianist Isidor Philipp. After Debussy's death, many pianists sought out Philipp for advice on playing his pieces.
Musical development.
From the start, though clearly talented, Debussy was argumentative and experimental. He challenged the rigid teaching of the Academy, favoring instead dissonances and intervals that were frowned upon. Like Georges Bizet, he was a brilliant pianist and an outstanding sight reader, who could have had a professional career had he so wished. The pieces he played in public at this time included sonata movements by Beethoven, Schumann and Weber; and Chopin—the Ballade No. 2, a movement from the Piano Concerto No. 1, and the "Allegro de concert", a relatively little-known piece but one requiring an advanced technique (it was originally intended to be the opening movement of a third piano concerto).
During the summers of 1880, 1881, and 1882 Debussy accompanied the wealthy patroness of Pyotr Ilyich Tchaikovsky, Nadezhda von Meck, as she travelled with her family in Europe and Russia. The young composer's many musical activities during these vacations included playing four-hand pieces with von Meck at the piano, giving music lessons to her children, and performing in private concerts with some of her musician friends. Despite von Meck's closeness with Tchaikovsky, the Russian master appears to have had minimal effect on Debussy. In September 1880 she sent Debussy's "Danse bohémienne" for Tchaikovsky's perusal. A month later Tchaikovsky wrote back to her, "It is a very pretty piece, but it is much too short. Not a single idea is expressed fully, the form is terribly shriveled, and it lacks unity." Debussy did not publish the piece; the manuscript remained in the von Meck family, and it was sold to B. Schott's Sohne in Mainz, and published by them in 1932. A greater influence was Debussy's close friendship with Marie-Blanche Vasnier, a singer he met when he began working as an accompanist to earn some money. She and her husband, Henri, gave Debussy emotional and professional support. Henri Vasnier introduced him to the writings of influential French writers of the time, which gave rise to his first songs, settings of poems by Paul Verlaine, the son-in-law of his former teacher, Mme. Mauté de Fleurville.
As the winner of the 1884 Prix de Rome with his composition "L'enfant prodigue", Debussy received a scholarship to the Académie des Beaux-Arts, which included a four-year residence at the Villa Medici, the French Academy in Rome, to further his studies (1885–1887). According to letters to Marie-Blanche Vasnier, perhaps in part designed to gain her sympathy, he found the artistic atmosphere stifling, the company boorish, the food bad, and the monastic quarters "abominable". Neither did he delight in the pleasures of the "Eternal City", finding the Italian opera of Donizetti and Verdi not to his taste. Debussy was often depressed and unable to compose, but he was inspired by Franz Liszt, whose command of the keyboard he found admirable.
In June 1885, Debussy wrote of his desire to follow his own way, saying, "I am sure the Institute would not approve, for, naturally it regards the path which it ordains as the only right one. But there is no help for it! I am too enamoured of my freedom, too fond of my own ideas!"
Debussy finally composed four pieces that were sent to the Academy: the symphonic ode "Zuleima", based on a text by Heinrich Heine; the orchestral piece "Printemps"; the cantata "La damoiselle élue" (1887–1888), which was criticized by the Academy as "bizarre"; and the "Fantaisie" for piano and orchestra. The third piece was the first in which stylistic features of Debussy's later style emerged. The fourth piece was heavily based on César Franck's music and Debussy withdrew it. The Academy chided him for "courting the unusual" and hoped for something better from the gifted student. Even though Debussy's works showed the influence of Jules Massenet, Massenet concluded, "He is an enigma."
During his visits to Bayreuth in 1888–9, Debussy was exposed to Wagnerian opera, which had a lasting impact on his work. Debussy, like many young musicians of the time, responded positively to Richard Wagner's sensuousness, mastery of form, and striking harmonies. Wagner's extroverted emotionalism was not to be Debussy's way, but the German composer's influence is evident in "La damoiselle élue" and the 1889 piece "Cinq poèmes de Charles Baudelaire". Other songs of the period, notably the settings of Verlaine – "Ariettes oubliées", "Trois mélodies", and "Fêtes galantes" – are all in a more capricious style. Around this time, Debussy met Erik Satie, who proved a kindred spirit in his experimental approach to composition and to naming his pieces. During this period, both musicians were bohemians enjoying the same cafe society and struggling to stay afloat financially.
In 1889, at the Exposition Universelle in Paris, Debussy heard Javanese gamelan music. He incorporated gamelan scales, melodies, rhythms, and ensemble textures into some of his compositions, most notably "Pagodes" from his piano collection Estampes.
Personal life.
Debussy's private life was often turbulent. At the age of 18 he began an eight-year affair with Marie-Blanche Vasnier, wife of a Parisian civil servant. The relationship eventually faltered following his winning of the Prix de Rome in 1884 and obligatory residence in Rome.
On his permanent return to Paris and his parents' home on the avenue de Berlin (now rue de Liège) he began a tempestuous relationship with Gabrielle ('Gaby') Dupont, a tailor's daughter from Lisieux, soon cohabiting with her on the rue de Londres, and later the rue Gustave Doré. During this time he also had an affair with the singer Thérèse Roger, to whom he was briefly engaged. Such cavalier behaviour was widely condemned, and precipitated the end of his long friendship with Ernest Chausson. He ultimately left Dupont for her friend Rosalie ('Lilly') Texier, a fashion model whom he married in 1899, after threatening suicide if she refused him. However, although Texier was affectionate, practical, straightforward, and well liked by Debussy's friends and associates, he became increasingly irritated by her intellectual limitations and lack of musical sensitivity. In 1904, Debussy was introduced to Emma Bardac, wife of Parisian banker Sigismond Bardac, by her son Raoul, one of his students. In contrast to Texier, Bardac was a sophisticate, a brilliant conversationalist, and an accomplished singer. After despatching Lilly to her father's home at Bichain in Villeneuve-la-Guyard on 15 July 1904, Debussy secretly took Bardac to Jersey for a holiday. On their return to France, Debussy wrote to Texier from Dieppe on 11 August, informing her their marriage was over, but still making no mention of Bardac. Debussy briefly moved to an apartment at 10 avenue Alphand. On 14 October, five days before their fifth wedding anniversary, Texier attempted suicide, shooting herself in the chest with a revolver while standing in the Place de la Concorde; she survived, although the bullet remained lodged in her vertebrae for the rest of her life. The ensuing scandal was to alienate Debussy from many of his friends, whilst Bardac was disowned by her family.
In the spring of 1905, finding the hostility towards them intolerable, Debussy and Bardac (now pregnant) fled to England, via Jersey. Bardac's divorce was finalized in May. The couple settled at the Grand Hotel in Eastbourne from 24 July to 30 August 1905, where Debussy was to correct proofs to his symphonic suite "La mer", and celebrate his divorce from Texier on 2 August.
After a brief visit to London, the couple returned to Paris in September, buying a house in a courtyard development off the Avenue du Bois de Boulogne (now Avenue Foch), where Debussy was to reside for the rest of his life. Their daughter (the composer's only child) Claude-Emma was born there on 30 October. Her parents were eventually married in 1908, their troubled union enduring until Debussy's death in 1918. More affectionately known as 'Chouchou', Claude-Emma was possibly the only person Debussy ever loved, and a great musical inspiration to him (she was the dedicatee of his "Children's Corner" suite). Debussy was to remark towards the end of his life, when gravely ill, that were it not for Chouchou, he might have committed suicide. She outlived her father by scarcely a year, succumbing to the diphtheria epidemic of 1919 after her doctor administered the wrong treatment.
Death.
Debussy died of rectal cancer at his Paris home on 25 March 1918, at the age of 55. He had been diagnosed with the cancer in 1909 after experiencing haemorrhaging, and in December 1915 underwent one of the earliest colostomy operations ever performed. The operation achieved only a temporary respite, and occasioned him considerable frustration (he was to liken dressing in the morning to "all the labours of Hercules in one"). His death occurred in the midst of the aerial and artillery bombardment of Paris during the German Spring Offensive of World War I. The funeral procession made its way through deserted streets to Père Lachaise Cemetery as the German guns bombarded the city. The military situation in France was critical, and did not permit the honour of a public funeral with ceremonious graveside orations. Debussy's body was reinterred the following year in the small Passy Cemetery sequestered behind the Trocadéro, fulfilling his wish to rest 'among the trees and the birds'; his wife and daughter are buried with him.
Music.
Style.
Rudolph Reti points out these features of Debussy's music, which "established a new concept of tonality in European music":
He concludes that Debussy's achievement was the synthesis of monophonic based "melodic tonality" with harmonies, albeit different from those of "harmonic tonality".
The application of the term "impressionist" to Debussy and the music he influenced is a matter of intense debate within academic circles. One side argues that the term is a misnomer, an inappropriate label which Debussy himself opposed. In a letter of 1908, he wrote "I am trying to do 'something different'—an effect of reality ... what the imbeciles call 'impressionism', a term which is as poorly used as possible, particularly by the critics, since they do not hesitate to apply it to [J.M.W.] Turner, the finest creator of mysterious effects in all the world of art." The opposing side argues that Debussy may have been reacting to unfavourable criticism at the time, and the negativity that critics associated with impressionism. It can be argued that he would have been pleased with application of the current definition of impressionism to his music.
Early works.
Beginning in the 1890s, Debussy developed his own musical language largely independent of Wagner's style, collared in part from the dreamy, sometimes morbid romanticism of the Symbolist movement. Debussy became a frequent participant at Stéphane Mallarmé's Symbolist gatherings, where Wagnerism dominated the discussion. In contrast to the enormous works of Wagner and other late-romantic composers, however, around this time Debussy chose to write in smaller, more accessible forms. The "Deux arabesques" is an example of one of Debussy's earliest works, already developing his musical language. "Suite bergamasque" (1890) recalls rococo decorousness with a modern cynicism and puzzlement. This suite contains one of Debussy's most popular pieces, "Clair de Lune". Debussy's String Quartet in G minor (1893) paved the way for his later, more daring harmonic exploration. In this work he used the Phrygian mode as well as less standard scales, such as the whole-tone, which creates a sense of floating, ethereal harmony. Debussy was beginning to employ a single, continuous theme and break away from the traditional A-B-A form, with its restatements and amplifications, which had been a mainstay of classical music since Haydn.
Influenced by Mallarmé, Debussy wrote one of his most famous works, the revolutionary "Prélude à l'après-midi d'un faune", truly original in form and execution. In contrast to the large orchestras so favoured by late-romanticism, Debussy wrote this piece for a smaller ensemble, emphasizing instrumental colour and timbre. Despite Mallarmé himself, and colleague and friend Paul Dukas having been impressed by the piece, it was controversial at its premiere. Nevertheless "Prélude" established Debussy as one of the leading composers of the era.
Middle works.
The three "Nocturnes" (1899) include characteristic studies in veiled harmony and texture as demonstrated in "Nuages"; exuberance in "Fêtes"; and whole-tones in "Sirènes". Contrasting sharply with Wagnerian opera, Debussy's "Pelléas et Mélisande" premiered in 1902, after ten years of work. It would be his only complete opera. Based on the play by Maurice Maeterlinck, the opera proved to be an immediate success and immensely influential to younger French composers, including Maurice Ravel. These works brought a fluidity of rhythm and colour quite new to Western music.
"La mer" (1903–1905) essays a more symphonic form, with a finale that works themes from the first movement, although the middle movement, "Jeux de vagues", proceeds much less directly and with more variety of colour. Again, the reviews were sharply divided. Some critics thought the treatment to be less subtle and less mysterious than his previous works and even a step backward. Pierre Lalo complained "I neither hear, nor see, nor feel the sea". Others extolled its "power and charm", its "extraordinary verve and brilliant fantasy", and its strong colors and definite lines.
During this period Debussy wrote much for the piano. The set of pieces entitled "Pour le piano" (1901) uses rich harmonies and textures which would later prove important in jazz music. His first volume of "Images pour piano" (1904–1905) combine harmonic innovation with poetic suggestion: "Reflets dans l'eau" is a musical description of rippling water; "Hommage à Rameau", the second piece, is slow and yearningly nostalgic. It takes as its inspiration a melody from Jean-Philippe Rameau's 1737 "Castor et Pollux".
The evocative "Estampes" for piano (1903) give impressions of exotic locations. Debussy came into contact with Javanese gamelan music during the 1889 Paris "Exposition Universelle". "Pagodes" is the directly inspired result, aiming for an evocation of the pentatonic structures employed by the Javanese music. Debussy wrote his famous "Children's Corner Suite" (1908) for his beloved daughter, Claude-Emma, whom he nicknamed "Chouchou". The suite recalls classicism—the opening piece "Doctor Gradus ad Parnassum" refers to Muzio Clementi's collection of instructional piano compositions "Gradus ad Parnassum", as well as a new wave of American ragtime music. In the popular final piece of the suite, "Golliwogg's Cakewalk", Debussy also pokes fun at Richard Wagner by mimicking the opening bars of Wagner's prelude to "Tristan und Isolde".
The first book of "Préludes" (1910), twelve in total, proved to be his most successful work for piano. The Preludes are frequently compared to those of Chopin. Debussy's preludes are replete with rich, unusual and daring harmonies. They include the popular "La fille aux cheveux de lin" (The Girl with the Flaxen Hair) and "La Cathédrale Engloutie" (The Engulfed Cathedral). Debussy wanted people to respond intuitively to these pieces so he placed the titles at the end of each one in the hope that listeners would not make stereotype images as they listened.
Larger scaled works included his orchestral piece "Iberia" (1907), begun as a work for two pianos, a triptych medley of Spanish allusions and fleeting impressions and also the music for Gabriele D'Annunzio's mystery play "Le martyre de Saint Sébastien" (1911). A lush and dramatic work, written in only two months, it is remarkable in sustaining a late antique modal atmosphere that otherwise was touched only in relatively short piano pieces.
During this period, as Debussy gained more popularity, he was engaged as a conductor throughout Europe, most often performing "Pelléas", "La Mer", and "Prélude à l'après-midi d'un faune". He was also an occasional music critic to supplement his conducting fees and piano lessons. Debussy avoided analytical dissection and attempts to force images from music, "Let us at all costs preserve this magic peculiar to music, since of all the arts it is most susceptible to magic." He could be caustic and witty, sometimes sloppy and ill-informed. Debussy was for the most part enthusiastic about Richard Strauss and Stravinsky, worshipful of Chopin and Bach, the latter being acknowledged as "the one great master." His relationship to Beethoven was a complex one; he was said to refer to him as ""le vieux sourd"" (the old deaf one) and abjured one young pupil never to play Beethoven's music for "it is like somebody dancing on my grave." However, Debussy made other statements about Beethoven which seem to suggest an admiration tempered by critical views, as it was said: "Debussy liked Mozart, and he believed that Beethoven had terrifically profound things to say, but that he did not know how to say them, because he was imprisoned in a web of incessant restatement and of German aggressiveness." He also admired the works of Charles-Valentin Alkan. Schubert and Mendelssohn fared much worse, the latter being described as a "facile and elegant notary".
Late works.
Debussy's harmonies and chord progressions frequently exploit dissonances without any formal resolution. Unlike in his earlier work, he no longer hides discords in lush harmonies. The forms are far more irregular and fragmented. These chords that seemingly had no resolution were described by Debussy himself as "floating chords", and were used to set tone and mood in many of his works. The whole tone scale dominates much of Debussy's late music.
His two last volumes of works for the piano, the "Études" (1915) interprets similar varieties of style and texture purely as pianistic exercises and includes pieces that develop irregular form to an extreme as well as others influenced by the young Igor Stravinsky (a presence too in the suite "En blanc et noir" for two pianos, 1915). The rarefaction of these works is a feature of the last set of songs, the "Trois poèmes de Mallarmé" (1913), and of the Sonata for flute, viola and harp (1915), though the sonata and its companions also recapture the inquisitive Verlainian classicism.
With the sonatas of 1915–1917, there is a sudden shift in the style. These works recall Debussy's earlier music, in part, but also look forward, with leaner, simpler structures. Despite the thinner textures of the Violin Sonata (1917) there remains an undeniable richness in the chords themselves. This shift parallels the movement commonly known as neo-classicism which became popular after Debussy's death. Debussy planned a set of six sonatas, but this plan was cut short by his death in 1918 so that he only completed three (cello, flute-viola-harp and violin) sonatas.
The last orchestral work by Debussy, the ballet "Jeux" (1912) written for Sergei Diaghilev's Ballets Russes, contains some of his strangest harmonies and textures in a form that moves freely over its own field of motivic connection. At first "Jeux" was overshadowed by Igor Stravinsky's "The Rite of Spring", composed in the same year as "Jeux" and premiered only two weeks later by the same ballet company. Decades later, composers such as Pierre Boulez and Jean Barraqué pointed out parallels to Anton Webern's serialism in this work. Other late stage works, including the ballets "Khamma" (1912) and "La boîte à joujoux" (1913) were left with the orchestration incomplete, and were later completed by Charles Koechlin and André Caplet, who also helped Debussy with the orchestration of "Gigues" (from "Images pour orchestre") and "Le martyre de St. Sébastien".
The second set of "Préludes" for piano (1913) features Debussy at his most avant-garde, where he uses dissonant harmonies to evoke specific moods and images. Debussy consciously gives titles to each prelude that amplify the preludes' tonal ambiguity and dissonance. He uses scales such as the whole tone scale, musical modes, and the octatonic scale in his preludes that exaggerate this tonal ambiguity, making the key of each prelude almost indistinguishable at times. The second book of Preludes for piano represents Debussy's strong interest in the indefinite and esoteric.
Although "Pelléas" was Debussy's only completed opera, he began several opera projects which remained unfinished, his fading concentration, increasing procrastination, and failing health perhaps the reasons. He had finished some partial musical sketches and some unpublished libretti for operas based on Poe's "The Devil in the Belfry" ("Le diable dans le beffroi", 1902–?1912) and "The Fall of the House of Usher" ("La chute de la maison Usher", 1908–1917) as well as considered projects for operas based on Shakespeare's "As You Like It" and Joseph Bedier's "La Legende de Tristan".
Further plans, such as an American tour, more ballet scores, and revisions of Chopin and Bach works for re-publication, were all cut short by the outbreak of World War I and his poor health.
Mathematical structuring.
Some people have claimed that Debussy structured parts of his music mathematically. Roy Howat, for instance, has published a book contending that Debussy's works are structured around mathematical models even while using an apparent classical structure such as sonata form. Howat suggests that some of Debussy's pieces can be divided into sections that reflect the golden ratio, frequently by using the numbers of the standard Fibonacci sequence.
Influences.
Debussy had a wide range of influences. Among the Russian composers of his time, the most prominent influences were Tchaikovsky, Balakirev, Rimsky-Korsakov, Borodin and Mussorgsky. It can be inferred that from the Russians "Debussy acquired his taste for ancient and oriental modes and for vivid colorations, and a certain disdain for academic rules." Specifically, Mussorgsky's opera "Boris Godunov" directly influenced one of Debussy's most famous works, "Pelléas et Mélisande". In addition to the Russian composers, one of Debussy's biggest influences was Richard Wagner. According to Pierre Louys, Debussy "did not see 'what anyone can do beyond Tristan. After Debussy's Wagner phase, he started to become immensely interested in non-Western music. He was drawn to unorthodox approaches to composition that non-Western music used. Specifically, he was drawn to a Javanese Gamelan, which was a musical ensemble from the island of Java that played an array of unique instrumentation including gongs and metallophones. He first heard the gamelan at the 1889 Paris Exposition. Debussy was not as interested in directly citing his non-Western influence in his music, but instead used his non-Western influence to shape his unique musical style in more of a general way.
Debussy was just as influenced by other art forms as he was by music, if not more so. He took a strong interest in literature and visual art and used these mediums to help shape his unique musical style. Debussy was heavily influenced by the French symbolist movement, an art movement from the 1880s that influenced art forms such as poetry, visual art, and theatre. He shared the movement's interest in the esoteric and indefinite and rejection of naturalism and realism. Specifically, "the development of free verse in poetry and the disappearance of the subject or model in painting influenced Debussy to think about issues of musical form." Debussy became personally acquainted with writers and painters of the movement and based his own works off of those of the symbolists. One of Debussy's main influences was the famous poet Stéphane Mallarmé, who "held the idea of a 'musicalization' of poetry." In other words, Mallarmé drew strong connections between music and his poetry. Debussy wrote "Prélude à l'après-midi d'un faune", which was directly influenced by Mallarmé's poem "Afternoon of a Faun". Like the symbolists in respect to their own art forms, Debussy aimed to reject common techniques and approaches to composition and attempted to evoke more of a sensorial experience for the listener with his works. Since his time at the Paris Conservatoire, Debussy believed he had much more to learn from artists than from musicians who were primarily interested in their musical careers.
Above all, Debussy was inspired by nature and the impression it made on the mind; he called "mysterious Nature" his religion. He made a pantheistic profession of faith: 'I do not practice religion in accordance with the sacred rites. I have made mysterious Nature my religion. I do not believe that a man is any nearer to God for being clad in priestly garments, nor that one place in a town is better adapted to meditation than another. When I gaze at a sunset sky and spend hours contemplating its marvellous ever-changing beauty, an extraordinary emotion overwhelms me. Nature in all its vastness is truthfully reflected in my sincere though feeble soul. Around me are the trees stretching up their branches to the skies, the perfumed flowers gladdening the meadow, the gentle grass-carpetted earth, ... and my hands unconsciously assume an attitude of adoration. ... To feel the supreme and moving beauty of the spectacle to which Nature invites her ephemeral guests! ... that is what I call prayer.'
Contemporary painter James McNeill Whistler who lived in France for a period of time had a profound influence on Debussy. In 1894, Debussy wrote to violinist Eugène Ysaÿe describing his "Nocturnes" as "an experiment in the different combinations that can be obtained from one color—what a study in grey would be in painting." Although it is not known what it is meant by this statement, one can observe in his music a careful use of orchestral, textural, and harmonic 'shading'.
Influence on later composers.
Claude Debussy is widely regarded as one of the most influential composers of the 20th century. His innovative harmonies were influential to almost every major composer of the 20th century, particularly Maurice Ravel, Igor Stravinsky, Olivier Messiaen, Béla Bartók, Pierre Boulez, Henri Dutilleux, Ned Rorem, George Gershwin, and the minimalist music of Steve Reich and Philip Glass as well as the influential Japanese composer Toru Takemitsu. He also influenced many important figures in jazz, most notably Miles Davis, Duke Ellington, Bix Beiderbecke, George Shearing, Thelonious Monk, Bill Evans, Jimmy Giuffre, Antônio Carlos Jobim, and Herbie Hancock. Furthermore, he had a profound impact on contemporary soundtrack composers such as John Williams because Debussy's colourful and evocative style translated easily into an emotional language for use in motion picture scores. In 1999, The Art of Noise released a concept album titled "The Seduction of Claude Debussy". The group blended the music of Debussy with drum and bass, opera, hip hop, jazz, and narration, and described the album as "the soundtrack to a film that wasn't made about the life of Claude Debussy". In 2000, the band released "Reduction", a limited-edition album composed mainly of outtakes from this album.
Leopold Stokowski, in an article, pointed out the identification of composers including Debussy with the music of Giovanni Pierluigi da Palestrina, providing an inspiration for non-contrapuntal music.
On 22 August 2013, Debussy's birthday anniversary, Google dedicated a celebratory Google Doodle to him that played the first half of "Clair de Lune".
Eponyms.
Debussy's name has posthumously been given to a number of discoveries. These include:
Recordings.
Debussy participated in a handful of recordings, made in 1904, with soprano Mary Garden. He also made some piano rolls for Welte Mignon in 1913.

</doc>
<doc id="6261" url="http://en.wikipedia.org/wiki?curid=6261" title="Charles Baxter (author)">
Charles Baxter (author)

Charles Baxter (born May 13, 1947) is an American author of fiction, nonfiction and poetry.
Biography.
Baxter was born in Minneapolis, Minnesota, to John and Mary Barber (Eaton) Baxter. He graduated from Macalester College in Saint Paul. In 1974 he received his PhD in English from the University at Buffalo with a thesis on Djuna Barnes, Malcolm Lowry, and Nathanael West.
Career.
Teaching.
Baxter taught high school in Pinconning, Michigan for a year before beginning his university teaching career at Wayne State University in Detroit, Michigan. He then moved to the University of Michigan, where for many years he directed the Creative Writing MFA program. Many of his students have gone on to successful writing careers; they include Gretchen Mazur, Helen Fremont, Michael Byers, Jardine Libaire, Porter Shreve, Davy Rothbart, John Fulton, Marc Nesbitt, Patrick O'Keeffe, Jess Row, Francesca Delbano, Peter Orner, Heidi Julavits, Karl Iagnemma, Achy Obejas, James Morrison and Elwood Reid. He currently teaches at the University of Minnesota and in the Warren Wilson College MFA Program for Writers.
Writing.
Baxter's literary work is recognized and highlighted by Michigan State University in their Michigan Writers Series.

</doc>
<doc id="6262" url="http://en.wikipedia.org/wiki?curid=6262" title="Ceres">
Ceres

Ceres commonly refers to:
Ceres may refer to:

</doc>
<doc id="6267" url="http://en.wikipedia.org/wiki?curid=6267" title="Cultural imperialism">
Cultural imperialism

Cultural imperialism is defined as the cultural aspects of imperialism. Imperialism, here, is referring to the creation and maintenance of unequal relationships between civilizations favoring the more powerful civilization. Therefore, it can be defined as the practice of promoting and imposing a culture, usually of politically powerful nations over less potent societies. It is the cultural hegemony of those industrialized or economically influential countries, which determine general cultural values and standardize civilizations throughout the world. Many scholars employ the term, especially those in the fields of history, cultural studies, and postcolonial theory. The term is usually used in a pejorative sense, often in conjunction with a call to reject such influence. "Cultural imperialism" can take various forms, such as an attitude, a formal policy, military action, so long as it reinforces cultural hegemony.
Background and definitions.
The term emerged in the 1960s and has been a focus of research since at least the 1970s. Terms such as "media imperialism", "structural imperialism", "cultural dependency and domination", "cultural synchronization", "electronic colonialism", "ideological imperialism", and "economic imperialism" have all been used to describe the same basic notion of cultural imperialism.
Various academics give various definitions of the term. American media critic Herbert Schiller wrote: "The concept of cultural imperialism today best describes the sum of the processes by which a society is brought into the modern world system and how its dominating stratum is attracted, pressured, forced, and sometimes bribed into shaping social institutions to correspond to, or even promote, the values and structures of the dominating centre of the system. The public media are the foremost example of operating enterprises that are used in the penetrative process. For penetration on a significant scale the media themselves must be captured by the dominating/penetrating power. This occurs largely through the commercialization of broadcasting."
Tom McPhail defined "Electronic colonialism as the dependency relationship established by the importation of communication hardware, foreign-produced software, along with engineers, technicians, and related information protocols, that vicariously establish a set of foreign norms, values, and expectations which, in varying degrees, may alter the domestic cultures and socialization processes." Sui-Nam Lee observed that "communication imperialism can be defined as the process in which the ownership and control over the hardware and software of mass media as well as other major forms of communication in one country are singly or together subjugated to the domination of another country with deleterious effects on the indigenous values, norms and culture." Ogan saw "media imperialism often described as a process whereby the United States and Western Europe produce most of the media products, make the first profits from domestic sales, and then market the products in Third World countries at costs considerably lower than those the countries would have to bear to produce similar products at home."
Downing and Sreberny-Mohammadi state: "Imperialism is the conquest and control of one country by a more powerful one. Cultural imperialism signifies the dimensions of the process that go beyond economic exploitation or military force. In the history of colonialism, (i.e., the form of imperialism in which the government of the colony is run directly by foreigners), the educational and media systems of many Third World countries have been set up as replicas of those in Britain, France, or the United States and carry their values. Western advertising has made further inroads, as have architectural and fashion styles. Subtly but powerfully, the message has often been insinuated that Western cultures are superior to the cultures of the Third World."
Needless to say, all these authors agree that cultural imperialism promotes the interests of certain circles within the imperial powers, often to the detriment of the target societies.
The issue of cultural imperialism emerged largely from communication studies. However, cultural imperialism has been used as a framework by scholars to explain phenomena in the areas of international relations, anthropology, education, science, history, literature, and sports.
Theoretical foundations.
Many of today's academics that employ the term, "cultural imperialism," are heavily informed by the work of Foucault, Derrida, Said, and other poststructuralist and postcolonialist theorists. Within the realm of postcolonial discourse, "cultural imperialism" can be seen as the cultural legacy of colonialism, or forms of social action contributing to the continuation of Western hegemony. To some outside of the realm of this discourse, the term is critiqued as being unclear, unfocused, and/or contradictory in nature 
Michel Foucault.
The work of French philosopher and social theorist Michel Foucault has heavily influenced use of the term "cultural imperialism," particularly his philosophical interpretation of power and his concept of governmentality.
Following an interpretation of power similar to that of Machiavelli, Foucault defines power as immaterial, as a "certain type of relation between individuals" that has to do with complex strategic social positions that relate to the subject's ability to control its environment and influence those around itself. According to Foucault, power is intimately tied with his conception of truth. "Truth," as he defines it, is a "system of ordered procedures for the production, regulation, distribution, circulation, and operation of statements" which has a "circular relation" with systems of power. Therefore, inherent in systems of power, is always "truth," which is culturally specific, inseparable from ideology which often coincides with various forms of hegemony. "Cultural imperialism" may be an example of this.
Foucault's interpretation of governance is also very important in constructing theories of transnational power structure. In his lectures at the Collège de France, Foucault often defines governmentality as the broad art of "governing," which goes beyond the traditional conception of governance in terms of state mandates, and into other realms such as governing "a household, souls, children, a province, a convent, a religious order, a family". This relates directly back to Machiavelli's The Prince, and Foucault's aforementioned conceptions of truth and power. (i.e. various subjectivities are created through power relations that are culturally specific, which lead to various forms of culturally specific governmentality such as neoliberal governmentality.)
Edward Saïd.
Informed by the works of Noam Chomsky, Michel Foucault, and Antonio Gramsci, Edward Saïd is a founding figure of Post-colonialism, established with the book "Orientalism" (1978), a humanist critique of The Enlightenment, which criticizes Western knowledge of “The East” — specifically the English and the French constructions of what is and what is not “Oriental”. Whereby said “knowledge” then led to cultural tendencies towards a binary opposition of the Orient vs. the Occident, wherein one concept is defined in opposition to the other concept, and from which they emerge as of unequal value. In "Culture and Imperialism" (1993), the sequel to "Orientalism", Saïd proposes that, despite the formal end of the “age of empire” after the Second World War (1939–45), colonial imperialism left a cultural legacy to the (previously) colonized peoples, which remains in their contemporary civilizations; and that said "cultural imperialism" is very influential in the international systems of power.
Gayatri Chakravorty Spivak.
Another influential voice in discussing matters of "cultural imperialism" is the self-described " practical Marxist-feminist-deconstructionist," Gayatri Chakravorty Spivak. Spivak has published a number of works challenging the "legacy of colonialism" including "A Critique of Postcolonial Reason: Towards a History of the Vanishing Present" (1999), "Other Asias" (2005), and "Can the Subaltern Speak?" (1988).
In "Can the Subaltern Speak?" Spivak critiques common representations in the West of the Sati, as being controlled by authors other than the participants (specifically English colonizers and Hindu leaders). Because of this, Spivak argues that the subaltern, referring to the communities that participate in the Sati, are not able to represent themselves through their own voice.
In "A critique of Postcolonial Reason", Spivak argues that Western philosophy has a history of not only exclusion of the Subaltern from discourse, but also does not allow them to occupy the space of a fully human subject.
Contemporary ideas and debate.
"Cultural imperialism" can refer to either the forced acculturation of a subject population, or to the voluntary embracing of a foreign culture by individuals who do so of their own free will. Since these are two very different referents, the validity of the term has been called into question.
Cultural influence can be seen by the "receiving" culture as either a threat to or an enrichment of its cultural identity. It seems therefore useful to distinguish between cultural imperialism as an (active or passive) attitude of superiority, and the position of a culture or group that seeks to complement its own cultural production, considered partly deficient, with imported products.
The imported products or services can themselves represent, or be associated with, certain values (such as consumerism). According to one argument, the "receiving" culture does not necessarily perceive this link, but instead absorbs the foreign culture passively through the use of the foreign goods and services. Due to its somewhat concealed, but very potent nature, this hypothetical idea is described by some experts as ""banal imperialism"." For example, it is argued that while "American companies are accused of wanting to control 95 percent of the world's consumers", "cultural imperialism involves much more than simple consumer goods; it involved the dissemination of American principles such as freedom and democracy", a process which "may sound appealing" but which "masks a frightening truth: many cultures around the world are disappearing due to the overwhelming influence of corporate and cultural America".
Some believe that the newly globalised economy of the late 20th and early 21st century has facilitated this process through the use of new information technology. This kind of cultural imperialism is derived from what is called "soft power". The theory of electronic colonialism extends the issue to global cultural issues and the impact of major multi-media conglomerates, ranging from Viacom, Time-Warner, Disney, News Corp, Sony, to Google and Microsoft with the focus on the hegemonic power of these mainly United States-based communication giants.
American Cultural Imperialism: a gift or a threat?
When talking about cultural imperialism, it is often referred to the proliferation of Western moral concepts, products, and political beliefs around the globe. The United States are not currently the only cultural imperialists, but today, as a global economic and political superpower, the spread of American values in the entire world is at the leading edge of a wave of spread of Western goods and consumerist culture. Some people believe that the spread of American beliefs and concepts of universal values are beneficial to most nations because their propagations of ideas such as freedom, democracy, equality, and human rights are concepts that should be, in some people’s opinion, universal indeed. Proponents argue that their contributions of modern ways of thinking and standards of becoming part of the industrialized and modernized world, make world society better-off.[http://gsevenier.free.fr/culturalImperialism.html]
Others, on the contrary, consider this American cultural hegemony as a threat. Indeed they may be positively helping countries, but these benefits inevitably come at the cost of hurting local markets and local cultures. While traditional cultural values are progressively being wiped away, critics argue, the world is increasingly stepping towards a process of cultural synchronization in which a common global culture based on imperialists societies is becoming more evident. This cultural uniformity would predictably lead to the extinction of cultures and make the world less culturally rich and diverse.
Cultural diversity.
One of the reasons often given for opposing any form of cultural imperialism, voluntary or otherwise, is the preservation of cultural diversity, a goal seen by some as analogous to the preservation of ecological diversity. Proponents of this idea argue either that such diversity is valuable in itself, to preserve human historical heritage and knowledge, or instrumentally valuable because it makes available more ways of solving problems and responding to catastrophes, natural or otherwise.
Ideas relating to African colonization.
Of all the areas of the world that scholars have claimed to be adversely affected by imperialism, Africa is probably the most notable. In the expansive "age of imperialism" of the nineteenth century, scholars have argued that European colonization in Africa has led to the elimination of many various cultures, worldviews, and epistemologies. This, arguably has led to uneven development, and further informal forms of social control having to do with culture and imperialism. A variety of factors, scholars argue, lead to the elimination of cultures, worldviews, and epistemologies, such as "de-linguicization" (replacing native African languages with European ones) and devaluing ontologies that are not explicitly individualistic. One scholar, Ali A. Obdi, claims that imperialism inherently "involve extensively interactive regimes and heavy contexts of identity deformation, misrecognition, loss of self-esteem, and individual and social doubt in self-efficacy."(2000: 12) Therefore, all imperialism would always, already be cultural.
Ties to neoliberalism.
Neoliberalism is often critiqued by sociologists, anthropologists, and cultural studies scholars as being culturally imperialistic. Critics of neoliberalism, at times, claim that it is the newly predominant form of imperialism. Other Scholars, such as Elizabeth Dunn and Julia Elyachar have claimed that neoliberalism requires and creates its own form of governmentality.
In Dunn's work, "Privatizing Poland", she argues that the expansion of the multinational corporation, Gerber, into Poland in the 1990s imposed Western, neoliberal governmentality, ideologies, and epistemologies upon the post-soviet persons hired. Cultural conflicts occurred most notably the company's inherent individualistic policies, such as promoting competition among workers rather than cooperation, and in its strong opposition to what the company owners claimed was bribery.
In Elyachar's work, "Markets of Dispossession", she focuses on ways in which, in Cairo, NGOs along with INGOs and the state promoted neoliberal governmentality through schemas of economic development that relied upon "youth microentrepreneurs." Youth microentrepreneurs would receive small loans to build their own businesses, similar to the way that microfinance supposedly operates. Elyachar argues though, that these programs not only were a failure, but that they shifted cultural opinions of value (personal and cultural) in a way that favored Western ways of thinking and being 
Ties to development studies.
Often, methods of promoting development and social justice to are critiqued as being imperialistic, in a cultural sense. For example, Chandra Mohanty has critiqued Western feminism, claiming that it has created a misrepresentation of the "third world woman" as being completely powerless, unable to resist male dominance. Thus, this leads to the often critiqued narrative of the "white man" saving the "brown woman" from the "brown man." Other, more radical critiques of development studies, have to do with the field of study itself. Some scholars even question the intentions of those developing the field of study, claiming that efforts to "develop" the Global South were never about the South itself. Instead, these efforts, it is argued, were made in order to advance Western development and reinforce Western hegemony.
Ties to Media Effects Studies.
The core of cultural imperialism thesis is integrated with the political-economy traditional approach in media effects research. Critics of cultural imperialism commonly claim that non-Western cultures, particularly from the Third World, will forsake their traditional values and lose their cultural identities when they are solely exposed to Western media. Nonetheless, Michael B. Salwen, in his book "Critical Studies in Mass Communication" (1991), claims that cross-consideration and integration of empirical findings on cultural imperialist influences is very critical in terms of understanding mass media in the international sphere. He recognizes both of contradictory contexts on cultural imperialist impacts. 
The first context is where cultural imperialism imposes socio-political disruptions on developing nations. Western media can distort images of foreign cultures and provoke personal and social conflicts to developing nations in some cases. 
Another context is that peoples in developing nations resist to foreign media and preserve their cultural attitudes. Although he admits that outward manifestations of Western culture may be adopted, but the fundamental values and behaviors remain still. Furthermore, positive effects might occur when male-dominated cultures adopt the “liberation” of women with exposure to Western media and it stimulates ample exchange of cultural exchange.
Criticisms of "cultural imperialism theory".
Critics of scholars who discuss cultural imperialism have a number of critiques. "Cultural imperialism" is a term that is only used in discussions where cultural relativism and constructivism are generally taken as true. (One cannot critique promoting Western values if one believes that said values are absolutely correct. Similarly, one cannot argue that Western epistemology is unjustly promoted in non-Western societies if one believes that those epistemologies are absolutely correct.) Therefore, those who disagree with cultural relativism and/or constructivism may critique the employment of the term, "cultural imperialism" on those terms.
John Tomlinson provides a critique of cultural imperialism theory and reveals major problems in the way in which the idea of cultural, as opposed to economic or political, imperialism is formulated. In his book "Cultural Imperialism: A Critical Introduction", he delves into the much debated “media imperialism” theory. Summarizing research on the Third World’s reception of American television shows, he challenges the cultural imperialism argument, conveying his doubts about the degree to which US shows in developing nations actually carry US values and improve the profits of US companies. Tomlinson suggests that cultural imperialism is growing in some respects, but local transformation and interpretations of imported media products propose that cultural diversification is not at an end in global society. He explains that one of the fundamental conceptual mistakes of cultural imperialism is to take for granted that the distribution of cultural goods can be considered as cultural dominance. He thus supports his argument highly criticizing the concept that Americanization is occurring through global overflow of American television products. He points to a myriad of examples of television networks who have managed to dominate their domestic markets and that domestic programs generally top the ratings. He also doubts the concept that cultural agents are passive receivers of information. He states that movement between cultural/geographical areas always involves translation, mutation, adaptation, and the creation of hybridity.
Other major critiques are that the term is not defined well, and employs further terms that are not defined well, and therefore lacks explanatory power, that "cultural imperialism" is hard to measure, and that the theory of a legacy of colonialism is not always true.
Rothkopf on dealing with cultural dominance.
David Rothkopf, managing director of Kissinger Associates and an adjunct professor of international affairs at Columbia University (who also served as a senior US Commerce Department official in the Clinton Administration), wrote about cultural imperialism in his provocatively titled "In Praise of Cultural Imperialism?" in the summer 1997 issue of "Foreign Policy" magazine. Rothkopf says that the United States should embrace "cultural imperialism" as in its self-interest. But his definition of cultural imperialism stresses spreading the values of tolerance and openness to cultural change in order to avoid war and conflict between cultures as well as expanding accepted technological and legal standards to provide free traders with enough security to do business with more countries. Rothkopf's definition almost exclusively involves allowing individuals in other nations to accept or reject foreign cultural influences. He also mentions, but only in passing, the use of the English language and consumption of news and popular music and film as cultural dominance that he supports. Rothkopf additionally makes the point that globalization and the Internet are accelerating the process of cultural influence.
Culture is sometimes used by the organizers of society — politicians, theologians, academics, and families — to impose and ensure order, the rudiments of which change over time as need dictates. One need only look at the 20th century's genocides. In each one, leaders used culture as a political front to fuel the passions of their armies and other minions and to justify their actions among their people.
Rothkopf then cites genocide and in Armenia, Russia, the Holocaust, Cambodia, Bosnia and Herzegovina, Rwanda and East Timor as examples of culture (in some cases expressed in the ideology of "political culture" or religion) being misused to justify violence. He also acknowledges that cultural imperialism in the past has been guilty of forcefully eliminating the cultures of natives in the Americas and in Africa, or through use of the Inquisition, ""and during the expansion of virtually every empire."".The most important way to deal with cultural influence in any nation, according to Rothkopf, is to promote tolerance and allow, or even promote, cultural diversities that are compatible with tolerance and to eliminate those cultural differences that cause violent conflict:

</doc>
<doc id="6271" url="http://en.wikipedia.org/wiki?curid=6271" title="Chemical reaction">
Chemical reaction

A chemical reaction is a process that leads to the transformation of one set of chemical substances to another. Classically, chemical reactions encompass changes that only involve the positions of electrons in the forming and breaking of chemical bonds between atoms, with no change to the nuclei (no change to the elements present), and can often be described by a chemical equation. Nuclear chemistry is a sub-discipline of chemistry that involves the chemical reactions of unstable and radioactive elements where both electronic and nuclear changes may occur.
The substance (or substances) initially involved in a chemical reaction are called reactants or reagents. Chemical reactions are usually characterized by a chemical change, and they yield one or more products, which usually have properties different from the reactants. Reactions often consist of a sequence of individual sub-steps, the so-called elementary reactions, and the information on the precise course of action is part of the reaction mechanism. Chemical reactions are described with chemical equations, which graphically present the starting materials, end products, and sometimes intermediate products and reaction conditions.
Chemical reactions happen at a characteristic reaction rate at a given temperature and chemical concentration, and rapid reactions are often described as spontaneous, requiring no input of extra energy other than thermal energy. Non-spontaneous reactions run so slowly that they are considered to require the input of some type of additional energy (such as extra heat, light or electricity) in order to proceed to completion (chemical equilibrium) at human time scales.
Different chemical reactions are used in combinations during chemical synthesis in order to obtain a desired product. In biochemistry, a similar series of chemical reactions form metabolic pathways. These reactions are often catalyzed by protein enzymes. These enzymes increase the rates of biochemical reactions, so that metabolic syntheses and decompositions impossible under ordinary conditions may be performed at the temperatures and concentrations present within a cell.
The general concept of a chemical reaction has been extended to non-chemical reactions between entities smaller than atoms, including nuclear reactions, radioactive decays, and reactions between elementary particles as described by quantum field theory.
History.
Chemical reactions such as combustion in the fire, fermentation and the reduction of ores to metals were known since antiquity. Initial theories of transformation of materials were developed by Greek philosophers, such as the Four-Element Theory of Empedocles stating that any substance is composed of the four basic elements – fire, water, air and earth. In the Middle Ages, chemical transformations were studied by Alchemists. They attempted, in particular, to convert lead into gold, for which purpose they used reactions of lead and lead-copper alloys with sulfur.
The production of chemical substances that do not normally occur in nature has long been tried, such as the synthesis of sulfuric and nitric acids attributed to the controversial alchemist Jābir ibn Hayyān. The process involved heating of sulfate and nitrate minerals such as copper sulfate, alum and saltpeter. In the 17th century, Johann Rudolph Glauber produced hydrochloric acid and sodium sulfate by reacting sulfuric acid and sodium chloride. With the development of the lead chamber process in 1746 and the Leblanc process, allowing large-scale production of sulfuric acid and sodium carbonate, respectively, chemical reactions became implemented into the industry. Further optimization of sulfuric acid technology resulted in the contact process in 1880s, and the Haber process was developed in 1909–1910 for ammonia synthesis.
From the 16th century, researchers including Jan Baptist van Helmont, Robert Boyle and Isaac Newton tried to establish theories of the experimentally observed chemical transformations. The phlogiston theory was proposed in 1667 by Johann Joachim Becher. It postulated the existence of a fire-like element called "phlogiston", which was contained within combustible bodies and released during combustion. This proved to be false in 1785 by Antoine Lavoisier who found the correct explanation of the combustion as reaction with oxygen from the air.
Joseph Louis Gay-Lussac recognized in 1808 that gases always react in a certain relationship with each other. Based on this idea and the atomic theory of John Dalton, Joseph Proust had developed the law of definite proportions, which later resulted in the concepts of stoichiometry and chemical equations.
Regarding the organic chemistry, it was long believed that compounds obtained from living organisms were too complex to be obtained synthetically. According to the concept of vitalism, organic matter was endowed with a "vital force" and distinguished from inorganic materials. This separation was ended however by the synthesis of urea from inorganic precursors by Friedrich Wöhler in 1828. Other chemists who brought major contributions to organic chemistry include Alexander William Williamson with his synthesis of ethers and Christopher Kelk Ingold, who, among many discoveries, established the mechanisms of substitution reactions.
Equations.
Chemical equations are used to graphically illustrate chemical reactions. They consist of chemical or structural formulas of the reactants on the left and those of the products on the right. They are separated by an arrow (→) which indicates the direction and type of the reaction; the arrow is read as the word "yields". The tip of the arrow points in the direction in which the reaction proceeds. A double arrow () pointing in opposite directions is used for equilibrium reactions. Equations should be balanced according to the stoichiometry, the number of atoms of each species should be the same on both sides of the equation. This is achieved by scaling the number of involved molecules ("A, B, C" and "D" in a schematic example below) by the appropriate integers "a, b, c" and "d".
More elaborate reactions are represented by reaction schemes, which in addition to starting materials and products show important intermediates or transition states. Also, some relatively minor additions to the reaction can be indicated above the reaction arrow; examples of such additions are water, heat, illumination, a catalyst, etc. Similarly, some minor products can be placed below the arrow, often with a minus sign.
Retrosynthetic analysis can be applied to design a complex synthesis reaction. Here the analysis starts from the products, for example by splitting selected chemical bonds, to arrive at plausible initial reagents. A special arrow (⇒) is used in retro reactions.
Elementary reactions.
The elementary reaction is the smallest division into which a chemical reaction can be decomposed to, it has no intermediate products. Most experimentally observed reactions are built up from many elementary reactions that occur in parallel or sequentially. The actual sequence of the individual elementary reactions is known as reaction mechanism. An elementary reaction involves a few molecules, usually one or two, because of the low probability for several molecules to meet at a certain time.
The most important elementary reactions are unimolecular and bimolecular reactions. Only one molecule is involved in a unimolecular reaction; it is transformed by an isomerization or a dissociation into one or more other molecules. Such reactions require the addition of energy in the form of heat or light. A typical example of a unimolecular reaction is the cis–trans isomerization, in which the cis-form of a compound converts to the trans-form or vice versa.
In a typical dissociation reaction, a bond in a molecule splits (ruptures) resulting in two molecular fragments. The splitting can be homolytic or heterolytic. In the first case, the bond is divided so that each product retains an electron and becomes a neutral radical. In the second case, both electrons of the chemical bond remain with one of the products, resulting in charged ions. Dissociation plays an important role in triggering chain reactions, such as hydrogen–oxygen or polymerization reactions.
For bimolecular reactions, two molecules collide and react with each other. Their merger is called chemical synthesis or an addition reaction.
Another possibility is that only a portion of one molecule is transferred to the other molecule. This type of reaction occurs, for example, in redox and acid-base reactions. In redox reactions, the transferred particle is an electron, whereas in acid-base reactions it is a proton. This type of reaction is also called metathesis.
for example
Chemical equilibrium.
Most chemical reactions are reversible, that is they can and do run in both directions. The forward and reverse reactions are competing with each other and differ in reaction rates. These rates depend on the concentration and therefore change with time of the reaction: the reverse rate gradually increases and becomes equal to the rate of the forward reaction, establishing the so-called chemical equilibrium. The time to reach equilibrium depends on such parameters as temperature, pressure and the materials involved, and is determined by the minimum free energy. In equilibrium, the Gibbs free energy must be zero. The pressure dependence can be explained with the Le Chatelier's principle. For example, an increase in pressure due to decreasing volume causes the reaction to shift to the side with the fewer moles of gas.
The reaction yield stabilizes at equilibrium, but can be increased by removing the product from the reaction mixture or changed by increasing the temperature or pressure. A change in the concentrations of the reactants does not affect the equilibrium constant, but does affect the equilibrium position.
Thermodynamics.
Chemical reactions are determined by the laws of thermodynamics. Reactions can proceed by themselves if they are exergonic, that is if they release energy. The associated free energy of the reaction is composed of two different thermodynamic quantities, enthalpy and entropy:
Reactions can be exothermic, where ΔH is negative and energy is released. Typical examples of exothermic reactions are precipitation and crystallization, in which ordered solids are formed from disordered gaseous or liquid phases. In contrast, in endothermic reactions, heat is consumed from the environment. This can occur by increasing the entropy of the system, often through the formation of gaseous reaction products, which have high entropy. Since the entropy increases with temperature, many endothermic reactions preferably take place at high temperatures. On the contrary, many exothermic reactions such as crystallization occur at low temperatures. Changes in temperature can sometimes reverse the sign of the enthalpy of a reaction, as for the carbon monoxide reduction of molybdenum dioxide:
This reaction to form carbon dioxide and molybdenum is endothermic at low temperatures, becoming less so with increasing temperature. ΔH° is zero at , and the reaction becomes exothermic above that temperature.
Changes in temperature can also reverse the direction tendency of a reaction. For example, the water gas shift reaction
is favored by low temperatures, but its reverse is favored by high temperature. The shift in reaction direction tendency occurs at .
Reactions can also be characterized by the internal energy which takes into account changes in the entropy, volume and chemical potential. The latter depends, among other things, on the activities of the involved substances.
Kinetics.
The speed at which a reactions takes place is studied by reaction kinetics. The rate depends on various parameters, such as:
Several theories allow calculating the reaction rates at the molecular level. This field is referred to as reaction dynamics. The rate "v" of a first-order reaction, which could be disintegration of a substance A, is given by:
Its integration yields:
Here k is first-order rate constant having dimension 1/time, is concentration at a time "t" and [A0 is the initial concentration. The rate of a first-order reaction depends only on the concentration and the properties of the involved substance, and the reaction itself can be described with the characteristic half-life. More than one time constant is needed when describing reactions of higher order. The temperature dependence of the rate constant usually follows the Arrhenius equation:
where Ea is the activation energy and kB is the Boltzmann constant. One of the simplest models of reaction rate is the collision theory. More realistic models are tailored to a specific problem and include the transition state theory, the calculation of the potential energy surface, the Marcus theory and the Rice–Ramsperger–Kassel–Marcus (RRKM) theory.
Reaction types.
Four basic types.
Synthesis.
In a synthesis reaction, two or more simple substances combine to form a more complex substance. These reactions are in the general form:
Two or more reactants yielding one product is another way to identify a synthesis reaction. One example of a synthesis reaction is the combination of iron and sulfur to form iron(II) sulfide:
Another example is simple hydrogen gas combined with simple oxygen gas to produce a more complex substance, such as water.
Decomposition.
A decomposition reaction is the opposite of a synthesis reaction, where a more complex substance breaks down into its more simple parts. These reactions are in the general form:
One example of a decomposition reaction is the electrolysis of water to make oxygen and hydrogen gas:
Single replacement.
In a single replacement reaction, a single uncombined element replaces another in a compound; in order words, one element trades places with another element in a compound These reactions come in the general form of:
One example of a single displacement reaction is when magnesium replaces hydrogen in water to make magnesium hydroxide and hydrogen gas:
Double replacement.
In a double replacement reaction, the anions and cations of two compounds switch places and form two entirely different compounds. These reactions are in the general form:
For example, when barium chloride (BaCl2) and magnesium sulfate (MgSO4) react, the SO42- anion switches places with the 2Cl- anion, giving the compounds BaSO4 and MgCl2.
Another example of a double displacement reaction is the reaction of lead(II) nitrate with potassium iodide to form lead(II) iodide and potassium nitrate:
Oxidation and reduction.
Redox reactions can be understood in terms of transfer of electrons from one involved species (reducing agent) to another (oxidizing agent). In this process, the former species is "oxidized" and the latter is "reduced". Though sufficient for many purposes, these descriptions are not precisely correct. Oxidation is better defined as an increase in oxidation state, and reduction as a decrease in oxidation state. In practice, the transfer of electrons will always change the oxidation state, but there are many reactions that are classed as "redox" even though no electron transfer occurs (such as those involving covalent bonds).
In the following redox reaction, hazardous sodium metal reacts with toxic chlorine gas to form the ionic compound sodium chloride, or common table salt:
In the reaction, sodium metal goes from an oxidation state of 0 (as it is a pure element) to +1: in other words, the sodium lost one electron and is said to have been oxidized. On the other hand, the chlorine gas goes from an oxidation of 0 (it is also a pure element) to -1: the chlorine gains one electron and is said to have been reduced. Because the chlorine is the one reduced, it is considered the electron acceptor, or in other words, induces oxidation in the sodium - thus the chlorine gas is considered the oxidizing agent. Conversely, the sodium is oxidized or is the electron donor, and thus induces reduction in the other species and is considered the "reducing agent".
Which of the involved reactants would be reducing or oxidizing agent can be predicted from the electronegativity of their elements. Elements with low electronegativity, such as most metals, easily donate electrons and oxidize – they are reducing agents. On the contrary, many ions with high oxidation numbers, such as , , , , can gain one or two extra electrons and are strong oxidizing agents.
The number of electrons donated or accepted in a redox reaction can be predicted from the electron configuration of the reactant element. Elements try to reach the low-energy noble gas configuration, and therefore alkali metals and halogens will donate and accept one electron respectively. Noble gases themselves are chemically inactive.
An important class of redox reactions are the electrochemical reactions, where electrons from the power supply are used as the reducing agent. These reactions are particularly important for the production of chemical elements, such as chlorine or aluminium. The reverse process in which electrons are released in redox reactions and can be used as electrical energy is possible and used in batteries.
Complexation.
In complexation reactions, several ligands react with a metal atom to form a coordination complex. This is achieved by providing lone pairs of the ligand into empty orbitals of the metal atom and forming dipolar bonds. The ligands are Lewis bases, they can be both ions and neutral molecules, such as carbon monoxide, ammonia or water. The number of ligands that react with a central metal atom can be found using the 18-electron rule, saying that the valence shells of a transition metal will collectively accommodate 18 electrons, whereas the symmetry of the resulting complex can be predicted with the crystal field theory and ligand field theory. Complexation reactions also include ligand exchange, in which one or more ligands are replaced by another, and redox processes which change the oxidation state of the central metal atom.
Acid-base reactions.
In the Brønsted–Lowry acid–base theory, an acid-base reaction involves a transfer of protons (H+) from one species (the acid) to another (the base). When a proton is removed from an acid, the resulting species is termed that acid's conjugate base. When the proton is accepted by a base, the resulting species is termed that base's conjugate acid. In other words, acids act as proton donors and bases act as proton acceptors according to the following equation:
The reverse reaction is possible, and thus the acid/base and conjugated base/acid are always in equilibrium. The equilibrium is determined by the acid and base dissociation constants ("K"a and "K"b) of the involved substances. A special case of the acid-base reaction is the neutralization where an acid and a base, taken at exactly same amounts, form a neutral salt.
Acid-base reactions can have different definitions depending on the acid-base concept employed. Some of the most common are:
Precipitation.
Precipitation is the formation of a solid in a solution or inside another solid during a chemical reaction. It usually takes place when the concentration of dissolved ions exceeds the solubility limit and forms an insoluble salt. This process can be assisted by adding a precipitating agent or by removal of the solvent. Rapid precipitation results in an amorphous or microcrystalline residue and slow process can yield single crystals. The latter can also be obtained by recrystallization from microcrystalline salts.
Solid-state reactions.
Reactions can take place between two solids. However, because of the relatively small diffusion rates in solids, the corresponding chemical reactions are very slow in comparison to liquid and gas phase reactions. They are accelerated by increasing the reaction temperature and finely dividing the reactant to increase the contacting surface area.
Photochemical reactions.
In photochemical reactions, atoms and molecules absorb energy (photons) of the illumination light and convert into an excited state. They can then release this energy by breaking chemical bonds, thereby producing radicals. Photochemical reactions include hydrogen–oxygen reactions, radical polymerization, chain reactions and rearrangement reactions.
Many important processes involve photochemistry. The premier example is photosynthesis, in which most plants use solar energy to convert carbon dioxide and water into glucose, disposing of oxygen as a side-product. Humans rely on photochemistry for the formation of vitamin D, and vision is initiated by a photochemical reaction of rhodopsin. In fireflies, an enzyme in the abdomen catalyzes a reaction that results in bioluminescence. Many significant photochemical reactions, such as ozone formation, occur in the Earth atmosphere and constitute atmospheric chemistry.
Catalysis.
In catalysis, the reaction does not proceed directly, but through a third substance known as catalyst. Unlike other reagents that participate in the chemical reaction, a catalyst is not consumed by the reaction itself; however, it can be inhibited, deactivated or destroyed by secondary processes. Catalysts can be used in a different phase (heterogeneous) or in the same phase (homogeneous) as the reactants. In heterogeneous catalysis, typical secondary processes include coking where the catalyst becomes covered by polymeric side products. Additionally, heterogeneous catalysts can dissolve into the solution in a solid–liquid system or evaporate in a solid–gas system. Catalysts can only speed up the reaction – chemicals that slow down the reaction are called inhibitors. Substances that increase the activity of catalysts are called promoters, and substances that deactivate catalysts are called catalytic poisons. With a catalyst, a reaction which is kinetically inhibited by a high activation energy can take place in circumvention of this activation energy.
Heterogeneous catalysts are usually solids, powdered in order to maximize their surface area. Of particular importance in heterogeneous catalysis are the platinum group metals and other transition metals, which are used in hydrogenations, catalytic reforming and in the synthesis of commodity chemicals such as nitric acid and ammonia. Acids are an example of a homogeneous catalyst, they increase the nucleophilicity of carbonyls, allowing a reaction that would not otherwise proceed with electrophiles. The advantage of homogeneous catalysts is the ease of mixing them with the reactants, but they may also be difficult to separate from the products. Therefore, heterogeneous catalysts are preferred in many industrial processes.
Reactions in organic chemistry.
In organic chemistry, in addition to oxidation, reduction or acid-base reactions, a number of other reactions can take place which involve covalent bonds between carbon atoms or carbon and heteroatoms (such as oxygen, nitrogen, halogens, etc.). Many specific reactions in organic chemistry are name reactions designated after their discoverers.
Substitution.
In a substitution reaction, a functional group in a particular chemical compound is replaced by another group. These reactions can be distinguished by the type of substituting species into a nucleophilic, electrophilic or radical substitution.
In the first type, a nucleophile, an atom or molecule with an excess of electrons and thus a negative charge or partial charge, replaces another atom or part of the "substrate" molecule. The electron pair from the nucleophile attacks the substrate forming a new bond, while the leaving group departs with an electron pair. The nucleophile may be electrically neutral or negatively charged, whereas the substrate is typically neutral or positively charged. Examples of nucleophiles are hydroxide ion, alkoxides, amines and halides. This type of reaction is found mainly in aliphatic hydrocarbons, and rarely in aromatic hydrocarbon. The latter have high electron density and enter nucleophilic aromatic substitution only with very strong electron withdrawing groups. Nucleophilic substitution can take place by two different mechanisms, SN1 and SN2. In their names, S stands for substitution, N for nucleophilic, and the number represents the kinetic order of the reaction, unimolecular or bimolecular.
The SN1 reaction proceeds in two steps. First, the leaving group is eliminated creating a carbocation. This is followed by a rapid reaction with the nucleophile.
In the SN2 mechanism, the nucleophile forms a transition state with the attacked molecule, and only then the leaving group is cleaved. These two mechanisms differ in the stereochemistry of the products. SN1 leads to the non-stereospecific addition and does not result in a chiral center, but rather in a set of geometric isomers ("cis/trans"). In contrast, a reversal (Walden inversion) of the previously existing stereochemistry is observed in the SN2 mechanism.
Electrophilic substitution is the counterpart of the nucleophilic substitution in that the attacking atom or molecule, an electrophile, has low electron density and thus a positive charge. Typical electrophiles are the carbon atom of carbonyl groups, carbocations or sulfur or nitronium cations. This reaction takes place almost exclusively in aromatic hydrocarbons, where it is called electrophilic aromatic substitution. The electrophile attack results in the so-called σ-complex, a transition state in which the aromatic system is abolished. Then, the leaving group, usually a proton, is split off and the aromaticity is restored. An alternative to aromatic substitution is electrophilic aliphatic substitution. It is similar to the nucleophilic aliphatic substitution and also has two major types, SE1 and SE2
In the third type of substitution reaction, radical substitution, the attacking particle is a radical. This process usually takes the form of a chain reaction, for example in the reaction of alkanes with halogens. In the first step, light or heat disintegrates the halogen-containing molecules producing the radicals. Then the reaction proceeds as an avalanche until two radicals meet and recombine.
Addition and elimination.
The addition and its counterpart, the elimination, are reactions which change the number of substitutents on the carbon atom, and form or cleave multiple bonds. Double and triple bonds can be produced by eliminating a suitable leaving group. Similar to the nucleophilic substitution, there are several possible reaction mechanisms which are named after the respective reaction order. In the E1 mechanism, the leaving group is ejected first, forming a carbocation. The next step, formation of the double bond, takes place with elimination of a proton (deprotonation). The leaving order is reversed in the E1cb mechanism, that is the proton is split off first. This mechanism requires participation of a base. Because of the similar conditions, both reactions in the E1 or E1cb elimination always compete with the SN1 substitution.
The E2 mechanism also requires a base, but there the attack of the base and the elimination of the leaving group proceed simultaneously and produce no ionic intermediate. In contrast to the E1 eliminations, different stereochemical configurations are possible for the reaction product in the E2 mechanism, because the attack of the base preferentially occurs in the anti-position with respect to the leaving group. Because of the similar conditions and reagents, the E2 elimination is always in competition with the SN2-substitution.
The counterpart of elimination is the addition where double or triple bonds are converted into single bonds. Similar to the substitution reactions, there are several types of additions distinguished by the type of the attacking particle. For example, in the electrophilic addition of hydrogen bromide, an electrophile (proton) attacks the double bond forming a carbocation, which then reacts with the nucleophile (bromine). The carbocation can be formed on either side of the double bond depending on the groups attached to its ends, and the preferred configuration can be predicted with the Markovnikov's rule. This rule states that "In the heterolytic addition of a polar molecule to an alkene or alkyne, the more electronegative (nucleophilic) atom (or part) of the polar molecule becomes attached to the carbon atom bearing the smaller number of hydrogen atoms."
If the addition of a functional group takes place at the less substituted carbon atom of the double bond, then the electrophilic substitution with acids is not possible. In this case, one has to use the hydroboration–oxidation reaction, where in the first step, the boron atom acts as electrophile and adds to the less substituted carbon atom. At the second step, the nucleophilic hydroperoxide or halogen anion attacks the boron atom.
While the addition to the electron-rich alkenes and alkynes is mainly electrophilic, the nucleophilic addition plays an important role for the carbon-heteroatom multiple bonds, and especially its most important representative, the carbonyl group. This process is often associated with an elimination, so that after the reaction the carbonyl group is present again. It is therefore called addition-elimination reaction and may occur in carboxylic acid derivatives such as chlorides, esters or anhydrides. This reaction is often catalyzed by acids or bases, where the acids increase by the electrophilicity of the carbonyl group by binding to the oxygen atom, whereas the bases enhance the nucleophilicity of the attacking nucleophile.
Nucleophilic addition of a carbanion or another nucleophile to the double bond of an alpha, beta unsaturated carbonyl compound can proceed via the Michael reaction, which belongs to the larger class of conjugate additions. This is one of the most useful methods for the mild formation of C–C bonds.
Some additions which can not be executed with nucleophiles and electrophiles, can be succeeded with free radicals. As with the free-radical substitution, the radical addition proceeds as a chain reaction, and such reactions are the basis of the free-radical polymerization.
Other organic reaction mechanisms.
In a rearrangement reaction, the carbon skeleton of a molecule is rearranged to give a structural isomer of the original molecule. These include hydride shift reactions such as the Wagner-Meerwein rearrangement, where a hydrogen, alkyl or aryl group migrates from one carbon to a neighboring carbon. Most rearrangements are associated with the breaking and formation of new carbon-carbon bonds. Other examples are sigmatropic reaction such as the Cope rearrangement.
Cyclic rearrangements include cycloadditions and, more generally, pericyclic reactions, wherein two or more double bond-containing molecules form a cyclic molecule. An important example of cycloaddition reaction is the Diels–Alder reaction (the so-called [4+2] cycloaddition) between a conjugated diene and a substituted alkene to form a substituted cyclohexene system.
Whether or not a certain cycloaddition would proceed depends on the electronic orbitals of the participating species, as only orbitals with the same sign of wave function will overlap and interact constructively to form new bonds. Cycloaddition is usually assisted by light or heat. These perturbations result in different arrangement of electrons in the excited state of the involved molecules and therefore in different effects. For example, the Diels-Alder reactions can be assisted by heat whereas the [2+2 cycloaddition is selectively induced by light. Because of the orbital character, the potential for developing stereoisomeric products upon cycloaddition is limited, as described by the Woodward–Hoffmann rules.
Biochemical reactions.
Biochemical reactions are mainly controlled by enzymes. These proteins can specifically catalyze a single reaction, so that reactions can be controlled very precisely. The reaction takes place in the active site, a small part of the enzyme which is usually found in a cleft or pocket lined by amino acid residues, and the rest of the enzyme is used mainly for stabilization. The catalytic action of enzymes relies on several mechanisms including the molecular shape ("induced fit"), bond strain, proximity and orientation of molecules relative to the enzyme, proton donation or withdrawal (acid/base catalysis), electrostatic interactions and many others.
The biochemical reactions that occur in living organisms are collectively known as metabolism. Among the most important of its mechanisms is the anabolism, in which different DNA and enzyme-controlled processes result in the production of large molecules such as proteins and carbohydrates from smaller units. Bioenergetics studies the sources of energy for such reactions. An important energy source is glucose, which can be produced by plants via photosynthesis or assimilated from food. All organisms use this energy to produce adenosine triphosphate (ATP), which can then be used to energize other reactions.
Applications.
Chemical reactions are central to chemical engineering where they are used for the synthesis of new compounds from natural raw materials such as petroleum and mineral ores. It is essential to make the reaction as efficient as possible, maximizing the yield and minimizing the amount of reagents, energy inputs and waste. Catalysts are especially helpful for reducing the energy required for the reaction and increasing its reaction rate.
Some specific reactions have their niche applications. For example, the thermite reaction is used to generate light and heat in pyrotechnics and welding. Although it is less controllable than the more conventional oxy-fuel welding, arc welding and flash welding, it requires much less equipment and is still used to mend rails, especially in remote areas.
Monitoring.
Mechanisms of monitoring chemical reactions depend strongly on the reaction rate. Relatively slow processes can be analyzed in situ for the concentrations and identities of the individual ingredients. Important tools of real time analysis are the measurement of pH and analysis of optical absorption (color) and emission spectra. A less accessible but rather efficient method is introduction of a radioactive isotope into the reaction and monitoring how it changes over time and where it moves to; this method is often used to analyze redistribution of substances in the human body. Faster reactions are usually studied with ultrafast laser spectroscopy where utilization of femtosecond lasers allows short-lived transition states to be monitored at time scaled down to a few femtoseconds.

</doc>
<doc id="6272" url="http://en.wikipedia.org/wiki?curid=6272" title="Charleston">
Charleston

Charleston most commonly refers to:
Charleston may also refer to:
Geography.
In Australia:
In Canada:
In New Zealand:
In United Kingdom:
In the United States:

</doc>
<doc id="6276" url="http://en.wikipedia.org/wiki?curid=6276" title="Casiquiare canal">
Casiquiare canal

The Casiquiare river is a distributary of the upper Orinoco flowing southward into the Rio Negro, in Venezuela, South America. As such, it forms a unique natural canal between the Orinoco and Amazon river systems. It is the largest river on the planet that links two major river systems, a so-called bifurcation. The area forms a water divide, more dramatically at regional flood stage.
Discovery.
In 1744 a Jesuit priest named Father Roman, while ascending the Orinoco River, met some Portuguese slave-traders from the settlements on the Rio Negro. He accompanied them on their return, by way of the Casiquiare canal, and afterwards retraced his route to the Orinoco. Charles Marie de La Condamine, seven months later, was able to give to the "Académie française" an account of Father Roman's voyage, and thus confirm the existence of this waterway, first reported by Father Acuña in 1639.
But little credence was given to Father Roman's statement until it was verified, in 1756, by the Spanish Boundary-line Commission of Yturriaga and Solano. In 1800 German scientist Alexander von Humboldt and French botanist Aimé Bonpland explored the river. During a 1924–25 expedition, Alexander H. Rice, Jr. of Harvard University traveled up the Orinoco, traversed the Casiquiare canal, and descended the Rio Negro to the Amazon at Manaus. It was the first expedition to use aerial photography and shortwave radio for mapping of the region. In 1968 the Casiquiare was navigated by an SRN6 hovercraft during a National Geographic expedition.
Geography.
The origin of the Casiquiare, at the River Orinoco, is below the mission of La Esmeralda at , and about above sea level. Its mouth at the Rio Negro, an affluent of the Amazon River, is near the town of San Carlos and is above sea level.
The general course is south-west, and its length, including windings, is about . Its width, at its bifurcation with the Orinoco, is approximately , with a current towards the Rio Negro of . However, as it gains in volume from the very numerous tributary streams, large and small, that it receives en route, its velocity increases, and in the wet season reaches , even in certain stretches. It broadens considerably as it approaches its mouth, where it is about . The volume of water the Casiquiare captures from the Orinoco is small in comparison to what it accumulates in its course.
In flood-time it is said to have a second connection with the Rio Negro by a branch, which it throws off to the westward, called the Itinivini, which leaves it at a point about above its mouth. In the dry season, it has shallows, and is obstructed by sandbanks, a few rapids and granite rocks. Its shores are densely wooded, and the soil more fertile than that along the Rio Negro. The general slope of the plains through which the canal runs is south-west, but those of the Rio Negro slope south-east.
The Casiquiare is not a sluggish canal on a flat tableland, but a great, rapid river which, if its upper waters had not found contact with the Orinoco, perhaps by cutting back, would belong entirely to the Negro branch of the Amazon.
To the west of the Casiquiare, there is a much shorter and easier portage between the Orinoco and Amazon basins, called the isthmus of Pimichin, which is reached by ascending the Terni branch of the Atabapo River, an affluent of the Orinoco. Although the Terni is somewhat obstructed, it is believed that it could easily be made navigable for small craft. The isthmus is across, with undulating ground, nowhere over , with swamps and marshes. It is much used for the transit of large canoes, which are hauled across it from the Terni river, and which reach the Rio Negro by the little stream called the Pimichin.
Hydrographic divide.
The Casiquiare canal–Orinoco River hydrographic divide is a representation of the hydrographic water divide that delineates the separation between the Orinoco Basin and the Amazon Basin. (The Orinoco Basin flows west–north–northeast into the Caribbean; the Amazon Basin flows east into the western Atlantic in the extreme northeast of Brazil.)
Essentially the river divide is a west-flowing, upriver section of Venezuela's Orinoco River with an outflow to the south into the Amazon Basin. This named outflow is the Casiquiare canal, which, as it heads downstream (southerly), picks up speed and also accumulates water volume.
The greatest manifestation of the divide is during floods. During flood stage, the Casiquiare's main outflow point into the Rio Negro is supplemented by an overflow that is a second, and more minor, entry river bifurcation into the Rio Negro and upstream from its major, common low-water entry confluence with the Rio Negro. At flood, the river becomes an area flow source, far more than a narrow confined river.
The Casiquiare canal connects the upper Orinoco, 9 miles below the mission of Esmeraldas, with the Rio Negro affluent of the Amazon River near the town of San Carlos.
The simplest description (besides the entire area-floodplain) of the water divide is a "south-bank Orinoco River strip" at the exit point of the Orinoco, also the origin of the Casiquiare canal. However during the Orinoco's flood stage, that single, simply defined "origin of the canal" is turned into a region, and an entire strip along the southern bank of the Orinoco River.

</doc>
