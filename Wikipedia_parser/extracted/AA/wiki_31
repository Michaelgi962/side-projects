<doc id="2493" url="http://en.wikipedia.org/wiki?curid=2493" title="Anthroposophy">
Anthroposophy

Anthroposophy, a philosophy founded by Rudolf Steiner, postulates the existence of an objective, intellectually comprehensible spiritual world accessible to direct experience through inner development. More specifically, it aims to develop faculties of perceptive imagination, inspiration and intuition through cultivating a form of thinking independent of sensory experience, and to present the results thus derived in a manner subject to rational verification. In its investigations of the spiritual world, anthroposophy aims to attain the precision and clarity attained by the natural sciences in their investigations of the physical world.
Anthroposophical ideas have been applied practically in many areas including Steiner/Waldorf education, special education (most prominently through the Camphill Movement), biodynamic agriculture, medicine, ethical banking, organizational development, and the arts. The Anthroposophical Society has its international center at the Goetheanum in Dornach, Switzerland.
History.
The early work of the founder of anthroposophy, Rudolf Steiner, culminated in his "Philosophy of Freedom" (also translated as "The Philosophy of Spiritual Activity" and "Intuitive Thinking as a Spiritual Path"). Here, Steiner developed a concept of free will based on inner experiences, especially those that occur in the creative activity of independent thought.
By the beginning of the twentieth century, Steiner's interests turned to explicitly spiritual areas of research. His work began to interest others interested in spiritual ideas; among these was the Theosophical Society. From 1900 on, thanks to the positive reception given to his ideas, Steiner focused increasingly on his work with the Theosophical Society becoming the secretary of its section in Germany in 1902. During the years of his leadership, membership increased dramatically, from a few individuals to sixty-nine Lodges.
By 1907, a split between Steiner and the mainstream Theosophical Society had begun to become apparent. While the Society was oriented toward an Eastern and especially Indian approach, Steiner was trying to develop a path that embraced Christianity and natural science. The split became irrevocable when Annie Besant, then president of the Theosophical Society, began to present the child Jiddu Krishnamurti as the reincarnated Christ. Steiner strongly objected and considered any comparison between Krishnamurti and Christ to be nonsense; many years later, Krishnamurti also repudiated the assertion. Steiner's continuing differences with Besant led him to separate from the Theosophical Society Adyar; he was followed by the great majority of the membership of the Theosophical Society's German Section, as well as members of other national sections.
By this time, Steiner had reached considerable stature as a spiritual teacher. He spoke about what he considered to be his direct experience of the Akashic Records (sometimes called the "Akasha Chronicle"), thought to be a spiritual chronicle of the history, pre-history, and future of the world and mankind. In a number of works, Steiner described a path of inner development he felt would let anyone attain comparable spiritual experiences. Sound vision could be developed, in part, by practicing rigorous forms of ethical and cognitive self-discipline, concentration, and meditation; in particular, a person's moral development must precede the development of spiritual faculties.
In 1912, the Anthroposophical Society was founded. After World War I, the Anthroposophical movement took on new directions. Projects such as schools, centers for those with special needs, organic farms and medical clinics were established, all inspired by anthroposophy.
In 1923, faced with differences between older members focusing on inner development and younger members eager to become active in the social transformations of the time, Steiner refounded the Society in an inclusive manner and established a School for Spiritual Science. As a spiritual basis for the refounded movement, Steiner wrote a "" which remains a central meditative expression of anthroposophical ideas.
Steiner died just over a year later, in 1925. The Second World War temporarily hindered the anthroposophical movement in most of Continental Europe, as the Anthroposophical Society and most of its daughter movements (e.g. Steiner/Waldorf education) were banned by the National Socialists (Nazis); virtually no anthroposophists ever joined the National Socialist Party.
By 2007, national branches of the Anthroposophical Society had been established in fifty countries, and about 10,000 institutions around the world were working on the basis of anthroposophy. In the same year, the Anthroposophical Society was called the "most important esoteric society in European history."
Etymology.
"Anthroposophy" is an amalgam of the Greek terms ("anthropos" = "human") and ("sophia" = "wisdom"). It is listed by Nathan Bailey (1742) as meaning "the knowledge of the nature of man" (OED). Authors whose usage of the term predates Steiner's include occultist Agrippa von Nettesheim, alchemist Thomas Vaughan ("Anthroposophia Theomagica"), and philosopher Robert Zimmermann.
Steiner began using the term in the early 1900s as an alternative to the term "theosophy" (divine wisdom), a term central to the Theosophical Society, with which Steiner was associated at the time, and to a long tradition of European esotericists. Steiner probably first encountered the word "anthroposophy" in the work of Zimmermann, some of whose lectures in the University of Vienna he had attended while a student.
Central ideas.
Spiritual knowledge and freedom.
Anthroposophical proponents aim to extend the clarity of the scientific method to phenomena of human soul-life and to spiritual experiences. This requires developing new faculties of objective spiritual perception, which Steiner maintained was possible for humanity today. The steps of this process of inner development he identified as consciously achieved "imagination", "inspiration" and "intuition". Steiner believed results of this form of spiritual research should be expressed in a way that can be understood and evaluated on the same basis as the results of natural science: "The anthroposophical schooling of thinking leads to the development of a non-sensory, or so-called supersensory consciousness, whereby the spiritual researcher brings the experiences of this realm into ideas, concepts, and expressive language in a form which people can understand who do not yet have the capacity to achieve the supersensory experiences necessary for individual research."
Steiner hoped to form a spiritual movement that would free the individual from any external authority: "The most important problem of all human thinking is this: to comprehend the human being as a personality grounded in him or herself." For Steiner, the human capacity for rational thought would allow individuals to comprehend spiritual research on their own and bypass the danger of dependency on an authority.
Steiner contrasted the anthroposophical approach with both conventional mysticism, which he considered lacking the clarity necessary for exact knowledge, and natural science, which he considered arbitrarily limited to investigating the outer world.
Nature of the human being.
In "Theosophy", Steiner suggested that human beings unite a physical body of a nature common to (and that ultimately returns to) the inorganic world; a life body (also called the etheric body), in common with all living creatures (including plants); a bearer of sentience or consciousness (also called the astral body), in common with all animals; and the ego, which anchors the faculty of self-awareness unique to human beings.
Anthroposophy describes a broad evolution of human consciousness. Early stages of human evolution possess an intuitive perception of reality, including a clairvoyant perception of spiritual realities. Humanity has progressively evolved an increasing reliance on intellectual faculties and a corresponding loss of intuitive or clairvoyant experiences, which have become atavistic. The increasing intellectualization of consciousness, initially a progressive direction of evolution, has led to an excessive reliance on abstraction and a loss of contact with both natural and spiritual realities. However, to go further requires new capacities that combine the clarity of intellectual thought with the imagination, and beyond this with consciously achieved inspiration and intuitive insights.
Anthroposophy speaks of the reincarnation of the human spirit: that the human being passes between stages of existence, incarnating into an earthly body, living on earth, leaving the body behind and entering into the spiritual worlds before returning to be born again into a new life on earth. After the death of the physical body, the human spirit recapitulates the past life, perceiving its events as they were experienced by the objects of its actions. A complex transformation takes place between the review of the past life and the preparation for the next life. The individual's karmic condition eventually leads to a choice of parents, physical body, disposition, and capacities that provide the challenges and opportunities that further development requires, which includes karmically chosen tasks for the future life.
Steiner described some conditions that determine the interdependence of a person's lives, or karma.
Evolution.
The anthroposophical view of evolution considers all animals to have evolved from an early, unspecialized form. As the least specialized animal, human beings have maintained the closest connection to the archetypal form; contrary to the Darwinian conception of human evolution, all other animals "devolve" from this archetype. The spiritual archetype originally created by spiritual beings was devoid of physical substance; only later did this descend into material existence on Earth. In this view, human evolution has accompanied the Earth's evolution throughout the existence of the Earth.
Anthroposophy took over from Theosophy a complex system of cycles of world development and human evolution. The evolution of the world is said to have occurred in cycles. The first phase of the world consisted only of heat. In the second phase, a more active condition, light, and a more condensed, gaseous state separate out from the heat. In the third phase, a fluid state arose, as well as a sounding, forming energy. In the fourth (current) phase, solid physical matter first exists. This process is said to have been accompanied by an evolution of consciousness which led up to present human culture.
Good as balance.
The anthroposophical view is that good is found in the balance between two polar, generally evil influences on world and human evolution. Two spiritual adversaries endeavour to tempt and corrupt humanity: these are often described through their mythological embodiments, Lucifer and his counterpart Ahriman, which have both positive and negative aspects. Lucifer is the light spirit, which "plays on human pride and offers the delusion of divinity", but also motivates creativity and spirituality; Ahriman is the dark spirit, which tempts human beings to "...deny link with divinity and to live entirely on the material plane", but also stimulates intellectuality and technology. Both figures exert a negative effect on humanity when their influence becomes misplaced or one-sided, yet their influences are necessary for human freedom to unfold.
Each human being has the task to find a balance between these opposing influences, and each is helped in this task by the mediation of the "Representative of Humanity", also known as the Christ being, a spiritual entity who stands between and harmonizes the two extremes.
Applications.
Applications of anthroposophy include:
Steiner/Waldorf education.
This is a pedagogical movement with over 1000 Steiner or Waldorf schools (the latter name stems from the first such school, founded in Stuttgart in 1919) located in some 60 countries; the great majority of these are independent (private) schools. Sixteen of the schools have been affiliated with the United Nations' UNESCO Associated Schools Project Network, which sponsors education projects that foster improved quality of education throughout the world, in particular in terms of its ethical, cultural, and international dimensions. Waldorf schools receive full or partial governmental funding in some European nations, Australia and in parts of the United States (as Waldorf method public or charter schools).
The schools are located in a wide variety of communities and cultures: from the impoverished "favelas" of São Paulo to the wealthy suburbs of New York City; in India, Egypt, Australia, the Netherlands, Mexico and South Africa. Though most of the early Waldorf schools were teacher-founded, the schools today are usually initiated and later supported by an active parent community. Waldorf education is one of the most visible practical applications of an anthroposophical view and understanding of the human being and has been characterized as "the leader of the international movement for a New Education,"
Biodynamic agriculture.
Biodynamic agriculture, the first intentional form of organic farming, began in the 1920s when Rudolf Steiner gave a series of lectures since published as "Agriculture". Steiner is considered one of the founders of the modern organic farming movement.
Anthroposophical medicine.
Steiner gave several series of lectures to physicians and medical students. Out of those grew a complementary medical movement intending to "extend the knowledge gained through the methods of the natural sciences of the present age with insights from spiritual science." This movement now includes hundreds of M.D.s, chiefly in Europe and North America, and has its own clinics, hospitals, and medical schools.
One of the most studied applications has been the use of mistletoe extracts in cancer therapy. The extracts are generally no longer used to reduce or inhibit tumor growth, for which verifiable results have been found in vitro and in animal studies but not in humans, but instead to improve the patients' quality of life and to reduce tumor-induced symptoms and the side-effects of chemotherapy and radiotherapy. According to the National Cancer Institute, "Mistletoe extract has been shown to kill cancer cells in the laboratory and to affect the immune system. However, there is limited evidence that mistletoe's effects on the immune system help the body fight cancer... At present, the use of mistletoe cannot be recommended outside the context of well-designed clinical trials."
Special needs education and services.
In 1922, Ita Wegman founded an anthroposophical center for special needs education, the Sonnenhof, in Switzerland. In 1940, Karl König founded the Camphill Movement in Scotland. The latter in particular has spread widely, and there are now over a hundred Camphill communities and other anthroposophical homes for children and adults in need of special care in about 22 countries around the world. Both Karl König, Thomas Weihs and others have written extensively on these ideas underlying Special education.
Architecture.
Steiner himself designed around thirteen buildings, many of them significant works in a unique, organic—expressionist architectural style. Foremost among these are his designs for the two Goetheanum buildings in Dornach, Switzerland. Thousands of further buildings have been built by later generations of anthroposophic architects.
Architects who have been strongly influenced by the anthroposophic style include Imre Makovecz in Hungary, Hans Scharoun and Joachim Eble in Germany, Erik Asmussen in Sweden, Kenji Imai in Japan, Thomas Rau, Anton Alberts and Max van Huut in the Netherlands, Christopher Day and Camphill Architects in the UK, Thompson and Rose in America, Denis Bowman in Canada, and Walter Burley Griffin and Gregory Burgess in Australia.
One of the most famous contemporary buildings by an anthroposophical architect is ING House, an ING Bank building in Amsterdam, which has received several awards for its ecological design and approach to a self-sustaining ecology as an autonomous building and example of sustainable architecture.
Eurythmy.
In the arts, Steiner's new art of eurythmy gained early renown. Eurythmy seeks to renew the spiritual foundations of dance, revealing speech and music in visible movement. There are now active stage groups and training centers, mostly of modest proportions, in approximately 16 countries.
Social finance.
Around the world today are a number of banks, companies, charities, and schools for developing co-operative forms of business using Steiner's ideas about economic associations, aiming at harmonious and socially responsible roles in the world economy. The first anthroposophic bank was the "Gemeinschaftsbank für Leihen und Schenken" in Bochum, Germany, founded in 1974. Socially responsible banks founded out of anthroposophy in the English-speaking world include Triodos Bank, founded in 1980 and active in the UK, Netherlands, Germany, Belgium, and Spain, La Nef in France and RSF Social Finance in San Francisco.
Organizational development, counselling and biography work.
Bernard Lievegoed, a psychiatrist, founded a new method of individual and institutional development oriented towards humanizing organizations and linked with Steiner's ideas of the threefold social order. This work is represented by the NPI Institute for Organizational Development in the Netherlands and sister organizations in many other countries. Various forms of biographic and counselling work have been developed on the basis of anthroposophy.
Speech and drama.
There are also anthroposophical movements to renew speech and drama, the most important of which are based in the work of Marie Steiner-von Sivers ("speech formation", also known as "Creative Speech") and the "Chekhov Method" originated by Michael Chekhov (nephew of Anton Chekhov).
Social goals.
For a period after World War I, Steiner was extremely active and well known in Germany, in part because he lectured widely proposing social reforms. Steiner was a sharp critic of nationalism, which he saw as outdated, and a proponent of achieving social solidarity through individual freedom. A petition proposing a radical change in the German constitution and expressing his basic social ideas (signed by Herman Hesse, among others) was widely circulated. His main book on social reform is "Toward Social Renewal".
Anthroposophy continues to aim at reforming society through maintaining and strengthening the independence of the spheres of cultural life, human rights and the economy. It emphasizes a particular ideal in each of these three realms of society:
Esoteric path.
Paths of spiritual development.
According to Steiner, a real spiritual world exists, out of which the material one gradually condensed and evolved. Steiner held that the spiritual world can be researched in the right circumstances through direct experience, by persons practicing rigorous forms of ethical and cognitive self-discipline. Steiner described many exercises he said were suited to strengthening such self-discipline; the most complete exposition of these is found in his book "How To Know Higher Worlds". The aim of these exercises is to develop higher levels of consciousness through meditation and observation. Details about the spiritual world, Steiner suggested, could on such a basis be discovered and reported, though no more infallibly than the results of natural science.
Steiner regarded his research reports as being important aids to others seeking to enter into spiritual experience. He suggested that a combination of spiritual exercises (for example, concentrating on an object such as a seed), moral development (control of thought, feelings and will combined with openness, tolerance and flexibility) and familiarity with other spiritual researchers' results would best further an individual's spiritual development. He consistently emphasised that any inner, spiritual practice should be undertaken in such a way as not to interfere with one's responsibilities in outer life. Steiner distinguished between what he considered were true and false paths of spiritual investigation.
In anthroposophy, artistic expression is also treated as a potentially valuable bridge between spiritual and material reality.
Prerequisites to and stages of inner development.
Steiner's stated prerequisites to beginning on a spiritual path include a willingness to take up serious cognitive studies, a respect for factual evidence, and a responsible attitude. Central to progress on the path itself is a harmonious cultivation of the following qualities:
Steiner sees meditation as a concentration and enhancement of the power of thought. By focusing consciously on an idea, feeling or intention the meditant seeks to arrive at pure thinking, a state exemplified by but not confined to pure mathematics. In Steiner's view, conventional sensory-material knowledge is achieved through relating perception and concepts. The anthroposophic path of esoteric training articulates three further stages of supersensory knowledge, which do not necessarily follow strictly sequentially in any single individual's spiritual progress.
Spiritual exercises.
Steiner described numerous exercises he believed would bring spiritual development; other anthroposophists have added many others. A central principle is that "for every step in spiritual perception, three steps are to be taken in moral development." According to Steiner, moral development reveals the extent to which one has achieved control over one's inner life and can exercise it in harmony with the spiritual life of other people; it shows the real progress in spiritual development, the fruits of which are given in spiritual perception. It also guarantees the capacity to distinguish between false perceptions or illusions (which are possible in perceptions of both the outer world and the inner world) and true perceptions: i.e., the capacity to distinguish in any perception between the influence of subjective elements (i.e., viewpoint) and the objective reality the perception points at.
Place in Western philosophy.
Steiner built upon Goethe's conception of an imaginative power capable of synthesizing the sense-perceptible form of a thing (an image of its outer appearance) and the concept we have of that thing (an image of its inner structure or nature). Steiner added to this the conception that a further step in the development of thinking is possible when the thinker observes his or her own thought processes. "The organ of observation and the observed thought process are then identical, so that the condition thus arrived at is simultaneously one of perception through thinking and one of thought through perception."
Thus, in Steiner's view, we can overcome the subject-object divide through inner activity, even though all human experience begins by being conditioned by it. In this connection, Steiner examines the step from thinking determined by outer impressions to what he calls sense-free thinking. He characterizes thoughts he considers without sensory content, such as mathematical or logical thoughts, as free deeds. Steiner believed he had thus located the origin of free will in our thinking, and in particular in sense-free thinking.
Some of the epistemic basis for Steiner's later anthroposophical work is contained in the seminal work, Philosophy of Freedom. In his early works, Steiner sought to overcome what he perceived as the dualism of Cartesian idealism and Kantian subjectivism by developing Goethe's conception of the human being as a natural-supernatural entity, that is: natural in that humanity is a product of nature, supernatural in that through our conceptual powers we extend nature's realm, allowing it to achieve a reflective capacity in us as philosophy, art and science. Steiner was one of the first European philosophers to overcome the subject-object split in Western thought. Though not well known among philosophers, his philosophical work was taken up by Owen Barfield (and through him influenced the Inklings, an Oxford group of Christian writers that included J. R. R. Tolkien and C. S. Lewis) and Richard Tarnas.
Christian and Jewish mystical thought have also influenced the development of anthroposophy.
Union of science and spirit.
Steiner believed in the possibility of applying the clarity of scientific thinking to spiritual experience, which he saw as deriving from an objectively existing spiritual world. Steiner identified mathematics, which attains certainty through thinking itself, thus through inner experience rather than empirical observation, as the basis of his epistemology of spiritual experience.
Relationship to religion.
Christ as the center of earthly evolution.
Steiner's writing, though appreciative of all religions and cultural developments, emphasizes Western tradition as having evolved to meet contemporary needs. He describes Christ and his mission on earth of bringing individuated consciousness as having a particularly important place in human evolution, whereby:
Thus, anthroposophy considers there to be a being who unifies all religions, and who is not represented by any particular religious faith. This being is, according to Steiner, not only the Redeemer of the Fall from Paradise, but also the unique pivot and meaning of earth's evolutionary processes and of human history. To describe this being, Steiner periodically used terms such as the "Representative of Humanity" or the "good spirit" rather than any denominational term.
This view has certain similarities to the concepts of Christogenesis advocated by Pierre Teilhard de Chardin.
Divergence from conventional Christian thought.
Steiner's views of Christianity diverge from conventional Christian thought in key places, and include gnostic elements:
Judaism.
Rudolf Steiner wrote and lectured on Judaism and Jewish issues for much of his life. In the 1880s and 1890s, he took part in debates on anti-semitism and on assimilation. He was a fierce opponent of anti-semitism and supported the unconditional acceptance and integration of the Jews in Europe. He also supported Émile Zola's position in the Dreyfus affair. In his later life, Steiner was accused by the Nazis of being a Jew, and Adolf Hitler called anthroposophy "Jewish methods." The anthroposophical institutions in Germany were banned during Nazi rule and several anthroposophists sent to concentration camps.
Steiner emphasized Judaism's central importance to the constitution of the modern era in the West. Important early anthroposophists who were Jewish included Karl König, the founder of the Camphill movement, and a majority of the executive board of the original Anthroposophical Society. Martin Buber and Hugo Bergmann, who viewed Steiner's social ideas as a solution to the Arab–Jewish conflict, were also influenced by anthroposophy.
There are several anthroposophical organisations in Israel, including the anthroposophical kibbutz Harduf, founded by Jesaiah Ben-Aharon. A number of these organizations are striving to foster positive relationships between the Arab and Jewish populations: The Harduf Waldorf school includes both Jewish and Arab faculty and students, and has extensive contact with the surrounding Arab communities. In Hilf near Haifa, there is a joint Arab-Jewish Waldorf kindergarten, the first joint Arab-Jewish kindergarten in Israel.
Christian Community.
Towards the end of Steiner's life, a group of theology students (primarily Lutheran, with some Roman Catholic members) approached Steiner for help in reviving Christianity, in particular "to bridge the widening gulf between modern science and the world of spirit." They approached a notable Lutheran pastor, Friedrich Rittelmeyer, who was already working with Steiner's ideas, to join their efforts. Out of their co-operative endeavor, the "Movement for Religious Renewal", now generally known as The Christian Community, was born. Steiner emphasized that he considered this movement, and his role in creating it, to be independent of his anthroposophical work, as he wished anthroposophy to be independent of any particular religion or religious denomination.
Reception.
Supporters.
Anthroposophy's supporters include Pulitzer Prize-winning and Nobel Laureate Saul Bellow, Andrei Bely, Joseph Beuys, Owen Barfield, architect Walter Burley Griffin, Wassily Kandinsky, Nobel Laureates Selma Lagerlöf Andrei Tarkovsky, Bruno Walter, and Right Livelihood Award winners Sir George Trevelyan and Ibrahim Abouleish. Albert Schweitzer was a friend of Steiner's supportive of his ideals for cultural renewal.
Scientific basis.
Though Rudolf Steiner studied natural science at the Vienna Technical University at the undergraduate level, his doctorate was in epistemology and very little of his work is directly concerned with the empirical sciences. In his mature work, when he did refer to science it was often to present phenomenological or Goethean science as an alternative to what he considered the materialistic science of his contemporaries.
His primary interest was in applying the methodology of science to realms of inner experience and the spiritual worlds (Steiner's appreciation that the essence of science is its method of inquiry is unusual among esotericists), and Steiner called anthroposophy "Geisteswissenschaft" (lit.: Science of the mind, or cultural or spiritual science), a term generally used in German to refer to the humanities and social sciences; in fact, the term "science" is used more broadly in Europe as a general term that refers to any exact knowledge.
Whether this is a sufficient basis for anthroposophy to be considered a spiritual science has been a matter of controversy. As Freda Easton explained in her study of Waldorf schools, "Whether one accepts anthroposophy as a science depends upon whether one accepts Steiner's interpretation of a science that extends the consciousness and capacity of human beings to experience their inner spiritual world." Sven Ove Hansson has disputed anthroposophy's claim to a scientific basis, stating that its ideas are not empirically derived and neither reproducible nor testable.
Carlo Willmann points out that as, on its own terms, anthroposophical methodology offers no possibility of being falsified except through its own procedures of spiritual investigation, no intersubjective validation is possible by conventional scientific methods; it thus cannot stand up to positivistic science's criticism. Peter Schneider calls such objections untenable on the grounds that if a non-sensory, non-physical realm exists, then according to Steiner the experiences of pure thinking possible within the normal realm of consciousness would already be experiences of that, and it would be impossible to exclude the possibility of empirically grounded experiences of other supersensory content.
Olav Hammer suggests that anthroposophy carries scientism "to lengths unparalleled in any other Esoteric position" due to its dependence upon claims of clairvoyant experience, its subsuming natural science under "spiritual science", and its development of what Hammer calls "fringe" sciences such as anthroposophical medicine and biodynamic agriculture justified partly on the basis of the ethical and ecological values they promote, rather than purely on a scientific basis.
Though Steiner saw that spiritual vision itself is difficult for others to achieve, he recommended open-mindedly exploring and rationally testing the results of such research; he also urged others to follow a spiritual training that would allow them directly to apply the methods he used eventually to achieve comparable results. Some results of Steiner's research have been investigated and supported by scientists working to further and extend scientific observation in directions suggested by an anthroposophical approach.
Religious nature.
As an explicitly spiritual movement, anthroposophy has sometimes been called a religious philosophy. In 2005, a California federal court ruled that a group alleging that anthroposophy is a religion for Establishment Clause purposes did not provide any legally admissible evidence in support of this view; the case is under appeal. In 2000, a French court ruled that a government minister's description of anthroposophy as a cult was defamatory.
Statements on race.
Anthroposophical ideas have been criticized from both sides in the race debate:
The Anthroposophical Society in America has stated:
"We explicitly reject any racial theory that may be construed to be part of Rudolf Steiner's writings. The Anthroposophical Society in America is an open, public society and it rejects any purported spiritual or scientific theory on the basis of which the alleged superiority of one race is justified at the expense of another race."

</doc>
<doc id="2494" url="http://en.wikipedia.org/wiki?curid=2494" title="Aurochs">
Aurochs

The aurochs ( or ; pl. aurochs, or rarely aurochsen, aurochses), also urus, ure ("Bos primigenius"), the ancestor of domestic cattle, is an extinct type of large wild cattle that inhabited Europe, Asia and North Africa. The species survived in Europe until the last recorded aurochs died in the Jaktorów Forest, Poland in 1627.
During the Neolithic Revolution, which occurred during the early Holocene, there were at least two aurochs domestication events: one related to the Indian subspecies, leading to zebu cattle; the other one related to the Eurasian subspecies, leading to taurine cattle. Other species of wild bovines were also domesticated, namely the wild water buffalo, gaur, and banteng. In modern cattle, numerous breeds share characteristics of the aurochs, such as a dark colour in the bulls with a light eel stripe along the back (the cows being lighter), or a typical aurochs-like horn shape.
Taxonomy.
The aurochs was variously classified as "Bos primigenius", "Bos taurus", or, in old sources, "Bos urus". However, in 2003, the International Commission on Zoological Nomenclature "conserved the usage of 17 specific names based on wild species, which are pre-dated by or contemporary with those based on domestic forms", confirming "Bos primigenius" for the aurochs. Taxonomists who consider domesticated cattle a subspecies of the wild aurochs should use "B. primigenius taurus"; those who consider domesticated cattle to be a separate species may use the name "B. taurus", which the Commission has kept available for that purpose.
Etymology.
The words "aurochs", "urus", and "wisent" have all been used synonymously in English. However, the extinct aurochs/urus is a completely separate species from the still-extant wisent, also known as European bison. The two were often confused, and some 16th-century illustrations of aurochs and wisents have hybrid features.
The word "urus" (; plural "uri") is a Latin word, but was borrowed into Latin from Germanic (cf. Old English/Old High German "ūr", Old Norse "úr"). In German, OHG "ūr" was compounded with "ohso" "ox", giving "ūrohso", which became early modern "Aurochs". The modern form is "Auerochs".
The word "aurochs" was borrowed from early modern German, replacing archaic "urochs", also from an earlier form of German. The word is invariable in number in English, though sometimes back-formed singular "auroch" and innovated plural "aurochses" occur. The use in English of the plural form "" is nonstandard, but mentioned in "The Cambridge Encyclopedia of the English Language". It is directly parallel to the German plural and recreates by analogy the same distinction as English "ox" vs. "oxen".
Evolution.
During the Pliocene, the colder climate caused an extension of open grassland, which led to the evolution of large grazers, such as wild bovines. Bos acutifrons is an extinct species of cattle that has been suggested as an ancestor for the aurochs. 
The oldest aurochs remains have been dated to about 2 million years ago, in India. The Indian subspecies was the first to appear. During the Pleistocene, the species migrated west into the Middle East (western Asia) as well as to the east. They reached Europe about 270,000 years ago. The South Asian domestic cattle, or zebu, descended from Indian aurochs at the edge of the Thar Desert; the zebu is resistant to drought. Domestic yak, gayal and Javan cattle do not descend from aurochs.
The first complete mitochondrial genome (16,338 base pairs) DNA sequence analysis of "Bos primigenius" from an archaeologically verified and exceptionally well preserved aurochs bone sample was published in 2010.
Three wild subspecies of aurochs are recognized. Only the Eurasian subspecies survived until recent times.
Description.
The appearance of the aurochs has been reconstructed from skeletal material, historical descriptions and contemporaneous depictions, such as cave paintings, engravings or Sigismund von Herberstein’s illustration. The work by Charles Hamilton Smith is a copy of a painting owned by a merchant in Augsburg, which may date to the 16th century. Scholars have proposed that Smith's illustration was based on a cattle/aurochs hybrid, or an aurochs-like breed. The aurochs was depicted in prehistoric cave paintings and described in Julius Caesar's "The Gallic War".
Size
The aurochs was one of the largest herbivores in postglacial Europe, comparable to the wisent, the European bison. The size of an aurochs appears to have varied by region: in Europe, northern populations were bigger on average than those from the south. For example, during the Holocene, aurochs from Denmark and Germany had an average height at the shoulders of in bulls and in cows, while aurochs populations in Hungary had bulls reaching . The body mass of aurochs appeared to have showed some variability. Some individuals were comparable in weight to the wisent and the banteng, reaching around , whereas those from the late-middle Pleistocene are estimated to have weighed up to , as much as the largest gaur (the largest extant bovid). The sexual dimorphism between bull and cow was strongly expressed, with the cows being significantly shorter than bulls on average.
Horns
Because of the massive horns, the frontal bones of aurochs were elongated and broad. The horns of the aurochs were characteristic in size, curvature and orientation. They were curved in three directions: upwards and outwards at the base, then swinging forwards and inwards, then inwards and upwards. Aurochs horns could reach in length and between in diameter. The horns of bulls were larger, with the curvature more strongly expressed than in cows. The horns grew from the skull at a 60° angle to the muzzle, facing forwards.
Body shape
The proportions and body shape of the aurochs were strikingly different from many modern cattle breeds. For example, the legs were considerably longer and more slender, resulting in a shoulder height that nearly equalled the trunk length. The skull, carrying the large horns, was substantially larger and more elongated than in most cattle breeds. As in other wild bovines, the body shape of the aurochs was athletic and, especially in bulls, showed a strongly expressed neck and shoulder musculature. Therefore the forehand was larger than the rear, similar to the Wisent but unlike many domestic cattle. Even in carrying cows, the udder was small and hardly visible from the side; this feature is equal to that of other wild bovines. 
Coat colour
The coat colour of the aurochs can be reconstructed by using historical and contemporary depictions. In his letter to Conrad Gesner (1602), Anton Schneeberger describes the aurochs, a description that agrees with cave paintings in Lascaux and Chauvet. Calves were born a chestnut colour. Young bulls changed their coat colour at a few months' old to a very deep brown or black, with a white eel stripe running down the spine. Cows retained the reddish-brown colour. Both sexes had a light-coloured muzzle. Some North African engravings show aurochs with a light-colored "saddle" on the back, but otherwise there is no evidence of variation in coat colour throughout its range. A passage from Mucante (1596), describing the “wild ox” as gray, but is ambiguous and may refer to the wisent. Egyptian grave paintings show cattle with a reddish-brown coat colour in both sexes, with a light saddle, but the horn shape of these suggest that they may depict domestic cattle. Remains of aurochs hair were not known until the early 1980s.
Colour of forelocks
Some primitive cattle breeds display similar coat colours to the aurochs, including the black colour in bulls with a light eel stripe, a pale mouth, and similar sexual dimorphism in colour. A feature often attributed to the aurochs is blond forehead hairs. Historical descriptions tell that the aurochs had long and curly forehead hair, but none mentions a certain colour for it. Cis van Vuure (2005) says that, although the color is present in a variety of primitive cattle breeds, it is probably a discolouration that appeared after domestication. The gene responsible for this feature has not yet been identified. Zebu breeds show lightly coloured inner sides of the legs and belly, caused by the so-called Zebu-tipping gene. It has not been tested if this gene is present in remains of the wild form of the zebu, the Indian aurochs.
Ecology and behaviour.
Like many bovids, aurochs formed herds for at least one part of the year. These probably did not number much more than thirty. If aurochs had similar social behaviour as their descendents, social status was gained through displays and fights, in which cows engaged as well as bulls. Indeed it was reported that aurochs bulls often had severe fights. As in other wild cattle, ungulates that form unisexual herds, there was considerable sexual dimorphism. Ungulates that form herds containing animals of both sexes, such as horses, have more weakly developed sexual dimorphism.
During the mating season, which probably took place during the late summer or early autumn, the bulls had severe fights, and evidence from the forest of Jaktorów shows these could lead to death. In autumn, aurochs fed up for the winter and got fatter and shinier than during the rest of the year, according to Schneeberger. Calves were born in spring. According to Schneeberger the mother stayed at the calf's side until it was strong enough to join and keep up with the herd on the feeding grounds.
Calves were vulnerable to wolves, while healthy adult aurochs probably did not have to fear these predators. In prehistoric Europe, North Africa and Asia, big cats, like lions and tigers, and hyenas were additional predators that probably preyed on aurochs.
Historical descriptions, like Caesar’s "De Bello Gallico" or Schneeberger, tell that aurochs were swift and fast, and could be very aggressive. According to Schneeberger, aurochs were not concerned when a man approached. But, teased or hunted, an aurochs could get very aggressive and dangerous, and throw the teasing person into the air, as he described in a 1602 letter to Gesner.
Habitat.
There is no consensus concerning the habitat of the aurochs. While some authors think that the habitat selection of the aurochs was comparable to the African Forest Buffalo, others describe the species as inhabiting open grassland and helping maintain open areas by grazing, together with other large herbivores. With its hypsodont jaw, the aurochs was probably a grazer and had a food selection very similar to domestic cattle. It was not a browser like many deer species, nor a semi-intermediary feeder like the wisent. Comparisons of the isotope levels of Mesolithic aurochs and domestic cattle bones showed that aurochs probably inhabited wetter areas than domestic cattle. Schneeberger describes that, during winter, the aurochs ate twigs and acorns in addition to grasses.
After the beginning of the Common Era, the habitat of aurochs became more fragmented because of the steadily growing human population. During the last centuries of its existence, the aurochs was limited to remote regions, such as floodplain forests or marshes, where there were no competing domestic herbivores and less hunting pressure.
Relationship with humans.
Domestication.
The aurochs, which ranged throughout much of Eurasia and Northern Africa during the late Pleistocene and early Holocene, is widely accepted as the wild ancestor of modern cattle. Archaeological evidence shows that domestication occurred independently in the Near East and the Indian subcontinent between 10,000–8,000 years ago, giving rise to the two major domestic taxa observed today: humpless "Bos taurus" (taurine) and humped "Bos indicus" (zebu), respectively. This is confirmed by genetic analyses of matrilineal mitochondrial DNA sequences, which reveal a marked differentiation between modern "Bos taurus" and "Bos indicus" haplotypes, demonstrating their derivation from two geographically and genetically divergent wild populations. It is possible that there was a third domestication event from another form of the Aurochs in Africa. The Sanga cattle, a not-humped zebu like cattle type, is commonly believed to originate from crosses between humped-zebus with taurine cattle breeds. However, there is archaeological evidence that these cattle were domesticated independently in Africa and that bloodlines of taurine and zebu cattle were introduced only within the last few hundreds years.
Domestication of the aurochs began in the southern Caucasus and northern Mesopotamia from about the 6th millennium BC. Genetic evidence suggests that aurochs were independently domesticated in India and possibly also in northern Africa. Domesticated cattle and aurochs are so different in size that they have been regarded as separate species; however, large ancient cattle and aurochs "are difficult to classify because morphological traits have overlapping distributions in cattle and aurochs and diagnostic features are identified only in horn and some cranial element."
A Mitochondrial DNA study suggests that all domesticated taurine cattle originated from about 80 wild female aurochs. Those animals lived in Iran 10,500 years ago.
Comparison of aurochs bones with those of modern cattle has provided many insights about the aurochs. Remains of the beast, from specimens believed to have weighed more than a ton, have been found in Mesolithic sites around Goldcliff, Wales.
Though aurochs became extinct in Britain during the Bronze age, analysis of bones from aurochs that lived at about the same time as domesticated cattle showed no genetic contribution to modern breeds. As a result of this study, modern European cattle were thought to have descended directly from the Near East domestication. Another study found distinct similarities between modern breeds and Italian aurochs specimens, which suggested that the previously tested British aurochs were not a good model of the diversity of aurochs genetics. It also suggests possible North African and European aurochs contributions to domestic breeds. Further genetic tests have shown that domestic cattle in Europe are of Near Eastern origin. This indicates that the European aurochs was not domesticated, nor did it interbreed with the imported Near Eastern cattle.
Indian cattle (zebu), although domesticated eight to ten thousand years ago, are related to aurochs that diverged from the Near Eastern ones some 200,000 years ago. African cattle are thought to have descended from aurochs more closely related to the Near Eastern ones. The Near East and African aurochs groups are thought to have split some 25,000 years ago, probably 15,000 years before domestication. The "Turano-Mongolian" type of cattle now found in Northern China, Mongolia, Korea and Japan may represent a fourth domestication event (and a third event among "Bos taurus"–type aurochs). This group may have diverged from the Near East group some 35,000 years ago. Whether these separate genetic populations would have equated to separate subspecies is unclear.
The maximum range of the aurochs was from Europe (excluding Ireland and northern Scandinavia), to northern Africa, the Middle East, India and Central Asia. Until at least 3,000 years ago, the aurochs was also found in Eastern China, where it is recorded at the Dingjiabao Reservoir in Yangyuan County. Most remains in China are known from the area east of 105° E, but the species has also been reported from the eastern margin of the Tibetan plateau, close to the Heihe River.
Extinction.
By the 13th century AD, the aurochs’ range was restricted to Poland, Lithuania, Moldavia, Transylvania and East Prussia. The right to hunt large animals on any land was restricted first to nobles and then, gradually, to only the royal households. As the population of aurochs declined, hunting ceased, and the royal court used gamekeepers to provide open fields for grazing for the aurochs. The gamekeepers were exempted from local taxes in exchange for their service. Poaching aurochs was punishable by death. According to the royal survey in 1564, the gamekeepers knew of 38 animals. The last recorded live aurochs, a female, died in 1627 in the Jaktorów Forest, Poland, from natural causes. The causes of extinction were unrestricted hunting, a narrowing of habitat due to the development of farming, and diseases transmitted by domestic cattle.
Cattle resembling the aurochs.
While all the wild subspecies are extinct, "Bos primigenius" lives on in domesticated cattle and attempts are being made to breed similar types suitable for filling the extinct subspecies′ role in the wild.
Less-derived cattle breeds.
Because some cattle breeds have been changed more than other breeds, certain breeds (all belonging to the so-called landraces) bear a greater resemblance to the aurochs. These breeds are not very productive from the economical point of view, as they do not give as much milk or meat as others. Most of the ″primitive″ phenotypes are facing extinction, because farmers give them up for economic reasons or crossbreed them with more productive dairy and meat cattle. Very hardy and robust, the primitive breeds are sometimes used in nature conservation programs, where they can fill the place of their wild ancestor in the ecology. Primitive breeds include, for example: Caldela, Limia Cattle, Maremmana primitivo, Maronesa, Pajuna Cattle, Rhodopian Shorthorn, Sayaguesa Cattle, Spanish Fighting Bull, and Tudanca Cattle.
Breeding of aurochs-like cattle.
The idea of breeding back the aurochs was first proposed in the 19th century by Felix Pawel Jarocki. In the 1920s a first attempt was undertaken by the Heck brothers in Germany with the aim of breeding an effigy (a look-alike) of the aurochs. Starting in the 1990s, grazing and rewilding projects brought new impetus to the idea and new breeding-back efforts came underway, this time with the aim of recreating an animal not only with the looks but also with the behaviour and the ecological impact of the aurochs, in order to be able to fill the ecological role of the aurochs.
Heck cattle
In the 1920s, two German zoo directors (in Berlin and Munich), the brothers Heinz and Lutz Heck, began a selective breeding program to breed back the aurochs into existence from the descendant domestic cattle. Their plan was based on the concept that a species is not extinct as long as all its genes are still present in a living population. The result is the breed called Heck cattle. It resembles what is known about the appearance of the aurochs in colour and, in some cases, also horn shape.
Taurus Project
The ABU ("Arbeitsgemeinschaft Biologischer Umweltschutz"), a conservation group in Germany, started to crossbreed Heck cattle with southern-European primitive breeds in 1996, with the goal to increase the aurochs-likeness of certain Heck cattle herds. These crossbreeds are called Taurus cattle. It is aimed to bring in aurochs-like features that are supposedly missing in Heck cattle, using Sayaguesa Cattle, Chianina and, to a lesser extent, Spanish Fighting Cattle (Lidia). The same is done in the Hungarian national park Hortobágy, additionally using Hungarian Grey cattle and Watusi, in Lille Vildmose National Park in Denmark, using only Chianina and Sayaguesa so far, and in Latvia.
Tauros Programme
The Dutch-based Tauros Programme, initially TaurOs Project, is trying to DNA-sequence breeds of primitive cattle to find gene sequences that match those found in ″ancient DNA″ from aurochs samples. The modern cattle would be selectively bred to try to produce the aurochs-type genes in a single animal. Starting around 2007, Tauros programme selected a number of primitive breeds mainly from Iberia and Italy, such as Sayaguesa Cattle, Maremmana primitivo, Pajuna Cattle, Limia Cattle, Maronesa, Tudanca Cattle and others, which already bear considerable resemblance to the aurochs in certain features. Numerous crossbreed calves have been born already.
Uruz Project
The newest of the back-breeding efforts, the Uruz Project, was started in 2013 by the initiator of the Tauros Project and the True Nature Foundation, an organization that wants to create a sound economic foundation underneath ecological restoration and rewilding. It differs from the other projects in that it will use a limited set of cattle breeds with known aurochs characteristics, it will adhere to a strict crossbreeding strategy following the rules of Mendelian inheritance, and when needed the project will make use of genome editing. By doing this the project hopes to reduce the amount of generations needed and reduce the amount of unwanted recessive genes and throwbacks. Its preliminary plans called for the use of Sayaguesa, Maremmana primitivo or Hungarian steppe cattle, Chianina and Watusi. A first breeding herd so far consisting of three Chianina cows and a Watusi bull was started at Kloster Lorsch in Germany in December 2013, another herd using Barrosã is being set up in northern Portugal.
Other projects
Scientists of the Polish Foundation for Recreating the Aurochs (PFOT) in Poland hope to use DNA from bones in museums to recreate the aurochs. They plan to return this animal to the forests of Poland. The project has gained the support of the Polish Ministry of the Environment. They plan research on ancient preserved DNA. Other research projects have extracted ″ancient″ DNA over the past twenty years and their results have been published in such periodicals as "Nature" and "PNAS". Polish scientists believe that modern genetics and biotechnology make it possible to recreate an animal almost identical to the aurochs. They say this research will lead to examining the causes of the extinction of the aurochs, and help prevent a similar occurrence with domestic cattle.
In culture.
The aurochs was an important game animal appearing in both Paleolithic European and Mesopotamian cave paintings, such as those found at Lascaux and Livernon in France. Aurochs existed into the Iron Age in Anatolia and the Near East, where is was worshiped as a sacred animal, the Lunar Bull, associated with the Great Goddess and later with Mithras. In 2012, an archaeological mission of the British Museum, led by Lebanese archaeologist Claude Doumet Serhal, discovered at the site of the old American school in Sidon, Lebanon, the remains of wild animal bones, including those of an aurochs, dating from the late fourth-early third millennium. A 1999 archaeological dig in Peterborough, England, uncovered the skull of an aurochs. The front part of the skull had been removed but the horns remained attached. The supposition is that the killing of the aurochs in this instance was a sacrificial act.
Also during antiquity, the aurochs was regarded as an animal of cultural value. Aurochs are depicted on the Ishtar Gate. Aurochs horns were often used by Romans as hunting horns. Aurochs were among those wild animals caught for fights ("venationes") in arenas. Julius Caesar wrote about aurochs in "Gallic War" Chapter 6.28:
"...those animals which are called uri. These are a little below the elephant in size, and of the appearance, color, and shape of a bull. Their strength and speed are extraordinary; they spare neither man nor wild beast which they have espied. These the Germans take with much pains in pits and kill them. The young men harden themselves with this exercise, and practice themselves in this sort of hunting, and those who have slain the greatest number of them, having produced the horns in public, to serve as evidence, receive great praise. But not even when taken very young can they be rendered familiar to men and tamed. The size, shape, and appearance of their horns differ much from the horns of our oxen. These they anxiously seek after, and bind at the tips with silver, and use as cups at their most sumptuous entertainments."
The ancient name of the Estonian town of Rakvere, "Tarwanpe" or "Tarvanpea", probably derives from "Auroch's head" ("Tarvan pea") in ancient Estonian.
The Hebrew Bible contains numerous references to the untameable strength of "re'em", translated as "bullock" or "wild-ox" in Jewish translations and translated rather poorly in the King James Version as "unicorn" but recognized from the last century by Hebrew scholars as the aurochs.
When the aurochs became rarer, hunting it became a privilege of the nobility and a sign of a high social status. In the Nibelungenlied, the killing of aurochs by Siegfried is described: ""Darnach schlug er schiere einen Wisent und einen Elch, starker Ure viere und einen grimmen Schelch"", meaning ""After that, he defeated one wisent and one elk, four aurochs and one Schelch"" - the background of the "Schelch" is dubious. Aurochs horns were commonly used as drinking horns by the nobility, which led to the fact that many aurochs horn sheaths are preserved today (albeit often discoloured). Furthermore, there is a painting by Willem Kalf depicting an aurochs horn. The horns of the last aurochs bulls, which died in 1620, were ornamented with gold and are located at the Livrustkammaren in Stockholm today.
Schneeberger writes that aurochs were hunted with arrows, nets and hunting dogs. With immobilized aurochs, a ritual was practised that might be regarded as cruel nowadays: the curly hair on the forehead was cut from the skull of the living animal. Belts were made out of this hair and were believed to increase the fertility of women. When the aurochs was slaughtered, a cross-like bone was extracted from the heart. This bone, which is also present in domestic cattle, contributed to the mystique of the animal and magical powers have been attributed to it.
In eastern Europe, where the aurochs survived until nearly 400 years ago, the aurochs has left traces in fixed expressions. In Russia, a drunken person behaving badly was described as "behaving like an aurochs", whereas in Poland, big strong people were characterized as being "a bloke like an aurochs".
In Central Europe the aurochs features in toponyms and heraldic coats of arms. For example, the names Ursenbach and Aurach am Hongar are derived from the aurochs. An aurochs head, the traditional arms of the German region Mecklenburg, figures in the coat of arms of Mecklenburg-Vorpommern. The aurochs (Romanian "bour", from Latin "būbalus") was also the symbol of Moldavia; nowadays they can be found in the coat of arms of both Romania and Moldova. In modern-day Romania, there are villages named Boureni. The horn of the aurochs is a charge of the coat of arms of Tauragė, Lithuania, (the name itself of Tauragė is a compound of "taũras" "auroch" and "ragas" "horn"). It is also present in the emblem of Kaunas, Lithuania, and was part of the emblem of Bukovina during its time as an Austro-Hungarian "Kronland". The Swiss Canton of Uri is named after the aurochs; its yellow flag shows a black aurochs head. East Slavic surnames Turenin, Turishchev, Turov, Turovsky originate from the Slavic name of the species "tur". In Slovakia there are toponyms like Turany, Turíčky, Turie, Turie Pole, Turík, Turová (villages), Turiec (river and region), Turská dolina (valley) and others. Turopolje, a large lowland floodplain south of the Sava river in Croatia, got its name from the once-abundant aurochs (Croatian: ).
In 2002, a 3.5-m-high and 7.1-m-long statue of an aurochs was erected in Rakvere, Estonia for the town's 700th birthday. The sculpture, made by artist Tauno Kangro, has become a symbol of the town.
Aurochs are frequently mentioned in the "A Song of Ice and Fire" series of fantasy novels by George R. R. Martin, in which roasted aurochs are sometimes served at banquets.
In the 2012 movie "Beasts of the Southern Wild", the six-year-old main character imagines aurochs, though the fantasy creatures are portrayed by "costumed" Vietnamese Pot-Bellied piglets.
Notes.
This article incorporates Creative Commons CC-BY-2.5 text from reference.

</doc>
<doc id="2499" url="http://en.wikipedia.org/wiki?curid=2499" title="Asynchronous Transfer Mode">
Asynchronous Transfer Mode

Asynchronous Transfer Mode (ATM) is, according to the ATM Forum, "a telecommunications concept defined by ANSI and ITU (formerly CCITT) standards for carriage of a complete range of user traffic, including voice, data, and video signals". ATM was developed to meet the needs of the Broadband Integrated Services Digital Network, as defined in the late 1980s, and designed to unify telecommunication and computer networks. It was designed for a network that must handle both traditional high-throughput data traffic (e.g., file transfers), and real-time, low-latency content such as voice and video. The reference model for ATM approximately maps to the three lowest layers of the ISO-OSI reference model: network layer, data link layer, and physical layer. ATM is a core protocol used over the SONET/SDH backbone of the public switched telephone network (PSTN) and Integrated Services Digital Network (ISDN), but its use is declining in favour of all IP.
ATM provides functionality that is similar to both circuit switching and packet switching networks: ATM uses asynchronous time-division multiplexing, and encodes data into small, fixed-sized packets (ISO-OSI frames) called "cells." This differs from approaches such as the Internet Protocol or Ethernet that use variable sized packets and frames. ATM uses a connection-oriented model in which a virtual circuit must be established between two endpoints before the actual data exchange begins. These virtual circuits may be “permanent”, i.e. dedicated connections that are usually preconfigured by the service provider, or “switched”, i.e. set up on a per-call basis using signalling and disconnected when the call is terminated. 
Layer 2 – Datagrams.
In the ISO-OSI reference model data link layer (layer 2), the basic transfer units are generically called frames. In ATM these frames are of a fixed (53 octets or bytes) length and specifically called "cells".
Cell size.
If a speech signal is reduced to packets, and it is forced to share a link with bursty data traffic (traffic with some large data packets) then no matter how small the speech packets could be made, they would always encounter full-size data packets. Under normal queuing conditions the cells might experience maximum queuing delays. To avoid this issue, all ATM packets, or "cells," are the same small size. In addition, the fixed cell structure means that ATM can be readily switched by hardware without the inherent delays introduced by software switched and routed frames.
Thus, the designers of ATM utilized small data cells to reduce jitter (delay variance, in this case) in the multiplexing of data streams. Reduction of jitter (and also end-to-end round-trip delays) is particularly important when carrying voice traffic, because the conversion of digitized voice into an analogue audio signal is an inherently real-time process, and to do a good job, the decoder (codec) that does this needs an evenly spaced (in time) stream of data items. If the next data item is not available when it is needed, the codec has no choice but to produce silence or guess — and if the data is late, it is useless, because the time period when it should have been converted to a signal has already passed.
At the time of the design of ATM, 155 Mbit/s Synchronous Digital Hierarchy (SDH) with 135 Mbit/s payload was considered a fast optical network link, and many plesiochronous digital hierarchy (PDH) links in the digital network were considerably slower, ranging from 1.544 to 45 Mbit/s in the USA, and 2 to 34 Mbit/s in Europe.
At this rate, a typical full-length 1500 byte (12000-bit) data packet would take 77.42 µs to transmit. In a lower-speed link, such as a 1.544 Mbit/s T1 line, a 1500 byte packet would take up to 7.8 milliseconds.
A queuing delay induced by several such data packets might exceed the figure of 7.8 ms several times over, in addition to any packet generation delay in the shorter speech packet. This was clearly unacceptable for speech traffic, which needs to have low jitter in the data stream being fed into the codec if it is to produce good-quality sound. A packet voice system can produce this low jitter in a number of ways:
The design of ATM aimed for a low-jitter network interface. However, "cells" were introduced into the design to provide short queuing delays while continuing to support datagram traffic. ATM broke up all packets, data, and voice streams into 48-byte chunks, adding a 5-byte routing header to each one so that they could be reassembled later. The choice of 48 bytes was political rather than technical. When the CCITT (now ITU-T) was standardizing ATM, parties from the United States wanted a 64-byte payload because this was felt to be a good compromise in larger payloads optimized for data transmission and shorter payloads optimized for real-time applications like voice; parties from Europe wanted 32-byte payloads because the small size (and therefore short transmission times) simplify voice applications with respect to echo cancellation. Most of the European parties eventually came around to the arguments made by the Americans, but France and a few others held out for a shorter cell length. With 32 bytes, France would have been able to implement an ATM-based voice network with calls from one end of France to the other requiring no echo cancellation. 48 bytes (plus 5 header bytes = 53) was chosen as a compromise between the two sides. 5-byte headers were chosen because it was thought that 10% of the payload was the maximum price to pay for routing information. ATM multiplexed these 53-byte cells instead of packets which reduced worst-case cell contention jitter by a factor of almost 30, reducing the need for echo cancellers.
The structure of an ATM cell.
An ATM cell consists of a 5-byte header and a 48-byte payload. The payload size of 48 bytes was chosen as described above.
ATM defines two different cell formats: UNI (User-Network Interface) and NNI (Network-Network Interface). Most ATM links use UNI cell format.
ATM uses the PT field to designate various special kinds of cells for operations, administration and management (OAM) purposes, and to delineate packet boundaries in some AALs.
Several ATM link protocols use the HEC field to drive a CRC-based framing algorithm, which allows locating the ATM cells with no overhead beyond what is otherwise needed for header protection. The 8-bit CRC is used to correct single-bit header errors and detect multi-bit header errors. When multi-bit header errors are detected, the current and subsequent cells are dropped until a cell with no header errors is found.
A UNI cell reserves the GFC field for a local flow control/submultiplexing system between users. This was intended to allow several terminals to share a single network connection, in the same way that two Integrated Services Digital Network (ISDN) phones can share a single basic rate ISDN connection. All four GFC bits must be zero by default.
The NNI cell format replicates the UNI format almost exactly, except that the 4-bit GFC field is re-allocated to the VPI field, extending the VPI to 12 bits. Thus, a single NNI ATM interconnection is capable of addressing almost 212 VPs of up to almost 216 VCs each (in practice some of the VP and VC numbers are reserved).
Cells in practice.
ATM supports different types of services via ATM adaptation layers (AAL). Standardized AALs include AAL1, AAL2, and AAL5, and the rarely used AAL3 and AAL4. AAL1 is used for constant bit rate (CBR) services and circuit emulation. Synchronization is also maintained at AAL1. AAL2 through AAL4 are used for variable bit rate (VBR) services, and AAL5 for data. Which AAL is in use for a given cell is not encoded in the cell. Instead, it is negotiated by or configured at the endpoints on a per-virtual-connection basis.
Following the initial design of ATM, networks have become much faster. A 1500 byte (12000-bit) full-size Ethernet frame takes only 1.2 µs to transmit on a 10 Gbit/s network, reducing the need for small cells to reduce jitter due to contention. Some consider that this makes a case for replacing ATM with Ethernet in the network backbone. However, it should be noted that the increased link speeds by themselves do not alleviate jitter due to queuing. Additionally, the hardware for implementing the service adaptation for IP packets is expensive at very high speeds. Specifically, at speeds of OC-3 and above, the cost of segmentation and reassembly (SAR) hardware makes ATM less competitive for IP than Packet Over SONET (POS); because of its fixed 48-byte cell payload, ATM is not suitable as a data link layer "directly" underlying IP (without the need for SAR at the data link level) since the OSI layer on which IP operates must provide a maximum transmission unit (MTU) of at least 576 bytes. SAR performance limits mean that the fastest IP router ATM interfaces are STM16 - STM64 which actually compares, while POS can operate at OC-192 (STM64) with higher speeds expected in the future.
On slower or congested links (622 Mbit/s and below), ATM does make sense, and for this reason most asymmetric digital subscriber line (ADSL) systems use ATM as an intermediate layer between the physical link layer and a Layer 2 protocol like PPP or Ethernet.
At these lower speeds, ATM provides a useful ability to carry multiple logical circuits on a single physical or virtual medium, although other techniques exist, such as Multi-link PPP and Ethernet VLANs, which are optional in VDSL implementations. DSL can be used as an access method for an ATM network, allowing a DSL termination point in a telephone central office to connect to many internet service providers across a wide-area ATM network. In the United States, at least, this has allowed DSL providers to provide DSL access to the customers of many internet service providers. Since one DSL termination point can support multiple ISPs, the economic feasibility of DSL is substantially improved.
Why virtual circuits?
ATM operates as a channel-based transport layer, using virtual circuits (VCs). This is encompassed in the concept of the Virtual Paths (VP) and Virtual Channels. Every ATM cell has an 8- or 12-bit Virtual Path Identifier (VPI) and 16-bit Virtual Channel Identifier (VCI) pair defined in its header. The VCI, together with the VPI, is used to identify the next destination of a cell as it passes through a series of ATM switches on its way to its destination. The length of the VPI varies according to whether the cell is sent on the user-network interface (on the edge of the network), or if it is sent on the network-network interface (inside the network).
As these cells traverse an ATM network, switching takes place by changing the VPI/VCI values (label swapping). Although the VPI/VCI values are not necessarily consistent from one end of the connection to the other, the concept of a circuit "is" consistent (unlike IP, where any given packet could get to its destination by a different route than the others). ATM switches use the VPI/VCI fields to identify the Virtual Channel Link (VCL) of the next network that a cell needs to transit on its way to its final destination. The function of the VCI is similar to that of the data link connection identifier (DLCI) in frame relay and the Logical Channel Number & Logical Channel Group Number in X.25.
Another advantage of the use of virtual circuits comes with the ability to use them as a multiplexing layer, allowing different services (such as voice, Frame Relay, n* 64 channels, IP). The VPI is useful for reducing the switching table of some virtual circuits which have common paths.
Using cells and virtual circuits for traffic engineering.
Another key ATM concept involves the traffic contract. When an ATM circuit is set up each switch on the circuit is informed of the traffic class of the connection.
ATM traffic contracts form part of the mechanism by which "quality of service" (QoS) is ensured. There are four basic types (and several variants) which each have a set of parameters describing the connection.
VBR has real-time and non-real-time variants, and serves for "bursty" traffic. Non-real-time is sometimes abbreviated to vbr-nrt.
Most traffic classes also introduce the concept of Cell Delay Variation Tolerance (CDVT), which defines the "clumping" of cells in time.
Traffic policing.
To maintain network performance, networks may apply traffic policing to virtual circuits to limit them to their traffic contracts at the entry points to the network, i.e. the user–network interfaces (UNIs) and network-to-network interfaces (NNIs): Usage/Network Parameter Control (UPC and NPC). The reference model given by the ITU-T and ATM Forum for UPC and NPC is the generic cell rate algorithm (GCRA), which is a version of the leaky bucket algorithm. CBR traffic will normally be policed to a PCR and CDVt alone, whereas VBR traffic will normally be policed using a dual leaky bucket controller to a PCR and CDVt and an SCR and Maximum Burst Size (MBS). The MBS will normally be the packet (SAR-SDU) size for the VBR VC in cells.
If the traffic on a virtual circuit is exceeding its traffic contract, as determined by the GCRA, the network can either drop the cells or mark the Cell Loss Priority (CLP) bit (to identify a cell as potentially redundant). Basic policing works on a cell by cell basis, but this is sub-optimal for encapsulated packet traffic (as discarding a single cell will invalidate the whole packet). As a result, schemes such as Partial Packet Discard (PPD) and Early Packet Discard (EPD) have been created that will discard a whole series of cells until the next packet starts. This reduces the number of useless cells in the network, saving bandwidth for full packets. EPD and PPD work with AAL5 connections as they use the end of packet marker: the ATM User-to-ATM User (AUU) Indication bit in the Payload Type field of the header, which is set in the last cell of a SAR-SDU
Traffic shaping.
Traffic shaping usually takes place in the network interface card (NIC) in user equipment, and attempts to ensure that the cell flow on a VC will meet its traffic contract, i.e. cells will not be dropped or reduced in priority at the UNI. Since the reference model given for traffic policing in the network is the GCRA, this algorithm is normally used for shaping as well, and single and dual leaky bucket implementations may be used as appropriate.
Types of virtual circuits and paths.
ATM can build virtual circuits and virtual paths either statically or dynamically. Static circuits (permanent virtual circuits or PVCs) or paths (permanent virtual paths or PVPs) require that the circuit is composed of a series of segments, one for each pair of interfaces through which it passes.
PVPs and PVCs, though conceptually simple, require significant effort in large networks. They also do not support the re-routing of service in the event of a failure. Dynamically built PVPs (soft PVPs or SPVPs) and PVCs (soft PVCs or SPVCs), in contrast, are built by specifying the characteristics of the circuit (the service "contract") and the two end points.
Finally, ATM networks create and remove switched virtual circuits (SVCs) on demand when requested by an end piece of equipment. One application for SVCs is to carry individual telephone calls when a network of telephone switches are inter-connected using ATM. SVCs were also used in attempts to replace local area networks with ATM.
Virtual circuit routing.
Most ATM networks supporting SPVPs, SPVCs, and SVCs use the Private Network Node Interface or the Private Network-to-Network Interface (PNNI) protocol. PNNI uses the same shortest-path-first algorithm used by OSPF and IS-IS to route IP packets to share topology information between switches and select a route through a network. PNNI also includes a very powerful summarization mechanism to allow construction of very large networks, as well as a call admission control (CAC) algorithm which determines the availability of sufficient bandwidth on a proposed route through a network in order to satisfy the service requirements of a VC or VP.
Call admission and connection establishment.
A network must establish a connection before two parties can send cells to each other. In ATM this is called a virtual circuit (VC). It can be a permanent virtual circuit (PVC), which is created administratively on the end points, or a switched virtual circuit (SVC), which is created as needed by the communicating parties. SVC creation is managed by signaling, in which the requesting party indicates the address of the receiving party, the type of service requested, and whatever traffic parameters may be applicable to the selected service. "Call admission" is then performed by the network to confirm that the requested resources are available and that a route exists for the connection.
Reference model.
ATM defines three layers:
Deployment.
ATM became popular with telephone companies and many computer makers in the 1990s. However, even by the end of the decade, the better price/performance of Internet Protocol-based products was competing with ATM technology for integrating real-time and bursty network traffic.
Companies such as FORE Systems focused on ATM products, while other large vendors such as Cisco Systems provided ATM as an option.
After the burst of the dot-com bubble, some still predicted that "ATM is going to dominate".
However, in 2005 the ATM Forum, which had been the trade organization promoting the technology, merged with groups promoting other technologies, and eventually became the Broadband Forum.
Wireless ATM or Mobile ATM.
While ATM is a great technology for high speed networks, the wireless version of it
takes the form of Wireless ATM or Mobile ATM. Basically, the core network is ATM while the access network is wireless. ATM cells are still being transmitted from base stations to mobile terminals. Mobility functions are performed at an ATM switch in the core network, known as "crossover switch", which is similar to the MSC (mobile switching center) of GSM Networks. The advantage of Wireless ATM is its high bandwidth and high speed handoffs done at Layer 2. In the early 1990s, Bell Labs and NEC
Research Labs worked actively in this field. Andy Hopper from Cambridge University
Computer Laboratory also worked in this area. There was a Wireless ATM Forum formed
to standardize the technology behind Wireless ATM Networks. The forum was supported
by several telecommunication companies, including NEC, Fujitsu, AT&T, etc. Mobile
ATM aimed to provide high speed multimedia communications technology, capable of
delivering broadband mobile communications beyond that of GSM and WLANs.

</doc>
<doc id="2500" url="http://en.wikipedia.org/wiki?curid=2500" title="Anus">
Anus

The anus (, which is from the Proto-Indo-European "ano–", meaning "ring" ) is an opening at the opposite end of an animal's digestive tract from the mouth. Its function is to control the expulsion of feces, unwanted semi-solid matter produced during digestion, which, depending on the type of animal, may include: matter which the animal cannot digest, such as bones; food material after all the nutrients have been extracted, for example cellulose or lignin; ingested matter which would be toxic if it remained in the digestive tract; and dead or excess gut bacteria and other endosymbionts. 
Amphibians, reptiles, and birds use the same orifice (known as the cloaca) for excreting liquid and solid wastes, for copulation and egg-laying. Monotreme mammals also have a cloaca, which is thought to be a feature inherited from the earliest amniotes via the therapsids. Marsupials have a single orifice for excreting both solids and liquids and, in females, a separate vagina for reproduction. Female placental mammals have completely separate orifices for defecation, urination, and reproduction; males have one opening for defecation and another for both urination and reproduction, although the channels flowing to that orifice are almost completely separate.
The development of the anus was an important stage in the evolution of multicellular animals. It appears to have happened at least twice, following different paths in protostomes and deuterostomes. This accompanied or facilitated other important evolutionary developments: the bilaterian body plan, the coelom, and metamerism, in which the body was built of repeated "modules" which could later specialize, such as the heads of most arthropods, which are composed of fused, specialized segments.
Development.
In animals at least as complex as an earthworm, the embryo forms a dent on one side, the blastopore, which deepens to become the archenteron, the first phase in the growth of the gut. In deuterostomes, the original dent becomes the anus while the gut eventually tunnels through to make another opening, which forms the mouth. The protostomes were so named because it was thought that in their embryos the dent formed the mouth first ("proto–" meaning "first") and the anus was formed later at the opening made by the other end of the gut. More recent research, however, shows that in protostomes the edges of the dent close up in the middle, leaving openings at the ends which become the mouth and anus.

</doc>
<doc id="2501" url="http://en.wikipedia.org/wiki?curid=2501" title="Appendix">
Appendix

Appendix may refer to:
In documents:
In anatomy:
In music:
In journalism:

</doc>
<doc id="2502" url="http://en.wikipedia.org/wiki?curid=2502" title="Acantharea">
Acantharea

The Acantharea (Acantharia) are a group of radiolarian protozoa, distinguished mainly by their skeletons.
Structure.
These are composed of strontium sulfate crystals, which do not fossilize, and take the form of either ten diametric or twenty radial spines. The central capsule is made up of microfibrils arranged into twenty plates, each with a hole through which one spine projects, and there is also a microfibrillar cortex linked to the spines by myonemes. These assist in flotation, together with the vacuoles in the ectoplasm, which often contain zooxanthellae.
Classification by spine arrangement.
The arrangement of the spines is very precise, and is described by what is called the Müllerian law. This is easiest to describe in terms of lines of latitude and longitude - the spines lie on the intersections between five of the former, symmetric about an equator, and eight of the latter, spaced uniformly. Each line of longitude carries either two "tropical" spines or one "equatorial" and two "polar" spines, in alternation. The way that the spines are joined together at the center of the cell varies and is one of the primary characteristics by which acanthareans are classified.
The axopods are fixed in number.
Life cycle.
Adults are usually multinucleated. Reproduction is thought to take place by formation of swarmer cells (formerly referred to as "spores"), which may be flagellate. Not all life cycle stages have been observed, and study of these organisms has been hampered mainly by an inability to maintain these organisms in culture through successive generations.

</doc>
<doc id="2503" url="http://en.wikipedia.org/wiki?curid=2503" title="African National Congress">
African National Congress

The African National Congress (ANC) is the Republic of South Africa's governing political party, supported by its Tripartite Alliance with the Congress of South African Trade Unions (COSATU) and the South African Communist Party (SACP), since the establishment of multi-racial democracy in April 1994. It defines itself as a "disciplined force of the left".
Members founded the organisation as the South African Native National Congress (SANNC) on 8 January 1912 at the Waaihoek Wesleyan Church in Bloemfontein to increase the rights of the black South African population. John Dube, its first president, and poet and author Sol Plaatje were among its founding members. The organisation became the ANC in 1923 and formed a military wing, the Umkhonto we Sizwe (Spear of the Nation) in 1961.
It has been the ruling party of post-apartheid South Africa on the national level since 1994. It increased its majority in the South African general election. Further increases in 2004, with 69.7% of the votes. In 2009 its share of the vote reduced slightly, but it remained the dominant party with 65.9% of the votes, and decreased again in 2014 when it garnered 62.15%.
History.
The founding of the SANNC was in direct response to injustice against black South Africans at the hands of the government then in power. It can be said that the SANNC had its origins in a pronouncement by Pixley ka Isaka Seme who said in 1911, "Forget all the past differences among Africans and unite in one national organisation." The SANNC was founded the following year on 8 January 1912.
The government of the newly formed Union of South Africa began a systematic oppression of black people in South Africa. The Land Act was promulgated in 1913 forcing many non-whites from their farms into the cities and towns to work, and to restrict their movement within South Africa.
By 1919, the SANNC was leading a campaign against passes (an ID which non-whites had to possess). However, it then became dormant in the mid-1920s. During that time, black people were also represented by the ICU and the previously white-only Communist party. In 1923, the organisation became the African National Congress, and in 1929 the ANC supported a militant mineworkers' strike.
By 1927, J.T. Gumede (president of the ANC) proposed co-operation with the Communists in a bid to revitalise the organisation, but he was voted out of power in the 1930s. This led to the ANC becoming largely ineffectual and inactive, until the mid-1940s when the ANC was remodelled as a mass movement.
The ANC responded militarily to attacks on the rights of black South Africans, as well as calling for strikes, boycotts, and defiance. This led to a later Defiance Campaign in the 1950s, a mass movement of resistance to apartheid. The government tried to stop the ANC by banning party leaders and enacting new laws to stop the ANC, however these measures ultimately proved to be ineffective.
In 1955, the Congress of the People officially adopted the Freedom Charter, stating the core principles of the South African Congress Alliance, which consisted of the African National Congress and its allies the South African Communist Party (SACP), the South African Indian Congress, the South African Congress of Democrats (COD) and the Coloured People's Congress. The government claimed that this was a communist document, and consequently leaders of the ANC and Congress were arrested. 1960 saw the Sharpeville massacre, in which 69 people were killed when police opened fire on anti-apartheid protesters.
Whites eventually joined the fight against apartheid, leading many Black nationalists to break away from the ANC.
During apartheid there was violence between the ANC and the Inkatha Freedom Party. For example between 1985 and 1989, 5,000 civilians were killed in fighting between the two parties. Massacres by each of the other's supporters included the Shell House massacre and the Boipatong massacre.
The ANC and its members were officially removed from the United States terrorism watch list in 2008.
Umkhonto we Sizwe.
Umkhonto we Sizwe or MK, translated "Spear of the Nation", was the military wing of the ANC. Partly in response to the Sharpeville massacre of 1960, individual members of the ANC found it necessary to consider violence to combat what passive protest had failed to quell. A significant portion of ANC leadership and party as a whole therefore agreed that this violence was needed to combat increasing backlash from the government.
Some ANC members were upset by the actions of the MK, and refused to accept violence as necessary for the ending of apartheid, but these individuals became a minority as the militant leaders such as Nelson Mandela gained significant popularity. Many consider their actions to be criminal, but the MK deemed the means justified by the end goal of ending apartheid. The MK committed terrorist acts to achieve their aims, and MK was responsible for the deaths of both civilians and members of the military. Acts of terrorism committed by the MK include the Church Street bombing and the Magoo's Bar bombing.
In co-operation with the South African Communist Party, MK was founded in 1961.
Ideology.
The ANC deems itself a force of national liberation in the post-apartheid era; it officially defines its agenda as the "National Democratic Revolution". The ANC is a member of the Socialist International. It also sets forth the redressing of socio-economic differences stemming from colonial- and apartheid-era policies as a central focus of ANC policy.
The National Democratic Revolution (NDR) is described as a process through which the National Democratic Society (NDS) is achieved; a society in which people are intellectually, socially, economically and politically empowered. The drivers of the NDR are also called the motive forces and are defined as the elements within society that gain from the success of the NDR. Using contour plots or concentric circles the centre represents the elements in society that gain the most out of the success of the NDR. Moving away from the centre results in the reduction of the gains that those elements derive. It is generally believed that the force that occupies the centre of those concentric circles in countries with low unemployment is the working class while in countries with higher levels of unemployment it is the unemployed. Some of the many theoreticians that have written about the NDR include Joe Slovo, Joel Netshitenzhe and Tshilidzi Marwala.
In 2004 the ANC declared itself to be a social democratic party.
Tripartite Alliance.
The ANC holds a historic alliance with the South African Communist Party (SACP) and Congress of South African Trade Unions (COSATU), known as the "Tripartite Alliance". The SACP and COSATU have not contested any election in South Africa, but field candidates through the ANC, hold senior positions in the ANC, and influence party policy and dialogue. During Mbeki's presidency, the government took a more pro-capitalist stance, often running counter to the demands of the SACP and COSATU.
2008 schism.
Following Zuma's accession to the ANC leadership in 2007 and Mbeki's resignation as president in 2008, the Mbeki faction of former ministers led by Mosiuoa Lekota split away from the ANC to form the Congress of the People.
2013 NUMSA split.
On 20 December 2013, a special congress of the National Union of Metalworkers of South Africa (NUMSA), the country's biggest trade union with 338,000 members, voted to withdraw support from the ANC and SACP, and form a socialist party to protect the interests of the working class. NUMSA secretary general Irvin Jim condemned the ANC and SACP's support for big business and stated: "It is clear that the working class cannot any longer see the ANC or the SACP as its class allies in any meaningful sense."
ANC flag.
The ANC flag is composed of three stripes – black, green and gold. Black symbolises the native people of South Africa, green represents the land and gold represents the mineral and other natural wealth of South Africa. This flag was also the battle flag of the Umkhonto we Sizwe. The official party flag also has the emblem of the party incorporated onto the flag.
Party list.
Politicians in the party win a place in parliament by being on the "Party List", which is drawn up before the elections and enumerates, in order, the party's preferred MPs. The number of seats allocated is proportional to the popular national vote, and this determines the cut-off point.
The ANC has also gained members through the controversial floor crossing process.
Although most South African parties announced their candidate list for provincial premierships in the 2009 election, the ANC did not, as it is not required for parties to do so.
Role of the ANC in resolving the conflict.
The ANC represented the main opposition to the government during apartheid and therefore they played a major role in resolving the conflict through participating in the peacemaking and peace-building processes. Initially intelligence agents of the National Party met in secret with ANC leaders, including Nelson Mandela, to judge whether conflict resolution was possible. Discussions and negotiations took place leading to the eventual unbanning of the ANC and other opposing political parties by then President de Klerk on 2 February 1990. These initial meetings were the first crucial steps towards resolution.
The next official step towards rebuilding South Africa was the Groote Schuur Minute where the government and the ANC agreed on a common commitment towards the resolution of the existing climate of violence and intimidation, as well as a commitment to stability and to a peaceful process of negotiations. The ANC negotiated the release of political prisoners and the indemnity from prosecution for returning exiles and moreover channels of communication were established between the Government and the ANC.
Later the Pretoria Minute represented another step towards resolution where agreements at Groote Schuur were reconsolidated and steps towards setting up an interim government and drafting a new constitution were established as well as suspension of the military wing of the ANC – the Umkhonto we Sizwe. This step helped end much of the violence within South Africa. Another agreement that came out of the Pretoria Minute was that both parties would try and raise awareness that a new way of governance was being created for South Africa, and that further violence would only hinder this process. However, violence still continued in Kwazulu-Natal, which violated the trust between Mandela and de Klerk. Moreover, internal disputes in the ANC prolonged the war as consensus on peace was not reached.
The next significant steps towards resolution were the Repeal of the Population Registration Act, the repeal of the Group Areas and the Native Land Acts and a catch-all Abolition of Racially Based Land Measures Act was passed. These measures ensured no one could claim, or be deprived of, any land rights on the basis of race.
In December 1991 the Convention for a Democratic South Africa (CODESA) was held with the aim of establishing an interim government. However, a few months later in June 1992 the Boipatong massacre occurred and all negotiations crumbled as the ANC pulled out. After this negotiations proceeded between two agents, Cyril Ramaphosa of the ANC, and Roelf Meyer of the National Party. In over 40 meetings the two men discussed and negotiated over many issues including the nature of the future political system, the fate of over 40,000 government employees and if/how the country would be divided. The result of these negotiations was an interim constitution that meant the transition from apartheid to democracy was a constitutional continuation and that the rule of law and state sovereignty remained intact during the transition, which was vital for stability within the country. A date was set for the first democratic elections on 27 April 1994. The ANC won 62.5% of the votes and has been in power ever since.
Criticism.
Controversy over corrupt members.
The most prominent corruption case involving the ANC relates to a series of bribes paid to companies involved in the ongoing R55 billion Arms Deal saga, which resulted in a long term jail sentence to former Deputy President Jacob Zuma's legal adviser Schabir Shaik. Schabir Shaik was released after about two years on the basis that he was terminally ill. Zuma, now the State president, was charged with fraud, bribery and corruption in the Arms Deal, but the charges were subsequently withdrawn by the National Prosecuting Authority of South Africa due to their delay in prosecution. The ANC has also been criticised for its subsequent abolition of the Scorpions, the multidisciplinary agency that investigated and prosecuted organised crime and corruption, and was heavily involved in the investigation into Zuma and Shaik.
Tony Yengeni, in his then position as chief whip of the ANC and also head of the Parliaments defence committee has recently been named as being involved in a R6 million bribe with the German company ThyssenKrupp over the purchase of four corvettes for the SANDF. German detectives raided the offices of the German company and found documentation linking Yengeni to the bribe
Other recent corruption issues include the sexual misconduct and criminal charges of Beaufort West municipal manager Truman Prince, and the Oilgate scandal, in which millions of Rand in funds from a state-owned company were allegedly funnelled into ANC coffers.
The ANC has also been accused of using government and civil society to fight its political battles against opposition parties such as the Democratic Alliance. The result has been a number of complaints and allegations that none of the political parties truly represent the interests of the poor. This has resulted in the "No Land! No House! No Vote!" Campaign which becomes very prominent each time the country holds elections.
Controversy over wasteful expenditure.
The ANC spent over R1 billion of taxpayers' money on luxury vehicles, expensive hotels, banquets, advertising and other "wasteful expenditure" between August 2009 and April 2010. The main thrust behind this reporting is the official opposition in the country, the Democratic Alliance (DA), which kept a tally of the expenditure called "The Wasteful Expenditure Monitor".
According to the DA, this money could have:
The ANC Northern Cape premier, Sylvia Lucas, in her first 10 weeks in office, spent R53,159.00 of taxpayers money on "fast food" at outlets such as Spur, Nandos, KFC and Wimpy.
Condemnation over Secrecy Bill.
In late 2011 the ANC was heavily criticised over the passage of the Protection of State Information Bill, which opponents claimed would improperly restrict the freedom of the press. Opposition to the bill included otherwise ANC-aligned groups such as COSATU. Notably, Nelson Mandela and other Nobel laureates Nadine Gordimer, Archbishop Desmond Tutu, and F. W. de Klerk have expressed disappointment with the bill for not meeting standards of constitutionality and aspirations for freedom of information and expression.
Role in the Marikana killings.
The ANC have been criticised for its role in failing to prevent the 16 August 2012 massacre of Lonmin miners at Marikana in the North West. Some allege that Police Commissioner Riah Phiyega and Police Minister Nathi Mthethwa, a close confidant of Jacob Zuma, may have given the go ahead for the police action against the miners on that day.
Commissioner Phiyega of the ANC came under further criticism as being insensitive and uncaring when she was caught smiling and laughing during the Farlam Commission's video playback of the 'massacre'.
Archbishop Desmond Tutu has announced that he no longer can bring himself to exercise a vote for the ANC as it is no longer the party that he and Nelson Mandela fought for, and that the party has now lost its way, and is in danger of becoming a corrupt entity in power.

</doc>
<doc id="2504" url="http://en.wikipedia.org/wiki?curid=2504" title="Amphetamine">
Amphetamine

Uses.
Medical.
<onlyinclude></onlyinclude>
Enhancing performance.
<onlyinclude></onlyinclude>
Contraindications.
<onlyinclude></onlyinclude>
Side effects.
Side effects of amphetamine are varied, and the amount of amphetamine consumed is the primary factor in determining the likelihood and severity of side effects. Amphetamine products such as Adderall, Dexedrine, and their generic equivalents are currently approved by the USFDA for long-term therapeutic use. Recreational use of amphetamine generally involves doses much larger, and therefore has a greater risk of serious side effects than dosages used for therapeutic reasons.
<onlyinclude></onlyinclude>
Overdose.
<onlyinclude></onlyinclude>
Interactions.
Many types of substances are known to interact with amphetamine, resulting in altered drug action or metabolism of amphetamine, the interacting substance, or both. Inhibitors of the enzymes that metabolize amphetamine (i.e., CYP2D6 and flavin containing monooxygenase) will prolong its elimination half-life. Amphetamine also interacts with , particularly monoamine oxidase A inhibitors, since both MAOIs and amphetamine increase plasma catecholamines; therefore, concurrent use of both is dangerous. Amphetamine will modulate the activity of most psychoactive drugs. In particular, amphetamine may decrease the effects of sedatives and depressants and increase the effects of stimulants and antidepressants. Amphetamine may also decrease the effects of antihypertensives and antipsychotics due to its effects on blood pressure and dopamine respectively. There is no significant effect on consuming amphetamine with food in general, but the pH of gastrointestinal content and urine affects the absorption and excretion of amphetamine, respectively. Acidic substances reduce the absorption of amphetamine and increase urinary excretion, and alkaline substances do the opposite. Due to the effect pH has on absorption, amphetamine also interacts with gastric acid reducers such as proton pump inhibitors and H2 antihistamines, which increase gastrointestinal pH.
Pharmacology.
Pharmacodynamics.
Amphetamine exerts its behavioral effects by altering the use of monoamines as neuronal signals in the brain, primarily in catecholamine neurons in the reward and executive function pathways of the brain, collectively known as the mesocorticolimbic projection. The concentrations of the main neurotransmitters involved in reward circuitry and executive functioning, dopamine and norepinephrine, increase dramatically in a dose-dependent manner by amphetamine due to its effects on monoamine transporters. The reinforcing and task saliency effects of amphetamine are mostly due to enhanced dopaminergic activity in the mesolimbic pathway.
Amphetamine has been identified as a potent full agonist of trace amine-associated receptor 1 (TAAR1), a and G protein-coupled receptor (GPCR) discovered in 2001, which is important for regulation of brain monoamines. Activation of increases production via adenylyl cyclase activation and inhibits monoamine transporter function. Monoamine autoreceptors (e.g., D2 short, presynaptic α2, and presynaptic 5-HT1A) have the opposite effect of TAAR1, and together these receptors provide a regulatory system for monoamines. Notably, amphetamine and trace amines bind to TAAR1, but not monoamine autoreceptors. Imaging studies indicate that monoamine reuptake inhibition by amphetamine and trace amines is site specific and depends upon the presence of in the associated monoamine neurons. As of 2010, of TAAR1 and the dopamine transporter (DAT) has been visualized in rhesus monkeys, but of TAAR1 with the norepinephrine transporter (NET) and the serotonin transporter (SERT) has only been evidenced by messenger RNA (mRNA) expression.
In addition to the neuronal monoamine transporters, amphetamine also inhibits vesicular monoamine transporter 2 (VMAT2), SLC22A3, and SLC22A5. SLC22A3 is an extraneuronal monoamine transporter that is present in astrocytes and SLC22A5 is a high-affinity carnitine transporter. Amphetamine also mildly inhibits both the CYP2A6 and CYP2D6 liver enzymes. Amphetamine is known to strongly induce cocaine and amphetamine regulated transcript (CART) gene expression, a neuropeptide involved in feeding behavior, stress, and reward, which induces observable increases in neuronal development and survival "in vitro". The CART receptor has yet to be identified, but there is significant evidence that CART binds to a unique . Amphetamine also inhibits monoamine oxidase at very high doses, resulting in less dopamine and phenethylamine metabolism and consequently higher concentrations of synaptic monoamines.
The full profile of amphetamine's short-term drug effects is derived through increased cellular communication or neurotransmission of dopamine, serotonin, norepinephrine, epinephrine, histamine, CART peptides, acetylcholine, and glutamate, which it effects through interactions with , , and .
Dextroamphetamine is a more potent agonist of than levoamphetamine. Consequently, dextroamphetamine produces greater stimulation than levoamphetamine, roughly three to four times more, but levoamphetamine has slightly stronger cardiovascular and peripheral effects.
Dopamine.
In certain brain regions, amphetamine increases the concentration of dopamine in the synaptic cleft. Amphetamine can enter the presynaptic neuron either through or by diffusing across the neuronal membrane directly. As a consequence of DAT uptake, amphetamine produces competitive reuptake inhibition at the transporter. Upon entering the presynaptic neuron, amphetamine activates which, through protein kinase A (PKA) and protein kinase C (PKC) signaling, causes DAT phosphorylation. Phosphorylation by either protein kinase can result in DAT internalization ( reuptake inhibition), but phosphorylation alone induces reverse transporter function (dopamine efflux). Through direct activation of G protein-coupled inwardly-rectifying potassium channels and indirect activation of D2 autoreceptors (via increased dopamine release), reduces the firing rate of post-synaptic dopamine receptors, preventing a hyper-dopaminergic state. 
Amphetamine is also a substrate for the presynaptic vesicular monoamine transporter, . Following amphetamine uptake at VMAT2, the synaptic vesicle releases dopamine molecules into the cytosol in exchange. Subsequently, the cytosolic dopamine molecules exit the presynaptic neuron via reverse transport at .
Norepinephrine.
Similar to dopamine, amphetamine dose-dependently increases the level of synaptic norepinephrine, the direct precursor of epinephrine. Based upon neuronal expression, amphetamine is thought to affect norepinephrine analogously to dopamine. In other words, amphetamine induces TAAR1-mediated efflux and reuptake inhibition at phosphorylated , competitive NET reuptake inhibition, and norepinephrine release from .
Serotonin.
Amphetamine exerts analogous, yet less pronounced, effects on serotonin as on dopamine and norepinephrine. Amphetamine affects serotonin via and, like norepinephrine, is thought to phosphorylate via .
Acetylcholine.
Amphetamine has no direct effect on acetylcholine, but several studies have noted that acetylcholine release increases after its use. In lab animals, high doses of amphetamine greatly increase acetylcholine levels in certain brain regions, including the hippocampus and caudate nucleus. In humans, a similar phenomenon occurs via the cholinergic–dopaminergic link, mediated by the neuropeptide ghrelin, in the ventral tegmentum. This heightened cholinergic activity leads to increased nicotinic receptor activation in the ; a factor which likely contributes to the nootropic effects of amphetamine.
Other relevant activity.
Extracellular levels of glutamate, the primary excitatory neurotransmitter in the brain, have been shown to increase upon exposure to amphetamine. This cotransmission effect was found in the mesolimbic pathway, an area of the brain implicated in reward, where amphetamine is known to affect dopamine neurotransmission. Amphetamine also induces effluxion of histamine from synaptic vesicles in mast cells and histaminergic neurons through .
Pharmacokinetics.
The oral bioavailability of amphetamine varies with gastrointestinal pH; it is well absorbed from the gut, and bioavailability is typically over 75% for dextroamphetamine. Amphetamine is a weak base with a pKa of ; consequently, when the pH is basic, more of the drug is in its lipid soluble free base form, and more is absorbed through the lipid-rich cell membranes of the gut epithelium. Conversely, an acidic pH means the drug is predominantly in a water soluble cationic (salt) form, and less is absorbed. Approximately of amphetamine circulating in the bloodstream is bound to plasma proteins.
The half-life of amphetamine enantiomers differ and vary with urine pH. At normal urine pH, the half-lives of dextroamphetamine and levoamphetamine are  hours and  hours, respectively. An acidic diet will reduce the enantiomer half-lives to  hours; an alkaline diet will increase the range to  hours. The immediate-release and extended release variants of salts of both isomers reach peak plasma concentrations at 3 hours and 7 hours post-dose respectively. Amphetamine is eliminated via the kidneys, with of the drug being excreted unchanged at normal urinary pH. When the urinary pH is basic, amphetamine is in its free base form, so less is excreted. When urine pH is abnormal, the urinary recovery of amphetamine may range from a low of 1% to a high of 75%, depending mostly upon whether urine is too basic or acidic, respectively. Amphetamine is usually eliminated within two days of the last oral dose. Apparent half-life and duration of effect increase with repeated use and accumulation of the drug.
The prodrug lisdexamfetamine is not as sensitive to pH as amphetamine when being absorbed in the gastrointestinal tract; following absorption into the blood stream, it is converted by red blood cell-associated enzymes to dextroamphetamine via hydrolysis. The elimination half-life of lisdexamfetamine is generally less than one hour.
CYP2D6, dopamine β-hydroxylase, flavin-containing monooxygenase, butyrate-CoA ligase, and glycine N-acyltransferase are the enzymes known to metabolize amphetamine or its metabolites in humans. Amphetamine has a variety of excreted metabolic products, including , , , benzoic acid, hippuric acid, norephedrine, and phenylacetone. Among these metabolites, the active sympathomimetics are , , and norephedrine. The main metabolic pathways involve aromatic para-hydroxylation, aliphatic alpha- and beta-hydroxylation, N-oxidation, N-dealkylation, and deamination. The known pathways and detectable metabolites in humans include the following:
Related endogenous compounds.
<onlyinclude></onlyinclude>
Physical and chemical properties.
Amphetamine is a methyl homolog of the mammalian neurotransmitter phenethylamine with the chemical formula . The carbon atom adjacent to the primary amine is a stereogenic center, and amphetamine is composed of a racemic 1:1 mixture of two enantiomeric mirror images. This racemic mixture can be separated into its optical isomers: levoamphetamine and dextroamphetamine. Physically, at room temperature, the pure free base of amphetamine is a mobile, colorless, and volatile liquid with a characteristically strong amine odor, and acrid, burning taste. Frequently prepared solid salts of amphetamine include amphetamine aspartate, hydrochloride, phosphate, saccharate, and sulfate, the last of which is the most common amphetamine salt. Amphetamine is also the parent compound of its own structural class, which includes a number of psychoactive derivatives. In organic chemistry, amphetamine is an excellent chiral ligand for the stereoselective synthesis of .
Derivatives.
Amphetamine derivatives, often referred to as "amphetamines" or "substituted amphetamines", are a broad range of chemicals that contain amphetamine as a "backbone". The class includes stimulants like methamphetamine, serotonergic empathogens like MDMA (ecstasy), and decongestants like ephedrine, among other subgroups. This class of chemicals is sometimes referred to collectively as the "amphetamine family."
Synthesis.
Since the first preparation was reported in 1887, many synthetic routes to amphetamine have been developed. Many are based on classic organic reactions. One such example is the Friedel–Crafts alkylation of chlorobenzene by allyl chloride to yield beta chloropropylbenzene which is then reacted with ammonia to produce racemic amphetamine (method 1). Another example employs the Ritter reaction (method 2). In this route, allylbenzene is reacted acetonitrile in sulfuric acid to yield an organosulfate which in turn is treated with sodium hydroxide to give amphetamine via an acetamide intermediate. A third route starts with which through a double alkylation with methyl iodide followed by benzyl chloride can be converted into acid. This synthetic intermediate can be transformed into amphetamine using either a Hofmann or Curtius rearrangement (method 3).
A significant number of amphetamine syntheses feature a reduction of a nitro, imine, oxime or other nitrogen-containing functional group. In one such example, a Knoevenagel condensation of benzaldehyde with nitroethane yields . The double bond and nitro group of this intermediate is reduced using either catalytic hydrogenation or by treatment with lithium aluminium hydride (method 4). Another method is the reaction of phenylacetone with ammonia, producing an imine intermediate that is reduced to the primary amine using hydrogen over a palladium catalyst or lithium aluminum hydride (method 5).
The most common route of both legal and illicit amphetamine synthesis employs a non-metal reduction known as the Leuckart reaction (method 6). In the first step, a reaction between phenylacetone and formamide, either using additional formic acid or formamide itself as a reducing agent, yields . This intermediate is then hydrolyzed using hydrochloric acid, and subsequently basified, extracted with organic solvent, concentrated, and distilled to yield the free base. The free base is then dissolved in an organic solvent, sulfuric acid added, and amphetamine precipitates out as the sulfate salt.
A number of chiral resolutions have been developed to separate the two enantiomers of amphetamine. For example, racemic amphetamine can be treated with to form a diastereoisomeric salt which is fractionally crystallized to yield dextroamphetamine. Chiral resolution remains the most economical method for obtaining optically pure amphetamine on a large scale. In addition, several enantioselective syntheses of amphetamine have been developed. In one example, optically pure is condensed with phenylacetone to yield a chiral schiff base. In the key step, this intermediate is reduced by catalytic hydrogenation with a transfer of chirality to the carbon atom alpha to the amino group. Cleavage of the benzylic amine bond by hydrogenation yields optically pure dextroamphetamine.
Detection in body fluids.
Amphetamine is frequently measured in urine or blood as part of a drug test for sports, employment, poisoning diagnostics, and forensics. Techniques such as immunoassay, which is the most common form of amphetamine test, may cross-react with a number of sympathomimetic drugs. Chromatographic methods specific for amphetamine are employed to prevent false positive results. Chiral separation techniques may be employed to help distinguish the source of the drug, whether prescription amphetamine, prescription amphetamine prodrugs, (e.g., selegiline), over-the-counter drug products (e.g., Vicks Vapoinhaler) or illicitly obtained substituted amphetamines. Several prescription drugs produce amphetamine as a metabolite, including benzphetamine, clobenzorex, famprofazone, fenproporex, lisdexamfetamine, mesocarb, methamphetamine, prenylamine, and selegiline, among others. These compounds may produce positive results for amphetamine on drug tests. Amphetamine is generally only detectable by a standard drug test for approximately 24 hours, although a high dose may be detectable for two to four days.
For the assays, a study noted that an enzyme multiplied immunoassay technique (EMIT) assay for amphetamine and methamphetamine may produce more false positives than liquid chromatography–tandem mass spectrometry. Gas chromatography–mass spectrometry (GC–MS) of amphetamine and methamphetamine with the derivatizing agent chloride allows for the detection of methamphetamine in urine. GC–MS of amphetamine and methamphetamine with the chiral derivatizing agent Mosher's acid chloride allows for the detection of both dextroamphetamine and dextromethamphetamine in urine. Hence, the latter method may be used on samples that test positive using other methods to help distinguish between the various sources of the drug.
History, society, and culture.
Amphetamine was first synthesized in 1887 in Germany by Romanian chemist Lazăr Edeleanu who named it "phenylisopropylamine"; its stimulant effects remained unknown until 1927, when it was independently resynthesized by Gordon Alles and reported to have sympathomimetic properties. Amphetamine had no pharmacological use until 1934, when Smith, Kline and French began selling it as an inhaler under the trade name Benzedrine as a decongestant. During World War II, amphetamines and methamphetamine were used extensively by both the Allied and Axis forces for their stimulant and performance-enhancing effects. As the addictive properties of the drug became known, governments began to place strict controls on the sale of amphetamine. For example, during the early 1970s in the United States, amphetamine became a schedule II controlled substance under the Controlled Substances Act. In spite of strict government controls, amphetamine has been used legally or illicitly by people from a variety of backgrounds, including authors, musicians, mathematicians, and athletes. Amphetamine is still illegally synthesized today in clandestine labs and sold on the black market, primarily in European countries. Outside Europe, the illicit market for amphetamine is much smaller than the market for methamphetamine.
Legal status.
As a result of the United Nations Convention on Psychotropic Substances, amphetamine became a schedule II controlled substance, as defined in the treaty, in all (183) state parties. Consequently, it is heavily regulated in most countries. Some countries, such as South Korea and Japan, have banned substituted amphetamines even for medical use. In other nations, such as Canada (schedule I drug), the United States (schedule II drug), Thailand (category 1 narcotic), and United Kingdom (class B drug), amphetamine is in a restrictive national drug schedule that allows for its use as a medical treatment.
Pharmaceutical products.
The only currently prescribed amphetamine formulation that contains both enantiomers is Adderall. Amphetamine is also prescribed in enantiopure and prodrug form as dextroamphetamine and lisdexamfetamine respectively. Lisdexamfetamine is structurally different from amphetamine, and is inactive until it metabolizes into dextroamphetamine. The free base of racemic amphetamine was previously available as Benzedrine, Psychedrine, and Sympatedrine. Levoamphetamine was previously available as Cydril. All current amphetamine pharmaceuticals are salts due to the comparatively high volatility of the free base. Some of the current brands and their generic equivalents are listed below.

</doc>
<doc id="2506" url="http://en.wikipedia.org/wiki?curid=2506" title="Asynchronous communication">
Asynchronous communication

In telecommunications, asynchronous communication is transmission of data, generally without the use of an external clock signal, where data can be transmitted intermittently rather than in a steady stream. Any timing required to recover data from the communication symbols is encoded within the symbols. A notable exception is the RS-232 port, and some derivatives, which are asynchronous, but still have an external clock signal available, although not commonly used. The most significant aspect of asynchronous communications is that data is not transmitted at regular intervals, thus making possible variable bit rate, and that the transmitter and receiver clock generators do not have to be exactly synchronized all the time.
Physical layer.
In asynchronous serial communication the physical protocol layer, the data blocks are code words of a certain word length, for example octets (bytes) or ASCII characters, delimited by start bits and stop bits. A variable length space can be inserted between the code words. No bit synchronization signal is required. This is sometimes called character oriented communication. Examples are the RS-232C serial standard, and MNP2 and V.2 modems and older.
Data link layer and higher.
Asynchronous communication at the data link layer or higher protocol layers is known as statistical multiplexing or packet mode communication, for example asynchronous transfer mode (ATM). In this case the asynchronously transferred blocks are called data packets, for example ATM cells. The opposite is circuit switched communication, which provides constant bit rate, for example ISDN and SONET/SDH.
The packets may be encapsulated in a data frame, with a frame synchronization bit sequence indicating the start of the frame, and sometimes also a bit synchronization bit sequence, typically 01010101, for identification of the bit transition times. Note that at the physical layer, this is considered as synchronous serial communication. Examples of packet mode data link protocols that can be/are transferred using synchronous serial communication are the HDLC, Ethernet, PPP and USB protocols.
Application layer.
An asynchronous communication service or application does not require a constant bit rate. Examples are file transfer, email and the World Wide Web. An example of the opposite, a synchronous communication service, is realtime streaming media, for example IP telephony, IP-TV and video conferencing.
Electronically mediated communication.
Electronically mediated communication is often asynchronous in that the participants do not communicate concurrently. Examples include email and bulletin board systems, where participants send or post messages at different times. The term acquired currency in online learning, where exchanges between teachers and students are often asynchronous instead of synchronous (that is, simultaneous), as they would be in face-to-face or telephone conversations.

</doc>
<doc id="2508" url="http://en.wikipedia.org/wiki?curid=2508" title="Artillery">
Artillery

Artillery is a class of large military weapons built to fire munitions far beyond the range of infantry's small arms. Early artillery development focused on the ability to breach fortifications, and led to heavy, fairly immobile siege engines. As technology improved, lighter, more mobile field artillery developed for battlefield use. This development continues today; modern self-propelled artillery vehicles are highly mobile weapons of great versatility providing the largest share of an army's total firepower.
In its earliest sense, the word artillery referred to any group of soldiers primarily armed with some form of manufactured weapon or armour. Since the introduction of gunpowder and cannon, the word "artillery" has largely meant cannon and in contemporary usage it usually refers to shell-firing guns, howitzers, mortars and rockets. In common speech, the word artillery is often used to refer to individual devices along with their accessories and fittings, although these assemblages are more properly called "equipments". By association, artillery may also refer to the arm of service that customarily operates such engines.
Artillery is arguably the most lethal form of land-based armament currently employed and has been since at least the early industrial revolution. The vast majority of combat deaths in the Napoleonic Wars, World War I and World War II were caused by artillery. In 1944, Joseph Stalin said in a speech that artillery was "the God of War".
Artillery piece.
Although not called as such, machines performing the role recognizable as artillery have been employed in warfare since antiquity. The first references in the western historical tradition begin at Syracuse in 399 BC and these devices were widely employed by the Roman Legions in Republican times well before the Christian era. Until the introduction of gunpowder into western warfare artillery depended upon mechanical energy to operate and this severely limited the kinetic energy of the projectiles while also requiring the construction of very large apparatus to store sufficient energy. For comparison, a Roman 1st century BC catapult using stones of 6.55 kg fired with a kinetic energy of 16,000 joules, while a mid-19th century 12 pound cannon firing projectiles of 4.1 kg fired the projectile with a kinetic energy of 240,000 joules.
For much of artillery's history during the Middle Ages and the Early modern period, artillery pieces on land were moved with the assistance of horse teams. During the more recent Modern era and in the Post-Modern period the artillery crew has used wheeled or tracked vehicles as a mode of transportation. Artillery used by naval forces has changed significantly also, with missiles replacing guns in surface warfare.
Over the course of military history, projectiles were manufactured from a wide variety of materials, made in a wide variety of shapes, and used different means of inflicting physical damage and casualties to defeat specific types of targets. The engineering designs of the means of delivery have likewise changed significantly over time, and have become some of the most complex technological application today.
In some armies, the weapon of artillery is the projectile, not the piece that fires it. The process of delivering fire onto the target is called gunnery. The actions involved in operating the piece are collectively called "serving the gun" or "detachment" by the gun crew, constituting either direct or indirect artillery fire. The manner in which artillery units or formations are employed is called artillery support, and may at different periods in history refer to weapons designed to be fired from ground, sea, and even air-based weapons platforms.
Crew.
Although the term also describes soldiers and sailors with the primary function of using artillery weapons, the individuals who operate them are called gunners whatever their rank, however 'gunner' is the lowest rank in artillery arms. There is no generally recognised generic term for a gun, howitzer, mortar, and so forth: some armies use 'artillery piece', while others use 'gun'. The projectiles fired are typically either 'shot' (if solid) or 'shell' if not. Shell is a widely used generic term for a projectile, which is a component of munitions.
The term 'artillery' is also applied to a combat arm of most military services when used organizationally to describe units and formations of the national armed forces that operate the weapons.
The gunners and their guns are usually grouped in teams called either 'crews' or 'detachments'. Several such crews and teams with other functions are combined into a unit of artillery usually called a battery, although sometimes called a company. Batteries are roughly equivalent to a company in the infantry, and are combined into larger military organizations for administrative and operational purpose.
During military operations the role of field artillery is to provide close support to other arms in combat or to attack targets. The latter role is typically achieved by delivering either high explosive munitions to inflict casualties on the enemy from casing fragments and other debris and blast, or by demolition of enemy positions, equipment and vehicles. Fire may be directed by an artillery observer or called onto map coordinates.
Military doctrine has played a significant influence on the core engineering design considerations of artillery ordnance through its history, in seeking to achieve a balance between delivered volume of fire with ordnance mobility. However, during the modern period the consideration of protecting the gunners also arose due to the late-19th century introduction of the new generation of infantry weapons using conoidal bullet, better known as the Minié ball, with a range almost as long as that of field artillery.
The gunners' increasing proximity to and participation in direct combat against other combat arms and attacks by aircraft made the introduction of a gun shield necessary. The problems of how to employ a fixed or horse towed gun in mobile warfare necessitated the development of new methods of transporting the artillery into combat. Two distinct forms of artillery developed: the towed gun, which was used primarily to attack or defend a fixed line; and the self-propelled gun, which was designed to accompany a mobile force and provide continuous fire support. These influences have guided the development of artillery ordnance, systems, organisations, and operations until the present, with artillery systems capable of providing support at ranges from as little as 100 m to the intercontinental ranges of ballistic missiles. The only combat in which artillery is unable to take part in is close quarters combat.
Etymology.
The word as used in the current context originated in the Middle Ages. One suggestion is that it comes from the Old French "atellier" meaning "to arrange", and "attillement" meaning "equipment".
From the 13th century an "artillier" referred to a builder of any war equipment, and for the next 250 years the sense of the word "artillery" covered all forms of military weapons. Hence the naming of the Honourable Artillery Company an essentially infantry unit until the 19th century. Another suggestion is that comes from the Italian "arte de tirare" (art of shooting) coined by one of the first theorists on the use of artillery, Niccolo Tartaglia.
History.
Mechanical systems used for throwing ammunition in ancient warfare, also known as "engines of war", like the catapult, onager, trebuchet, and ballista, are also referred to by military historians as artillery.
Invention of gunpowder.
The first documented record of artillery with gunpowder propellant used on the battlefield was on January 28, 1132, when General Han Shizhong of the Song Dynasty used escalade and Huochong to capture a city in Fujian. Early Chinese artillery had vase-like shapes. This includes the "long range awe inspiring" cannon dated from 1350 and found in the 14th century Ming Dynasty treatise "Huolongjing". With the development of better metallurgy techniques, later cannons abandoned the vase shape of early Chinese artillery. This change can be seen in the bronze "thousand ball thunder cannon," an early example of field artillery. These small, crude weapons diffused into the Middle East (the "madfaa", see also the German Wikipedia and "midfa") and reached Europe in the 13th century, in a very limited manner.
In Asia, Mongols adopted the Chinese artillery and used it effectively in the great conquest. By late 14th century, Chinese rebels used organized artillery and cavalry to push Mongols out.
As small smooth-bore tubes these were initially cast in iron or bronze around a core, with the first drilled bore ordnance recorded in operation near Seville in 1247. They fired lead, iron, or stone balls, sometimes large arrows and on occasions simply handfuls of whatever scrap came to hand. During the Hundred Years' War, these weapons became more common, initially as the bombard and later the cannon. Cannon were always muzzle-loaders. While there were many early attempts at breech-loading designs, a lack of engineering knowledge rendered these even more dangerous to use than muzzle-loaders.
Expansion of artillery use.
In 1415, the Portuguese invaded the Mediterranean port town of Ceuta. While it is difficult to confirm the use of firearms in the siege of the city, it is known the Portuguese defended it thereafter with firearms, namely "bombardas", "colebratas", and "falconetes". In 1419, Sultan Abu Sa'id led an army to reconquer the fallen city, and Moroccans brought cannons and used them in the assault on Ceuta. Finally, hand-held firearms and riflemen appear in Morocco, in 1437, in an expedition against the people of Tangiers. It is clear these weapons had developed into several different forms, from small guns to large artillery pieces.
The artillery revolution in Europe caught on during the Hundred Years' War and changed the way that battles were fought. In the preceding decades, the English had even used a gunpowder-like weapon in military campaigns against the Scottish. However, at this time, the cannons used in battle were very small and not particularly powerful. Cannons were only useful for the defense of a castle, as demonstrated at Breteuil in 1356, when the besieged English used a cannon to destroy an attacking French assault tower. By the end of the 14th century, cannon were only powerful enough to knock in roofs, and could not penetrate castle walls.
However, a major change occurred between 1420 and 1430, when artillery became much more powerful and could now batter strongholds and fortresses quite efficiently. The English, French, and Burgundians all advanced in military technology, and as a result the traditional advantage that went to the defense in a siege was lost. The cannon during this period were elongated, and the recipe for gunpowder was improved to make it three times as powerful as before. These changes led to the increased power in the artillery weapons of the time.
Joan of Arc encountered gunpowder weaponry several times. When she led the French against the English at the Battle of Tourelles, in 1430, she faced heavy gunpowder fortifications, and yet her troops prevailed in that battle. In addition, she led assaults against the English-held towns of Jargeau, Meung, and Beaugency, all with the support of large artillery units. When she led the assault on Paris, Joan faced stiff artillery fire, especially from the suburb of St. Denis, which ultimately led to her defeat in this battle. In April 1430, she went to battle against the Burgundians, whose support was purchased by the English.
At this time, the Burgundians had the strongest and largest gunpowder arsenal among the European powers, and yet the French, under Joan of Arc's leadership, were able to beat back the Burgundians and defend themselves. As a result, most of the battles of the Hundred Years' War that Joan of Arc participated in were fought with gunpowder artillery.
The army of Mehmet the Conqueror, which conquered Constantinople in 1453, included both artillery and foot soldiers armed with gunpowder weapons. The Ottomans brought to the siege sixty-nine guns in fifteen separate batteries and trained them at the walls of the city. The barrage of Ottoman cannon fire lasted forty days, and they are estimated to have fired 19,320 times.
Artillery also played a decisive role in the Battle of St. Jakob an der Birs of 1444.
Smoothbores.
Bombards were of value mainly in sieges. A famous Turkish example used at the siege of Constantinople in 1453 weighed 19 tons, took 200 men and sixty oxen to emplace, and could fire just seven times a day. The Fall of Constantinople was perhaps ""the first event of supreme importance whose result was determined by the use of artillery"" when the huge bronze cannons of Mehmed II breached the city's walls, ending the Byzantine Empire, according to Sir Charles Oman.
Bombards developed in Europe were massive smoothbore weapons distinguished by their lack of a field carriage, immobility once emplaced, highly individual design, and noted unreliability (in 1460 James II, King of Scots, was killed when one exploded at the siege of Roxburgh). Their large size precluded the barrels being cast and they were constructed out of metal staves or rods bound together with hoops like a barrel, giving their name to the gun barrel.
The use of the word "cannon" marks the introduction in the 15th century of a dedicated field carriage with axle, trail and animal-drawn limber—this produced mobile field pieces that could move and support an army in action, rather than being found only in siege and static defences. The reduction in the size of the barrel was due to improvements in both iron technology and gunpowder manufacture, while the development of trunnions – projections at the side of the cannon as an integral part of the cast – allowed the barrel to be fixed to a more movable base, and also made raising or lowering the barrel much easier.
The first land-based mobile weapon is usually credited to Jan Žižka, who deployed his oxen-hauled cannon during the Hussite Wars of Bohemia (1418–1424). However cannons were still large and cumbersome. With the rise of musketry in the 16th century, cannon were largely (though not entirely) displaced from the battlefield—the cannon were too slow and cumbersome to be used and too easily lost to a rapid enemy advance.
The combining of shot and powder into a single unit, a cartridge, occurred in the 1620s with a simple fabric bag, and was quickly adopted by all nations. It speeded loading and made it safer, but unexpelled bag fragments were an additional fouling in the gun barrel and a new tool—a worm—was introduced to remove them. Gustavus Adolphus is identified as the general who made cannon an effective force on the battlefield—pushing the development of much lighter and smaller weapons and deploying them in far greater numbers than previously. The outcome of battles was still determined by the clash of infantry.
Shells, explosive-filled fused projectiles, were also developed in the 17th century. The development of specialized pieces—shipboard artillery, howitzers and mortars—was also begun in this period. More esoteric designs, like the multi-barrel "ribauldequin" (known as "organ guns"), were also produced.
The 1650 book by Kazimierz Siemienowicz ""Artis Magnae Artilleriae pars prima"" was one of the most important contemporary publications on the subject of artillery. For over two centuries this work was used in Europe as a basic artillery manual.
One of the most significant effects of artillery during this period was however somewhat more indirect – by easily reducing to rubble any medieval-type fortification or city wall (some which had stood since Roman times), it abolished millennia of siege-warfare strategies and styles of fortification building. This led, among other things, to a frenzy of new bastion-style fortifications to be built all over Europe and in its colonies, but also had a strong integrating effect on emerging nation-states, as kings were able to use their newfound artillery superiority to force any local dukes or lords to submit to their will, setting the stage for the absolutist kingdoms to come.
Modern Rocket artillery can trace its heritage back to the Mysorean rockets of India. Their first recorded use was in 1780 during the battles of the Second, Third and Fourth Mysore Wars. The wars fought between the British East India Company and the Kingdom of Mysore in India made use of the rockets as a weapon. In the Battle of Pollilur (1780), the Siege of Seringapatam (1792) and in Battle of Seringapatam in 1799 these rockets were used with considerable effect against the British."After the wars, several Mysore rockets were sent to England, and from 1801, William Congreve copied the rockets with minor modifications as the Congreve rocket which were used effectively during the Napoleonic Wars and the War of 1812.
Napoleonic artillery.
Cannons continued to become smaller and lighter—Frederick II of Prussia deployed the first genuine light artillery during the Seven Years' War.
Jean-Baptiste de Gribeauval, a French artillery engineer, introduced the standardization of cannon design in the mid-18th century. He developed a field howitzer whose gun barrel, carriage assembly and ammunition specifications were made uniform for all French cannons. The standardized interchangeable parts of these cannons down to the nuts, bolts and screws made their mass production and repair much easier. Another major change at this time was the development of a flintlock firing mechanism for the cannons to replace the old method of igniting powder in the cannon touchhole. The flintlock was a far more reliable (and safe) mechanism.
These improvements in the French artillery were essential for the later military successes of Napoleon. Napoleon, himself a former artillery officer, perfected the tactic of massed artillery batteries unleashed upon a critical point in his enemies' line as a prelude to a decisive infantry and cavalry assault.
Modern artillery.
The development of modern artillery occurred in the mid to late 19th century as a result of the convergence of various improvements in the underlying technology. Advances in metallurgy allowed for the construction of breech-loading rifled guns that could fire at a much greater muzzle velocity.
After the British artillery was shown up in the Crimean War as having barely changed since the Napoleonic Wars the industrialist William Armstrong was awarded a contract by the government to design a new piece of artillery. Production started in 1855 at the Elswick Ordnance Company and the Royal Arsenal at Woolwich, and the outcome was the revolutionary Armstrong Gun, which marked the birth of modern artillery. Three of its features particularly stand out.
First, the piece was rifled, which allowed for a much more accurate and powerful action. Although rifling had been tried on small arms since the 15th century, the necessary machinery to accurately rifle artillery was only available by the mid-19th century. Martin von Wahrendorff, and Joseph Whitworth independently produced rifled cannon in the 1840s, but it was Armstrong's gun that was first to see widespread use during the Crimean War. The cast iron shell of the Armstrong gun was similar in shape to a Minié ball and had a thin lead coating which made it fractionally larger than the gun's bore and which engaged with the gun's rifling grooves to impart spin to the shell. This spin, together with the elimination of windage as a result of the tight fit, enabled the gun to achieve greater range and accuracy than existing smooth-bore muzzle-loaders with a smaller powder charge.
His gun was also a breech-loader. Although attempts at breech-loading mechanisms had been made since medieval times, the essential engineering problem was that the mechanism couldn't withstand the explosive charge. It was only with the advances in metallurgy and precision engineering capabilities during the Industrial Revolution that Armstrong was able to construct a viable solution. The gun combined all the properties that make up an effective artillery piece. The gun was mounted on a carriage in such a way as to return the gun to firing position after the recoil.
What made the gun really revolutionary lay in the technique of the construction of the gun barrel that allowed it to withstand much more powerful explosive forces. The "built-up" method involved assembling the barrel with wrought-iron (later mild steel was used) tubes of successively smaller diameter. The tube would then be heated to allow it to expand and fit over the previous tube. When it cooled the gun would contract although not back to its original size, which allowed an even pressure along the walls of the gun which was directed inward against the outward forces that the gun firing exerted on the barrel.
Another innovative feature, more usually associated with 20th-century guns, was what Armstrong called its "grip", which was essentially a squeeze bore; the 6 inches of the bore at the muzzle end was of slightly smaller diameter, which centered the shell before it left the barrel and at the same time slightly swaged down its lead coating, reducing its diameter and slightly improving its ballistic qualities.
Armstrong's system was adopted in 1858, initially for "special service in the field" and initially he only produced smaller artillery pieces, 6-pounder (2.5 in/64 mm) mountain or light field guns, 9-pounder (3 in/76 mm) guns for horse artillery, and 12-pounder (3 inches /76 mm) field guns.
The first cannon to contain all 'modern' features is generally considered to be the French 75 of 1897. It was the first field gun to include a hydro-pneumatic recoil mechanism, which kept the gun's trail and wheels perfectly still during the firing sequence. Since it did not need to be re-aimed after each shot, the crew could fire as soon as the barrel returned to its resting position. In typical use, the French 75 could deliver fifteen rounds per minute on its target, either shrapnel or melinite high-explosive, up to about 5 miles (8,500 m) away. Its firing rate could even reach close to 30 rounds per minute, albeit only for a very short time and with a highly experienced crew. These were rates that contemporary bolt action rifles could not match. The gun used cased ammunition, was breech-loading, had modern sights, a self-contained firing mechanism and hydro-pneumatic recoil dampening.
Indirect Fire.
See article Indirect fire. The firing of a projectile without relying on direct line of sight between the gun and the target, possibly dates back to the 16th century. Early battlefield use of indirect fire may have occurred at Paltzig in July 1759 when the Russian artillery fired over the tops of trees, and at the Battle of Waterloo where a battery of the Royal Horse Artillery fired an indirect Shrapnel barrage against advancing French troops.
In 1882 a Russian officer, Lieutenant Colonel KG Guk, published "Indirect Fire for Field Artillery" that provided a practical method of using aiming points for indirect fire by describing, "all the essentials of aiming points, crest clearance, and corrections to fire by an observer".
A few years later the Richtfläche (lining-plane) sight was invented in Germany and provided a means of indirect laying in azimuth, complementing the clinometers for indirect laying in elevation which already existed. Despite conservative opposition within the German army, indirect fire was adopted as doctrine by the 1890s. In the early 1900s Goertz in Germany developed an optical sight for azimuth laying. It quickly replaced the lining-plane; in English it became the 'Dial Sight' (UK) or 'Panoramic Telescope' (US).
The British halfheartedly experimented with indirect fire techniques since the 1890s, but with the onset of the Boer War they were the first to apply the theory in practice in 1899, although they had to improvise without a lining-plane sight.
In the next 15 years leading up to World War I, the techniques of indirect fire became available for all types of artillery. Indirect fire was the defining characteristic of 20th century artillery and led to undreamt of changes in the amount of artillery, its tactics, organisation and techniques most of which occurred during World War I. 
An implication of indirect fire and improving guns was increasing range between gun and target, this increased the time of flight and the vertex of the trajectory. The result was decreasing accuracy (the increasing distance between the target and the mean point of impact of the shells aimed at it) caused by the increasing effects of non-standard conditions. Indirect firing data was based on standard conditions including a specific muzzle velocity, zero wind, air temperature and density, and propellant temperature. In practice this standard combination of conditions almost never existed, they varied throughout the day and day to day, and the greater the time of flight the greater the inaccuracy. An added complication was the need for survey to accurately fix the coordinates of the gun position and provide accurate orientation for the guns. Of course targets had to be accurately located but by 1916 air photo interpretation techniques enabled this and ground survey techniques could sometimes be used.
In 1914 the methods of correcting firing data for the actual conditions were often convoluted, and the availability of data about actual conditions was rudimentary or non-existent, the assumption was that fire would always be ranged (adjusted). British heavy artillery worked energetically to progressively solve all these problems from late 1914 onwards and by early 1918 had effective processes in place for both field and heavy artillery. These processes enabled 'map-shooting', later called 'predicted fire', it meant that effective fire could be delivered against an accurately located target without ranging. Nevertheless the mean point of impact was still some tens of yards from the target-centre aiming point. It was not precision fire but it was good enough for concentrations and barrages. These processes remain in use into the 21st Century with refinements to calculations enabled by computers and improved data capture about non-standard conditions.
The British major-general Henry Hugh Tudor pioneered armour and artillery cooperation at the breakthrough Battle of Cambrai. The improvements in accommodating non-standard conditions allowed effective predicted. Major General J. B. A. Bailey, British Army (retired) wrote.
An estimated 75,000 French soldiers were casualties of friendly artillery in the four years of World War I.
Precision artillery.
Modern artillery is most obviously distinguished by its long range, firing an explosive shell or rocket and a mobile carriage for firing and transport. However, its most important characteristic is the use of indirect fire, whereby the firing equipment is aimed without seeing the target through its sights. Indirect fire emerged at the beginning of the 20th century and was greatly enhanced by the development of predicted fire methods in World War I. However, indirect fire was area fire; it was and is not suitable for destroying point targets; its primary purpose is area suppression. Nevertheless by the late 1970s precision munitions started to appear, notably the US 155mm Copperhead and its Soviet 152mm equivalent that had success in Indian service. These relied on laser designation to 'illuminate' the target that the shell homed onto. However, in the early 21st Century the Global Positioning System (GPS) enabled relatively cheap and accurate guidance for shells and missiles, notably the US 155mm Excalibur and the 227mm GMLRS rocket. The introduction of these led to a new issue, the need for very accurate three dimensional target coordinates - the mensuration process.
Weapons covered by the term 'modern artillery' include "cannon" artillery (such as howitzer, mortar, and field gun) and rocket artillery. Certain smaller-caliber mortars are more properly designated small arms rather than artillery, albeit indirect-fire small arms. This term also came to include coastal artillery which traditionally defended coastal areas against seaborne attack and controlled the passage of ships. With the advent of powered flight at the start of the 20th century, artillery also included ground-based anti-aircraft batteries.
The term "artillery" has traditionally not been used for projectiles with internal guidance systems, preferring the term "missilery", though some modern artillery units employ surface-to-surface missiles. Advances in terminal guidance systems for small munitions has allowed large-caliber guided projectiles to be developed, blurring this distinction.
Ammunition.
One of the most important role of logistics is the supply of munitions as a primary type of artillery consumable, their storage and the provision of fuses, detonators and warheads at the point where artillery troops will assemble the charge, projectile, bomb or shell.
A round of artillery ammunition comprises four components:
Fuzes.
The normal artillery spelling is "fuze". Fuzes are the devices that trigger explosion of the artillery ammunition charge. Broadly there are four main types:
Most artillery fuzes are nose fuzes. However, base fuzes have been used with armour piercing shells and for squash head (HESH or HEP) anti-tank shells. At least one nuclear shell and its non-nuclear spotting version also used a multi-deck mechanical time fuze fitted into its base.
Impact fuzes were, and in some armies remain, the standard fuze for high explosive (HE) rounds. Their default action is normally 'superquick', some have had a 'graze' action which allows them to penetrate light cover and others have 'delay'. Delay fuzes allow the shell to penetrate the ground before exploding. Armor- or concrete-piercing fuzes are specially hardened. During World War I and later, ricochet fire with delay or graze fuzed HE shells, fired with a flat angle of descent, was used to achieve airburst.
HE shells can be fitted with other fuzes. Airburst fuzes usually have a combined airburst and impact function. However, until the introduction of proximity fuzes, the airburst function was mostly used with cargo munitions—for example shrapnel, illumination, and smoke. The larger calibers of anti-aircraft artillery are almost always used airburst. Airburst fuzes have to have the fuze length (running time) set on them. This is done just before firing using either a wrench or a fuze setter pre-set to the required fuze length.
Early airburst fuzes used igniferous timers which lasted into the second half of the 20th century. Mechanical time fuzes appeared in the early part of the century. These required a means of powering them. The Thiel mechanism used a spring and escapement (i.e. 'clockwork'), Junghans used centrifugal force and gears, and Dixi used centrifugal force and balls. From about 1980, electronic time fuzes started replacing mechanical ones for use with cargo munitions.
Proximity fuzes have been of two types: photo-electric or radar. The former was not very successful and seems only to have been used with British anti-aircraft artillery 'unrotated projectiles' (rockets) in World War II. Radar proximity fuzes were a big improvement over the mechanical (time) fuzes which they replaced. Mechanical time fuzes required an accurate calculation of their running time, which was affected by non-standard conditions. With HE (requiring a burst 20 to above the ground), if this was very slightly wrong the rounds would either hit the ground or burst too high. Accurate running time was less important with cargo munitions that burst much higher.
The first radar proximity fuzes (codenamed 'VT') were initially used against aircraft in World War II. Their ground use was delayed for fear of the enemy recovering 'blinds' (artillery shells which failed to detonate) and copying the fuze. The first proximity fuzes were designed to detonate about above the ground. These air-bursts are much more lethal against personnel than ground bursts because they deliver a greater proportion of useful fragments and deliver them into terrain where a prone soldier would be protected from ground bursts.
However, proximity fuzes can suffer premature detonation because of the moisture in heavy rain clouds. This led to 'controlled variable time' (CVT) after World War II. These fuzes have a mechanical timer that switched on the radar about 5 seconds before expected impact, they also detonated on impact.
The proximity fuze emerged on the battlefields of Europe in late December 1944. They have become known as the U.S. Artillery's "Christmas present", and were much appreciated when they arrived during the Battle of the Bulge. They were also used to great effect in anti-aircraft projectiles in the Pacific against "kamikaze" as well as in Britain against V-1 flying bombs.
Electronic multi-function fuzes started to appear around 1980. Using solid-state electronics they were relatively cheap and reliable, and became the standard fitted fuze in operational ammunition stocks in some western armies. The early versions were often limited to proximity airburst, albeit with height of burst options, and impact. Some offered a go/no-go functional test through the fuze setter.
Later versions introduced induction fuze setting and testing instead of physically placing a fuze setter on the fuze. The latest, such as Junghan's DM84U provide options giving, superquick, delay, a choice of proximity heights of burst, time and a choice of foliage penetration depths.
A new type of artillery fuze will appear soon. In addition to other functions these offer some course correction capability, not full precision but sufficient to significantly reduce the dispersion of the shells on the ground.
Projectiles.
The projectile is the munition or "bullet" fired downrange. This may or may not be an explosive device. Traditionally, projectiles have been classified as "shot" or "shell", the former being solid and the latter having some form of "payload".
Shells can also be divided into three configurations: bursting, base ejection or nose ejection. The latter is sometimes called the shrapnel configuration. The most modern is base ejection, which was introduced in World War I. Both base and nose ejection are almost always used with airburst fuzes. Bursting shells use various types of fuze depending on the nature of the payload and the tactical need at the time.
Payloads have included:
Propellant.
Most forms of artillery require a propellant to propel the projectile at the target. Propellant is always a low explosive, this means it deflagrates instead of detonating, as with high explosives. The shell is accelerated to a high velocity in a very short time by the rapid generation of gas from the burning propellant. This high pressure is achieved by burning the propellant in a contained area, either the chamber of a gun barrel or the combustion chamber of a rocket motor.
Until the late 19th century the only available propellant was black powder. Black powder had many disadvantages as a propellant; it has relatively low power, requiring large amounts of powder to fire projectiles, and created thick clouds of white smoke that would obscure the targets, betray the positions of guns and make aiming impossible. In 1846 nitrocellulose (also known as guncotton) was discovered, and the high explosive nitroglycerin was discovered at much the same time. Nitrocellulose was significantly more powerful than black powder, and was smokeless. Early guncotton was unstable however, and burned very fast and hot, leading to greatly increased barrel wear. Widespread introduction of smokeless powder would wait until the advent of the double-base powders, which combine nitrocellulose and nitroglycerin to produce powerful, smokeless, stable propellant.
Many other formulations were developed in the following decades, generally trying to find the optimum characteristics of a good artillery propellant; low temperature, high energy, non corrosive, highly stable, cheap, and easy to manufacture in large quantities. Broadly, modern gun propellants are divided into three classes: single-base propellants which are mainly or entirely nitrocellulose based, double-base propellants composed of a combination of nitrocellulose and nitroglycerin, and triple base composed of a combination of nitrocellulose and nitroglycerin and Nitroguanidine.
Artillery shells fired from a barrel can be assisted to greater range in three ways:
Propelling charges for tube artillery can be provided in one of two ways: either as cartridge bags or in metal cartridge cases. Generally anti-aircraft artillery and smaller caliber (up to 3" or 76.2 mm) guns use metal cartridge cases that include the round and propellant, similar to a modern rifle cartridge. This simplifies loading and is necessary for very high rates of fire. Bagged propellant allows the amount of powder to be raised or lowered depending on the range to the target. it also makes handling of larger shells easier. Each requires a totally different type of breech to the other. A metal case holds an integral primer to initiate the propellant and provides the gas seal to prevent the gases leaking out of the breech, this is called obturation. With bagged charges the breech itself provides obturation and holds the primer. In either case the primer is usually percussion but electrical is also used and laser ignition is emerging. Modern 155 mm guns have a primer magazine fitted to their breech.
Artillery ammunition has four classifications according to use:
Field artillery system.
Because field artillery mostly uses indirect fire the guns have to be part of a system that enables them to attack targets invisible to them in accordance with the combined arms plan.
The main functions in the field artillery system are:
Organisationally and spatially these functions can be arranged in many ways. Since the creation of modern indirect fire different armies have done it differently at different times and in different places. Technology is often a factor but so are military-social issues, the relationships between artillery and other arms, and the criteria by which military capability, efficiency and effectiveness are judged. Cost is also an issue because artillery is expensive due to the large quantities of ammunition that it uses and its level of manpower.
Communications underpin the artillery system, they have to be reliable and in real-time to link the various elements. During the 20th century communications used flags, morse code by radio, line and lights, voice and teleprinter by line. Radio has included HF, VHF, satellite and radio relay as well as modern tactical trunk systems. In western armies at least radio communications are now usually encrypted.
The emergence of mobile and man-portable radios after World War I had a major impact on artillery because it enable fast and mobile operations with observers accompanying the infantry or armoured troops. In World War II some armies fitted their self-propelled guns with radios. However, sometimes in the first half of the 20th century hardcopy artillery fire plans and map traces were distributed.
Data communications can be especially important for artillery because by using structured messages and defined data types fire control messages can be automatically routed and processed by computers. For example a target acquisition element can send a message with target details which is automatically routed through the tactical and technical fire control elements to deliver firing data to the gun's laying system and the gun automatically laid. As tactical data networks become pervasive they will provide any connected soldier with a means for reporting target information and requesting artillery fire.
Command is the authority to allocate resources, typically by assigning artillery formations or units. Terminology and its implications vary widely. However, very broadly, artillery units are assigned in direct support or in general support. Typically, the former mostly provide close support to manoeuvre units while the latter may provide close support and or depth fire, notably counter-battery. Generally, 'direct support' also means that the artillery unit provides artillery observation and liaison teams to the supported units. Sometimes direct support units are placed under command of the regiment/brigade they support. General support units may be grouped into artillery formations for example, brigades even divisions, or multi-battalion regiments, and usually under command of division, corps or higher HQs. General support units tend to be moved to where they are most required at any particular time. Artillery command may impose priorities and constraints to support their combined arms commander's plans.
Target acquisition can take many forms, it is usually observation in real time but may be the product of analysis. Artillery observation teams are the most common means of target acquisition. However, air observers have been use since the beginning of indirect fire and were quickly joined by air photography. Target acquisition may also be by anyone that can get the information into the artillery system. Targets may be visible to forward troops or in depth and invisible to them.
Observation equipment can vary widely in its complexity.
Control, sometimes called tactical fire control, is primarily concerned with 'targeting' and the allotment of fire units to targets. This is vital when a target is within range of many fire units and the number of fire units needed depends on the nature of the target, and the circumstances and purpose of its engagement. Targeting is concerned with selecting the right weapons in the right quantities to achieve the required effects on the target. Allotment attempts to address the artillery dilemma—important targets are rarely urgent and urgent targets are rarely important. Of course importance is a matter of perspective; what is important to a divisional commander is rarely the same as what is important to an infantry platoon commander.
Broadly, there are two situations: fire against opportunity targets and targets whose engagement is planned as part of a particular operation. In the latter situation command assigns fire units to the operation and an overall artillery fire planner makes a plan, possibly delegating resources for some parts of it to other planners. Fire plans may also involve use of non-artillery assets such as mortars and aircraft.
Control of fire against opportunity targets is an important differentiator between different types of artillery system. In some armies only designated artillery HQs have the tactical fire control authority to order fire units to engage a target, all 'calls for fire' being requests to these HQs. This authority may also extend to deciding the type and quantity of ammunition to be used. In other armies an 'authorised observer' (for example, artillery observation team or other target acquisition element) can order fire units to engage. In the latter case a battery observation team can order fire to their own battery and may be authorised to order fire to their own battalion and sometimes to many battalions. For example a divisional artillery commander may authorise selected observers to order fire to the entire divisional artillery. When observers or cells are not authorised they can still request fire.
Armies that apply forward tactical control generally put the majority of the more senior officers of artillery units forward in command observation posts or with the supported arm. Those that do not use this approach tend to put these officers close to the guns. In either case the observation element usually controls fire in detail against the target, such as adjusting it onto the target, moving it and co-ordinating it with the supported arm as necessary to achieve the required effects.
Firing data has to be calculated and is the key to indirect fire, the arrangements for this have varied widely. In the end firing data has two components: quadrant elevation and azimuth, to these may be added the size of propelling charge and the fuze setting. The process to produce firing data this is sometimes called technical fire control. Before computers, some armies set the range on the gun's sights, which mechanically corrected it for the gun's muzzle velocity. For the first few decades of indirect fire, the firing data were often calculated by the observer who then adjusted the fall of shot onto the target.
However, the need to engage targets at night, in depth or hit the target with the first rounds quickly led to predicted fire being developed in World War I. Predicted fire existed alongside the older method. After World War II predicted methods were invariably applied but the fall of shot usually needed adjustment because of inaccuracy in locating the target, the proximity of friendly troops or the need to engage a moving target. Target location errors were significantly reduced once laser rangefinders, orientation and navigation devices were issued to observation parties.
In predicted fire the basic geospatial data of range, angle of sight and azimuth between a fire unit and its target was produced and corrected for variations from the 'standard conditions'. These variations included barrel wear, propellant temperature, different projectiles weights that all affected the muzzle velocity, and air temperature, density, wind speed & direction and rotation of the earth that affect the shell in flight. The net effect of variations can also be determined by shooting at an accurately known point, a process called 'registration'.
All these calculations to produce a quadrant elevation (or range) and azimuth were done manually by highly trained soldiers using instruments, tabulated data, data of the moment and approximations until battlefield computers started appearing in the 1960s and 1970s. While some early calculators copied the manual method (typically substituting polynomials for tabulated data), computers use a different approach. They simulate a shell's trajectory by 'flying' it in short steps and applying data about the conditions affecting the trajectory at each step. This simulation is repeated until it produces a quadrant elevation and azimuth that lands the shell within the required 'closing' distance of the target co-ordinates. NATO has a standard ballistic model for computer calculations and has expanded the scope of this into the NATO Armaments Ballistic Kernel (NABK) within the SG2 Shareable (Fire Control) Software Suite (S4).
Technical fire control has been performed in various places, but mostly in firing batteries. However, in the 1930s the French moved it to battalion level and combined it with some tactical fire control. This was copied by the US. Nevertheless most armies seemed to have retained it within firing batteries and some duplicated the technical fire control teams in a battery to give operational resilience and tactical flexibility. Computers reduced the number of men needed and enabled decentralisation of technical fire control to autonomous sub-battery fire units such as platoons, troops or sections, although some armies had sometimes done this with their manual methods. Computation on the gun or launcher, integrated with their laying system, is also possible. MLRS led the way in this.
A fire unit is the smallest artillery or mortar element, consisting of one or more weapon systems, capable of being employed to execute a fire assigned by a tactical fire controller. Generally it is a battery, but sub-divided batteries are quite common, and in some armies very common. On occasions a battery of 6 guns has been 6 fire units. Fire units may or may not occupy separate positions. Geographically dispersed fire units may or may not have an integral capability for technical fire control.
Specialist services provide data need for predicted fire. Increasingly, they are provided from within firing units. These services include:
Logistic services, supply of artillery ammunition has always been a major component of military logistics. Up until World War I some armies made artillery responsible for all forward ammunition supply because the load of small arms ammunition was trivial compared to artillery. Different armies use different approaches to ammunition supply, which can vary with the nature of operations. Differences include where the logistic service transfers artillery ammunition to artillery, the amount of ammunition carried in units and extent to which stocks are held at unit or battery level. A key difference is whether supply is 'push' or 'pull'. In the former the 'pipeline' keeps pushing ammunition into formations or units at a defined rate. In the latter units fire as tactically necessary and replenish to maintain or reach their authorised holding (which can vary), so the logistic system has to be able to cope with surge and slack.
Artillery has always been equipment intensive and for centuries artillery provided its own artificers to maintain and repair their equipment. Most armies now place these services in specialist branches with specialist repair elements in batteries and units.
Classification of artillery.
Artillery types can be categorised in several ways, for example by type or size of weapon or ordnance, by role or by organizational arrangements.
Types of ordnance.
The types of cannon artillery are generally distinguished by the velocity at which they fire projectiles.
Types of artillery:
Modern field artillery can also be split into two other categories: towed and self-propelled. As the name suggests, towed artillery has a prime mover, usually a jeep or truck, to move the piece, crew, and ammunition around. Self-propelled howitzers are permanently mounted on a carriage or vehicle with room for the crew and ammunition and are thus capable of moving quickly from one firing position to another, both to support the fluid nature of modern combat and to avoid counter-battery fire. There are also mortar carrier vehicles, many of which allow the mortar to be removed from the vehicle and be used dismounted, potentially in terrain in which the vehicle cannot navigate, or in order to avoid detection.
Organizational types.
At the beginning of the modern artillery period, the late 19th century, many armies had three main types of artillery, in some case they were sub-branches within the artillery branch in others they were separate branches or corps. There were also other types excluding the armament fitted to warships:
After World War I many nations merged these different artillery branches, in some cases keeping some as sub-branches. Naval artillery disappeared apart from that belonging to marines. However, two new branches of artillery emerged during that war and its aftermath, both used specialised guns (and a few rockets) and used direct not indirect fire, in the 1950s and 1960s both started to make extensive use of missiles:
However, the general switch by artillery to indirect fire before and during World War I led to a reaction in some armies. The result was accompanying or infantry guns. These were usually small, short range guns, that could be easily man-handled and used mostly for direct fire but some could use indirect fire. Some were operated by the artillery branch but under command of the supported unit. In World War II they were joined by self-propelled assault guns, although other armies adopted infantry or close support tanks in armoured branch units for the same purpose, subsequently tanks generally took on the accompanying role.
Equipment types.
The three main types of artillery "gun" are guns, howitzers and mortars. During the 20th century, guns and howitzers have steadily merged in artillery use, making a distinction between the terms somewhat meaningless. By the end of the 20th century, true guns with calibers larger than about 60 mm had become very rare in artillery use, the main users being tanks, ships, and a few residual anti-aircraft and coastal guns. The term "cannon" is a United States generic term that includes guns, howitzers and mortars; it is not used in other English speaking armies.
The traditional definitions differentiated between guns and howitzers in terms of maximum elevation (well less than 45° as opposed to close to or greater than 45°), number of charges (one or more than one charge), and having higher or lower muzzle velocity, sometimes indicated by barrel length. These three criteria give eight possible combinations, of which guns and howitzers are but two. However, modern "howitzers" have higher velocities and longer barrels than the equivalent "guns" of the first half of the 20th century.
True guns are characterized by long range, having a maximum elevation significantly less than 45°, a high muzzle velocity and hence a relatively long barrel, smooth bore (no rifling) and a single charge. The latter often led to fixed ammunition where the projectile is locked to the cartridge case. There is no generally accepted minimum muzzle velocity or barrel length associated with a gun. 
Howitzers can fire at maximum elevations at least close to 45°; elevations up to about 70° are normal for modern howitzers. Howitzers also have a choice of charges, meaning that the same elevation angle of fire will achieve a different range depending on the charge used. They have rifled bores, lower muzzle velocities and shorter barrels than equivalent guns. All this means they can deliver fire with a steep angle of descent. Because of their multi-charge capability, their ammunition is mostly separate loading (the projectile and propellant are loaded separately).
That leaves six combinations of the three criteria, some of which have been termed gun howitzers. A term first used in the 1930s when howitzers with a relatively high maximum muzzle velocities were introduced, it never became widely accepted, most armies electing to widen the definition of "gun" or "howitzer". By the 1960s, most equipments had maximum elevations up to about 70°, were multi-charge, had quite high maximum muzzle velocities and relatively long barrels.
Mortars are simpler. The modern mortar originated in World War I and there were several patterns. After that war, most mortars settled on the Stokes pattern, characterized by a short barrel, smooth bore, low muzzle velocity, elevation angle of firing generally greater than 45°, and a very simple and light mounting using a "baseplate" on the ground. The projectile with its integral propelling charge was dropped down the barrel from the muzzle to hit a fixed firing pin. Since that time, a few mortars have become rifled and adopted breech loading.
There are other recognized typifying characteristics for artillery. One such characteristic is the type of obturation used to seal the chamber and prevent gases escaping through the breech. This may use a metal cartridge case that also holds the propelling charge, a configuration called "QF" or "quickfiring" by some nations. The alternative does not use a metal cartridge case, the propellant being merely bagged or in combustible cases with the breech itself providing all the sealing. This is called "BL" or "breech loading" by some nations.
A second characteristic is the form of propulsion. Modern equipment can either be towed or self-propelled (SP). A towed gun fires from the ground and any inherent protection is limited to a gun shield. Towing by horse teams lasted throughout World War II in some armies, but others were fully mechanized with wheeled or tracked gun towing vehicles by the outbreak of that war. The size of a towing vehicle depends on the weight of the equipment and the amount of ammunition it has to carry.
A variation of towed is portee, where the vehicle carries the gun which is dismounted for firing. Mortars are often carried this way. A mortar is sometimes carried in an armored vehicle and can either fire from it or be dismounted to fire from the ground. Since the early 1960s it has been possible to carry lighter towed guns and most mortars by helicopter. Even before that, they were parachuted or landed by glider from the time of the first airborne trials in the USSR in the 1930s.
In an SP equipment, the gun is an integral part of the vehicle that carries it. SPs first appeared during World War I, but did not really develop until World War II. They are mostly tracked vehicles, but wheeled SPs started to appear in the 1970s. Some SPs have no armor and carry little or no ammunition. Armoured SPs usually carry a useful ammunition load. Early armoured SPs were mostly a "casemate" configuration, in essence an open top armored box offering only limited traverse. However, most modern armored SPs have a full enclosed armored turret, usually giving full traverse for the gun. Many SPs cannot fire without deploying stabilizers or spades, sometimes hydraulic. A few SPs are designed so that the recoil forces of the gun are transferred directly onto the ground through a baseplate. A few towed guns have been given limited self-propulsion by means of an auxiliary engine.
Two other forms of tactical propulsion were used in the first half of the 20th century: Railways or transporting the equipment by road, as two or three separate loads, with disassembly and re-assembly at the beginning and end of the journey. Railway artillery took two forms, railway mountings for heavy and super-heavy guns and howitzers and armored trains as "fighting vehicles" armed with light artillery in a direct fire role. Disassembled transport was also used with heavy and super heavy weapons and lasted into the 1950s.
Caliber categories.
A third form of artillery typing is to classify it as "light", "medium", "heavy" and various other terms. It appears to have been introduced in World War I, which spawned a very wide array of artillery in all sorts of sizes so a simple categorical system was needed. Some armies defined these categories by bands of calibers. Different bands were used for different types of weapons—field guns, mortars, anti-aircraft guns and coast guns.
Modern operations.
List of countries in order of amount of artillery:
Artillery is used in a variety of roles depending on its type and caliber. The general role of artillery is to provide "fire support"—"the application of fire, coordinated with the manoeuvre of forces to destroy, "neutralize" or "suppress" the enemy". This NATO definition, of course, makes artillery a supporting arm although not all NATO armies agree with this logic. The "italicised" terms are NATO's.
Unlike rockets, guns (or howitzers as some armies still call them) and mortars are suitable for delivering "close supporting fire". However, they are all suitable for providing "deep supporting fire" although the limited range of many mortars tends to exclude them from the role. Their control arrangements and limited range also mean that mortars are most suited to "direct supporting fire". Guns are used either for this or "general supporting fire" while rockets are mostly used for the latter. However, lighter rockets may be used for direct fire support. These rules of thumb apply to NATO armies.
Modern mortars, because of their lighter weight and simpler, more transportable design, are usually an integral part of infantry and, in some armies, armor units. This means they generally do not have to "concentrate" their fire so their shorter range is not a disadvantage. Some armies also consider infantry operated mortars to be more responsive than artillery, but this is a function of the control arrangements and not the case in all armies. However, mortars have always been used by artillery units and remain with them in many armies, including a few in NATO.
In NATO armies artillery is usually assigned a tactical mission that establishes its relationship and responsibilities to the formation or units it is assigned to. It seems that not all NATO nations use the terms and outside NATO others are probably used. The standard terms are: "direct support", "general support", "general support reinforcing" and "reinforcing". These tactical missions are in the context of the command authority: "operational command", "operational control", "tactical command" or "tactical control".
In NATO direct support generally means that the directly supporting artillery unit provides observers and liaison to the manoeuvre troops being supported, typically an artillery battalion or equivalent is assigned to a brigade and its batteries to the brigade's battalions. However, some armies achieve this by placing the assigned artillery units under command of the directly supported formation. Nevertheless, the batteries' fire can be "concentrated" onto a single target, as can the fire of units in range and with the other tactical missions.
Application of fire.
There are several dimensions to this subject. The first is the notion that fire may be against an "opportunity" target or may be "prearranged". If it is the latter it may be either "on-call" or "scheduled". Prearranged targets may be part of a "fire plan". Fire may be either "observed" or "unobserved", if the former it may be "adjusted", if the latter then it has to be "predicted". Observation of adjusted fire may be directly by a forward observer or indirectly via some other "target acquisition" system.
NATO also recognises several different types of fire support for tactical purposes:
These purposes have existed for most of the 20th century, although their definitions have evolved and will continue to do so, lack of "suppression" in "counterbattery" is an omission. Broadly they can be defined as either:
Two other NATO terms also need definition:
The tactical purposes also include various "mission verbs", a rapidly expanding subject with the modern concept of "effects based operations".
"Targeting" is the process of selecting target and matching the appropriate response to them taking account of operational requirements and capabilities. It requires consideration of the type of fire support required and the extent of coordination with the supported arm. It involves decisions about:
The "targeting" process is the key aspect of tactical fire control. Depending on the circumstances and national procedures it may all be undertaken in one place or may be distributed. In armies practicing control from the front, most of the process may be undertaken by a forward observer or other target acquirer. This is particularly the case for a smaller target requiring only a few fire units. The extent to which the process is formal or informal and makes use of computer based systems, documented norms or experience and judgement also varies widely armies and other circumstances.
Surprise may be essential or irrelevant. It depends on what effects are required and whether or not the target is likely to move or quickly improve its protective posture. During World War II UK researchers concluded that for impact fuzed munitions the relative risk were as follows:
Airburst munitions significantly increase the relative risk for lying men, etc. Historically most casualties occur in the first 10–15 seconds of fire, i.e. the time needed to react and improve protective posture, however, this is less relevant if airburst is used.
There are several ways of making best use of this brief window of maximum vulnerability:
Counter-battery fire.
Modern Counter-battery fire developed in World War I, with the objective of defeating the enemy's artillery. Typically such fire was used to suppress enemy batteries when they were or were about to interfere with the activities of friendly forces (such as a to prevent enemy defensive artillery fire against an impending attack) or to systematically destroy enemy guns. In World War I the latter required air observation. The first indirect counter-battery fire was in May 1900 by an observer in a balloon.
Enemy artillery can be detected in two ways, either by direct observation of the guns from the air or by ground observers (including specialist reconnaissance), or from their firing signatures. This includes radars tracking the shells in flight to determine their place of origin, sound ranging detecting guns firing and resecting their position from pairs of microphones or cross-observation of gun flashes using observation by human observers or opto-electronic devices, although the widespread adoption of 'flashless' propellant limited the effectiveness of the latter.
Once hostile batteries have been detected they may be engaged immediately by friendly artillery or later at an optimum time, depending on the tactical situation and the counter-battery policy. Air strike is another option. In some situations the task is to locate all active enemy batteries for attack using a counter-battery fire at the appropriate moment in accordance with a plan developed by artillery intelligence staff. In other situation counter-battery fire may occur whenever a battery is located with sufficient accuracy.
Modern counter-battery target acquisition uses unmanned aircraft, counter-battery radar, ground reconnaissance and sound-ranging. Counter-battery fire may be adjusted by some of the systems, for example the operator of an unmanned aircraft can 'follow' a battery if it moves. Defensive measures by batteries include frequently changing position or constructing defensive earthworks, the tunnels used by North Korea being an extreme example. Counter-measures include air defence against aircraft and attacking counter-battery radars physically and electronically.
Field artillery team.
'Field Artillery Team' is a US term and the following description and terminology applies to the US, other armies are broadly similar but differ in significant details. The 'Field Artillery System' above gives a more comprehensive description. Modern field artillery (post–World War I) has three distinct parts: the forward observer (or FO), the fire direction center (FDC) and the actual guns themselves. The forward observer observes the target using tools such as binoculars, laser rangefinders, designators and call back fire missions on his radio, or relays the data through a portable computer via an encrypted digital radio connection protected from jamming by computerized frequency hopping. A lesser known part of the team is the FAS or Field Artillery Survey team which setups up the "Gun Line" for the cannons. Today most artillery battalions use a(n) "Aiming Circle" which allows for faster setup and more mobility. FAS teams are still used for checks and balances purposes and if a gun battery has issues with the "Aiming Circle" a FAS team will do it for them.
The FO can communicate directly with the battery FDC, of which there is one per each battery of 4–8 guns. Otherwise the several FOs communicate with a higher FDC such as at a Battalion level, and the higher FDC prioritizes the targets and allocates fires to individual batteries as needed to engage the targets that are spotted by the FOs or to perform preplanned fires.
The Battery FDC computes firing data—ammunition to be used, powder charge, fuse settings, the direction to the target, and the quadrant elevation to be fired at to reach the target, what gun will fire any rounds needed for adjusting on the target, and the number of rounds to be fired on the target by each gun once the target has been accurately located—to the guns. Traditionally this data is relayed via radio or wire communications as a warning order to the guns, followed by orders specifying the type of ammunition and fuse setting, direction, and the elevation needed to reach the target, and the method of adjustment or orders for fire for effect (FFE). However in more advanced artillery units, this data is relayed through a digital radio link.
Other parts of the field artillery team include meteorological analysis to determine the temperature, humidity and pressure of the air and wind direction and speed at different altitudes. Also radar is used both for determining the location of enemy artillery and mortar batteries and to determine the precise actual strike points of rounds fired by battery and comparing that location with what was expected to compute a registration allowing future rounds to be fired with much greater accuracy.
Time on Target.
A technique called Time on Target was developed by the British Army in North Africa at the end of 1941 and early 1942 particularly for counter-battery fire and other concentrations, it proved very popular. It relied on BBC time signals to enable officers to synchronize their watches to the second because this avoided the need to use military radio networks and the possibility of losing surprise, and the need for field telephone networks in the desert. With this technique the time of flight from each fire unit (battery or troop) to the target is taken from the range or firing tables, or the computer and each engaging fire unit subtracts its time of flight from the TOT to determine the time to fire. An executive order to fire is given to all guns in the fire unit at the correct moment to fire. When each fire unit fires their rounds at their individual firing time all the opening rounds will reach the target area almost simultaneously. This is especially effective when combined with techniques that allow fires for effect to be made without preliminary adjusting fires.
MRSI.
This is a modern version of the earlier "time on target" concept in which fire from different weapons was timed to arrive on target at the same time. It is possible for artillery to fire several shells per gun at a target and have all of them arrive simultaneously, which is called MRSI (Multiple Rounds Simultaneous Impact). This is because there is more than one trajectory for the rounds to fly to any given target: typically one is below 45 degrees from horizontal and the other is above it, and by using different size propelling charges with each shell, it is possible to create multiple trajectories. Because the higher trajectories cause the shells to arc higher into the air, they take longer to reach the target and so if the shells are fired on these trajectories for the first volleys (starting with the shell with the most propellant and working down) and then after the correct pause more volleys are fired on the lower trajectories, the shells will all arrive at the same time. This is useful because many more shells can land on the target with no warning. With traditional volleys along the same trajectory, anybody at the target area may have time (however long it takes to reload and re-fire the guns) to take cover between volleys. However, guns capable of burst fire can deliver several rounds in 10 seconds if they use the same firing data for each, and if guns in more than one location are firing on one target they can use Time on Target procedures so that all their shells arrive at the same time and target.
To engage targets using MRSI requires two things, firstly guns with the requisite rate of fire and sufficient different size propelling charges, secondly a fire control computer that has been designed to compute such missions and the data handling capability that allows all the firing data to be produced, sent to each gun and then presented to the gun commander in the correct order. The number of rounds that can be delivered in MRSI depends primarily on the range to the target and the rate of fire, for maximum rounds the range is limited to that of lowest propelling charge that will reach the target.
Examples of guns with a rate of fire that makes them suitable for MRSI includes UK's AS-90, South Africa's Denel G6-52 (which can land six rounds simultaneously at targets at least away), Germany's Panzerhaubitze 2000 (which can land five rounds simultaneously at targets at least away) and Slovakia's 155 mm SpGH ZUZANA model 2000. The Archer project (Developed by BAE-Systems in Sweden), a 155 mm howitzer on a wheeled chassis claiming to be able to deliver up to 7 shells on target simultaneously from the same gun. The 120 mm twin barrel AMOS mortar system, joint developed by Hägglunds (Sweden) Patria (company) (Finland), is capable of 7 + 7 shells MRSI. The United States Crusader program (now cancelled) was slated to have MRSI capability. It is unclear how many fire control computers have the necessary capabilities.
Two-round MRSI firings were a popular artillery demonstration in the 1960s, where well trained detachments could show off their skills for spectators.
Air burst.
The destructiveness of artillery bombardments can be enhanced when some or all of the shells are set for airburst, meaning that they explode in the air above the target instead of upon impact. This can be accomplished either through time fuses or proximity fuses. Time fuses use a precise timer to detonate the shell after a preset delay. This technique is tricky and slight variations in the functioning of the fuse can cause it to explode too high and be ineffective, or to strike the ground instead of exploding above it. Since December 1944 (Battle of the Bulge), proximity fuzed artillery shells have been available that take the guesswork out of this process. These embody a miniature, low powered radar transmitter in the fuse to detect the ground and explode them at a predetermined height above it. The return of the weak radar signal completes an electrical circuit in the fuze which explodes the shell. The proximity fuse itself was developed by the British to increase the effectiveness of anti-aircraft warfare.
This is a very effective tactic against infantry and light vehicles, because it scatters the fragmentation of the shell over a larger area and prevents it from being blocked by terrain or entrenchments that do not include some form of robust overhead cover. Combined with TOT or MRSI tactics that give no warning of the incoming rounds, these rounds are especially devastating because many enemy soldiers are likely to be caught in the open. This is even more so if the attack is launched against an assembly area or troops moving in the open rather than a unit in an entrenched tactical position.

</doc>
<doc id="2510" url="http://en.wikipedia.org/wiki?curid=2510" title="Arnulf of Carinthia">
Arnulf of Carinthia

Arnulf of Carinthia (850 – 8 December 899) was the Carolingian King of East Francia from 887, the disputed King of Italy from 894 and the disputed Holy Roman Emperor from 22 February 896 until his death at Regensburg, Bavaria.
Career.
Birth and Illegitimacy.
Arnulf was the son of Carloman, King of Bavaria, and his wife Liutswind, perhaps of Carantanian origin, and possibly the sister of Ernst, Count of the Bavarian Nordgau Margraviate in the area of the Upper Palatinate, or perhaps the burgrave of Passau, as some sources say. After Arnulf's birth, Carloman married, before 861, a daughter of that same Count Ernst, who died after 8 August 879. As it is mainly West-Franconian historiography that speaks of Arnulf's illegitimacy, it is quite feasible that the two females are one and the same person and that Carloman later on actually married Liutswind, thus legitimizing his son.
Early Years.
Arnulf was granted the Duchy of Carinthia, a Frankish vassal state and successor of the ancient Principality of Carantania, by his father Carloman, after Carloman had become reconciled with his own father Louis the German and was created King of Bavaria. Arnulf spent his childhood on the "Mosaburch" or Mosapurc, which is widely believed to be Moosburg in Carinthia, only a few miles away from one of the Imperial residences, the Carolingian Kaiserpfalz at Karnburg, which before as "Krnski grad" had been the residence of the Carantanian princes. Arnulf kept his seat here and from later events it may be inferred that the Carantanians, from an early time, treated him as their own Duke. Later, after he had been crowned King of East Francia, Arnulf turned his old territory of Carinthia into the March of Carinthia, a part of the Duchy of Bavaria.
After Carloman was incapacitated by a stroke in 879, Louis the Younger inherited Bavaria, Charles the Fat was given the Kingdom of Italy and Arnulf was confirmed in Carinthia by an agreement with Carloman. Bavaria, however, was ruled more or less by Arnulf. Arnulf had in fact ruled Bavaria during the summer and autumn of 879 while his father arranged his succession and he himself was granted "Pannonia," in the words of the "Annales Fuldenses", or "Carantanum," in the words of Regino of Prüm. The division of the realm was confirmed in 880 on Carloman’s death.
When, in 882, Engelschalk II rebelled against the Margrave of Pannonia, Aribo, and ignited the so-called Wilhelminer War, Arnulf supported him and even accepted his and his brother's homage. This ruined Arnulf's relationship with his uncle the Emperor and put him at war with Svatopluk of Moravia. Pannonia was invaded, but Arnulf refused to give up the young Wilhelminers. Arnulf did not make peace with Svatopluk until late 885, by which time the Moravian was a man of the emperor. Some scholars see this war as destroying Arnulf's hopes at succeeding Charles.
King of East Francia.
Arnulf took the leading role in the deposition of his uncle, the Emperor Charles the Fat. With the support of the nobles, Arnulf held a Diet at Tribur and deposed Charles in November 887, under threat of military action. Charles peacefully went into his involuntary retirement, but not without first chastising his nephew for his treachery and asking only for a few royal villas in Swabia, which Arnulf mercifully granted him, on which to live out his final months. Arnulf, having distinguished himself in the war against the Slavs was elected by the nobles of the realm (only the eastern realm, though Charles had ruled the whole of the Frankish lands) and assumed his title of King of East Francia.
Arnulf took advantage of the problems in West Francia upon the death of Charles The Fat to secure the territory of Lorraine, which he converted into a kingdom for his son, Zwentibold. In addition, in 889, Arnulf supported the claim of Louis the Blind to the kingdom of Provence, after receiving a personal appeal from Louis’ mother, Ermengard, who came to see Arnulf at Forchheim in May 889. Recognising the superiority of Arnulf’s position, in 888 Odo of France formally admitted the suzerainty of Arnulf. In 893, Arnulf switched his support from Odo to Charles the Simple after being persuaded by Fulk (Archbishop of Reims) that it was in his best interests. Arnulf then took advantage of the fighting that followed between Odo and Charles in 894, taking territory from West Francia and transferring it to his dominion. At one point, Charles was forced to flee to Arnulf and ask for his protection. His intervention forced Pope Formosus to get involved, as he was worried that a divided and war weary West Francia would be easy prey for the Normans.
In 895, Arnulf summoned both Charles and Odo to his presence at Worms. Charles’s advisers convinced him not to go, and he sent a representative in his place. Odo, on the other hand, personally attended, together with a large retinue, bearing many gifts for Arnulf. Angered by the non-appearance of Charles, he welcomed Odo at the Diet of Worms in May 895, and again supported Odo's claim to the West Francian throne. In this same assembly, he bestowed upon his illegitimate son Zwentibold, a crown as the King of Lotharingia.
Arnulf was not a negotiator, but a fighter. In 890 he was successfully battling the Slavs in Pannonia. In 891, the Danes invaded Lotharingia, and crushed an East Frankish army at Maastricht. At the decisive Battle of Leuven in September 891 in Lorraine, he repelled an invasion by the Normans (Northmen or Vikings), essentially ending their invasions on that front. The "Annales Fuldenses" report that the bodies of dead Northmen blocked the run of the river. After his victory, Arnulf built a new castle on an island in the Dijle river (Dutch: Dijle, English and French: Dyle).
As early as 880, Arnulf had designs on Great Moravia, and had the Frankish bishop Wiching of Nitra interfere with the missionary activities of Methodius, with the aim of preventing any potential for creating a unified Moravian nation. In 893 or 894, Great Moravia probably lost a part of its territory — present-day Western Hungary — to him. As a reward, Wiching became Arnulf’s chancellor in 892. Arnulf, however, failed to conquer the whole of Great Moravia when he attempted it in 892, 893, and 899. Yet Arnulf did achieve some successes, in particular in 895, when Bohemia broke away from Great Moravia and became his vassal. An accord was made between him and the Bohemian Duke Borivoj I (reigned 870-95); Bohemia was thus freed from the dangers of invasion. However, in his attempts to conquer Moravia, in 899 Arnulf invited across the Magyars who had settled in Pannonia, and with their help he imposed a measure of control on Moravia. While Arnulf remained alive, the Magyars refrained from any overt acts of pillage, but with Arnulf’s death, they proceeded to invade Italy in 900.
Like all early Germanic rulers, he was heavily involved in ecclesiastical disputes; in 895, at the Diet of Tribur, he presided over a dispute between the Episcopal sees of Bremen, Hamburg and Cologne over jurisdictional authority, which saw Bremen and Hamburg remain a combined see, independent of the see of Cologne.
King of Italy and Holy Roman Emperor.
In Italy, the Iron Crown of Lombardy was being fought over between Guy III of Spoleto and Berengar of Friuli. Berengar had been crowned king in 887, but Guy was in his turn crowned in 889. While Pope Stephen V supported Guy, crowning him Roman Emperor in 891, Arnulf threw his support behind Berengar.
In 893, a new pope, Formosus, not trusting the newly crowned co-emperors Guy and Lambert, sent an embassy to Omuntesberch, where Arnulf was holding a Diet with Svatopluk, to request Arnulf come and liberate Italy, where he would be crowned in Rome. Arnulf met the "Primores" of the Kingdom of Italy, dismissed them with gifts and promised to enter Italy. Arnulf sent his son Zwentibold with a Bavarian army to join Berengar of Friuli. They defeated Guy, but were bought off and left in autumn. Arnulf then personally led an army across the Alps early in 894. In January 894 Bergamo fell, and Count Ambrose, Guy’s representative in the city, was hung from a tree by the city’s gate.
Conquering all of the territory north of the Po, he forced the surrender of Milan and then drove Guy out of Pavia, where he was crowned King of Italy, but went no further before Guy died suddenly in late autumn, and fever incapacitated his troops. His march northward through the Alps was interrupted by Rudolph, King of Transjurane Burgundy, and it was only with great difficulty that Arnulf crossed the mountain range. In retaliation, Arnulf ordered his illegitimate son Zwentibold to ravage Burgundy. In the meantime, Lambert and his mother Ageltrude travelled to Rome to receive papal confirmation of his imperial succession, but Formosus, still desiring to crown Arnulf, was imprisoned in Castel Sant'Angelo.
In September 895, a new embassy arrived in Regensburg beseeching Arnulf's aid. In October, Arnulf undertook his second campaign into Italy. He crossed the Alps quickly and took Pavia, but then he continued slowly, garnering support among the nobility of Tuscany. First Maginulf, Count of Milan, and then Walfred, Count of Pavia, joined him. Eventually even the Margrave Adalbert II abandoned Lambert. Finding Rome locked against him and held by Ageltrude, he had to take the city by force on 21 February 896, freeing the pope. Arnulf was greeted at the Ponte Milvio by the Roman Senate who escorted him into the Leonine City, where he was received by Pope Formosus on the steps of the Santi Apostoli.
On 22 February 896, Formosus led the king into the church, anointed and crowned him, and saluted him as "Augustus". Arnulf then proceeded to the Basilica of Saint Paul Outside the Walls, where he received the homage of the Roman people, who swore “never to hand over the city to Lambert or his mother Ageltrude”. Arnulf then proceeded to exile to Bavaria two leading senators, Constantine and Stephen, who had helped Ageltrude seize the city. Leaving one of his vassals, Farold, to hold Rome, Arnulf marched on Spoleto, where Ageltrude had fled to join Lambert. On his way down, Arnulf suffered a stroke, forcing him to call off his campaign and return to Bavaria.
Arnulf only retained power in Italy as long as he was personally there. On his way north, he stopped at Pavia where he crowned his illegitimate son Ratold, sub-King of Italy, after which he left Ratold in Milan in an attempt to preserve his hold on Italy. That same year, Formosus died, leaving Lambert once again in power, and both he and Berengar killed any officials who had been put in place by Arnulf, as Ratold also fled from Milan to Bavaria. Rumours of the time made Arnulf's condition to be a result of poisoning at the hand of Ageltrude. On his return to Germany, he exercised very little further control in Italy for the rest of his life, although his agents in Rome did not prevent the accession of Pope Stephen VI in 896. Although he eventually became a supporter of the claims of Lambert, he initially gave his support to Arnulf.
Final Years.
With his return to Germany in 896, Arnulf found that his physical ill health - he suffered from (morbus pediculosis – infestation of lice under the eyelid) - meant he was unable to deal with the problems besetting his reign. Italy was lost, raiders from Moravia and Hungary were continually raiding his lands, and Lotharingia was in revolt against Zwentibold. He was also plagued by escalating violence and power struggles between the lower German nobility.
On 8 December 899 Arnulf of Carinthia died at Ratisbon (today known as Regensburg), Bavaria, Germany.
On Arnulf's death, he was succeeded as a king of the East Franks by his son by his wife Ota (died 903), Louis the Child. When his only legitimate son, Louis the Child, died in 911 at age 17 or 18, the eastern (German) branch of the house of Charlemagne ceased to exist. Arnulf had the nobility also recognize the rights of his illegitimate sons Zwentibold and Ratold as his successors. Zwentibold, whom he had made King of Lotharingia in 895, continued to rule there until the next year (900).
Arnulf is entombed in St. Emmeram's Basilica at Regensburg, which is now known as Schloss Thurn und Taxis, the palace of the Princes of Thurn und Taxis.

</doc>
<doc id="2511" url="http://en.wikipedia.org/wiki?curid=2511" title="Alexanderplatz">
Alexanderplatz

Alexanderplatz () is a large public square and transport hub in the central Mitte district of Berlin, near the Fernsehturm. Berliners often call it simply Alex, referring to a larger neighbourhood stretching from "Mollstraße" in the northeast to "Spandauer Straße" and the Red City Hall in the southwest.
History.
Early history.
Originally a cattle market outside the city fortifications, it was named in honor of a visit of the Russian Emperor Alexander I to Berlin on 25 October 1805 by order of King Frederick William III of Prussia. The square gained a prominent role in the late 19th century with the construction of the Stadtbahn station of the same name and a nearby market hall, followed by the opening of a department store of Hermann Tietz in 1904, becoming a major commercial centre. The U-Bahn station of the present-day U2 line opened on 1 July 1913.
Its heyday was in the 1920s, when together with Potsdamer Platz it was at the heart of Berlin's nightlife, inspiring the 1929 novel "Berlin Alexanderplatz" (see 1920s Berlin) and the two films based thereon, Piel Jutzi's 1931 film and Rainer Werner Fassbinder's 15½ hour second adaptation, released in 1980. About 1920 the city's authorities started a rearrangement of the increasing traffic flows laying out a roundabout, accompanied by two buildings along the Stadtbahn viaduct, "Alexanderhaus" and "Berolinahaus" finished in 1932 according to plans designed by Peter Behrens.
East Germany.
Alexanderplatz has been subject to redevelopment several times in its history, most recently during the 1960s, when it was turned into a pedestrian zone and enlarged as part of the German Democratic Republic's redevelopment of the city centre. It is surrounded by several notable structures including the Fernsehturm (TV Tower), the second tallest structure in the European Union.
"Alex" also accommodates the Park Inn Berlin and the World Time Clock, a continually rotating installation that shows the time throughout the globe, and Hermann Henselmann's "Haus des Lehrers". During the Peaceful Revolution of 1989, the Alexanderplatz demonstration on 4 November was the largest demonstration in the history of East Germany.
After German reunification.
Since German reunification, Alexanderplatz has undergone a gradual process of change with many of the surrounding buildings being renovated. Despite the reconstruction of the tram line crossing, it has retained its socialist character, including the much-graffitied "Fountain of Friendship between Peoples" ("Brunnen der Völkerfreundschaft"), a popular venue.
In 1993, architect Hans Kollhoff's master plan for a major redevelopment including the construction of several skyscrapers was published. Due to a lack of demand it is unlikely these will be constructed, with the exception (announced January 2014) of a 39-storey residential tower designed by Frank Gehry on which work is expected to begin in 2015. However, beginning with the reconstruction of the "Kaufhof" department store in 2004, and the biggest underground railway station of Berlin, some buildings will be redesigned and new structures built on the square's south-eastern side. Sidewalks were expanded to shrink one of the avenues, a new underground garage was built, and commuter tunnels meant to keep pedestrians off the streets were removed. The surrounding buildings now house chain stores, fast-food restaurants, and fashion discounters. The "Alexa" shopping mall, with approximately 180 stores opened nearby during 2007 and a large "Saturn" electronic store was built and is open on Alexanderplatz since 2008.
Many historic buildings are located in the vicinity of Alexanderplatz. The traditional seat of city government, the Rotes Rathaus, or "Red City Hall", is located nearby, as was the former East German parliament building, the Palast der Republik, demolition of which began in February 2006 and has been completed. The reconstruction of the Baroque Stadtschloss near Alexanderplatz has been in planning for several years.
Alexanderplatz is also the name of the S-Bahn and U-Bahn stations there.

</doc>
<doc id="2512" url="http://en.wikipedia.org/wiki?curid=2512" title="Asian Development Bank">
Asian Development Bank

The Asian Development Bank (ADB) is a regional development bank established on 22 August 1966 which is headquartered in Metro Manila, Philippines to facilitate economic development of countries in Asia. The bank admits the members of the United Nations Economic and Social Commission for Asia and the Pacific (UNESCAP, formerly known as the United Nations Economic Commission for Asia and the Far East) and non-regional developed countries. From 31 members at its establishment, ADB now has 67 members - of which 48 are from within Asia and the Pacific and 19 outside. ADB was modeled closely on the World Bank, and has a similar weighted voting system where votes are distributed in proportion with member's capital subscriptions.
By the end of 2012, both the United States and Japan hold the two largest proportions of shares each at 12.78%. China holds 5.45%, India holds 5.36%.
Organization.
The highest policy-making body of the bank is the "Board of Governors" composed of one representative from each member state. The Board of Governors, in turn, elect among themselves the 12 members of the "Board of Directors" and their deputy. Eight of the 12 members come from regional (Asia-Pacific) members while the others come from non-regional members.
The Board of Governors also elect the bank's "President" who is the chairperson of the Board of Directors and manages ADB. The president has a term of office lasting five years, and may be reelected. Traditionally, and because Japan is one of the largest shareholders of the bank, the president has always been Japanese.
The most recent president was Takehiko Nakao, who succeeded Haruhiko Kuroda in 2013.
The headquarters of the bank is at 6 ADB Avenue, Mandaluyong City, Metro Manila, Philippines, and it has representative offices around the world. The bank employs 3,051 people, of which 1,463 (48%) are from the Philippines.
History.
1962-1972.
ADB was originally conceived by some influential Japanese who formulated a "private plan" for a regional development bank in 1962, which was later endorsed by the government. The Japanese felt that its interest in Asia was not served by the World Bank and wanted to establish a bank in which Japan was institutionally advantaged. Once the ADB was founded in 1966, Japan took a prominent position in the bank; it received the presidency and some other crucial "reserve positions" such as the director of the administration department. By the end of 1972, Japan contributed $173.7 million (22.6% of the total) to the ordinary capital resources and $122.6 million (59.6% of the total) to the special funds. In contrast, the United States contributed only $1.25 million for the special fund.
The ADB served Japan's economic interests because its loans went largely to Indonesia, Thailand, Malaysia, South Korea and the Philippines, the countries with which Japan had crucial trading ties; these nations accounted for 78.48% of the total ADB loans in 1967-72. Moreover, Japan received tangible benefits, 41.67% of the total procurements in 1967-76. Japan tied its special funds contributions to its preferred sectors and regions and procurements of its goods and services, as reflected in its $100 million donation for the Agricultural Special Fund in April 1968.
Takeshi Watanabe served as the first ADB president from 1966 to 1972.
1972-1986.
Japan's share of cumulative contributions increased from 30.4 percent in 1972 to 35.5 percent in 1981 and 41.9 percent in 1986. In addition, Japan was a crucial source of ADB borrowing, 29.4 percent (out of $6,729.1 million) in 1973-86, compared to 45.1 percent from Europe and 12.9 percent from the United States. Japanese presidents Inoue Shiro (1972–76) and Yoshida Taroichi (1976–81) took the spotlight. Fujioka Masao, the fourth president (1981–90), adopted an assertive leadership style. He announced an ambitious plan to expand the ADB into a high-impact development agency. His plan and banking philosophy led to increasing friction with the U.S. directors, with open criticism from the Americans at the 1985 annual meeting.
During this period there was a strong parallel institutional tie between the ADB and the Japanese Ministry of Finance, particularly the International Finance Bureau (IFB).
Since 1986.
Its share of cumulative contributions increased from 41.9 percent in 1986 to 50.0 per- cent in 1993. In addition, Japan has been a crucial lender to the ADB, 30.4 percent of the total in 1987-93, compared to 39.8 percent from Europe and 11.7 percent from the United States. However, different from the previous period, Japan has become more assertive since the mid-1980s. Japan's plan was to use the ADB as a conduit for recycling its huge surplus capital and a "catalyst" for attracting private Japanese capital to the region. After the 1985 Plaza Accord, Japanese manufacturers were pushed by high yen to move to Southeast Asia. The ADB played a role in channeling Japanese private capital to Asia by improving local infrastructure.
The ADB also committed itself to increasing loans for social issues such as education, health and population, urban development and environment, to 40 percent of its total loans from around 30 percent at the time.
Lending.
The ADB offers "hard" loans from ordinary capital resources (OCR) on commercial terms, and the Asian Development Fund (ADF) affiliated with the ADB extends "soft" loans from special fund resources with concessional conditions. For OCR, members subscribe capital, including paid-in and callable elements, a 50 percent paid-in ratio for the initial subscription, 5 percent for the Third General Capital Increase (GCI) in 1983 and 2 percent for the Fourth General Capital Increase in 1994. The ADB borrows from international capital markets with its capital as guarantee.
In 2009, ADB obtained member-contributions for its Fifth General Capital Increase of 200%, in response to a call by G20 leaders to increase resources of multilateral development banks so as to support growth in developing countries amid the global financial crisis. For 2010 and 2011, a 200% GCI allows lending of $12.5-13.0 billion in 2010 and about $11.0 billion in 2011. With this increase, the bank's capital base has tripled from $55 billion to $165 billion.
Effectiveness.
Given ADB's annual lending volume, the return on investment in lesson-learning for operational and developmental impact is likely to be high; maximizing it is a legitimate concern. All projects funded by ADB are evaluated to find out what results are being achieved, what improvements should be considered, and what is being learned.
There are two types of evaluation: independent and self-evaluation. Self-evaluation is conducted by the units responsible for designing and implementing country strategies, programs, projects, or technical assistance activities. It comprises several instruments, including project/program performance reports, midterm review reports, technical assistance or project/program completion reports, and country portfolio reviews. All projects are self-evaluated by the relevant units in a project completion report. ADB’s project completion reports are publicly disclosed on ADB’s Internet site. Client governments are required to prepare their own project completion reports.
Independent evaluation is a foundation block of organizational learning: It is essential to transfer increased amounts of relevant and high-quality knowledge from experience into the hands of policy makers, designers, and implementers. ADB’s Independent Evaluation Department (IED) conducts systematic and impartial assessment of policies, strategies, country programs, and projects, including their design, implementation, results, and associated business processes to determine their relevance, effectiveness, efficiency, and sustainability following prescribed methods and guidelines. It also validates self-evaluations. By this process of evaluation, ADB demonstrates three elements of good governance: accountability, by assessing the effectiveness of ADB's operations; transparency, by independently reviewing operations and publicly reporting findings and recommendations; and improved performance, by helping ADB and its clients learn from experience to enhance ongoing and future operations.
Operations evaluation has changed from the beginnings of evaluation in ADB in 1978. Initially, the focus was on assessing after completion the extent to which projects had achieved their expected economic and social benefits. Operations evaluation now shapes decision making throughout the project cycle and in ADB as a whole. Since the establishment of its independence in 2004, IED reports directly to ADB’s Board of Directors through the Board's Development Effectiveness Committee. Behavioral autonomy, avoidance of conflicts of interest, insulation from external influence, and organizational independence have made evaluation a dedicated tool—governed by the principles of usefulness, credibility, transparency, and independence—for greater accountability and making development assistance work better. "Independent Evaluation at the Asian Development Bank" presents a perspective of evaluation in ADB from the beginnings and looks to a future in which knowledge management plays an increasingly important role.
In recent years, there has been a major shift in the nature of IED's work program from a dominance of evaluations of individual projects to one focusing on broader and more strategic studies. To select priority topics for evaluation studies, IED seeks input from the Development Effectiveness Committee, ADB Management, and the heads of ADB departments and offices. The current thrusts are to improve the quality of evaluations by using more robust methodologies; give priority to country/sector assistance program evaluations; increase the number of joint evaluations; validate self-evaluations to shorten the learning cycle; conduct more rigorous impact evaluations; develop evaluation capacity, both in ADB and in DMCs; promote portfolio performance; evaluate business processes; and disseminate findings and recommendations and ensure their use. IED's work program has also been reinterpreted to emphasize organizational learning in a more clearly defined results architecture and results framework. It entails conducting and disseminating strategic evaluations (in consultation with stakeholders), harmonizing performance indicators and evaluation methodologies, and developing capacity in evaluation and evaluative thinking. All evaluation studies are publicly disclosed on IED's website (some evaluations of private sector operations are redacted to protect commercially confidential information). IED's evaluation resources are displayed by resource type, topic, region and country, and date. Learnings are also gathered in an online Evaluation Information System offering a database of lessons, recommendations, and ADB Management responses to these. Details of ongoing evaluations and updates on their progress are made public too.
Beginning 2006, acting within the knowledge management framework of ADB, IED has applied knowledge management to lesson learning, using knowledge performance metrics.
"Learning Lessons in ADB" sets the strategic framework for knowledge management in operations evaluation. Improvements have been made that hold promise not only in IED but, more importantly, vis-à-vis its interfaces with other departments and offices in ADB, developing member countries, and the international evaluation community. In the medium term, IED will continue to improve the organizational culture, management system, business processes, information technology solutions, community of practice, and external relations and networking for lesson learning. Among the new knowledge products and services developed, "Learning Curves" are handy, two-paged quick references designed to feed findings and recommendations from evaluation to a broader range of clients "Evaluation News" report on events in monitoring and evaluation. "Evaluation Presentations" offer short photographic or Powerpoint displays on evaluation topics. "Auditing the Lessons Architecture" highlights the contribution that knowledge audits can make to organizational learning and organizational health.
Of the 1,106 ADB-funded projects evaluated and rated so far (as of December 2007), 65% were assessed as being successful, 27% partly successful and 8% as unsuccessful.
Criticism.
Since the ADB's early days, critics have charged that the two major donors, Japan and the United States, have had extensive influence over lending, policy and staffing decisions.
Oxfam Australia has criticized the Asian Development Bank of insensitivity to local communities. "Operating at a global and international level, these banks can undermine people's human rights through projects that have detrimental outcomes for poor and marginalized communities." The bank also received criticism from the United Nations Environmental Program, stating in a report that "much of the growth has bypassed more than 70 percent of its rural population, many of whom are directly dependent on natural resources for livelihoods and incomes."
There had been criticism that ADB's large scale projects cause social and environmental damage due to lack of oversight. One of the most controversial ADB-related projects is Thailand's Mae Moh coal-fired power station. Environmental and human rights activists say ADB's environmental safeguards policy as well as policies for indigenous peoples and involuntary resettlement, while usually up to international standards on paper, are often ignored in practice, are too vague or weak to be effective, or are simply not enforced by bank officials.
The bank has been criticized over its role and relevance in the food crisis.The ADB has been accused by civil society of ignoring warnings leading up the crisis and also contributing to it by pushing loan conditions that many say unfairly pressure governments to deregulate and privatize agriculture, leading to problems such as the rice supply shortage in Southeast Asia.
The bank has also been criticized by Vietnam War veterans for funding projects in Laos, because of the United States' 15% stake in the bank, underwritten by taxes. Laos became a communist country after the U.S. withdrew from Vietnam, and the Laotian Civil War was won by the Pathet Lao, which is widely understood to have been supported by the North Vietnamese Army.
In 2009, the bank endorsed a $2.9 billion funding strategy for proposed projects in India. The projects in this strategy were only indicative and still needed to be further approved by the bank's board of directors; however, PRC Foreign Ministry spokesman Qin Gang claimed, "The Asian Development Bank, regardless of the major concerns of China, approved the India Country Partnership strategy which involves the territorial dispute between China and India. China expresses its strong dissatisfaction over this... The bank's move not only seriously tarnishes its own name, but also undermines the interests of its members."
United Nations Development Business.
The United Nations launched Development Business in 1978 with the support of the Asian Development Bank, the World Bank, and many other major development banks from around the world. Today, Development Business is the primary publication for all major multilateral development banks, United Nations agencies, and several national governments, many of whom have made the publication of their tenders and contracts in Development Business a mandatory requirement.
Strategy 2020.
Strategy 2020 is The Long-Term Strategic Framework of the Asian Development and wide strategic framework to guide all its operations to 2020.
Members.
[Development Bank (DMC Stages).png|right|thumb|300px|Asian Development Bank - Developing Member Countries (DMC) graduation stages
ADB has 67 members (as of 2 February 2007): 48 members from the Asian and Pacific Region, 19 members from Other Regions. Notable non-members are Bahrain, the Democratic People's Republic of Korea, Iran, Iraq, Jordan, Kuwait, Lebanon, Oman, Saudi Arabia and Yemen. "Names are as recognized by ADB." The year after a member's name indicates the year of membership. At the time a country ceases to be a member, the Bank shall arrange for the repurchase of such country's shares by the Bank as a part of the settlement of accounts with such country in accordance with the provisions of paragraphs 3 and 4 of Article 43.

</doc>
<doc id="2514" url="http://en.wikipedia.org/wiki?curid=2514" title="Aswan">
Aswan

Aswan (; ' ; Ancient Egyptian: '; '; '), formerly spelled "Assuan", is a city in the south of Egypt, the capital of the Aswan Governorate.
Aswan is a busy market and tourist centre located just north of the Aswan Dams on the east bank of the Nile at the first cataract. The modern city has expanded and includes the formerly separate community on the island of Elephantine.
History.
Aswan is the ancient city of Swenett, which in antiquity was the frontier town of Ancient Egypt facing the south. Swenett is supposed to have derived its name from an Egyptian goddess with the same name. This goddess later was identified as Eileithyia by the Greeks and Lucina by the Romans during their occupation of Ancient Egypt because of the similar association of their goddesses with childbirth, and of which the import is "the opener". The ancient name of the city also is said to be derived from the Egyptian symbol for "trade".
Because the Ancient Egyptians oriented toward the origin of the life-giving waters of the Nile in the south, Swenet was the first town in the country, and Egypt always was conceived to "open" or begin at Swenet. The city stood upon a peninsula on the right (east) bank of the Nile, immediately below (and north of) the first cataract of the flowing waters, which extend to it from Philae. Navigation to the delta was possible from this location without encountering a barrier.
The stone quarries of ancient Egypt located here were celebrated for their stone, and especially for the granitic rock called Syenite. They furnished the colossal statues, obelisks, and monolithal shrines that are found throughout Egypt, including the pyramids; and the traces of the quarrymen who wrought in these 3,000 years ago are still visible in the native rock. They lie on either bank of the Nile, and a road, in length, was cut beside them from Syene to Philae.
Swentet was equally important as a military station as that of a place of traffic. Under every dynasty it was a garrison town; and here tolls and customs were levied on all boats passing southwards and northwards. Around AD 330, the legion stationed here received a bishop from Alexandria; this later became the Coptic Diocese of Syene. The city is mentioned by numerous ancient writers, including Herodotus, Strabo, Stephanus of Byzantium, Ptolemy, Pliny the Elder, De architectura, and it appears on the Antonine Itinerary. It also is mentioned in the Book of Ezekiel and the Book of Isaiah.
The latitude of the city that would become Aswan – located at 24° 5′ 23″ – was an object of great interest to the ancient geographers. They believed that it was seated immediately under the tropic, and that on the day of the summer solstice, a vertical staff cast no shadow. They noted that the sun's disc was reflected in a well at noon. This statement is only approximately correct; at the summer solstice, the shadow was only 1/400th of the staff, and so could scarcely be discerned, and the northern limb of the Sun's disc would be nearly vertical.
Eratosthenes used measurements at Aswan (Elephantine) to determine the circumference of Earth, using Syene as the originating point and Alexandria as the terminal point of a measured arc (based on shadow length at the solstice).
The Nile is nearly wide above Aswan. From this frontier town to the northern extremity of Egypt, the river flows for more than without bar or cataract. The voyage from Aswan to Alexandria usually took 21 to 28 days in favourable weather.
Climate.
Köppen-Geiger climate classification system classifies its climate as hot desert (BWh). Aswan and Luxor have the hottest summer days of any other city in Egypt. Aswan's climate ranges from mild in the winter to very hot in the summer with absolutely no rain all year. There is maybe or of rain every 5 years. Aswan is one of the driest inhabited places in the world; as of early 2001, the last rain there was seven years earlier. , the last rainfall was thunderstorms on May 13, 2006, January 2010 and October 2012 [http://www.egyptindependent.com/news/heavy-rains-sweep-aswan. In Nubian settlements, they generally do not bother to roof all of the rooms in their houses.
In winter the temperatures average from at night to during the day. In summer the temperature averages at night to during the day
The highest record temperature was on May 22, 1973 and the lowest record temperature was on January 6, 1989.
Education.
In 1999, South Valley University was inaugurated and it has three branches; Aswan, Qena and Hurghada. It was the first university in Upper Egypt and it was organized in departmental basis. The university grew steadily and now it is firmly established as a major institution of higher education in Upper Egypt. Aswan branch of Assiut University began in 1973 with the Faculty of Education and in 1975 the Faculty of Science was opened. Aswan branch has five faculties namely; Science, Education, Engineering, Arts, Social Works and Institute of Energy. The Faculty of Science in Aswan has six departments. Each department has one educational programme: Chemistry, Geology, Physics and Zoology. Except Botany Department, which has three educational programmes: Botany, Environmental Sciences and Microbiology; and Mathematics Department, which has two educational programmes: Mathematics and Computer Science. The Faculty of Science awards the following degrees: Bachelor of Science in nine educational programmes, Higher Diploma, Master of Science and Philosophy Doctor of Science. Over 100 academic staff members are employed in.
Transport.
Aswan is served by the Aswan International Airport. Train and bus service is also available. Taxi and rickshaw are used for transport here.

</doc>
<doc id="2519" url="http://en.wikipedia.org/wiki?curid=2519" title="Adelaide of Italy">
Adelaide of Italy

Adelaide of Italy (931 – 16 December 999), also called Adelaide of Burgundy, was the second wife of Holy Roman Emperor Otto the Great and was crowned as the Holy Roman Empress with him by Pope John XII in Rome on February 2, 962. She was the daughter-in-law of St. Queen Matilda of East Francia. Empress Adelaide was perhaps the most prominent European woman of the 10th century; she was regent of the Holy Roman Empire as the guardian of her grandson in 991-995.
Life.
Born in Orbe, today in Switzerland, she was the daughter of Rudolf II of Burgundy, a member of the Elder House of Welf, and Bertha of Swabia. Her first marriage, at the age of fifteen, was to the son of her father's rival in Italy, Lothair II, the nominal King of Italy; the union was part of a political settlement designed to conclude a peace between her father and Hugh of Provence, the father of Lothair. They had a daughter, Emma of Italy.
Marriage to Otto I.
The Calendar of Saints states that her first husband was poisoned by the holder of real power, his successor, Berengar of Ivrea, who attempted to cement his political power by forcing her to marry his son, Adalbert; when she refused and fled, she was tracked down and imprisoned for four months at Como.
According to Adelheid's contemporary biographer, Odilo of Cluny, she managed to escape from captivity. After a time spent in the marshes nearby, she was rescued and taken to a "certain impregnable fortress," likely the fortified town of Canossa near Reggio. She managed to send an emissary to throw herself on the mercy of Otto the Great. His brothers were equally willing to save the dowager queen, but Otto got an army into the field. The widow met her rescuer at the old Lombard capital of Pavia and they married in 951. They had four children, as discussed below. Pope John XII crowned Otto Holy Roman Emperor in Rome on February 2, 962, and, breaking tradition, also crowned Adelheid as Holy Roman Empress.
In Germany, the crushing of a revolt in 953 by Liudolf, Otto's son by his first marriage, cemented Adelheid's position, for she retained all her dower lands. She and their eleven-year old son, the crown prince who became Otto II, accompanied Otto in 966 on his third expedition to Italy, where Otto restored the newly elected Pope John XIII to his throne (and executed some of the Roman rioters who had deposed him). Adelheid remained in Rome for six years while Otto ruled his kingdom from Italy. Their son Otto II was crowned co-emperor in 967, then married the Byzantine princess Theophanu in April 972, resolving the conflict between the two empires in southern Italy, as well as ensuring the imperial succession. Adelheid and her husband then returned to Germany, where Otto died in May 973, at the same Membleben palace where his father had died 37 years earlier.
Retirement.
Adelaide had long entertained close relations with Cluny, then the center of the movement for ecclesiastical reform, and in particular with its abbots Majolus and Odilo. She retired to a nunnery she had founded in c. 991 at Selz in Alsace. Though she never became a nun, she spent the rest of her days there in prayer. On her way to Burgundy to support her nephew Rudolf III against a rebellion, she died at Selz Abbey on December 16, 999, days short of the millennium she thought would bring the Second Coming of Christ. She had constantly devoted herself to the service of the church and peace, and to the empire as guardian of both; she also interested herself in the conversion of the Slavs. She was thus a principal agent—almost an embodiment—of the work of the Catholic Church during the Early Middle Ages in the construction of the religion-culture of western Europe.
A part of her relics are preserved in a shrine in Hanover. Her feast day, December 16, is still kept in many German dioceses.
Issue.
In 947, Adelaide was married to King Lothair II of Italy. The union produced one child:
In 951, Adelaide was married to King Otto I, the future Holy Roman Emperor. The union produced four children:
Legacy.
Adelaïde is the heroine of Gioacchino Rossini's 1817 opera, "Adelaide di Borgogna" and William Bernard McCabe's 1856 novel "Adelaide, Queen of Italy, or The Iron Crown".
Adelaide is a featured figure on Judy Chicago's installation piece "The Dinner Party", being represented as one of the 999 names on the "Heritage Floor."

</doc>
<doc id="2524" url="http://en.wikipedia.org/wiki?curid=2524" title="Airbus A300">
Airbus A300

The Airbus A300 is a short- to medium-range wide-body jet airliner that was developed and manufactured by Airbus. Released in 1972 as the world's first twin-engined widebody, it was the first product of Airbus Industrie, a consortium of European aerospace companies, now a subsidiary of Airbus Group. The A300 can typically seat 266 passengers in a two-class layout, with a maximum range of when fully loaded, depending on model.
Launch customer Air France introduced the type into service on 30 May 1974. Production of the A300 ceased in July 2007, along with its smaller A310 derivative. Freighter sales for which the A300 competed are to be fulfilled by a new A330-200F derivative.
The third production A300 is now a zero gravity plane and travels to airshows around Europe.
Development.
The requirements were stated in 1966 by Frank Kolk, an American Airlines executive, for a Boeing 727 replacement on busy short- to medium-range routes such as United States transcontinental flights. His brief included a passenger capacity of 250 to 300 seated in a twin-aisle configuration and fitted with two engines, with the capability of carrying full passenger loads without penalty from high-altitude airports like Denver. American manufacturers responded with widebody trijets, the McDonnell Douglas DC-10 and the Lockheed L-1011 Tristar, as twinjets were banned from many routes by the FAA.
On 26 September 1967, the British, French, and German governments signed a Memorandum of Understanding to start development of the 300-seat Airbus A300. An earlier announcement had been made in July 1967, but at that time the announcement had been clouded by the British Government's support for the Airbus, which coincided with its refusal to back British Aircraft Corporation's (BAC) proposed competitor, a development of the BAC 1-11, despite a preference for the latter expressed by British European Airways (BEA).
In the months following this agreement, both the French and British governments expressed doubts about the aircraft. Another problem was the requirement for a new engine to be developed by Rolls-Royce; the triple-spool RB207 of 47,500 lbf. In December 1968, the French and British partner companies (Sud Aviation and Hawker Siddeley) proposed a revised configuration, the 250-seat Airbus A250. Renamed the A300B, the aircraft would not require new engines, reducing development costs. To attract potential US customers, American General Electric CF6-50 engines powered the A300 instead of the British RB207. The British government withdrew from the venture; however, the British firm Hawker-Siddeley stayed on as a contractor, developing the wings for the A300, which were pivotal in later versions' impressive performance from short domestic to long intercontinental flights. (Years later, through British Aerospace, the UK re-entered the consortium.)
Airbus Industrie was formally set up in 1970 following an agreement between Aérospatiale (France) and the antecedents to Deutsche Aerospace (Germany). They were joined by the Spanish CASA in 1971. Each company would deliver its sections as fully equipped, ready-to-fly assemblies.
The first prototype A300 was unveiled on 28 September 1972, making its maiden flight from Toulouse–Blagnac Airport on 28 October that year, which was later commemorated on a French three franc stamp. The first production model, the A300B2, entered service in 1974 followed by the A300B4 one year later. Initially the success of the consortium was poor, but by 1979 there were 81 aircraft in service. It was the launch of the A320 in 1987 that established Airbus in the aircraft market – over 400 orders were placed before it flew, compared to 15 for the A300 in 1972.
The A300 was the first airliner to use just-in-time manufacturing techniques. Complete aircraft sections were manufactured by consortium partners all over Europe. These were airlifted to the final assembly line at Toulouse-Blagnac by a fleet of Boeing 377-derived Aero Spacelines Super Guppy aircraft. Originally devised as a way to share the work among Airbus' partners without the expense of two assembly lines, it turned out to be a more efficient way of building aircraft (more flexible and reduced costs) as opposed to building the whole aircraft at one site.
Design.
Airbus partners employed the latest technology, some derived from the Concorde. On entry into service in 1974, the A300 was a very advanced plane and influenced later subsonic airliner designs. The technological highlights include:
Later A300s incorporate other advanced features such as:
All these made the A300 a more economical substitute for widebody trijets such as McDonnell Douglas DC-10 and Lockheed L-1011 for short to medium routes. On the early versions, Airbus used the same engines and similar major systems as the DC-10.
Operational history.
After the launch, sales of the A300 were weak for some years, with most orders going to airlines that had an obligation to favour the domestically made product – notably Air France and Lufthansa. At one stage, Airbus had 16 "whitetail" A300s – completed but unsold aircraft – sitting on the tarmac. Germanair was the world's first charter airline and Indian Airlines was the world's first domestic airline to purchase the A300. These have now been retired.
In 1974, Korean Air ordered 4 A300s, becoming the first non-European international airline to order Airbus aircraft. Airbus saw South-East Asia as a vital market ready to be opened up and believed Korean Air to be the 'key'.
It was becoming clear that the whole concept of a short haul widebody was flawed. Airlines operating the A300 on short haul routes were forced to reduce frequencies to try and fill the aircraft. As a result they lost passengers to airlines operating more frequent narrow body flights. The supposed widebody comfort which it was assumed passengers would demand was illusory. Eventually, Airbus had to build its own narrowbody aircraft (the A320) to compete with the Boeing 737 and McDonnell Douglas DC-9/MD-80. The saviour of the A300 was the advent of Extended Range Twin Operations (ETOPS), a revised FAA rule which allows twin-engined airliners to fly long-distance routes that were previously off-limits to them. This enabled Airbus to develop the aircraft as a medium/long range airliner.
In 1977, US carrier Eastern Air Lines leased four A300s as an in-service trial. Frank Borman, ex-astronaut and the then CEO, was impressed that the A300 consumed 30% less fuel than his fleet of Tristars and then ordered 23 of the type (This order is often cited as the point at which Airbus came to be seen as a serious competitor to the large American aircraft-manufacturers Boeing and McDonnell Douglas). This was followed by an order from Pan Am. From then on, the A300 family sold well, eventually reaching a total of 878 delivered aircraft.
In December 1977, AeroCóndor Colombia became the first Airbus operator in Latin America, leasing one Airbus A300, named "Ciudad de Barranquilla".
The aircraft found particular favour with Asian airlines, being bought by Japan Air System, Korean Air, China Eastern Airlines, Thai Airways International, Singapore Airlines, Malaysia Airlines, Philippine Airlines, Garuda Indonesia, China Airlines, Pakistan International Airlines, Indian Airlines, Trans Australia Airlines and many others. As Asia did not have restrictions similar to the FAA 60-minutes rule for twin-engine airliners which existed at the time, Asian airlines used A300s for routes across the Bay of Bengal and South China Sea.
In 1977, the A300B4 became the first ETOPS compliant aircraft – its high performance and safety standards qualified it for Extended Twin Engine Operations over water, providing operators with more versatility in routing. In 1982 Garuda Indonesia became the first airline to fly the A300B4-200FF. By 1981, Airbus was growing rapidly, with over 300 aircraft sold and options for 200 more planes for over forty airlines. Alarmed by the success of the A300, Boeing responded with the new Boeing 767.
The A300 provided Airbus the experience of manufacturing and selling airliners competitively. The basic fuselage of the A300 was later stretched (A330 and A340), shortened (A310), or modified into derivatives (A300-600ST "Beluga" Super Transporter).The largest freight operator of the A300 is FedEx Express, which, as of January 2012, had 71 A300 aircraft in service. UPS Airlines also operates freighter versions of the A300. The final version was the A300-600R and is rated for 180-minute ETOPS. The A300 has enjoyed renewed interest in the secondhand market for conversion to freighters. The freighter versions – either new-build A300-600s or converted ex-passenger A300-600s, A300B2s and B4s – account for most of the world freighter fleet after the Boeing 747 freighter.
In March 2006 Airbus announced the closure of the A300/A310 line making them the first Airbus aircraft to be discontinued. The final production A300 made its initial flight on 18 April 2007 and was delivered on 12 July 2007. It was an A300F freighter for FedEx. Airbus has announced a support package to keep A300s flying commercially until at least 2025.
Variants.
A300B1.
Only two were built: the first prototype, registered F-WUAB, then F-OCAZ, and a second aircraft, F-WUAC, which was leased in November 1974 to Trans European Airways (TEA) and re-registered OO-TEF. TEA instantly subleased the aircraft for six weeks to Air Algérie, but continued to operate the aircraft until 1990. It had accommodation for 300 passengers (TEA) or 323 passengers (Air Algérie) with a maximum weight of 132,000 kg and two General Electric CF6-50A engines of 220 kN thrust.
A300B2.
The first production version. Powered by General Electric CF6 or Pratt & Whitney JT9D engines (the same engines that powered the Boeing 747-100, "the original jumbo jet") of between 227 and 236 kN thrust, it entered service with Air France in May 1974. The prototype A300B2 made its first flight on 28 June 1973 and was certificated by the French and German authorities on 15 March 1974 and FAA approval followed on 30 May 1974. The first production A300B2 (A300 number 5) made its maiden flight on 15 April 1974 and was handed over to Air France a few weeks later on 10 May 1974. The A300B2 entered revenue service on 23 May 1974 between Paris and London.
A300B4.
The major production version. Features a centre fuel tank for increased fuel capacity (47,500 kg). Production of the B2 and B4 totalled 248. The first A300B4 (the 9th A300) flew on 25 December 1974 and was certified on 26 March 1975. The first delivery was made to Germanair (which later merged into Hapag Lloyd) on 23 May 1975.
A300-600.
Officially designated A300B4-600, this version is nearly the same length as the B2 and B4 but has increased space because it uses the A310 rear fuselage and tail. It has higher power CF6-80 or Pratt & Whitney PW4000 engines and uses the Honeywell 331-250 auxiliary power unit (APU). The A300-600 entered service in 1983 with Saudi Arabian Airlines and a total of 313 A300-600s (all versions) have been sold. The A300-600 also has a similar cockpit to the A310, eliminating the need for a flight engineer. The FAA issues a single type rating which allows operation of both the A310 and A300-600.
A300B10 (A310).
Introduced a shorter fuselage, a new, higher aspect ratio wing, smaller tail and two crew operation. It is available in standard −200 and the Extended range −300 with range in both passenger and full cargo versions.
It is also available as a military tanker/transport serving the Canadian Forces and German Air Force. Sales total 260, although five of these (ordered by Iraqi Airways) were never built.
Incidents and accidents.
As of Aug. 2013, the A300 has been involved in 60 accidents and incidents, including 31 hull-losses and 1,436 fatalities.
Aircraft preserved.
Four A300s are preserved today:
Specifications.
Sources:
Deliveries.
"Data through end of December 2007."

</doc>
<doc id="2526" url="http://en.wikipedia.org/wiki?curid=2526" title="Agostino Carracci">
Agostino Carracci

Agostino Carracci (or Caracci) (16 August 1557 – 22 March 1602) was an Italian painter and printmaker. He was the brother of the more famous Annibale and cousin of Lodovico Carracci. 
He posited the ideal in nature, and was the founder of the competing school to the more gritty view of nature as expressed by Caravaggio. He was one of the founders of the Accademia degli Incamminati along with his brother, Annibale Carracci, and cousin, Ludovico Carracci. The academy helped propel painters of the School of Bologna to prominence.
Life.
Agostino Carracci was born in Bologna, and trained at the workshop of the architect Domenico Tibaldi. Starting from 1574 he worked as a reproductive engraver, copying works of 16th century masters such as Federico Barocci, Tintoretto, Antonio Campi, Veronese and Correggio. He also produced some original prints, including two etchings.
He travelled to Venice (1582, 1587–1589) and Parma (1586–1587). Together with Annibale and Ludovico he worked in Bologna on the fresco cycles in Palazzo Fava ("Histories of Jason and Medea", 1584) and Palazzo Magnani ("Histories of Romulus", 1590–1592). In 1592 he also painted the "Communion of St. Jerome", now in the Pinacoteca di Bologna and considered his masterwork. From 1586 is his altarpiece of the "Madonna with Child and Saints", in the National Gallery of Parma.
In 1598 Carracci joined his brother Annibale in Rome, to collaborate on the decoration of the Gallery in Palazzo Farnese. From 1598–1600 is a "triple Portrait", now in Naples, an example of genre painting. 
In 1600 he was called to Parma by Duke Ranuccio I Farnese to began the decoration of the Palazzo del Giardino, but he died before it was finished.
Agostino's son Antonio Carracci was also a painter, and attempted to compete with his father's Academy.

</doc>
<doc id="2528" url="http://en.wikipedia.org/wiki?curid=2528" title="Adenylate cyclase">
Adenylate cyclase

Adenylate cyclase (, also commonly known as adenylyl cyclase) is an enzyme with key regulatory roles in essentially all cells. It is the most polyphyletic known enzyme: six distinct classes have been described, all catalyzing the same reaction but representing unrelated gene families with no known sequence or structural homology. The best known AC class is class III or AC-III (Roman numerals are used for classes). AC-III occurs widely in eukaryotes and has important roles in many human tissues.
All classes of AC catalyze the conversion of ATP to 3',5'-cyclic AMP (cAMP) and pyrophosphate. Divalent cations (usually Mg) are generally required and appear to be closely involved in the enzymatic mechanism. The cAMP produced by AC then serves as a regulatory signal via specific cAMP-binding proteins, either transcription factors or other enzymes (e.g., cAMP-dependent kinases).
Class I AC.
Class I AC's occur in many bacteria including "E. coli". This was the first class of AC to be characterized. It was observed that "E. coli" deprived of glucose produce cAMP that serves as an internal signal to activate expression of genes for importing and metabolizing other sugars. cAMP exerts this effect by binding the transcription factor CRP, also known as CAP. Class I AC's are large cytosolic enzymes (~100 kDa) with a large regulatory domain (~50 kDa) that indirectly senses glucose levels. As of 2012, no crystal structure is available for class I AC.
Class II AC.
These AC's are toxins secreted by pathogenic bacteria such as "Bacillus anthracis" and "Bordetella pertussis" during infection. These bacteria also secrete proteins that enable the AC-II to enter host cells, where the exogenous AC activity undermines normal cellular processes. The genes for Class II AC's are known as cyaA. Several crystal structures are known for AC-II enzymes.
Class III AC.
These AC's are the most familiar based on extensive study due to their important roles in human health. They are also found in some bacteria, notably "Mycobacterium tuberculosis" where they appear to have a key role in pathogenesis. Most AC-III's are integral membrane proteins involved in transducing extracellular signals into intracellular responses. A Nobel Prize was awarded to Earl Sutherland in 1971 for discovering the key role of AC-III in human liver, where adrenaline indirectly stimulates AC to mobilize stored energy in the "fight or flight" response. The effect of adrenaline is via a G protein signaling cascade, which transmits chemical signals from outside the cell across the membrane to the inside of the cell (cytoplasm). The outside signal (in this case, adrenaline) binds to a receptor, which transmits a signal to the G protein, which transmits a signal to adenylate cyclase, which transmits a signal by converting adenosine triphosphate to cyclic adenosine monophosphate (cAMP). cAMP is known as a second messenger.
cAMP (cyclic adenosine monophosphate) is an important molecule in eukaryotic signal transduction, a so-called second messenger. Adenylate cyclases are often activated or inhibited by G proteins, which are coupled to membrane receptors and thus can respond to hormonal or other stimuli. Following activation of adenylate cyclase, the resulting cAMP acts as a second messenger by interacting with and regulating other proteins such as protein kinase A and cyclic nucleotide-gated ion channels.
Photoactivatable adenylate cyclase (PAC) was discovered in "E. gracilis" and can be expressed in other organisms through genetic manipulation. Shining blue light on a cell containing PAC activates it and abruptly increases the rate of conversion of ATP to cAMP. This is a useful technique for researchers in neuroscience because it allows them to quickly increase the intracellular cAMP levels in particular neurons, and to study the effect of that increase in neural activity on the behavior of the organism. For example, PAC expression in certain neurons has been shown to alter the grooming behavior in fruit flies exposed to blue light. Channelrhodopsin-2 is also used in a similar fashion.
AC-III structure.
Most class III adenylyl cyclases are transmembrane proteins with 12 transmembrane segments. The protein is organized with 6 transmembrane segments, then the C1 cytoplasmic domain, then another 6 membrane segments, and then a second cytoplasmic domain called C2. The important parts for function are the N-terminus and the C1 and C2 regions. The C1a and C2a subdomains are homologous and form an intramolecular 'dimer' that forms the active site. In "Mycobacterium tuberculosis", the AC-III polypeptide is only half as long, comprising one 6-transmembrane domain followed by a cytoplasmic domain, but two of these form a functional homodimer that resembles the mammalian architecture.
Types of AC-III.
There are ten known isoforms of adenylate cyclases in mammals:
These are also sometimes called simply AC1, AC2, etc., and, somewhat confusingly, sometimes Roman numerals are used for these isoforms that all belong to the overall AC class III. They differ mainly in how they are regulated, and are differentially expressed in various tissues throughout mammalian development.
AC-III regulation.
Adenylate cyclase is dually regulated by G proteins (Gs stimulating activity and Gi inhibiting it), and by forskolin, as well as other isoform-specific effectors:
In neurons, calcium-sensitive adenylate cyclases are located next to calcium ion channels for faster reaction to Ca2+ influx; they are suspected of playing an important role in learning processes. This is supported by the fact that adenylate cyclases are "coincidence detectors", meaning that they are activated only by several different signals occurring together. In peripheral cells and tissues adenylate cyclases appear to form molecular complexes with specific receptors and other signaling proteins in an isoform-specific manner.
Class IV.
AC-IV was first reported in the bacterium "Aeromonas hydrophila", and the structure of the AC-IV from "Yersinia pestis" has been reported. These are the smallest of the AC enzyme classes; the AC-IV from "Yersinia" is a dimer of 19 kDa subunits with no known regulatory components.
Class V and VI.
These forms of AC have been reported in specific bacteria ("Prevotella ruminicola" and "Rhizobium etti", respectively) and have not been extensively characterized.

</doc>
<doc id="2529" url="http://en.wikipedia.org/wiki?curid=2529" title="Alexandra">
Alexandra

Alexandra (Greek: ) is the feminine form of the given name Alexander, which is a romanization of the Greek name ("Alexandros"). Etymologically, the name is a compound of the Greek verb ("alexein") "to defend" and ("andros"), the genitive of the noun ("anēr") "man". Thus it may be roughly translated as "defender of man" or "protector of man". The name was one of the titles or epithets given to the Greek goddess Hera and as such is usually taken to mean "one who comes to save warriors". The earliest attested form of the name is the Mycenaean Greek , "a-re-ka-sa-da-ra", written in the Linear B syllabic script.

</doc>
<doc id="2536" url="http://en.wikipedia.org/wiki?curid=2536" title="Articolo 31">
Articolo 31

Articolo 31 was a popular band from Milan, Italy, melding hip hop, funk, pop and traditional Italian musical forms. They are one of the most popular Italian hip hop groups.
Band history.
Members are rapper J-Ax (real name Alessandro Aleotti) and DJ Jad (Vito Luca Perrini).
In the spoken intro of "Strade di Città", it is stated that the band is named after the article of the Irish constitution guaranteeing freedom of the press, although it must be noted that article 31 of the Irish constitution is not about the freedom of the press. They probably meant the Section 31 of the Broadcasting Authority Act.
Articolo 31 was one of the first hip hop groups in Italy, releasing one of the first Italian hip hop records, "Strade di città", in 1993. Soon, they signed with BMG Ricordi and started to mix their rap with pop music, obtaining great success and popularity. However, the underground hip hop scene positioned them as traitors. Their producer was Franco Godi, who also produced the music for the "Signor Rossi" animated series.
In 1997, DJ Gruff dissed Articolo 31 in a track titled "1 vs 2" appearing on the first album of the beatmaker Fritz da Cat, but Articolo 31's lawyers obtained a retraction of the record, and its reissue without the song.
In 2001, Articolo 31 collaborated with the American old school rapper Kurtis Blow on the album "XChé SI!". In the same year, they made the movie "Senza filtro" (in English, ""Without filter"").
With "Domani smetto", Articolo 31 left the world of hip hop and moved to rap with many influences; even more so than previously, this album found them rapping and singing to rock and pop tracks.
In recent years, J Ax and DJ Jad have each released solo records, and in 2006, the group declared an indefinite hiatus.
Their posse, "Spaghetti Funk", includes other popular performers like Space One and pop rappers Gemelli DiVersi.

</doc>
<doc id="2543" url="http://en.wikipedia.org/wiki?curid=2543" title="Alexander Kerensky">
Alexander Kerensky

Alexander Fyodorovich Kerensky (, ;  – 11 June 1970) was a lawyer and major political leader before the Russian Revolutions of 1917 belonging to a moderate socialist party, called Trudoviks.
After the February Revolution Kerensky served as Minister of Justice in the democratic Russian Provisional Government. In May he became Minister of War. In July he became the second Prime Minister until it was overthrown by the Bolsheviks in the October Revolution. He spent the remainder of his life in exile, dying in New York City at the age of 89.
Life and career.
Early life and activism.
Alexander Kerensky was born in Simbirsk (now Ulyanovsk) on the Volga River on 2 May 1881. His father, Fyodor Kerensky, was a teacher. and director of the local gymnasium. His mother, Nadezhda née Adler, was the daughter of a nobleman, Alexander Adler, head of the Topographical Bureau of the Kazan Military District.
Kerensky's father was the teacher of Vladimir Ulyanov (Lenin); members of the Kerensky and Ulyanov families were friends. In 1889, when Kerensky was eight, the family moved to Tashkent, where his father had been appointed the main inspector of public schools (superintendent). Alexander graduated with honors in 1899. The same year he entered St. Petersburg University, where he studied history and philology. The next year he switched to law and received a degree in 1904. In the same year he got married to Olga Lvovna Baranovskaya, the daughter of a Russian general. Kerensky joined the Narodnik and worked as a legal counsel to victims of Revolution of 1905. At the end of year he was jailed on suspicion of belonging to a militant group. Afterwards he gained a reputation for his work as a defense lawyer in a number of political trials of revolutionaries. In 1912 Kerensky became widely known when he visited the goldfields at the Lena River and published material about the Lena Minefields incident. He was a brilliant orator and skilled parliamentary leader of the socialist opposition to the government of Tsar Nicholas II.
Rasputin.
On 1 November 1916, at the opening of the Duma, Kerensky called the ministers "hired assassins" and "cowards" and said they were "guided by the contemptible Grishka Rasputin!"Mikhail Rodzianko, Zinaida Yusupova, Alexandra's sister Elisabeth, Grand Duchess Victoria and the Tsar's mother also tried to influence the Emperor or his stubborn wife to remove Grigori Rasputin, but without success. According to Kerensky, Rasputin had terrorized the Tsarina by threatening to return to his native village. 
People around Rasputin (his secretaries) were interested in strategic information.
After Rasputin had been murdered and buried in Tsarskoye Selo a group of soldiers were ordered by Kerensky to rebury the corpse at an unmarked spot in the countryside. But the truck broke down or was forced to stop because of the snow on Lesnoe Road.
February Revolution of 1917.
When the February Revolution broke out in 1917, Kerensky was one of its most prominent leaders: he was a member of the Provisional Committee of the State Duma and was elected vice-chairman of the Petrograd Soviet, and simultaneously became the first Minister of Justice in the newly formed Provisional Government. When the Soviet passed a resolution prohibiting its leaders from joining the government, Kerensky delivered a stirring speech at a Soviet meeting. Although the decision was never formalized, he was granted a "de facto" exemption and continued acting in both capacities.
After the first government crisis over Pavel Milyukov's secret note re-committing Russia to its original war aims on 2–4 May, Kerensky became the Minister of War and the dominant figure in the newly formed socialist-liberal coalition government. On 10 May (Julian calendar), Kerensky started for the front, and visited one division after another, urging the men to do their duty. His speeches were impressive and convincing for the moment, but had little lasting effect. Under Allied pressure to continue the war, he launched what became known as the Kerensky Offensive against the Austro-Hungarian/German South Army on 17 June (Julian calendar). At first successful, the offensive was soon stopped and then thrown back by a strong counter-attack. The Russian army suffered heavy losses and it was clear from the many incidents of desertion, sabotage, and mutiny that the army was no longer willing to attack.
Kerensky was heavily criticised by the military for his liberal policies, which included stripping officers of their mandates (handing overriding control to revolutionary inclined "soldier committees" instead), the abolition of the death penalty, and allowing various revolutionary agitators to be present at the front. Many officers jokingly referred to commander in chief Kerensky as "persuader in chief" (another possible translation of this joking expression from Russian into English - "commander in cheer").
On 2 July 1917, the first coalition collapsed over the question of Ukraine's autonomy. Following July Days unrest in Petrograd and suppression of the Bolsheviks, Kerensky succeeded Prince Lvov as Russia's Prime Minister. Following the Kornilov Affair at the end of August and the resignation of the other ministers, he appointed himself Supreme Commander-in-Chief as well.
Kerensky's next move, on 15 September, was to proclaim Russia a republic, which was quite contrary to the understanding that the Provisional Government should hold power only until the Constituent Assembly should meet to decide Russia's form of rule. He formed a five-member Directory, which consisted of himself, minister of foreign affairs Mikhail Tereshchenko, minister of war General Verkhovsky, minister of the navy Admiral Dmitry Verderevsky and minister of post and telegraph Nikitin. He retained his post in the final coalition government in October 1917 until it was overthrown by the Bolsheviks.
Kerensky's major challenge was that Russia was exhausted after three years of war, while the provisional government offered little motivation for a victory outside of continuing Russia's obligations towards its allies. Russia's continued involvement in the world war was not popular among the lower and middle classes and especially the soldiers. They had all believed that Russia would stop fighting when the Provisional Government took power, and now they felt deceived. Furthermore, Vladimir Lenin and his Bolshevik party were promising "peace, land, and bread" under a communist system. The army was disintegrating owing to a lack of discipline, leading to desertion in large numbers. By autumn 1917, an estimated two million men had unofficially left the army.
Kerensky and the other political leaders continued their obligation to Russia's allies by continuing involvement in World War I, fearing that the economy, already under huge stress from the war effort, might become increasingly unstable if vital supplies from France and the United Kingdom were cut off. The dilemma of whether to withdraw was a great one, and Kerensky's inconsistent and impractical policies further destabilized the army and the country at large.
Furthermore, Kerensky adopted a policy that isolated the right-wing conservatives, both democratic and monarchist-oriented. His philosophy of "no enemies to the left" greatly empowered the Bolsheviks and gave them a free hand, allowing them to take over the military arm or "voyenka" of the Petrograd and Moscow Soviets. His arrest of Kornilov and other officers left him without strong allies against the Bolsheviks, who ended up being Kerensky's strongest and most determined adversaries, as opposed to the right wing, which evolved into the White movement.
October Revolution of 1917.
During the Kornilov Affair, Kerensky had distributed arms to the Petrograd workers, and by November most of these armed workers had gone over to the Bolsheviks. On 1917 the Bolsheviks launched the second Russian revolution of the year. Kerensky's government in Petrograd had almost no support in the city. Only one small force, a 137 soldier strong subdivision of 2nd company of the First Petrograd Women's Battalion, also known as The Women's Death Battalion, was willing to fight for the government against the Bolsheviks, as the battalion commander ordered the majority of the troops back to their encampment, but this force was overwhelmed by the numerically superior pro-Bolshevik forces and defeated and captured. It took less than 20 hours before the Bolsheviks had taken over the government.
Kerensky escaped the Bolsheviks and went to Pskov, where he rallied some loyal troops for an attempt to retake the city. His troops managed to capture Tsarskoe Selo, but were beaten the next day at Pulkovo. Kerensky narrowly escaped, and spent the next few weeks in hiding before fleeing the country, eventually arriving in France. During the Russian Civil War he supported neither side, as he opposed both the Bolshevik regime and the White Movement.
Life in exile.
Kerensky lived in Paris until 1940, engaged in the endless splits and quarrels of the exiled Russian politicians. In 1939, Kerensky married the former Australian journalist Lydia "Nell" Tritton. When Germany occupied France in 1940, they emigrated to the United States. Tritton and Kerensky married at Martins Creek, Pennsylvania.
When his wife became terminally ill in 1945, he travelled with her to Brisbane, Australia, and lived there with her family. She suffered a stroke in February, and he remained there until her death on 10 April 1946. Kerensky returned to the United States, where he spent the rest of his life.
After the German-led invasion of the Soviet Union in 1941, Kerensky offered his support to Joseph Stalin.
Kerensky eventually settled in New York City, but spent much of his time at the Hoover Institution at Stanford University in California, where he both used and contributed to the Institution's huge archive on Russian history, and where he taught graduate courses. He wrote and broadcast extensively on Russian politics and history. His last public speech was delivered at Kalamazoo College, in Kalamazoo, Michigan.
Kerensky died at his home in New York City in 1970, one of the last surviving major participants in the turbulent events of 1917. The local Russian Orthodox Churches in New York refused to grant Kerensky burial, because of his Freemasonry and because it saw him as largely responsible for Russia falling to the Bolsheviks. A Serbian Orthodox Church also refused burial. Kerensky's body was flown to London where he was buried at Putney Vale's non-denominational cemetery.
Personal life.
Kerensky was married to Olga Lvovna Baranovskaya and they had two sons, Oleg and Gleb, who both went on to become engineers. Kerensky and Olga were divorced in 1939 and soon after he married Lydia Ellen (Nelle) Tritton (1899-1946).

</doc>
<doc id="2544" url="http://en.wikipedia.org/wiki?curid=2544" title="Ansgar">
Ansgar

Saint Ansgar (8 September 801 – 3 February 865), also known as Saint Anschar, was an Archbishop of Hamburg-Bremen. The see of Hamburg was designated a mission to bring Christianity to Northern Europe, and Ansgar became known as the "Apostle of the North".
Life.
Ansgar was the son of a noble family, born near Amiens. After his mother’s early death, Ansgar was brought up in Corbie Abbey, and made rapid progress in his education. According to the "Vita Ansgarii" ("Life of Ansgar"), when the little boy learned in a vision that his mother was in the company of Saint Mary, his careless attitude toward spiritual matters changed to seriousness ("Life of Ansgar", 1). His pupil, successor, and eventual biographer Rimbert considered the visions of which this was the first to be the main motivation of the saint's life. 
Ansgar was a product of the phase of Christianization of Saxony (present day Northern Germany) begun by Charlemagne and continued by his son and successor, Louis the Pious. When Saxony was no longer the focus of Christianization, what is now Denmark fell under the sweeping missionary gaze, with a group of monks including Ansgar sent back to Jutland with the baptized exiled king Harald Klak. Ansgar returned two years later after educating young boys who had been purchased because Harald had possibly been driven out of his kingdom. In 822 Ansgar was one of a number of missionaries sent to found the abbey of Corvey (New Corbie) in Westphalia, and there became a teacher and preacher. Then in 829 in response to a request from the Swedish king Björn at Hauge for a mission to the Swedes, Louis appointed Ansgar missionary. With an assistant, the friar Witmar, he preached and made converts for six months at Birka, on Lake Mälaren. They organized a small congregation there with the king's steward, Hergeir, as its most prominent member. In 831 he returned to Louis' court at Worms and was appointed to the Archbishopric of Hamburg.
This was a new archbishopric with a see formed from those of Bremen and Verden, plus the right to send missions into all the northern lands and to consecrate bishops for them. Ansgar was consecrated in November 831, and, the arrangements having been at once approved by Gregory IV, he went to Rome to receive the pallium directly from the hands of the pope and to be named legate for the northern lands. This commission had previously been bestowed upon Ebbo, Archbishop of Reims, but the jurisdiction was divided by agreement, with Ebbo retaining Sweden for himself. For a time Ansgar devoted himself to the needs of his own diocese, which was still missionary territory with but a few churches. He founded a monastery and a school in Hamburg; the school was intended to serve the Danish mission, but accomplished little. 
After Louis died in 840, his empire was divided and Ansgar lost the abbey of Turholt, which had been given as an endowment for his work. Then in 845, the Danes unexpectedly raided Hamburg, destroying all the church's treasures and books and leaving the entire diocese unrestorable. Ansgar now had neither see nor revenue. Many of his helpers deserted him, but the new king, Louis the German, came to his aid; after failing to recover Turholt for him, in 847 he awarded him the vacant diocese of Bremen, where he took up residence in 848. However, since Hamburg had been an archbishopric, the sees of Bremen and Hamburg were combined for him. This presented canonical difficulties and also aroused the anger of the Bishop of Cologne, to whom Bremen had been suffragan, but after prolonged negotiations, Pope Nicholas I approved the union of the two dioceses in 864.
Through all this political turmoil, Ansgar continued his mission to the northern lands. The Danish civil war compelled him to establish good relations with two kings, Horik the Elder and his son, Horik II. Both assisted him until his death (Wood, 124-125). He was able to secure recognition of Christianity as a tolerated religion and permission to build a church in Sleswick. He did not forget the Swedish mission, and spent two years there in person (848-850), at the critical moment when a pagan reaction was threatened, which he succeeded in averting. In 854, Ansgar returned to Sweden. Now king Olof ruled in Birka. According to Rimbert, he was well disposed to Christianity. On a Viking raid to Apuole (current village in Lithuania) in Courland, the Swedes prayed, and with God's help they plundered the Curonians
Ansgar died and was buried in Bremen in 865. 
His life story was written by his successor as archbishop, Rimbert, in the "Vita Ansgarii".
Visions.
Although a historical document and primary source written by a man whose existence can be proven historically, the "Vita Ansgarii" ("The Life of Ansgar") aims above all to demonstrate Ansgar's sanctity. It is partly concerned with Ansgar's visions, which, according to the author Rimbert, encouraged and assisted Ansgar's remarkable missionary feats. 
Through the course of this work, Ansgar repeatedly embarks on a new stage in his career following a vision. According to Rimbert, his early studies and ensuing devotion to the ascetic life of a monk were inspired by a vision of his mother in the presence of Saint Mary. Again, when the Swedish people were left without a priest for some time, he begged King Horik to help him with this problem; then after receiving his consent, consulted with Bishop Gautbert to find a suitable man. The two together sought the approval of King Louis, which he granted when he learned that they were in agreement on the issue. Ansgar was convinced he was commanded by heaven to undertake this mission, and was influenced by a vision he received when he was concerned about the journey, in which he met a man who reassured him of his purpose and informed him of a prophet that he would meet, the abbot Adalhard, who would instruct him in what was to happen. In the vision, he searched for and found Adalhard, who commanded, "Islands, listen to me, pay attention, remotest peoples", which Ansgar interpreted as God’s will that he go to the Scandinavian countries as "most of that country consisted of islands, and also, when 'I will make you the light of the nations so that my salvation may reach to the ends of the earth' was added, since the end of the world in the north was in Swedish territory". 
Statues dedicated to him stand in Hamburg and Copenhagen as well as a stone cross at Birka. A crater on the Moon, Ansgarius, has been named for him. His feast day is 3 February.
External links.
<br>

</doc>
<doc id="2546" url="http://en.wikipedia.org/wiki?curid=2546" title="Automated theorem proving">
Automated theorem proving

Automated theorem proving (also known as ATP or automated deduction) is a subfield of automated reasoning and mathematical logic dealing with proving a limited set of mathematical theorems by computer programs. Automated reasoning over mathematical proof was a major impetus for the development of computer science.
Logical foundations.
While the roots of formalised logic go back to Aristotle, the end of the 19th and early 20th centuries saw the development of modern logic and formalised mathematics. Frege's "Begriffsschrift" (1879) introduced both a complete propositional calculus and what is essentially modern predicate logic. His "Foundations of Arithmetic", published 1884, expressed (parts of) mathematics in formal logic. This approach was continued by Russell and Whitehead in their influential "Principia Mathematica", first published 1910–1913, and with a revised second edition in 1927. Russell and Whitehead thought they could derive all mathematical truth using axioms and inference rules of formal logic, in principle opening up the process to automatisation. In 1920, Thoralf Skolem simplified a previous result by Leopold Löwenheim, leading to the Löwenheim–Skolem theorem and, in 1930, to the notion of a Herbrand universe and a Herbrand interpretation that allowed (un)satisfiability of first-order formulas (and hence the validity of a theorem) to be reduced to (potentially infinitely many) propositional satisfiability problems.
In 1929, Mojżesz Presburger showed that the theory of natural numbers with addition and equality (now called Presburger arithmetic in his honor) is decidable and gave an algorithm that could determine if a given sentence in the language was true or false.
However, shortly after this positive result, Kurt Gödel published "On Formally Undecidable Propositions of Principia Mathematica and Related Systems" (1931), showing that in any sufficiently strong axiomatic system there are true statements which cannot be proved in the system. This topic was further developed in the 1930s by Alonzo Church and Alan Turing, who on the one hand gave two independent but equivalent definitions of computability, and on the other gave concrete examples for undecidable questions.
First implementations.
Shortly after World War II, the first general purpose computers became available. In 1954, Martin Davis programmed Presburger's algorithm for a JOHNNIAC vacuum tube computer at the Princeton Institute for Advanced Study. According to Davis, "Its great triumph was to prove that the sum of two even numbers is even". More ambitious was the Logic Theory Machine, a deduction system for the propositional logic of the "Principia Mathematica", developed by Allen Newell, Herbert A. Simon and J. C. Shaw. Also running on a JOHNNIAC, the Logic Theory Machine constructed proofs from a small set of propositional axioms and three deduction rules: modus ponens, (propositional) variable substitution, and the replacement of formulas by their definition. The system used heuristic guidance, and managed to prove 38 of the first 52 theorems of the "Principia".
The "heuristic" approach of the Logic Theory Machine tried to emulate human mathematicians, and could not guarantee that a proof could be found for every valid theorem even in principle. In contrast, other, more systematic algorithms achieved, at least theoretically, completeness for first-order logic. Initial approaches relied on the results of Herbrand and Skolem to convert a first-order formula into successively larger sets of propositional formulae by instantiating variables with terms from the Herbrand universe. The propositional formulas could then be checked for unsatisfiability using a number of methods. Gilmore's program used conversion to disjunctive normal form, a form in which the satisfiability of a formula is obvious.
Decidability of the problem.
Depending on the underlying logic, the problem of deciding the validity of a formula varies from trivial to impossible. For the frequent case of propositional logic, the problem is decidable but Co-NP-complete, and hence only exponential-time algorithms are believed to exist for general proof tasks. For a first order predicate calculus, with no ("proper") axioms, Gödel's completeness theorem states that the theorems (provable statements) are exactly the logically valid well-formed formulas, so identifying valid formulas is recursively enumerable: given unbounded resources, any valid formula can eventually be proven.
However, "invalid" formulas (those that are "not" entailed by a given theory), cannot always be recognized. For example, by Gödel's incompleteness theorem, we know that any theory whose proper axioms are true for the natural numbers cannot prove all first order statements true for the natural numbers, even if the list of proper axioms is allowed to be infinite enumerable. It follows that an automated theorem prover may fail to terminate while searching for a proof. Despite these theoretical limits, in practice, theorem provers can solve many hard problems, even in these undecidable logics.
Related problems.
A simpler, but related, problem is proof verification, where an existing proof for a theorem is certified valid. For this, it is generally required that each individual proof step can be verified by a primitive recursive function or program, and hence the problem is always decidable.
Since the proofs generated by automated theorem provers are typically very large, the problem of proof compression is crucial and various techniques aiming at making the prover's output smaller, and consequently more easily understandable and checkable, have been developed.
Proof assistants require a human user to give hints to the system. Depending on the degree of automation, the prover can essentially be reduced to a proof checker, with the user providing the proof in a formal way, or significant proof tasks can be performed automatically. Interactive provers are used for a variety of tasks, but even fully automatic systems have proved a number of interesting and hard theorems, including at least one that has eluded human mathematicians for a long time, namely the Robbins conjecture. However, these successes are sporadic, and work on hard problems usually requires a proficient user.
Another distinction is sometimes drawn between theorem proving and other techniques, where a process is considered to be theorem proving if it consists of a traditional proof, starting with axioms and producing new inference steps using rules of inference. Other techniques would include model checking, which, in the simplest case, involves brute-force enumeration of many possible states (although the actual implementation of model checkers requires much cleverness, and does not simply reduce to brute force).
There are hybrid theorem proving systems which use model checking as an inference rule. There are also programs which were written to prove a particular theorem, with a (usually informal) proof that if the program finishes with a certain result, then the theorem is true. A good example of this was the machine-aided proof of the four color theorem, which was very controversial as the first claimed mathematical proof which was essentially impossible to verify by humans due to the enormous size of the program's calculation (such proofs are called non-surveyable proofs). Another example would be the proof that the game Connect Four is a win for the first player.
Industrial uses.
Commercial use of automated theorem proving is mostly concentrated in integrated circuit design and verification. Since the Pentium FDIV bug, the complicated floating point units of modern microprocessors have been designed with extra scrutiny. AMD, Intel and others use automated theorem proving to verify that division and other operations are correctly implemented in their processors.
First-order theorem proving.
First-order theorem proving is one of the most mature subfields of automated theorem proving. The logic is expressive enough to allow the specification of arbitrary problems, often in a reasonably natural and intuitive way. On the other hand, it is still semi-decidable, and a number of sound and complete calculi have been developed, enabling "fully" automated systems. More expressive logics, such as higher order logics, allow the convenient expression of a wider range of problems than first order logic, but theorem proving for these logics is less well developed.
Benchmarks and competitions.
The quality of implemented systems has benefited from the existence of a large library of standard benchmark examples — the Thousands of Problems for Theorem Provers (TPTP) Problem Library — as well as from the CADE ATP System Competition (CASC), a yearly competition of first-order systems for many important classes of first-order problems.
Some important systems (all have won at least one CASC competition division) are listed below.
Comparison.
"See also:" Proof assistant#Comparison and :Category:Theorem proving software systems

</doc>
<doc id="2547" url="http://en.wikipedia.org/wiki?curid=2547" title="Agent Orange">
Agent Orange

Agent Orange or Herbicide Orange (HO) is one of the herbicides and defoliants used by the U.S. military as part of its herbicidal warfare program, Operation Ranch Hand, during the Vietnam War from 1961 to 1971. It was a mixture of equal parts of two herbicides, 2,4,5-T and 2,4-D.
During the late 1940s and 1950s, the US and British collaborated on development of herbicides with potential applications in warfare. Some of those products were brought to market as herbicides. The British were the first to employ herbicides and defoliants to destroy the crops, bushes, and trees of communist insurgents in Malaya during the Malayan Emergency. These operations laid the groundwork for the subsequent use of Agent Orange and other defoliant formulations by the US.
In mid-1961, President Ngo Dinh Diem of South Vietnam asked the United States to conduct aerial herbicide spraying in his country. In August of that year, the South Vietnamese Air Force initiated herbicide operations with American help. But Diem's request launched a policy debate in the White House and the State and Defense Departments. However, U.S. officials considered using it, pointing out that the British had already used herbicides and defoliants during the Malayan Emergency in the 1950s. In November 1961, President John F. Kennedy authorized the start of Operation Ranch Hand, the codename for the U.S. Air Force's herbicide program in Vietnam.
Agent Orange was manufactured for the U.S. Department of Defense primarily by Monsanto Corporation and Dow Chemical. It was given its name from the color of the orange-striped barrels in which it was shipped, and was by far the most widely used of the so-called "Rainbow Herbicides". The 2,4,5-T used to produce Agent Orange was contaminated with 2,3,7,8-tetrachlorodibenzodioxin (TCDD), an extremely toxic dioxin compound. In some areas, TCDD concentrations in soil and water were hundreds of times greater than the levels considered safe by the U.S. Environmental Protection Agency.
In the absence of specific customary or positive international humanitarian law regarding herbicidal warfare, a draft convention, prepared by a Working Group set up within the Conference of the Committee on Disarmament (CCD), was submitted to the UN General Assembly in 1976. In that same year, the First Committee of the General Assembly decided to send the text of the draft convention to the General Assembly, which adopted Resolution 31/72 on December 10, 1976, with the text of the Convention attached as an annex thereto. The convention, namely the Environmental Modification Convention, was opened for signature and ratification on May 18, 1977, and entered into force in October 5, 1978. The convention prohibits the military or other hostile use of environmental modification techniques having widespread, long-lasting or severe effects. Many states do not regard this as a complete ban on the use of herbicides and defoliants in warfare but it does require case-by-case consideration.
Chemical description and toxicology.
Chemically, Agent Orange is an approximately 1:1 mixture of two phenoxyl herbicides – 2,4-dichlorophenoxyacetic acid (2,4-D) and 2,4,5-trichlorophenoxyacetic acid (2,4,5-T) – in iso-octyl ester form.
Numerous studies have examined health effects linked to Agent Orange, its component compounds, and its manufacturing byproducts.
Prior to the controversy surrounding Agent Orange, there was already a large body of scientific evidence linking 2,4,5-T to serious negative health effects and ecological damage. But in 1969, it was revealed to the public that the 2,4,5-T was contaminated with a dioxin, 2,3,7,8-tetrachlorodibenzodioxin (TCDD), and that the TCDD was causing many of the previously unexplained adverse health effects which were correlated with Agent Orange exposure. TCDD has been described as "perhaps the most toxic molecule ever synthesized by man". Internal memoranda revealed that Monsanto (a major manufacturer of 2,4,5-T) had informed the U.S. government in 1952 that its 2,4,5-T was contaminated. In the manufacture of 2,4,5-T, accidental overheating of the reaction mixture easily causes the product to condense into the toxic self-condensation product TCDD. At the time, precautions were not taken against this unintended side reaction, which caused also the Seveso disaster in Italy in 1976.
In 1979, Yale biologist Arthur Galston, who specialized in herbicide research, published a review of what was known at the time about the toxicity of TCDD. Even "vanishingly small" quantities of dioxin in the diet caused adverse health effects when tested on animals. Since then, TCDD has been comprehensively studied. It has been associated with increased neoplasms in every animal bioassay reported in the scientific literature. The National Toxicology Program has classified TCDD as "known to be a human carcinogen", frequently associated with soft-tissue sarcoma, non-Hodgkin's lymphoma, Hodgkin's lymphoma and chronic lymphocytic leukemia (CLL).
Of the two herbicides that make up Agent Orange, 2,4-D and 2,4,5-T, the latter is considered to be less biodegradable.
While degradation of 2,4,5-T with a half-life on a scale of days can be achieved by adding bacteria of a special strain, "no substantial degradation" was observed in the same soil without addition of bacteria. The half-life of dioxins in soil is more than 10 years, and that of TCDD in human fat tissue is about 7 years.
A 1969 report authored by K. Diane Courtney and others found 2,4,5-T could cause birth defects and stillbirths in mice. Several studies have shown an increased rate of cancer mortality for workers exposed to 2,4,5-T. In one such study, from Hamburg, Germany, the risk of cancer mortality increased by 170% after working for 10 years at the 2,4,5-T-producing section of a Hamburg manufacturing plant. Three studies have suggested prior exposure to Agent Orange poses an increased risk of acute myelogenous leukemia in the children of Vietnam veterans.
It has often been claimed that the contamination with dioxin was discovered only "later". However, prior to Operation Ranch Hand (1962-1971), health-risks had become apparent, from several accidents in 2,4,5-T-production in the U.S. and in Europe. The causes had been investigated, and results published in 1957, specifically stating "tetrachlordibenzodioxine proved very active." Additionally "Boehringer, which used the relatively safer low-temperature-process since 1957, in the same year warned the other producers of 2,4,5-TCP, which were using the high-temperature-process, pointing out the risk and providing suggestions how to avoid them."
Starting in 1991, Congress asked the Institute of Medicine to review the scientific literature on Agent Orange and the other herbicides used in Vietnam, including their active ingredients and the dioxin contaminant. The IOM found an association between dioxin exposure and diabetes.
The discovery of herbicides and defoliants and first use in war.
Several herbicides were discovered as part of efforts by the US and the British to develop herbicidal weapons for use during WWII. These included 2,4-D (2,4-dichlorophenoxyacetic acid), 2,4,5-T (coded LN-14, and also known as trioxone), MCPA (2-methyl-4-chlorophenoxyacetic acid, 1414B and 1414A, recoded LN-8 and LN-32), and isopropyl phenylcarbamate (1313, recoded LN-33).
In 1943, the U.S. Department of the Army contracted the University of Chicago to study the effects of 2,4-D and 2,4,5-T on cereal grains (including rice) and broadleaf crops. From these studies arose the concept of using aerial applications of herbicides to destroy enemy crops to disrupt their food supply. In early 1945, the U.S. Army ran tests of various 2,4-D and 2,4,5-T mixtures at the Bushnell Army Airfield in Florida, which is now listed as a Formerly Used Defense Site (FUDS). As a result, the U.S. began a full-scale production of 2,4-D and 2,4,5-T and would have used it against Japan in 1946 during Operation Downfall if the war had continued.
By the end of the war, the relationship between the two countries was well established. In the years after the war, the U.S. tested 1100 compounds and field trials of the more promising ones were done at British stations in India and Australia, to establish their effects in tropical conditions, as well as at the U.S.'s testing ground in Florida.
Between 1950 and 1952, for example, trials were conducted in Tanganyika, at Kikore and Stunyansa, to test arboricides and defoliants under tropical conditions. The chemicals involved were 2,4-D, 2,4,5-T, and endothal (3,6-endoxohexahydrophthalic acid). During 1952/53, the unit supervised the aerial spraying of 2,4,5-T over the Waturi peninsula in Kenya to assess the value of defoliants in the eradication of tsetse fly.
During the Malayan Emergency, Britain was the first nation to employ the use of herbicides and defoliants to destroy bushes, trees, and vegetation to deprive insurgents of cover and targeting food crops as part of a starvation campaign in the early 1950s. A detailed account of how the British experimented with the spraying of herbicides was written by two scientists, E.K. Woodford of Agricultural Research Council's Unit of Experimental Agronomy and H.G.H. Kearns of the University of Bristol.
After the Malayan conflict ended in 1960, the U.S. considered the British precedent in deciding that the use of defoliants was a legal tactic of warfare. Secretary of State Dean Rusk advised President John F. Kennedy that the British had established a precedent for warfare with herbicides in Malaya.
Use in the Vietnam War.
During the Vietnam War, between 1962 and 1971, the United States military sprayed nearly of chemical herbicides and defoliants in Vietnam, eastern Laos and parts of Cambodia, as part of the aerial defoliation program known as Operation Ranch Hand. Like the British did in Malaya, the goal was to defoliate rural/forested land, depriving guerrillas of food and cover and clearing sensitive areas such as around base perimeters. The program was also a part of a general policy of forced draft urbanization, which aimed to destroy the ability of peasants to support themselves in the countryside, forcing them to flee to the U.S. dominated cities, depriving the guerrillas of their rural support base.
Spraying was usually done either from helicopters or from low-flying C-123 Provider aircraft, fitted with sprayers and "MC-1 Hourglass" pump systems and chemical tanks. Spray runs were also conducted from trucks, boats, and backpack sprayers.
The first batch of herbicides was unloaded at Tan Son Nhut Air Base in South Vietnam, on January 9, 1962. U.S. Air Force records show at least 6,542 spraying missions took place over the course of Operation Ranch Hand. By 1971, 12 percent of the total area of South Vietnam had been sprayed with defoliating chemicals, at an average concentration of 13 times the recommended U.S. Department of Agriculture application rate for domestic use. In South Vietnam alone, an estimated 10 million hectares of agricultural land were ultimately destroyed. In some areas, TCDD concentrations in soil and water were hundreds of times greater than the levels considered safe by the U.S. Environmental Protection Agency.
The campaign destroyed of upland and mangrove forests and millions of acres of crops. Overall, more than 20% of South Vietnam's forests were sprayed at least once over a nine-year period.
In 1965, members of the U.S. Congress were told "crop destruction is understood to be the more important purpose ... but the emphasis is usually given to the jungle defoliation in public mention of the program." Military personnel were told they were destroying crops because they were going to be used to feed guerrillas. They later discovered nearly all of the food they had been destroying was not being produced for guerrillas; it was, in reality, only being grown to support the local civilian population. For example, in Quang Ngai province, 85% of the crop lands were scheduled to be destroyed in 1970 alone. This contributed to widespread famine, leaving hundreds of thousands of people malnourished or starving.
The U.S. military began targeting food crops in October 1962, primarily using Agent Blue; the American public was not made aware of the crop destruction programs until 1965 (and it was then believed that crop spraying had begun that spring). In 1965, 42 percent of all herbicide spraying was dedicated to food crops. The first official acknowledgement of the programs came from the State Department in March 1966.
Many experts at the time, including Arthur Galston, the biologist who developed and intensively studied 2,4,5-T and TCDD, opposed herbicidal warfare, due to concerns about the side effects to humans and the environment by indiscriminately spraying the chemical over a wide area. As early as 1966, resolutions were introduced to the United Nations charging that the U.S. was violating the 1925 Geneva Protocol, which regulated the use of chemical and biological weapons. The U.S. defeated most of the resolutions, arguing that Agent Orange was not a chemical or a biological weapon as it was considered a herbicide and a defoliant and it was used in effort to destroy plant crops to deprive the enemy of cover and not meant to target human beings. A weapon, by definition, is any device used to injure, defeat, or destroy living beings, structures, or systems, and Agent Orange did not qualify under that definition. It also argued that if the U.S. were to be charged for using Agent Orange, then Britain and its Commonwealth nations should be charged since they also used it widely during the Malayan Emergency in the 1950s. In 1969, during a debate in the First Committee of the UN General Assembly on the question of chemical and bacteriological (biological) weapons, Britain stated with respect to the then still draft Resolution 2603 (XXIV): “The evidence seems to us to be notably inadequate for the assertion that the use in war of chemical substances specifically toxic to plants is prohibited by international law.”
Effects on the Vietnamese people.
Health effects.
The Vietnam Red Cross reported as many as 3 million Vietnamese people have been affected by Agent Orange, including at least 150,000 children born with birth defects. According to Vietnamese Ministry of Foreign Affairs, 4.8 million Vietnamese people were exposed to Agent Orange, resulting in 400,000 people being killed or maimed, and 500,000 children born with birth defects. Women had higher rates of miscarriage and stillbirths, as did livestock such as cattle, water buffalo, and pigs. The Red Cross of Vietnam estimates that up to 1 million people are disabled or have health problems due to contaminated Agent Orange. The United States government has challenged these figures as being unreliable and unrealistically high.
Children in the areas where Agent Orange was used have been affected and have multiple health problems, including cleft palate, mental disabilities, hernias, and extra fingers and toes. In the 1970s, high levels of dioxin were found in the breast milk of South Vietnamese women, and in the blood of U.S. military personnel who had served in Vietnam. The most affected zones are the mountainous area along Truong Son (Long Mountains) and the border between Vietnam and Cambodia. The affected residents are living in substandard conditions with many genetic diseases.
About 28 of the former U.S. military bases in Vietnam where the herbicides were stored and loaded onto airplanes may still have high level of dioxins in the soil, posing a health threat to the surrounding communities. Extensive testing for dioxin contamination has been conducted at the former U.S. airbases in Da Nang, Phu Cat and Bien Hoa. Some of the soil and sediment on the bases have extremely high levels of dioxin requiring remediation. The Da Nang Airbase has dioxin contamination up to 350 times higher than international recommendations for action. The contaminated soil and sediment continue to affect the citizens of Vietnam, poisoning their food chain and causing illnesses, serious skin diseases and a variety of cancers in the lungs, larynx, and prostate.
Ecological effects.
About 17.8 percent——of the total forested area of Vietnam was sprayed during the war, which disrupted the ecological equilibrium. The persistent nature of dioxins, erosion caused by loss of tree cover and loss of seedling forest stock meant that reforestation was difficult (or impossible) in many areas. Many defoliated forest areas were quickly invaded by aggressive pioneer species (such as bamboo and cogon grass), which make it unlikely that the forests will be able to regenerate. Animal-species diversity was also impacted; in one study a Harvard biologist found 24 species of birds and five species of mammals in a sprayed forest, while in two adjacent sections of unsprayed forest there were 145 and 170 species of birds and 30 and 55 species of mammals.
Dioxins from Agent Orange have persisted in the Vietnamese environment since the war, settling in the soil and sediment and entering the food chain through animals and fish which feed in the contaminated areas. The movement of dioxins through the food web has resulted in bioconcentration and biomagnification. The areas most heavily contaminated with dioxins are former U.S. air bases.
Sociopolitical effects.
The RAND Corporation's "Memorandum 5446-ISA/ARPA" states: "the fact that the VC obtain most of their food from the neutral rural population dictates the destruction of civilian crops ... if they (the VC) are to be hampered by the crop destruction program, it will be necessary to destroy large portions of the rural economy – probably 50% or more".
Rural-to-urban migration rates dramatically increased in South Vietnam, as peasants escaped the war in the countryside by fleeing to the cities. The urban population in South Vietnam nearly tripled, growing from 2.8 million people in 1958 to 8 million by 1971. The rapid flow of people led to a fast-paced and uncontrolled urbanization; an estimated 1.5 million people were living in Saigon slums.
Effects on U.S. veterans.
Some studies showed that veterans who served in the South during the war have increased rates of cancer, and nerve, digestive, skin and respiratory disorders. Veterans from the south had higher rates of throat cancer, acute/chronic leukemia, Hodgkin's lymphoma and non-Hodgkin's lymphoma, prostate cancer, lung cancer, colon cancer, soft tissue sarcoma and liver cancer. With the exception of liver cancer, these are the same conditions the U.S. Veterans Administration has determined may be associated with exposure to Agent Orange/dioxin, and are on the list of conditions eligible for compensation and treatment.
Military personnel who loaded airplanes and helicopters used in Ranch Hand probably sustained some of the heaviest exposures. Members of the Army Chemical Corps, who stored and mixed herbicides and defoliated the perimeters of military bases, and mechanics who worked on the helicopters and planes, are also thought to have had some of the heaviest exposures. However, this same group of individuals has not shown remarkably higher incidences of the associated diseases. Others with potentially heavy exposures included members of U.S. Army Special Forces units who defoliated remote campsites, and members of U.S. Navy river units who cleared base perimeters. Military members who served on Okinawa also claim to have been exposed to the chemical but there is no verifiable evidence to corroborate these claims.
While in Vietnam, the veterans were told not to worry, and were persuaded the chemical was harmless. After returning home, Vietnam veterans began to suspect their ill health or the instances of their wives having miscarriages or children born with birth defects might be related to Agent Orange and the other toxic herbicides to which they were exposed in Vietnam. Veterans began to file claims in 1977 to the Department of Veterans Affairs for disability payments for health care for conditions they believed were associated with exposure to Agent Orange, or more specifically, dioxin, but their claims were denied unless they could prove the condition began when they were in the service or within one year of their discharge.
By April 1993, the Department of Veterans Affairs had only compensated 486 victims, although it had received disability claims from 39,419 soldiers who had been exposed to Agent Orange while serving in Vietnam.
Legal and diplomatic proceedings.
U.S. veterans class action lawsuit against manufacturers.
Since at least 1978, several lawsuits have been filed against the companies which produced Agent Orange, among them Dow Chemical, Monsanto, and Diamond Shamrock.
Hy Mayerson of The Mayerson Law Offices, P.C. was an early pioneer in Agent Orange litigation, working with environmental attorney Victor Yannacone in 1980 on the first class-action suits against wartime manufacturers of Agent Orange. In meeting Dr. Ronald A. Codario, one of the first civilian doctors to see afflicted patients, Mayerson, so impressed by the fact a physician would show so much interest in a Vietnam veteran, forwarded more than a thousand pages of information on Agent Orange and the effects of dioxin on animals and humans to Codario's office the day after he was first contacted by the doctor. The corporate defendants sought to escape culpability by blaming everything on the U.S. government.
The Mayerson law firm, with Sgt. Charles E. Hartz as their principal client, filed the first U.S. Agent Orange class-action lawsuit, in Pennsylvania in 1980, for the injuries military personnel in Vietnam suffered through exposure to toxic dioxins in the defoliant. Attorney Hy Mayerson co-wrote the brief that certified the Agent Orange Product Liability action as a class action, the largest ever filed as of its filing. Hartz's deposition was one of the first ever taken in America, and the first for an Agent Orange trial, for the purpose of preserving testimony at trial, as it was understood that Hartz would not live to see the trial because of a brain tumor that began to develop while he was a member of Tiger Force, Special Forces, and LRRPs in Vietnam. The firm also located and supplied critical research to the Veterans' lead expert, Dr. Ronald A. Codario, M.D., including about 100 articles from toxicology journals dating back more than a decade, as well as data about where herbicides had been sprayed, what the effects of dioxin had been on animals and humans, and every accident in factories where herbicides were produced or dioxin was a contaminant of some chemical reaction.
The chemical companies involved denied that there was a link between Agent Orange and the veterans' medical problems. However, on May 7, 1984, seven chemical companies settled the class-action suit out of court just hours before jury selection was to begin. The companies agreed to pay $180 million as compensation if the veterans dropped all claims against them. Slightly over 45% of the sum was ordered to be paid by Monsanto alone. Many veterans who were victims of Agent Orange exposure were outraged the case had been settled instead of going to court, and felt they had been betrayed by the lawyers. "Fairness Hearings" were held in five major American cities, where veterans and their families discussed their reactions to the settlement, and condemned the actions of the lawyers and courts, demanding the case be heard before a jury of their peers. Federal Judge Julius Weinstein refused the appeals, claiming the settlement was "fair and just". By 1989, the veterans' fears were confirmed when it was decided how the money from the settlement would be paid out. A totally disabled Vietnam veteran would receive a maximum of $12,000 spread out over the course of 10 years. Furthermore, by accepting the settlement payments, disabled veterans would become ineligible for many state benefits that provided far more monetary support than the settlement, such as food stamps, public assistance, and government pensions. A widow of a Vietnam veteran who died of Agent Orange exposure would only receive $3700.
In 2004, Monsanto spokesman Jill Montgomery said Monsanto should not be liable at all for injuries or deaths caused by Agent Orange, saying: "We are sympathetic with people who believe they have been injured and understand their concern to find the cause, but reliable scientific evidence indicates that Agent Orange is not the cause of serious long-term health effects."
New Jersey Agent Orange Commission.
In 1980, New Jersey created the New Jersey Agent Orange Commission, the first state commission created to study its effects. The commission's research project in association with Rutgers University was called "The Pointman Project". It was disbanded by Governor Christine Todd Whitman in 1996.
During Pointman I, commission researchers devised ways to determine small dioxin levels in blood. Prior to this, such levels could only be found in the adipose (fat) tissue. The project studied dioxin (TCDD) levels in blood as well as in adipose tissue in a small group of Vietnam veterans who had been exposed to Agent Orange and compared them to those of a matched control group; the levels were found to be higher in the former group.
The second phase of the project continued to examine and compare dioxin levels in various groups of Vietnam veterans, including Army, Marines and brown water riverboat Navy personnel.
U.S. Congress.
In 1991, Congress enacted the Agent Orange Act, giving the Department of Veterans Affairs the authority to declare certain conditions 'presumptive' to exposure to Agent Orange/dioxin, making these veterans who served in Vietnam eligible to receive treatment and compensation for these conditions. The same law required the National Academy of Sciences to periodically review the science on dioxin and herbicides used in Vietnam to inform the Secretary of Veterans Affairs about the strength of the scientific evidence showing association between exposure to Agent Orange/dioxin and certain conditions.
Through this process, the list of 'presumptive' conditions has grown since 1991, and currently the U.S. Department of Veterans Affairs has listed prostate cancer, respiratory cancers, multiple myeloma, type II diabetes, Hodgkin's disease, non-Hodgkin's lymphoma, soft tissue sarcoma, chloracne, porphyria cutanea tarda, peripheral neuropathy, chronic lymphocytic leukemia, and spina bifida in children of veterans exposed to Agent Orange as conditions associated with exposure to the herbicide. This list now includes B cell leukemias, such as hairy cell leukemia, Parkinson's disease and ischemic heart disease, these last three having been added on August 31, 2010. Several highly placed individuals in government are voicing concerns about whether some of the diseases on the list should, in fact, actually have been included.
U.S.–Vietnamese government negotiations.
In 2002, Vietnam and the U.S. held a joint conference on Human Health and Environmental Impacts of Agent Orange. Following the conference, the U.S. National Institute of Environmental Health Sciences (NIEHS) began scientific exchanges between the U.S. and Vietnam, and began discussions for a joint research project on the human health impacts of Agent Orange.
These negotiations broke down in 2005, when neither side could agree on the research protocol and the research project was cancelled. More progress has been made on the environmental front. In 2005, the first U.S.-Vietnam workshop on remediation of dioxin was held.
Starting in 2005, the U.S. Environmental Protection Agency (EPA) began to work with the Vietnamese government to measure the level of dioxin at the Da Nang Airbase. Also in 2005, the Joint Advisory Committee on Agent Orange, made up of representatives of Vietnamese and U.S. government agencies, was established. The committee has been meeting yearly to explore areas of scientific cooperation, technical assistance and environmental remediation of dioxin.
A breakthrough in the diplomatic stalemate on this issue occurred as a result of United States President George W. Bush's state visit to Vietnam in November 2006. In the joint statement, President Bush and President Triet agreed "further joint efforts to address the environmental contamination near former dioxin storage sites would make a valuable contribution to the continued development of their bilateral relationship."
On May 25, 2007, President Bush signed the U.S. Troop Readiness, Veterans' Care, Katrina Recovery, and Iraq Accountability Appropriations Act, 2007 into law for the wars in Iraq and Afghanistan that included an earmark of $3 million specifically for funding for programs for the remediation of dioxin 'hotspots' on former U.S. military bases, and for public health programs for the surrounding communities; some authors consider this to be completely inadequate, pointing out that the U.S. airbase in Da Nang, alone, will cost $14 million to clean up, and that three others are estimated to require $60 million for cleanup. The appropriation was renewed in the fiscal year 2009 and again in FY 2010. An additional $12 million was appropriated in the fiscal year 2010 in the Supplemental Appropriations Act and a total of $18.5 million appropriated for fiscal year 2011.
Secretary of State Hillary Clinton stated during a visit to Hanoi in October 2010 that the U.S. government would begin work on the clean-up of dioxin contamination at the Da Nang airbase.
In June 2011, a ceremony was held at Da Nang airport to mark the start of U.S.-funded decontamination of dioxin hotspots in Vietnam. $32m has so far been allocated by the U.S. Congress to fund the program.
A $43 million project began in the summer of 2012, as Vietnam and the U.S. forge closer ties to boost trade and counter China's rising influence in the disputed South China Sea.
Vietnamese victims class action lawsuit in U.S. courts.
On January 31, 2004, a victim's rights group, the Vietnam Association for Victims of Agent Orange/dioxin (VAVA), filed a lawsuit in the United States District Court for the Eastern District of New York in Brooklyn, against several U.S. companies for liability in causing personal injury, by developing, and producing the chemical, and claimed that the use of Agent Orange violated the 1907 Hague Convention on Land Warfare, 1925 Geneva Protocol, and the 1949 Geneva Conventions. Dow Chemical and Monsanto were the two largest producers of Agent Orange for the U.S. military, and were named in the suit, along with the dozens of other companies (Diamond Shamrock, Uniroyal, Thompson Chemicals, Hercules, etc.). On March 10, 2005, Judge Jack B. Weinstein of the Eastern District – who had presided over the 1984 U.S. veterans class-action lawsuit – dismissed the lawsuit, ruling there was no legal basis for the plaintiffs' claims. He concluded Agent Orange was not considered a poison under international law at the time of its use by the U.S.; the U.S. was not prohibited from using it as a herbicide; and the companies which produced the substance were not liable for the method of its use by the government. Weinstein used the British example to help dismissed the claims of people exposed to Agent Orange in their suit against the chemical companies that had supplied it.
George Jackson stated that "if the Americans were guilty of war crimes for using Agent Orange in Vietnam, then the British would be also guilty of war crimes as well since they were the first nation to deploy the use of herbicides and defoliants in warfare and used them on a large scale throughout the Malayan Emergency. Not only was there no outcry by other states in response to Britain's use, but the U.S. viewed it as establishing a precedent for the use of herbicides and defoliants in jungle warfare." The U.S. government was also not a party in the lawsuit, due to sovereign immunity, and the court ruled the chemical companies, as contractors of the U.S. government, shared the same immunity.
The case was appealed and heard by the Second Circuit Court of Appeals in Manhattan on June 18, 2007. Three judges on the Second Circuit Court of Appeals upheld Weinstein's ruling to dismiss the case. They ruled that, though the herbicides contained a dioxin (a known poison), they were not intended to be used as a poison on humans. Therefore, they were not considered a chemical weapon and thus not a violation of international law. A further review of the case by the whole panel of judges of the Court of Appeals also confirmed this decision. The lawyers for the Vietnamese filed a petition to the U.S. Supreme Court to hear the case. On March 2, 2009, the Supreme Court denied certiorari and refused to reconsider the ruling of the Court of Appeals.
In a November 2004 Zogby International poll of 987 people, 79% of respondents thought the U.S. chemical companies which produced Agent Orange defoliant should compensate U.S. soldiers who were affected by the toxic chemical used during the war in Vietnam. Also, 51% said they supported compensation for Vietnamese Agent Orange victims.
Help for those affected in Vietnam.
To assist those who have been affected by Agent Orange/dioxin, the Vietnamese have established "peace villages", which each host between 50 and 100 victims, giving them medical and psychological help. As of 2006, there were 11 such villages, thus granting some social protection to fewer than a thousand victims. U.S. veterans of the war in Vietnam and individuals who are aware and sympathetic to the impacts of Agent Orange have supported these programs in Vietnam. An international group of veterans from the U.S. and its allies during the Vietnam War working with their former enemy — veterans from the Vietnam Veterans Association — established the Vietnam Friendship Village outside of Hanoi.
The center provides medical care, rehabilitation and vocational training for children and veterans from Vietnam who have been affected by Agent Orange. In 1998, The Vietnam Red Cross established the Vietnam Agent Orange Victims Fund to provide direct assistance to families throughout Vietnam that have been affected. In 2003, the Vietnam Association of Victims of Agent Orange (VAVA) was formed. In addition to filing the lawsuit against the chemical companies, VAVA provides medical care, rehabilitation services and financial assistance to those injured by Agent Orange.
The Vietnamese government provides small monthly stipends to more than 200,000 Vietnamese believed affected by the herbicides; this totaled $40.8 million in 2008 alone. The Vietnam Red Cross has raised more than $22 million to assist the ill or disabled, and several U.S. foundations, United Nations agencies, European governments and nongovernmental organizations have given a total of about $23 million for site cleanup, reforestation, health care and other services to those in need.
Vuong Mo of the Vietnam News Agency described one of centers:
On June 16, 2010, members of the U.S.-Vietnam Dialogue Group on Agent Orange/Dioxin unveiled a comprehensive 10-year Declaration and Plan of Action to address the toxic legacy of Agent Orange and other herbicides in Vietnam. The Plan of Action was released as an Aspen Institute publication and calls upon the U.S. and Vietnamese governments to join with other governments, foundations, businesses, and nonprofits in a partnership to clean up dioxin "hot spots" in Vietnam and to expand humanitarian services for people with disabilities there. On September 16, 2010, Senator Patrick Leahy (D-VT) acknowledged the work of the Dialogue Group by releasing a statement on the floor of the United States Senate. The statement urges the U.S. government to take the Plan of Action's recommendations into account in developing a multi-year plan of activities to address the Agent Orange/dioxin legacy.
Use outside Malaya and Vietnam.
Australia.
Queensland
In 2008, Australian researcher Jean Williams claimed that cancer rates in the town of Innisfail, Queensland were 10 times higher than the state average due to secret testing of Agent Orange by the Australian military scientists during the Vietnam War. Williams, who had won the Order of Australia medal for her research on the effects of chemicals on U.S. war veterans, based her allegations on Australian government reports found in the Australian War Memorial's archives. A former soldier, Ted Bosworth, backed up the claims, saying that he had been involved in the secret testing. Neither Williams or Bosworth have produced verifiable evidence to support their claims. The Queensland health department determined that cancer rates in Innisfail were no higher than those in other parts of the state.
Brazil.
The Brazilian government used herbicides to defoliate a large section of the Amazon rainforest so that Alcoa could build the Tucuruí dam to power mining operations. Large areas of rainforest were destroyed, along with the homes and livelihoods of thousands of rural peasants and indigenous tribes.
Cambodia.
Agent Orange was used as a defoliant in eastern Cambodia during the Vietnam War, but its impacts are difficult to assess due to the chaos caused by the Khmer Rouge regime.
Canada.
New Brunswick
The U.S. military, with the permission of the Canadian government, tested herbicides, including Agent Orange, in the forests near the Canadian Forces Base Gagetown in New Brunswick for three days in 1966 and four days in 1967. Soldiers working on the base at that time were advised that the chemicals would have no harmful effects on them, to the point they would spray each other with the chemical to cool off. This information was incorrect and was the reason so many people were fighting for compensation for medical bills. Greg Thompson, Minister for Veterans Affairs worked hard with many veterans to make this possible. A lot of work was done by John Chisholm (a veteran) on behalf of many others to help with claims for the compensation package. On September 12, 2007, Greg Thompson, Minister of Veterans Affairs, announced that the government of Canada was offering a one-time ex gratia payment of $20,000 as the compensation package for Agent Orange exposure at CFB Gagetown.
On July 12, 2005, Merchant Law Group LLP on behalf of over 1,100 Canadian veterans and civilians who were living in and around the CFB Gagetown filed a lawsuit to pursue class action litigation concerning Agent Orange and Agent Purple with the Federal Court of Canada.
On August 4, 2009, the case was rejected by the court due to lack of evidence. The ruling was appealed.
In 2007 the Canadian government announced that a research and fact-finding program initiated in 2005 had found the base was safe.
Ontario
On February 17, 2011, the Toronto Star revealed that the same chemicals used to strip the jungles of Vietnam were also employed to clear extensive plots of Crown land in Northern Ontario. The same day, in response to the Toronto Star article, the Ontario provincial government launched a probe into the use of Agent Orange.
On February 18, 2011, Ontario's Ministry of Natural Resources widened the probe of Agent Orange spraying to include all areas of the province where government managed forests on Crown land.
The Toronto Star reported that, "records from the 1950s, 1960s and 1970s show forestry workers, often students and junior rangers, spent weeks at a time as human markers holding red, helium-filled balloons on fishing lines while low-flying planes sprayed toxic herbicides including an infamous chemical mixture known as Agent Orange on the brush and the boys below."
British Columbia
Records show tens of thousands of gallons of the toxic mixture were applied to clear brush near highways and along power lines in the late 1960s and early 1970s – and in some cases the substance was sprayed next to homes. In B.C., the mix of 2-4-D and 2-4-5-T was called "Type B Weed and Brush Killer" in government invoices. Sometimes, the engineers ordered 2-4-5-T by itself, and dubbed it "Type C Weed and Brush Killer."
In total, about 26,000 gallons of Type B Weed and Brush Killer were ordered between 1965 and 1972. About 10,000 gallons of Type C Weed and Brush Killer were ordered in the same time period. The barrels were shipped to all four of the regions of B.C. as designated by the Ministry of Highways: Kamloops, Nelson, Prince George and Vancouver.
In 1976, documents from BC Hydro show 2-4-5-T and 2-4-D was sprayed along Hydro lines Vernon-Monashee and Nicola-Brenda circuits. The documents also say "brushkiller" was sprayed in Pemberton and Daisy Lake.
Guam.
An analysis of chemicals present in the island’s soil together with resolutions passed by Guam’s legislature suggest that Agent Orange was among the herbicides routinely used on and around military bases Anderson Air Force Base, Naval Air Station Agana, Guam. Despite the evidence, the Department of Defense continues to deny that Agent Orange was ever stored or used on Guam. Several Guam veterans have collected an enormous amount of evidence to assist in their disability claims for direct exposure to dioxin containing herbicides such as 2,4,5-T which are similar to the illness associations and disability coverage that has become standard for those who were harmed by the same chemical contaminant of Agent Orange used in Vietnam.
Korea.
Agent Orange was used in Korea in the late 1960s. Republic of Korea troops were the only personnel involved in the spraying, which occurred along the Korean Demilitarized Zone (DMZ). "Citing declassified U.S. Department of Defense documents, Korean officials fear thousands of its soldiers may have come into contact with the herbicide in the late 1960s and early 1970s. According to one top government official, as many as '30,000 Korean veterans are suffering from illness related to their exposure'. The exact number of GIs who may have been exposed is unknown. But C. David Benbow, a North Carolina attorney who served as a sergeant with Co. C, 3rd Battalion, 23rd Infantry Regiment, 2nd Infantry Division, along the DMZ in 1968–69, estimates as many as '4,000 soldiers at any given time' could have been affected.".
In 1999, about 20,000 South Koreans filed two separated lawsuits against U.S. companies, seeking more than $5 billion in damages. After losing a decision in 2002, they filed an appeal.
In January 2006, the South Korean Appeals Court ordered Dow Chemical and Monsanto to pay $62 million in compensation to about 6,800 people. The ruling acknowledged that "the defendants failed to ensure safety as the defoliants manufactured by the defendants had higher levels of dioxins than standard", and, quoting the U.S. National Academy of Science report, declared that there was a "causal relationship" between Agent Orange and 11 diseases, including cancers of the lung, larynx and prostate. The judges failed to acknowledge "the relationship between the chemical and peripheral neuropathy, the disease most widespread among Agent Orange victims" according to the "Mercury News".
The United States local press KPHO-TV in Phoenix, Arizona alleged that the United States Army had buried Agent Orange in Camp Carroll, the U.S. Army base located in Gyeongsangbuk-do, Korea. It is based on the claim of three U.S. Army veterans. They claimed approximately 250 drums of Agent Orange were buried at Camp Carroll in 1978. The South Korean Ministry of Environment announced that they will request cooperative investigation at Camp Carroll officially. The USFK issued a statement that confirmed that barrels were buried there, but all (plus an additional 60 tons of soil) were removed in 1996.
Currently, veterans who provide evidence meeting VA requirements for service in Vietnam, and who can medically establish that anytime after this 'presumptive exposure' they developed any medical problems on the list of presumptive diseases, may receive compensation from the VA. Certain veterans who served in Korea and are able to prove they were assigned to certain specified around the DMZ during a specific time frame are afforded similar presumption. The differences in requirements between Vietnam and Korea service stem from the fact that congress has not made any laws to provide for the same sweeping presumption of exposure similar to the Agent Orange Act of 1991 for Korean veterans.
Laos.
Parts of Laos were sprayed with Agent Orange during the Vietnam War.
New Zealand.
The use of Agent Orange has been controversial in New Zealand, because of the exposure of New Zealand troops in Vietnam and because of the production of Agent Orange for Vietnam and other users at an Ivon Watkins-Dow chemical plant in Paritutu, New Plymouth. There have been continuing claims, as yet unproven, that the suburb of Paritutu has also been polluted; see New Zealand in the Vietnam War.
There are cases of New Zealand soldiers developing cancers such as bone cancer but none have been scientifically connected to exposure to herbicides.
Philippines.
Herbicide persistence studies of Agents Orange and White were conducted in the Philippines. The Philippine herbicide test program was conducted in cooperation with the University of the Philippines, College of Forestry and was described in a 1969 issue of The Philippine Collegian.
Johnston Atoll.
The U.S. Air Force operation to remove Herbicide Orange from Vietnam in 1972 was named Operation Pacer IVY while the operation to destroy the Agent Orange stored at Johnston Atoll in 1977 was named Operation Pacer HO. Operation Pacer IVY (InVentorY) collected Agent Orange in South Vietnam and removed it in 1972 aboard the ship for storage on Johnston Atoll. The Environmental Protection Agency (EPA) reports that 1,800,000 gallons of Herbicide Orange was stored at Johnson Island in the Pacific and 480,000 gallons at Gulfport Mississippi.
Research and studies were initiated to find a safe method to destroy the materials and it was discovered they could be incinerated safely under special conditions of temperature and dwell time. However, these herbicides were expensive and the Air Force wanted to resell its surplus instead of dumping it at sea. Among many methods tested, a possibility of salvaging the herbicides by reprocessing and filtering out the 2,3,7,8-tetrachlorodibenzo-p-dioxin (TCDD) contaminant with carbonized (charcoaled) coconut fibers. This concept was then tested in 1976 and a pilot plant constructed at Gulfport, Mississippi.
From July to September 1977 during Operation Pacer HO (Herbicide Orange), the entire stock of Agent Orange from both Herbicide Orange storage sites at Gulfport, Mississippi and Johnston Atoll was subsequently incinerated in four separate burns in the vicinity of Johnson Island aboard the Dutch-owned waste incineration ship .
As of 2004, some records of the storage and disposition of Agent Orange at Johnston Atoll have been associated with the historical records of Operation Red Hat.
Okinawa, Japan.
There have been dozens of reports in the press about use and/or storage of military formulated herbicides on Okinawa that are based upon statements by former U.S. service members that had been stationed on the island, photographs, government records, and unearthed storage barrels. The U.S. Department of Defense (DoD) has denied these allegations with statements by military officials and spokespersons, as well as a January 2013 report authored by Dr. Alvin Young that was released in April 2013.
In particular, the 2013 report refuted articles written by journalist Jon Mitchell as well as a statement from "An Ecological Assessment of Johnston Atoll" a 2003 publication produced by the United States Army Chemical Materials Agency that states, "in 1972, the U.S. Air Force also brought about 25,000 55-gallon (208L) drums of the chemical, Herbicide Orange (HO) to Johnston Island that originated from Vietnam and was stored on Okinawa." The 2013 report stated: "The authors of the report were not DoD employees, nor were they likely familiar with the issues surrounding Herbicide Orange or its actual history of transport to the Island." and detailed the transport phases and routes of Agent Orange from Vietnam to Johnston Atoll, none of which included Okinawa.
Further official confirmation of restricted (dioxin containing) herbicide storage on Okinawa appeared in a 1971 Fort Detrick report titled "Historical, Logistical, Political and Technical Aspects of the Herbicide/Defoliant Program", which mentioned that the environmental statement should consider "Herbicide stockpiles elsewhere in PACOM (Pacific Command) U.S. Government restricted materials Thailand and Okinawa (Kadena AFB)." The 2013 DoD report says that the environmental statement urged by the 1971 report was published in 1974 as "The Department of Air Force Final Environmental Statement", and that the latter did not find Agent Orange was held in either Thailand or Okinawa.
Thailand.
Agent Orange was tested by the United States in Thailand during the war in Southeast Asia. Buried drums were uncovered and confirmed to be Agent Orange in 1999. Workers who uncovered the drums fell ill while upgrading the airport near Hua Hin, 100 km south of Bangkok.
Vietnam-era Veterans whose service involved duty on or near the perimeters of military bases in Thailand anytime between February 28, 1961 and May 7, 1975 may have been exposed to herbicides and may qualify for VA benefits. A claim for direct exposure is possible if the individual can verify that they worked or lived in close proximity to the affected areas of the bases in Thailand.
A declassified Department of Defense report written in 1973, suggests that there was a significant use of herbicides on the fenced-in perimeters of military bases in Thailand to remove foliage that provided cover for enemy forces.
In 2013 VA determined that herbicides used on the Thailand base perimeters may have been tactical and procured from Vietnam, or a strong, commercial type resembling tactical herbicides.
United States.
The University of Hawaii has acknowledged extensive testing of Agent Orange on behalf of the United States Department of Defense in Hawaii along with mixtures of Agent Orange on Kaua'i Island in 1967-68 and on Hawaii Island in 1966; testing and storage in other U.S. locations has been documented by the United States Department of Veterans Affairs.
In 1971, the C-123 aircraft used for spraying Agent Orange were returned to the United States and assigned various East Coast USAF Reserve squadrons, and then employed in traditional airlift missions between 1972 and 1982. In 1994, testing by the Air Force identified some former spray aircraft as "heavily contaminated" with dioxin residue. Inquiries by aircrew veterans in 2011 brought a decision by the U.S. Department of Veterans Affairs opining that not enough dioxin residue remained to injure these post-Vietnam War veterans. On 26 January 2012, the U.S. Center For Disease Control's Agency for Toxic Substances and Disease Registry challenged this with their finding that former spray aircraft were indeed contaminated and the aircrews exposed to harmful levels of dioxin. In response to veterans' concerns, the VA in February 2014 referred the C-123 issue to the Institute of Medicine for a special study, with results expected in late 2014.
In 1978, the U.S. Environmental Protection Agency suspended spraying of Agent Orange in National Forests.
A December 2006 Department of Defense report listed Agent Orange testing, storage, and disposal sites at 32 locations throughout the United States, as well as in Canada, Thailand, Puerto Rico, Korea, and in the Pacific Ocean. The Veteran Administration has also acknowledged that Agent Orange was used domestically by U.S. forces in test sites throughout the United States. Eglin Air Force Base in Florida was one of the primary testing sites throughout the 1960s.
Cleanup programs.
On the 9th of August 2012, the United States and Vietnam began a cooperative cleaning up of the toxic chemical on part of Danang International Airport, marking the first time Washington has been involved in cleaning up Agent Orange in Vietnam. Danang was the primary storage site of the chemical. Two other cleanup sites the United States and Vietnam are looking at is Biên Hòa, in the southern province of Đồng Nai - a 'hotspot' for dioxin - and Phù Cát airport in the central province of Bình Định, says U.S. Ambassador to Vietnam David Shear. According to the Vietnamese newspaper Nhân Dân, the U.S. government provided $41 million to the project, which will reduce the contamination level in 73,000 m3 of soil by late 2016.

</doc>
<doc id="2551" url="http://en.wikipedia.org/wiki?curid=2551" title="Astronomical year numbering">
Astronomical year numbering

Astronomical year numbering is based on AD/CE year numbering, but follows normal decimal integer numbering more strictly. Thus, it has a year 0, the years before that are designated with negative numbers and the years after that are designated with positive numbers. Astronomers use the Julian calendar for years before 1582, including this year 0, and the Gregorian calendar for years after 1582 as exemplified by Jacques Cassini (1740), Simon Newcomb (1898) and Fred Espenak (2007).
The prefix AD and the suffixes CE, BC or BCE (Common Era, Before Christ or Before Common Era) are dropped. The year 1 BC/BCE is numbered 0, the year 2 BC is numbered −1, and in general the year "n" BC/BCE is numbered "−("n" − 1)" (a negative number equal to 1 − "n"). The numbers of AD/CE years are not changed and are written with either no sign or a positive sign; thus in general "n" AD/CE is simply "n" or +"n". For normal calculation a number zero is often needed, here most notably when calculating the number of years in a period that spans the epoch; the end years need only be subtracted from each other.
The system is so named due to its use in astronomy. Few other disciplines outside history deal with the time before year 1, some exceptions being dendrochronology, archaeology and geology, the latter two of which use 'years before the present'. Although the absolute numerical values of astronomical and historical years only differ by one before year 1, this difference is critical when calculating astronomical events like eclipses or planetary conjunctions to determine when historical events which mention them occurred.
Year zero usage.
In his Rudolphine Tables (1627), Johannes Kepler used a prototype of year zero which he labeled "Christi" (Christ's) between years labeled "Ante Christum" (Before Christ) and "Post Christum" (After Christ) on the mean motion tables for the Sun, Moon, Saturn, Jupiter, Mars, Venus and Mercury. Then in 1702 the French astronomer Philippe de la Hire used a year he labeled at the end of years labeled "ante Christum" (BC), and immediately before years labeled "post Christum" (AD) on the mean motion pages in his "Tabulæ Astronomicæ", thus adding the designation "0" to Kepler's "Christi". Finally, in 1740 the French astronomer Jacques Cassini , who is traditionally credited with the invention of year zero, completed the transition in his "Tables astronomiques", simply labeling this year "0", which he placed at the end of Julian years labeled "avant Jesus-Christ" (before Jesus Christ or BC), and immediately before Julian years labeled "après Jesus-Christ" (after Jesus Christ or AD).
Cassini gave the following reasons for using a year 0:
Fred Espanak of NASA lists 50 phases of the moon within year 0, showing that it is a full year, not an instant in time. <br> Jean Meeus gives the following explanation:
Signed years without year 0.
Although he used the usual French terms "avant J.-C." (before Jesus Christ) and "après J.-C." (after Jesus Christ) to label years elsewhere in his book, the Byzantine historian Venance Grumel used negative years (identified by a minus sign, −) to label BC years and unsigned positive years to label AD years in a table, possibly to save space, without a year 0 between them.
Version 1.0 of the XML Schema language, often used to describe data interchanged between computers in XML, includes built-in primitive datatypes date and dateTime. Although these are defined in terms of ISO 8601 which uses the proleptic Gregorian calendar and therefore should include a year 0, the XML Schema specification states that there is no year zero. Version 1.1 of the defining recommendation realigned the specification with ISO 8601 by including a year zero, despite
the problems arising from the lack of backwards compatibility.

</doc>
<doc id="2552" url="http://en.wikipedia.org/wiki?curid=2552" title="Adam of Bremen">
Adam of Bremen

Adam of Bremen (also: Adamus Bremensis) was a German medieval chronicler. He lived and worked in the second half of the eleventh century. He is most famous for his chronicle "Gesta Hammaburgensis Ecclesiae Pontificum" ("Deeds of Bishops of the Hamburg Church").
Background.
Little is known of his life other than hints from his own chronicles. He is believed to have come from Meissen (Latin "Misnia") in Saxony. The dates of his birth and death are uncertain, but he was probably born before 1050 and died on 12 October of an unknown year (Possibly 1081, latest 1085). From his chronicles it is apparent that he was familiar with a number of authors. The honorary name of "Magister Adam" shows that he had passed through all the stages of a higher education. It is probable that he was taught at the "Magdeburger Domschule".
In 1066 or 1067 he was invited by archbishop Adalbert of Hamburg to join the Church of Bremen. Adam was accepted among the capitulars of Bremen, and by 1069 he appeared as director of the cathedral's school. Soon thereafter he began to write the history of Bremen/Hamburg and of the northern lands in his "Gesta".
His position and the missionary activity of the church of Bremen allowed him to gather information on the history and the geography of Northern Germany. A stay at the court of Svend Estridson gave him the opportunity to find information about the history and geography of Denmark, and the Scandinavian countries.
Bremen was a major trading town, and ships, traders and missionaries went from there to many different locations. The earlier archbishopric seat in Hamburg had been attacked and destroyed several times, and thereafter the sees of Hamburg and Bremen were combined for protection. For three hundred years, beginning with bishop Ansgar, the Hamburg-Bremen archbishopric had been designated as the "Mission of the North" and had jurisdiction over all missions in Scandinavia, North-Western Russia, Iceland and Greenland. Then the archbishop of Hamburg-Bremen had a falling-out with the pope and in 1105 a separate archbishopric for the North was established in Lund.
Gesta.
Adam of Bremen's best-known work is the "Gesta Hammaburgensis Ecclesiae Pontificum" ("Deeds of Bishops of the Hamburg Church"), which he began only after the death of the archbishop Adalbert. It consists of four volumes about the history of the archbishopric of Hamburg-Bremen, and the isles of the north. The first three mainly consist of history and the last one is mainly on geography. Adam based his works in part on Einhard, Cassiodorus, and other earlier historians, as he had the whole library of the church of Bremen at his fingertips. The first edition was completed in 1075/1076.
The first book gives a history from 788 onwards of the Church in Hamburg-Bremen, and the Christian mission in the North. This is the chief source of knowledge of the North until the thirteenth century. The second book continues the history, and also deals with German history between 940 and 1045. The third book is about the deeds of archbishop Adalbert and is considered a milestone in medieval biographical writing.
The fourth book, "Descriptio insularum Aquilonis", completed approximately in 1075, is about the geography, people and customs of Scandinavia, as well as updates of the progress of Christian missionaries there. The description of the Uppsala temple is one of the most famous excerpts of the Gesta, however as no archaeological site has ever been found, one can wonder if Adam’s description is linked to reality.
Adam also presents idolatry and human sacrifice as religious practice:
Adam was a supporter of converting the Northern people. Scandinavia had only just recently been explored by missionaries, and since the fourth book was perhaps created to inspire and guide future missionaries, its detailed descriptions make it one of the most important sources about pre-Christian Scandinavia. It is also the first known European record (in chapter 38) that mentions Vínland (Winland) island (insula), a location somewhere on the North-East coast of what is now the United States.

</doc>
<doc id="2553" url="http://en.wikipedia.org/wiki?curid=2553" title="Ab urbe condita">
Ab urbe condita

Ab urbe condita (related to Anno Urbis Conditae: AUC or a.u.c. or a.u.) is a Latin phrase meaning "from the founding of the City (Rome)", traditionally dated to 753 BC. AUC is a year-numbering system used by some ancient Roman historians to identify particular Roman years. Renaissance editors sometimes added AUC to Roman manuscripts they published, giving the false impression that the Romans usually numbered their years using the AUC system. In fact, modern historians use AUC much more frequently than the Romans themselves did. The dominant method of identifying Roman years in Roman times was to name the two consuls who held office that year. The regnal year of the emperor was also used to identify years, especially in the Byzantine Empire after 537 when Justinian required its use. Examples of continuous numbering include counting by regnal year, principally found in the writings of German authors, for example Mommsen's "History of Rome", and (most ubiquitously) in the Anno Domini year-numbering system.
Significance.
From Emperor Claudius (reigned AD 41–54) onwards, Varro's calculation (see below) superseded other contemporary calculations. Celebrating the anniversary of the city became part of imperial propaganda. Claudius was the first to hold magnificent celebrations in honour of the city's anniversary, in 48 AD, 800 years after the founding of the city. Hadrian and Antoninus Pius held similar celebrations, in 121 AD and 147/148 AD respectively.
During 248 AD, Philip the Arab celebrated Rome's first millennium, together with Ludi saeculares for Rome's alleged tenth saeculum. Coins from his reign commemorate the celebrations. A coin by a contender for the imperial throne, Pacatianus, explicitly states "Year one thousand and first", which is an indication that the citizens of the Empire had a sense of the beginning of a new era, a "Saeculum Novum".
When the Roman Empire turned Christian during the following century, this imagery came to be used in a more metaphysical sense, and removed legal impediments to the development and public use of the "Anno Domini" dating system, which came into general use during the reign of Charlemagne.
Calculation by Varro.
The traditional date for the founding of Rome of 21 April 753 BC, was initiated by Varro. Varro may have used the consular list with its mistakes, and called the year of the first consuls "245 "ab urbe condita"", accepting the 244-year interval from Dionysius of Halicarnassus for the kings after the foundation of Rome. The correctness of Varro's calculation has not been proven scientifically but is still used worldwide.
Relationship with Anno Domini.
The Anno Domini year numbering was developed by a monk named Dionysius Exiguus in Rome during 525, as a result of his work on calculating the date of Easter. In his Easter table the year 532 AD was equated with the regnal year 248 of Emperor Diocletian. The table counted the years starting from the presumed birth of Christ, rather than the accession of the emperor Diocletian on 20 November 284, or as stated by Dionysius: ""sed magis elegimus ab incarnatione Domini nostri Jesu Christi annorum tempora praenotare..."" Blackburn and Holford-Strevens review interpretations of Dionysius which place the Incarnation in 2 BC, 1 BC, or 1 AD. It was later calculated (from the historical record of the succession of Roman consuls) that the year 1 AD corresponds to the Roman year 754 AUC, based on Varro's epoch. This however resulted in that year not corresponding with the lifetimes of historical figures reputed to be alive, or otherwise mentioned in connection with the Christian incarnation, e.g. Herod the Great or Quirinius.
Alternative calculations.
According to Velleius Paterculus the foundation of Rome occurred 437 years after the capture of Troy by the Achaeans (1182 BC). It occurred soon before an eclipse of the Sun that was observed at Rome on 25 June 745 BC and had a magnitude of 50.3%. Its beginning occurred at 16:38, its middle at 17:28, and its end at 18:16.
However, according to Lucius Tarrutius of Firmum, Romulus and Remus were conceived in the womb on the 23rd day of the Egyptian month Choiac, at the time of a total eclipse of the Sun. (This eclipse occurred on 15 June 763 BC, with a magnitude of 62.5% at Rome. Its beginning occurred at 6:49, its middle at 7:47 and its end at 8:51.) They were born on the 21st day of the month Thoth. The first day of Thoth fell on 2 March in that year. Rome was founded on the ninth day of the month Pharmuthi, which was 21 April, as universally agreed. The Romans add that about the time Romulus started to build the city, an eclipse of the Sun was observed by Antimachus, the Teian poet, on the 30th day of the lunar month. This eclipse on 25 June 745 BC (see above) had a magnitude of 54.6% at Teos, Asia Minor. It started at 17:49; it was still eclipsed at sunset, at 19:20. Romulus vanished in the 54th year of his life, on the Nones of Quintilis (July), on a day when the Sun was darkened. The day became like night, which sudden darkness was believed to be an eclipse of the Sun. It occurred on 17 July 709 BC, with a magnitude of 93.7%, beginning at 5:04 and ending at 6:57. (All these eclipse data have been calculated by Prof. Aurél Ponori-Thewrewk, retired director of the Planetarium of Budapest.) Plutarch placed it in the 37th year from the foundation of Rome, on the fifth of our July, then called Quintilis, also states that Romulus ruled for 37 years. He was either slain by the senate or disappeared during the 38th year of his reign. Most of these data have been recorded by Plutarch, Florus, Cicero, Dio (Dion) Cassius and Dionysius of Halicarnassus (L. 2). Dio in his "Roman History" (Book I) confirms this data by telling that Romulus was in his 18th year of age when he had initiated Rome. Thus, three eclipse calculations may be evidence for the suggestion that Romulus reigned from 746 BC to 709 BC, and Rome was founded during 745 BC.
Q. Fabius Pictor (c. 250 BC) tells that Roman consuls started for the first time 239 years after Rome's foundation. Livy gives almost the same, 240 years for that interval. Polybius tells that 28 years after the expulsion of the last Persian king Xerxes crossed over to Greece, and that event is fixed to 478 BC by two solar eclipses.

</doc>
<doc id="2558" url="http://en.wikipedia.org/wiki?curid=2558" title="ARY Group">
ARY Group

The ARY Group is a Dubai-based holding company founded by a Pakistani businessman, Haji Abdul Razzak Yaqoob. Abdul is the Chief Executive Officer and owner of the Group. ARY is a diversified group with interests in several sectors, though it is most famous for its contribution to Pakistani television.
Companies under the ARY Group:

</doc>
<doc id="2559" url="http://en.wikipedia.org/wiki?curid=2559" title="Arapawa Island">
Arapawa Island

Arapawa Island is a small island located in the Marlborough Sounds, at the north east tip of the South Island of New Zealand.
The island has a land area of . Queen Charlotte Sound defines its western side, while to the south lies Tory Channel, which is on the sea route from Wellington in the North Island to Picton.
History.
It was from a hill on Arapawa Island in 1770 that Captain James Cook first saw the sea passage from the Pacific Ocean to the Tasman Sea, which was named Cook Strait. This discovery banished the fond notion of geographers that there existed a great southern continent, Terra Australis. A monument at Cook's Lookout was erected in 1970.
From the late 1820s until the mid-1960s, Arapawa Island was a base for whaling in the Sounds. John Guard established a shore station at Te Awaiti in 1827 for right whales. Later, the station at Perano Head on the east coast of the island was used to hunt humpback whales from 1911 to 1964 (see Whaling in New Zealand). The houses built by the Perano family are now operated as tourist accommodations.
Aircraft accident.
An elevated power cable from the mainland to Arapawa Island over Tory Channel was struck by an Air Albatross Cessna 402 commuter aircraft in 1985. The crash was witnessed by many passengers on an inter-island Cook Strait ferry. The ferry immediately stopped to dispatch a rescue lifeboat. Along with the two pilots, one entire family was lost, and all but a young girl from the other. No bodies were ever found. The sole survivor (Cindy Mosey) was travelling with her family and the other from Nelson to Wellington to attend a gymnastics competition. The Arapawa Island crash caused public confidence in Air Albatross to falter, contributing to the company going into liquidation in December of that year.
Conservation.
Parts of the island have been heavily cleared of native vegetation in the past through burning and logging, A number of pine forests were planted on the island. Wilding pines, an invasive species in some parts of New Zealand, are being poisoned on the island to allow the regenerating native vegetation to grow. About at Ruaomoko Point on the south-eastern portion of the island will be killed by drilling holes into the trees and injecting poison.
Arapawa Island is known for the breeds of pigs, sheep and goats found only on the island. These became established in the 19th century, but the origin of these breeds is uncertain, and a matter of some speculation. Common suggestions are that they are old English breeds introduced by the early whalers, or by Captain Cook or other early explorers. These breeds are now extinct in England, and the goats surviving in a sanctuary on the island are now also bred in other parts of New Zealand and in the northern hemisphere.

</doc>
<doc id="2560" url="http://en.wikipedia.org/wiki?curid=2560" title="Administrative law">
Administrative law

Administrative law is the body of law that governs the activities of administrative agencies of government. Government agency action can include rulemaking, adjudication, or the enforcement of a specific regulatory agenda. Administrative law is considered a branch of public law. As a body of law, administrative law deals with the decision-making of administrative units of government (for example, tribunals, boards or commissions) that are part of a national regulatory scheme in such areas as police law, international trade, manufacturing, the environment, taxation, broadcasting, immigration and transport. Administrative law expanded greatly during the twentieth century, as legislative bodies worldwide created more government agencies to regulate the increasingly complex social, economic and political spheres of human interaction.
Civil law countries often have specialized courts, administrative courts, that review these decisions. The plurality of administrative decisions contested in administrative courts are related to taxation.
Administrative law in common law countries.
Generally speaking, most countries that follow the principles of common law have developed procedures for judicial review that limit the reviewability of decisions made by administrative law bodies. Often these procedures are coupled with legislation or other common law doctrines that establish standards for proper rulemaking. Administrative law may also apply to review of decisions of so-called semi-public bodies, such as non-profit corporations, disciplinary boards, and other decision-making bodies that affect the legal rights of members of a particular group or entity.
While administrative decision-making bodies are often controlled by larger governmental units, their decisions could be reviewed by a court of general jurisdiction under some principle of judicial review based upon due process (United States) or fundamental justice (Canada). Judicial review of administrative decisions is different from an administrative appeal. When sitting in review of a decision, the Court will only look at the method in which the decision was arrived at, whereas in an administrative appeal the correctness of the decision itself will be examined, usually by a higher body in the agency. This difference is vital in appreciating administrative law in common law countries.
The scope of judicial review may be limited to certain questions of fairness, or whether the administrative action is "ultra vires". In terms of ultra vires actions in the broad sense, a reviewing court may set aside an administrative decision if it is unreasonable (under Canadian law, following the rejection of the "Patently Unreasonable" standard by the Supreme Court in Dunsmuir v. New Brunswick), "Wednesbury" unreasonable (under British law), or arbitrary and capricious (under U.S. Administrative Procedure Act and New York State law). Administrative law, as laid down by the Supreme Court of India, has also recognized two more grounds of judicial review which were recognized but not applied by English Courts viz. legitimate expectation and proportionality.
The powers to review administrative decisions are usually established by statute, but were originally developed from the royal prerogative writs of English law, such as the writ of mandamus and the writ of certiorari. In certain Common Law jurisdictions, such as India or Pakistan, the power to pass such writs is a Constitutionally guaranteed power. This power is seen as fundamental to the power of judicial review and an aspect of the independent judiciary.
United States.
In the United States, many government agencies are organized under the executive branch of government, although a few are part of the judicial or legislative branches.
In the federal government, the executive branch, led by the president, controls the federal executive departments, which are led by secretaries who are members of the United States Cabinet. The many important independent agencies of the United States government created by statutes enacted by Congress exist outside of the federal executive departments but are still part of the executive branch.
Congress has also created some special judicial bodies known as Article I tribunals to handle some areas of administrative law.
The actions of executive agencies and independent agencies are the main focus of American administrative law. In response to the rapid creation of new independent agencies in the early twentieth century (see discussion below), Congress enacted the Administrative Procedure Act (APA) in 1946. Many of the independent agencies operate as miniature versions of the tripartite federal government, with the authority to "legislate" (through rulemaking; see Federal Register and Code of Federal Regulations), "adjudicate" (through administrative hearings), and to "execute" administrative goals (through agency enforcement personnel). Because the United States Constitution sets no limits on this tripartite authority of administrative agencies, Congress enacted the APA to establish fair administrative law procedures to comply with the constitutional requirements of due process. Agency procedures are drawn from four sources of authority: the APA, organic statutes, agency rules, and informal agency practice.
The American Bar Association's official journal concerning administrative law is the "Administrative Law Review", a quarterly publication that is managed and edited by students at the Washington College of Law.
Historical development.
Stephen Breyer, a U.S. Supreme Court Justice since 1994, divides the history of administrative law in the United States into six discrete periods, according to his book, "Administrative Law & Regulatory Policy" (3d Ed., 1992):
Agriculture.
The agricultural sector is one of the most heavily regulated sectors in the U.S. economy, as it is regulated in various ways at the international, federal, state, and local levels. Consequently, administrative law is a significant component of the discipline of Agricultural Law. The United States Department of Agriculture and its myriad agencies such as the Agricultural Marketing Service are the primary sources of regulatory activity, although other administrative bodies such as the Environmental Protection Agency play a significant regulatory role as well.
Administrative law in civil law countries.
Unlike most Common-law jurisdictions, the majority of civil law jurisdictions have specialized courts or sections to deal with administrative cases which, as a rule, will apply procedural rules specifically designed for such cases and different from that applied in private-law proceedings, such as contract or tort claims.
France.
In France, most claims against the national or local governments are handled by administrative courts, which use the "Conseil d'État" (Council of State) as a court of last resort. The main administrative courts are the "tribunaux administratifs" and appeal courts are the "cours administratives d'appel". The French body of administrative law is called ""droit administratif"".
Germany.
Administrative law in Germany, called “Verwaltungsrecht”:de:Verwaltungsrecht (Deutschland), generally rules the relationship between authorities and the citizens and therefore, it establishes citizens’ rights and obligations against the
authorities. It is a part of the public law, which deals with the organization, the tasks and the acting of the public administration. It also contains rules, regulations, orders and decisions created by and related to administrative agencies, such as federal agencies, federal state authorities, urban administrations, but also admission offices and fiscal authorities etc. Administrative law in Germany follows three basic principles.
Administrative law in Germany can be divided into general administrative law and special administrative law.
General administrative law.
The general administration law is basically ruled in the Administrative Procedures Law (Verwaltungsverfahrensgesetz Other legal sources are the Rules of the Administrative Courts (Verwaltungsgerichtsordnung [VwGO), the social security code (Sozialgesetzbuch and the general fiscal law (Abgabenordnung [AO).
Administrative Procedures Law.
The Verwaltungsverfahrensgesetz (VwVfG), which was enacted in 1977, regulates the main administrative procedures of the federal government. It serves the purpose to ensure a treatment in accordance with the rule of law by the public authority. Furthermore, it contains the regulations for mass processes and expands the legal protection against the authorities. The VwVfG basically applies for the entire public administrative activities of federal agencies as well as federal state authorities, in case of making federal law. One of the central clause is § 35 VwVfG. It defines the administrative act, the most common form of action in which the public administration occurs against a citizen. The definition in § 35 [http://www.gesetze-im-internet.de/vwvfg/__35.html] says, that an administration act is characterized by the following features:
It is an official act of an authority in the field of public law to resolve an individual case with effect to the outside.
§§ 36 – 39, §§ 58 – 59 and § 80 VwV––fG rule the structure and the necessary elements of the
administrative act. § 48 and § 49 VwVfG have a high relevance in practice, as well. In these
paragraphs, the prerequisites for redemption of an unlawful administration act (§ 48 VwVfG [http://www.gesetze-im-internet.de/vwvfg/__48.html]) and
withdrawal of a lawful administration act (§ 49 VwVfG [http://www.gesetze-im-internet.de/vwvfg/__49.html]), are listed.
Other legal sources.
Administration procedural law (Verwaltungsgerichtsordnung ), which was enacted in 1960, rules the court procedures at the administrative court. The VwGO is divided into five parts, which are the constitution of the courts, action, remedies and retrial, costs and enforcement15 and final clauses and temporary arrangements.
In absence of a rule, the VwGO is supplemented by the code of civil procedure (Zivilprozessordnung and the judicature act (Gerichtsverfassungsgesetz [GVG). In addition to the regulation of the administrative procedure, the VwVfG also constitutes the legal protection in administrative law beyond the court procedure. § 68 VwVGO rules the preliminary proceeding, called “Vorverfahren” or “Widerspruchsverfahren”, which is a stringent prerequisite for the administrative procedure, if an action for rescission or a writ of mandamus against an authority is aimed. The preliminary proceeding gives each citizen, feeling unlawfully mistreated by an authority, the possibility to object and to force a review of an administrative act without going to court. The prerequisites to open the public law remedy are listed in § 40 I VwGO. Therefore, it is necessary to have the existence of a conflict in public law without any constitutional aspects and no assignment to another jurisdiction.
The social security code (Sozialgesetzbuch ) and the general fiscal law are less important for the administrative law. They supplement the VwVfG and the VwGO in the fields of taxation and social legislation, such as social welfare or financial support for students (BaFÖG) etc.
Special administrative law.
The special administrative law consists of various laws. Each special sector has its own law. The most important ones are the
In Germany, the highest administrative court for most matters is the federal administrative court Bundesverwaltungsgericht. There are federal courts with special jurisdiction in the fields of social security law (Bundessozialgericht) and tax law (Bundesfinanzhof).
Italy.
Administrative law in Italy, known as “Diritto amministrativo”, is a branch of public law, whose rules govern the organization of the public administration and the activities of the pursuit of the public interest of the public administration and the relationship between this and the citizens.
Its genesis is related to the principle of division of powers of the State. The administrative power, originally called "executive", is to organize resources and people whose function is devolved to achieve the public interest objectives as defined by the law.
The Netherlands.
In The Netherlands, administrative law provisions are usually contained in separate laws. There is however a single General Administrative Law Act ("Algemene wet bestuursrecht" or Awb) that applies both to the making of administrative decisions and the judicial review of these decisions in courts. On the basis of the Awb, citizens can oppose a decision ('besluit') made by an administrative agency ('bestuursorgaan') within the administration and apply for judicial review in courts if unsuccessful.
Unlike France or Germany, there are no special administrative courts of first instance in the Netherlands, but regular courts have an administrative "chamber" which specializes in administrative appeals. The courts of appeal in administrative cases however are specialized depending on the case, but most administrative appeals end up in the judicial section of the Council of State (Raad van State).
Before going to court, citizens must usually first object to the decision with the administrative body who made it. This is called "bezwaar". This procedure allows for the administrative body to correct possible mistakes themselves and is used to filter cases before going to court. Sometimes, instead of bezwaar, a different system is used called "administratief beroep" (administrative appeal). The difference with bezwaar is that administratief beroep is filed with a different administrative body, usually a higher ranking one, than the administrative body that made the primary decision. Administratief beroep is available only if the law on which the primary decision is based specifically provides for it. An example involves objecting to a traffic ticket with the district attorney ("officier van justitie"), after which the decision can be appealed in court.
Sweden.
In Sweden, there is a system of administrative courts that considers only administrative law cases, and is completely separate from the system of general courts. This system has three tiers, with 12 county administrative courts ("förvaltningsrätt") as the first tier, four administrative courts of appeal ("kammarrätt") as the second tier, and the Supreme Administrative Court of Sweden ("Högsta Förvaltningsdomstolen") as the third tier.
Migration cases are handled in a two-tier system, effectively within the system general administrative courts. Three of the administrative courts serve as migration courts ("migrationsdomstol") with the Administrative Court of Appeal in Stockholm serving as the Migration Court of Appeal ("Migrationsöverdomstolen").
Brazil.
In Brazil, unlike most Civil-law jurisdictions, there is no specialized court or section to deal with administrative cases. In 1998, a constitutional reform, led by the government of the President Fernando Henrique Cardoso, introduced regulatory agencies as a part of the executive branch. Since 1988, Brazilian administrative law has been strongly influenced by the judicial interpretations of the constitutional principles of public administration (art. 37 of Federal Constitution): legality, impersonality, publicity of administrative acts, morality and efficiency.
Chile.
The President of the Republic exercises the administrative function, in collaboration with several Ministries or other authorities with "ministerial rank". Each Ministry has one or more under-secretary that performs through public services the actual satisfaction of public needs. There is not a single specialized court to deal with actions against the Administrative entities, but instead there are several specialized courts and procedures of review.
People's Republic of China.
Administrative law in the People's Republic of China was virtually non-existent before the economic reform era initiated by Deng Xiaoping. Since the 1980s, the People's Republic of China has constructed a new legal framework for administrative law, establishing control mechanisms for overseeing the bureaucracy and disciplinary committees for the Communist Party of China. However, many have argued that the usefulness of these laws is vastly inadequate in terms of controlling government actions, largely because of institutional and systemic obstacles like a weak judiciary, poorly trained judges and lawyers, and corruption.
In 1990, the Administrative Supervision Regulations (行政检查条例) and the Administrative Reconsideration Regulations (行政复议条例) were passed. Both regulations have since been amended and upgraded into laws. The 1993 State Civil Servant Provisional Regulations (国家公务员暂行条例) changed the way government officials were selected and promoted, requiring that they pass exams and yearly appraisals, and introduced a rotation system. In 1994, the State Compensation Law (国家赔偿法) was passed, followed by the Administrative Penalties Law (行政处罚法) in 1996.
Ukraine.
As a homogeneous legal substance isolated in a system of jurisprudence, the administrative law of Ukraine is characterized as: (1) a branch of law; (2) a science; (3) a discipline.

</doc>
<doc id="2561" url="http://en.wikipedia.org/wiki?curid=2561" title="American political scandals">
American political scandals

American political scandals may refer to:

</doc>
<doc id="2563" url="http://en.wikipedia.org/wiki?curid=2563" title="Arthur Phillip">
Arthur Phillip

Captain (later Admiral) Arthur Phillip RN (11 October 173831 August 1814) was the first Governor of New South Wales and founder of the settlement which became Sydney.
After much experience at sea, including command of a ship that was saved in a storm by convicts, Phillip sailed with the First Fleet, as Governor-designate of the proposed British penal colony of New South Wales. In February 1788, he selected its location to be Port Jackson (now Sydney Harbour).
Phillip was a far-sighted governor, who soon saw that New South Wales would need a civil administration and a system for emancipating the convicts. But his plan to bring skilled tradesmen on the voyage had been rejected, and he faced immense problems of labour, discipline and supply. Also his friendly attitude towards the aborigines was sorely tested when they killed his gamekeeper, and he was not able to assert a clear policy about them.
The arrival of the Second and Third Fleets placed new pressures on the scarce local resources, but by the time Phillip sailed home in December 1792, the colony was taking shape, with official land-grants and systematic farming and water-supply.
Phillip retired in 1805, but continued to correspond with his friends in New South Wales and to promote the colony's interests.
Early life and naval career.
Arthur Phillip was born 11 October 1738 in London England, the son of Jacob Phillip, a Frankfurt-born language teacher, and his English wife, Elizabeth Breach. His father died a year after he was born. His mother Elizabeth was originally married to a sailor named Herbert who died at sea of yellow fever. Phillip's mother claimed him as the father of her son so he could be enrolled in the Greenwich Hospital School, part of Greenwich Hospital,a free school for the orphans of men lost at sea supported by Queen Mary
. The treatment of the students was spartan but educational. Phillip learned how to navigate, draw (a skill necessary for making navigation charts) .At the age of 13 was apprenticed to the merchant navy. He spoke a number of languages in addition to English, including French, German and Portuguese.
Seven Years' War and Spanish-Portuguese War.
Phillip joined the Royal Navy at about fifteen, and saw action at the outbreak of the Seven Years' War in the Mediterranean at the Battle of Minorca. In 1762 he was promoted to Lieutenant, but was placed on half pay when the Seven Years' War ended in 1763. During this period he married, and farmed in Lyndhurst, Hampshire.
In 1774 Phillip joined the Portuguese Navy as a captain, serving in the War against Spain. While with the Portuguese Navy, Phillip commanded a frigate, the "Nossa Senhora do Pilar." On this ship he took a detachment of troops from Rio de Janeiro to Colonia do Sacramento on the Rio de la Plata (opposite Buenos Aires) to relieve the garrison there. This voyage also conveyed a consignment of convicts assigned to carry out work at Colonia. During a storm encountered in the course of the voyage, the convicts assisted in working the ship and, on arrival at Colonia, Phillip recommended that they be rewarded for saving the ship by remission of their sentences. A garbled version of this eventually found its way into the English press when Phillip was appointed in 1786 to lead the expedition to Sydney. Phillip played a leading part in the capture of the Spanish ship San Agustín, on 19 April 1777, off Santa Catarina. The "San Agustin" was commissioned into the Portuguese Navy as the "Santo Agostinho", and command of her was given to Phillip. The action was reported in the English press: 
Madrid, Aug. 28. Letters from Lisbon bring the following Account from Rio Janeiro: That the St. Augustine, of 70 Guns, having being separated from the Squadron of M. Casa Tilly, was attacked by two Portugueze Ships, against which they defended themselves for a Day and a Night, but being next Day surrounded by the Portugueze Fleet, was obliged to surrender.
In 1778 Britain was again at war, and Phillip was recalled to active service, and in 1779 obtained his first command, HMS "Basilisk". He was promoted to captain in 1781, and was given command of .
In July 1782, in a change of government, Thomas Townshend became Secretary of State for Home and American Affairs, and assumed responsibility for organising an expedition against Spanish America. Like his predecessor, Lord Germain, he turned for advice to Arthur Phillip. A letter from Phillip to Sandwich of 17 January 1781 records Phillip's loan to Sandwich of his charts of the Plata and Brazilian coasts for use in organising the expedition. Phillip's plan was for a squadron of three ships of the line and a frigate to mount a raid on Buenos Aires and Monte Video, then to proceed to the coasts of Chile, Peru and Mexico to maraud, and ultimately to cross the Pacific to join the British Navy's East India squadron for an attack on Manila. The expedition, consisting of the "Grafton," 70 guns, "Elizabeth," 74 guns, "Europe," 64 guns, and the "Iphigenia" frigate, sailed on 16 January 1783, under the command of Commodore Robert Kingsmill. Phillip was given command of the 64-gun , or "Europe". Shortly after sailing, an armistice was concluded between Great Britain and Spain. Phillip learnt of this in April when he put in for storm repairs at Rio de Janeiro. Phillip wrote to Townshend from Rio de Janeiro on 25 April 1783, expressing his disappointment that the ending of the American War had robbed him of the opportunity for naval glory in South America.
After his return to England from India in April 1784, Phillip remained in close contact with Townshend, now Lord Sydney, and the Home Office Under Secretary, Evan Nepean. From October 1784 to September 1786 he was employed by Nepean, who was in charge of the Secret Service relating to the Bourbon Powers, France and Spain, to spy on the French naval arsenals at Toulon and other ports. There was fear that Britain would soon be at war with these powers as a consequence of the Batavian Revolution in the Netherlands.
At this time, Lord Sandwich, together with the President of the Royal Society, Sir Joseph Banks, was advocating establishment of a British colony in New South Wales. A colony there would be of great assistance to the British Navy in facilitating attacks on the Spanish possessions in Chile and Peru, as Banks's collaborators, James Matra, Captain Sir George Young and Sir John Call pointed out in written proposals on the subject. The British Government took the decision to found the Botany Bay colony in mid-1786. Lord Sydney, as Secretary of State for the Home Office, was the minister in charge of this undertaking, and in September 1786 he appointed Phillip commodore of the fleet which was to transport the convicts and soldiers who were to be the new settlers to Botany Bay. Upon arrival there, Phillip was to assume the powers of Captain General and Governor in Chief of the new colony. A subsidiary colony was to be founded on Norfolk Island, as recommended by Sir John Call, to take advantage for naval purposes of that island's native flax and timber. Phillip's fleet sailed from Portsmouth in May 1787.
Governor of New South Wales.
In October 1786, Phillip was appointed captain of and named Governor-designate of New South Wales, the proposed British colony on the east coast of Australia, by Lord Sydney, the Home Secretary.
Phillip had a very difficult time assembling the fleet which was to make the eight-month sea voyage to Australia. Everything a new colony might need had to be taken, since Phillip had no real idea of what he might find when he got there. There were few funds available for equipping the expedition. His suggestion that people with experience in farming, building and crafts be included was rejected. Most of the 772 convicts (of whom 732 survived the voyage) were petty thieves from the London slums. Phillip was accompanied by a contingent of marines and a handful of other officers who were to administer the colony.
The 11 ships of the First Fleet set sail on 13 May 1787. The leading ship, reached Botany Bay setting up camp on the Kurnell Peninsula, on 18 January 1788. Phillip soon decided that this site, chosen on the recommendation of Sir Joseph Banks, who had accompanied James Cook in 1770, was not suitable, since it had poor soil, no secure anchorage and no reliable water source. After some exploration Phillip decided to go on to Port Jackson, and on 26 January the marines and convicts were landed at Sydney Cove, which Phillip named after Lord Sydney.
Shortly after establishing the settlement at Port Jackson, on 15 February 1788, Phillip sent Lieutenant Philip Gidley King with 8 free men and a number of convicts to establish the second British colony in the Pacific at Norfolk Island. This was partly in response to a perceived threat of losing Norfolk Island to the French and partly to establish an alternative food source for the new colony.
The early days of the settlement were chaotic and difficult. With limited supplies, the cultivation of food was imperative, but the soils around Sydney were poor, the climate was unfamiliar, and moreover very few of the convicts had any knowledge of agriculture. Farming tools were scarce and the convicts were unwilling farm labourers. The colony was on the verge of outright starvation for an extended period. The marines, poorly disciplined themselves in many cases, were not interested in convict discipline. Almost at once, therefore, Phillip had to appoint overseers from among the ranks of the convicts to get the others working. This was the beginning of the process of convict emancipation which was to culminate in the reforms of Lachlan Macquarie after 1811.
Phillip showed in other ways that he recognised that New South Wales could not be run simply as a prison camp. Lord Sydney, often criticised as an ineffectual incompetent, had made one fundamental decision about the settlement that was to influence it from the start. Instead of just establishing it as a military prison, he provided for a civil administration, with courts of law. Two convicts, Henry and Susannah Kable, sought to sue Duncan Sinclair, the captain of Alexander, for stealing their possessions during the voyage. Convicts in Britain had no right to sue, and Sinclair had boasted that he could not be sued by them. Someone in Government obviously had a quiet word in Kable's ear, as when the court met and Sinclair challenged the prosecution on the ground that the Kables were felons, the court required him to prove it. As all the convict records had been left behind in England, he could not do so, and the court ordered the captain to make restitution. Further, soon after Lord Sydney appointed him governor of New South Wales Arthur Phillip drew up a detailed memorandum of his plans for the proposed new colony. In one paragraph he wrote: "The laws of this country will of course, be introduced in [New South Wales, and there is one that I would wish to take place from the moment his Majesty's forces take possession of the country: That there can be no slavery in a free land, and consequently no slaves", and he meant what he said. Nevertheless, Phillip believed in discipline, and floggings and hangings were commonplace, although Philip commuted many death sentences.
Phillip also had to adopt a policy towards the Eora Aboriginal people, who lived around the waters of Sydney Harbour. Phillip ordered that they must be well-treated, and that anyone killing Aboriginal people would be hanged. Phillip befriended an Eora man called Bennelong, and later took him to England. On the beach at Manly, a misunderstanding arose and Phillip was speared in the shoulder: but he ordered his men not to retaliate. Phillip went some way towards winning the trust of the Eora, although the settlers were at all times treated extremely warily. Soon, a virulent disease, smallpox that was believed to be on account of the white settlers, and other European-introduced epidemics, ravaged the Eora population.
The Governor's main problem was with his own military officers, who wanted large grants of land, which Phillip had not been authorised to grant. The officers were expected to grow food, but they considered this beneath them. As a result scurvy broke out, and in October 1788 Phillip had to send "Sirius" to Cape Town for supplies, and strict rationing was introduced, with thefts of food punished by hanging. Arthur Phillip quoted "The living conditions need to improve or my men won't work as hard, so I have come to a conclusion that I must hire surgeons to fix the convicts."
Stabilising the colony.
Despite Phillip's earlier order that Aboriginal Australians must never be slain, and his insistence that no retaliation be taken to avenge his own non-fatal spearing, Phillip's stance toward Aboriginals changed markedly after the death of his gamekeeper, John MacIntyre. After being fatally wounded by an Aboriginal man, on his deathbed, MacIntyre confessed to a priest that he had exhibited cruelty to Aboriginals. MacIntyre, suspected of hunting more than just game, was dreaded by Bennelong and other Aboriginals, and is believed to have been wounded in retribution for the Aboriginals he had killed. Nevertheless, Phillip, alarmed and outraged, made a surprising move, ordering that the Natives be made severe examples of. He ordered a party to capture six Natives the very next day, 14 December 1790, and put them to death.
Lieutenant William Dawes and colleague Watkin Tench, who were ordered to lead the revenge party, expressed disgust at the idea. Dawes and Tench had befriended the Aboriginals, and Dawes was even reported to have engaged in a relationship with an Aboriginal woman. Tench revealed in his journal that he had been given provisions for three days, ropes to bind the Aboriginal victims, and bags to collect their severed heads. However, the fleet was uncooperative. Phillip, growing frustrated with the burdens of upholding a colony and his health suffering, resigned soon after this episode.
By 1790 the situation had stabilised. The population of about 2,000 was adequately housed and fresh food was being grown. Phillip assigned a convict, James Ruse, land at Rose Hill (now Parramatta) to establish proper farming, and when Ruse succeeded he received the first land grant in the colony. Other convicts followed his example. "Sirius" was wrecked in March 1790 at the satellite settlement of Norfolk Island, depriving Phillip of vital supplies. In June 1790 the Second Fleet arrived with hundreds more convicts, most of them too sick to work.
By December 1790 Phillip was ready to return to England, but the colony had largely been forgotten in London and no instructions reached him, so he carried on. In 1791 he was advised that the government would send out two convoys of convicts annually, plus adequate supplies. But July, when the vessels of the Third Fleet began to arrive, with 2,000 more convicts, food again ran short, and he had to send a ship to Calcutta for supplies.
By 1792 the colony was well established, though Sydney remained an unplanned huddle of wooden huts and tents. The whaling industry was established, ships were visiting Sydney to trade, and convicts whose sentences had expired were taking up farming. John Macarthur and other officers were importing sheep and beginning to grow wool. The colony was still very short of skilled farmers, craftsmen and tradesmen, and the convicts continued to work as little as possible, even though they were working mainly to grow their own food.
In late 1792 Phillip, whose health was suffering from the poor diet, at last received permission to leave, and on 11 December 1792 he sailed in the ship "Atlantic", taking with him many specimens of plants and animals. He also took Bennelong and his friend Yemmerrawanyea, another young Indigenous Australian who, unlike Bennelong, would succumb to English weather and disease and not live to make the journey home. The European population of New South Wales at his departure was 4,221, of whom 3,099 were convicts. The early years of the colony had been years of struggle and hardship, but the worst was over, and there were no further famines in New South Wales. Phillip arrived in London in May 1793. He tendered his formal resignation and was granted a pension of £500 a year.
Later life.
Phillip's wife, Margaret, had died in 1792. Margaret Charlotte Phillip is buried with her companion Mrs Cane at St Beuno's Churchyard, Llanycil, Bala, Merionethshire. In 1794 he married Isabella Whitehead, and lived for a time at Bath. His health gradually recovered and in 1796 he went back to sea, holding a series of commands and responsible posts in the wars against the French. In January 1799 he became a Rear-Admiral. In 1805, aged 67, he retired from the Navy with the rank of Admiral of the Blue, and spent most of the rest of his life at Bath. He continued to correspond with friends in New South Wales and to promote the colony's interests with government officials. He died in Bath in 1814.
Phillip was buried in St Nicholas's Church, Bathampton. Forgotten for many years, the grave was discovered in 1897 and the Premier of New South Wales, Sir Henry Parkes who also started federation in Australia, had it restored. An annual service of remembrance is held here around Phillip's birthdate by the Britain–Australia Society to commemorate his life. A monument to Phillip in Bath Abbey Church was unveiled in 1937. Another was unveiled at St Mildred's Church, Bread St, London, in 1932; that church was destroyed in the London Blitz in 1940, but the principal elements of the monument were re-erected at the west end of Watling Street, near Saint Paul's Cathedral, in 1968. A different bust and memorial is inside the nearby church of St Mary-le-Bow. There is a statue of him in the Botanic Gardens, Sydney. There is a portrait of him by Francis Wheatley in the National Portrait Gallery, London.
Percival Serle wrote of Phillip in his "Dictionary of Australian Biography": 
Loss of remains.
In 2007, Geoffrey Robertson QC alleged that Phillip's remains are no longer in St Nicholas Church, Bathampton and have been lost: "...Captain Arthur Phillip is not where the ledger stone says he is: it may be that he is buried somewhere outside, it may simply be that he is simply lost. But he is not where Australians have been led to believe that he now lies." Robertson also believes it was a "disgraceful slur" on Phillip's legacy that he was not buried in one of England's great cathedrals and was relegated to a small village church. Robertson is campaigning for a rigorous search for the remains, which he believes should be re-interred in Australia.
In his honour.
His name is commemorated in Australia by Port Phillip, Phillip Island (Victoria), Phillip Island, the federal electorate of Phillip (1949–1993), the suburb of Phillip in Canberra, the Governor Phillip Tower building in Sydney, and many streets, parks and schools including a state high school in Parramatta.
Popular culture.
Phillip is a prominent character in Timberlake Wertenbaker's play "Our Country's Good", in which he commissions Lieutenant Ralph Clark to stage a production of "The Recruiting Officer". He is shown as compassionate and just, but receives little support from his fellow officers. He is also prominent in "Banished" and is played by David Wenham.
Phillip is referred to in the John Williamson song "Chains around my ankle".
Kate Grenville's 2008 novel "The Lieutenant" portrays Phillip through the character Commodore James Gilbert.

</doc>
<doc id="2564" url="http://en.wikipedia.org/wiki?curid=2564" title="April 10">
April 10


</doc>
<doc id="2573" url="http://en.wikipedia.org/wiki?curid=2573" title="Angus">
Angus

Angus () is one of the 32 local government council areas of Scotland, a registration county and a lieutenancy area. The council area borders Aberdeenshire, Dundee City and Perth and Kinross. Main industries include agriculture and fishing. Global pharmaceuticals company GSK has a significant presence in Montrose in the north of the county.
Angus was historically a county, known officially as Forfarshire from the 18th century until 1928. It remains a registration county and a lieutenancy area. In 1975 its administrative functions were transferred to the council district of the Tayside Region, and in 1995 further reform resulted in the establishment of the unitary Angus Council.
History.
Prehistory.
The area that now comprises Angus has been occupied since at least the Neolithic period. Material taken from postholes from an enclosure at Douglasmuir, near Friockheim, about five miles north of Arbroath have been radiocarbon dated to around 3500 BC The function of the enclosure is unknown, but may have been for agriculture or for ceremonial purposes.
Bronze age archaeology is to be found in abundance in the area. Examples include the short-cist burials found near West Newbigging, about a mile to the North of the town. These burials included pottery urns, a pair of silver discs and a gold armlet. Iron Age archaeology is also well represented, for example in the souterrain nearby Warddykes cemetery and at West Grange of Conan, as well as the better-known examples at Carlungie and Ardestie.
Mediaeval history.
The county is traditionally associated with the Pictish kingdom of Circinn, which is thought to have encompassed Angus and the Mearns. Bordering it were the kingdoms of Ce (Mar and Buchan) to the North, Fotla (Atholl) to the West, and Fib (Fife) to the South.
The most visible remnants of the Pictish age are the numerous sculptured stones that can be found throughout Angus. Of particular note are the collections found at Aberlemno, St Vigeans, Kirriemuir and Monifieth.
Angus shares borders with Kincardineshire to the north-east, Aberdeenshire to the north and Perthshire to the west. Southwards, it faces Fife across the Firth of Tay.
Angus is marketed as the birthplace of Scotland. The signing of the Declaration of Arbroath at Arbroath Abbey in 1320 marked Scotland's establishment as an independent nation. It is an area of rich history from Pictish times onwards. Notable historic sites in addition to Arbroath Abbey include Glamis Castle, Arbroath Signal Tower museum and the Bell Rock Light House.
Main industries include agriculture and fishing. Global pharmaceuticals company GSK has a significant presence in Montrose in the north of the county.
Demography.
Population structure.
In the 2001 census the population of Angus was recorded as 108,400. 20.14% were under the age of 16, 63.15% were between 16 and 65 and 18.05% were aged 65 or above.
Of the 16 to 74 age group, 32.84% had no formal qualifications, 27.08% were educated to 'O' Grade/Standard Grade level, 14.38% to Higher level, 7.64% to HND or equivalent level and 18.06% to degree level.
Language in Angus.
The most recent available census results (2001) show that Gaelic is spoken by 0.45% of the Angus population. This, similar to other lowland areas, is lower than the national average of 1.16%. These figures are self-reported and are not broken down into levels of fluency.
Historically, the dominant language in Angus was Pictish until the sixth to seventh centuries AD when the area became progressively gaelicised, with Pictish extinct by the mid-ninth century. Gaelic/Middle Irish began to retreat from lowland areas in the late-eleventh century and was absent from the Eastern lowlands by the fourteenth century. It was replaced there by Middle Scots, the contemporary local South Northern dialect of Modern Scots, while Gaelic persisted as a majority language in the highland Glens until the 19th century. Scottish English is now increasingly replacing Scots.
Angus Council are planning to raise the status of Gaelic in the county by adopting a series of measures, including bilingual road signage, communications, vehicle livery and staffing.
Government.
Local government.
Angus () is one of the 32 local government council areas of Scotland. In 1996, two-tier local government council was abolished and Angus was established as one of the replacement single-tier Council Areas.
The boundaries of the present council area are exactly the same as those of the county minus the City of Dundee.
The council area borders Aberdeenshire, Dundee City and Perth and Kinross.
Parliamentary representation.
Areas similar to that of the council area are covered by the Angus Westminster constituency for the UK Parliament and the area is also represented at the Scottish Parliament by both the Angus and North Tayside Holyrood constituencies.
Geography.
Angus can be split into three geographic areas. To the north and west, the topography is mountainous. This is the area of the five Angus Glens, which is sparsely populated and where the main industry is hill-farming. To the south and east the topography consists of rolling hills bordering the sea. This area is well populated, with the larger towns and the city of Dundee on the coast. In between lies Strathmore ("the Great Valley"), which is a fertile agricultural area noted for the growing of potatoes, soft fruit and the raising of Angus cattle. Montrose in the north east of the county is notable for its tidal basin.

</doc>
<doc id="2575" url="http://en.wikipedia.org/wiki?curid=2575" title="André the Giant">
André the Giant

André René Roussimoff (May 19, 1946 – January 27, 1993), best known as André the Giant, was a French professional wrestler and actor. His best-remembered acting role was that of Fezzik, the giant in the film "The Princess Bride". His size was a result of gigantism (after which he suffered from acromegaly) and led to his being called "The Eighth Wonder of the World".
In the World Wrestling Federation (now known as WWE), Roussimoff was a one-time WWF Champion and a one-time WWF World Tag Team Champion. In 1993, André was the inaugural inductee into the WWF Hall of Fame.
Early life.
André Roussimoff was born in Grenoble, France, to Boris and Mariann Roussimouff, a couple of Bulgarian and Polish ancestry. As a child, he displayed symptoms of his gigantism very early, reaching a height of 6'3" (190.5 cm) and a weight of by the age of 12. Unable to fit on the school bus, he was driven to school by playwright Samuel Beckett, a neighbor. Roussimoff was a good student, but he dropped out after the 8th grade since he did not think having a high school education was necessary for a farm laborer. He then worked on a farm, completed an apprenticeship in woodworking, and next worked in a factory that manufactured engines for hay balers. None of these brought him any satisfaction.
Professional wrestling career.
Early career.
At age 17, Roussimoff moved to Paris and was taught the art of professional wrestling by a local promoter who recognized the earning potential of Roussimoff's size. He trained at night and worked as a mover during the day to pay living expenses. Roussimoff was billed as "Géant Ferré", taken from the name of a mythical French giant, and began wrestling in Paris and nearby areas. Canadian promoter and wrestler Frank Valois met Roussimoff in 1966, becoming his business manager and adviser. Roussimoff began making a name for himself wrestling in the United Kingdom, Germany, Australia, New Zealand, and Africa.
He made his Japanese debut in 1970, billed as "Monster Roussimoff", wrestling for the International Wrestling Enterprise. Wrestling as both a singles and tag team competitor, he quickly won the company's tag team championship alongside Michael Nador. During his time in Japan, doctors first informed Roussimoff that he suffered from acromegaly.
Roussimoff next moved to Montréal, Québec, where he became an immediate success, regularly selling out the Montreal Forum. However, promoters eventually ran out of plausible opponents for him and, as the novelty of his size wore off, the gate receipts dwindled. Roussimoff wrestled numerous times in 1972 for Verne Gagne's American Wrestling Association (AWA) as a special attraction until Valois appealed to Vince McMahon Sr., founder of the World Wide Wrestling Federation (WWWF), for advice. McMahon suggested several changes. He felt Roussimoff should be portrayed as a large, immovable monster, and to enhance the perception of his size, McMahon discouraged Roussimoff from performing maneuvers such as dropkicks (although he was capable of performing such agile maneuvers before his health deteriorated in later life). He also began billing Roussimoff as "André the Giant" and set up a travel-intensive schedule, loaning him to wrestling associations around the world, to keep him from becoming overexposed in any area. Promoters had to guarantee André a certain amount of money as well as pay McMahon's WWF a booking fee.
World Wide Wrestling Federation/World Wrestling Federation.
Debut and various feuds (1973–1987).
On March 26, 1973, André debuted in the World Wide Wrestling Federation (later World Wrestling Federation) as a fan favorite, defeating Buddy Wolfe in New York's Madison Square Garden.
André was one of professional wrestling's most beloved "babyfaces" throughout the 1970s and early 1980s. As such, Gorilla Monsoon insisted that André was never defeated for 15 years by pinfall or submission prior to WrestleMania III. This, however, is not true. André actually had lost cleanly in matches outside WWF parameters: a pinfall loss in Mexico to Canek in 1984 and a submission loss in Japan to Antonio Inoki in June 1986. He also had sixty-minute time limit draws with the two other major world champions of the day, Harley Race and Nick Bockwinkel.
In 1976 André fought professional boxer Chuck Wepner in an unscripted boxer-vs-wrestler fight. The wild fight was shown via telecast as part of the undercard of the Muhammad Ali vs Antonio Inoki fight and ended when André threw Wepner over the top rope and outside the ring.
In 1980, he feuded with Hulk Hogan, wrestling him at Shea Stadium's Showdown at Shea and in Pennsylvania. The feud continued in Japan in 1982 and 1983.
In 1982, Vince McMahon, Sr. sold the World Wrestling Federation to his son, Vince McMahon, Jr.. As McMahon began to expand his newly acquired promotion to the national level, he required his wrestlers to appear exclusively for him. McMahon signed André to these terms in 1984, although he still allowed the Giant to work in Japan for New Japan Pro Wrestling (NJPW).
One of André's feuds pitted him against "the Mongolian Giant" Killer Khan. According to the storyline, Khan had snapped André's ankle during a match on May 2, 1981, in Rochester, New York, by leaping off the top rope and crashing down upon it with his knee-drop. In reality, André had broken his ankle getting out of bed the morning before the match. The injury and subsequent rehabilitation was worked into the existing André/Khan storyline. After a stay at Beth Israel Hospital in Boston, André returned with payback on his mind. The two battled on July 20, 1981, at Madison Square Garden in a match that resulted in a double disqualification. Their feud continued as fans filled arenas up and down the east coast to witness their matches. On November 14, 1981, at the Philadelphia Spectrum, André decisively defeated Khan in what was billed as a "Mongolian Stretcher Match", in which the loser must be taken to the dressing room on a stretcher.
Another feud involved a man who considered himself to be "the true giant" of wrestling: Big John Studd. Throughout the early to mid 1980s, André and Studd fought all over the world, battling to try to determine who the real giant of wrestling was. In December 1984, Studd took the feud to a new level when he and partner Ken Patera knocked out André during a televised tag team match and proceeded to cut off André's hair. After gaining revenge on Patera, André met Studd in a "Body Slam Challenge" at the first WrestleMania, held March 31, 1985, at Madison Square Garden in New York City. André slammed Studd to win the match and collect the $15,000 prize, then proceeded to throw cash to the fans before having the bag stolen from him by Studd's manager, Bobby "The Brain" Heenan.
The following year, at WrestleMania 2 on April 7, 1986, André continued to display his dominance by winning a twenty-man battle royal which featured top NFL stars and wrestlers. André last eliminated Bret Hart to win the contest.
After WrestleMania 2, André continued his feud with Studd and King Kong Bundy. At about this time, André requested a leave of absence to tend to his health—effects from his acromegaly were beginning to take their toll—as well as tour Japan. He had also gotten a part in the film "The Princess Bride". To explain Andre's absence, a storyline was developed to have Heenan—suggesting that André was secretly afraid of Studd and Bundy, whom Heenan bragged were unbeatable—challenge Andre and a partner of his choosing to wrestle Studd and Bundy in a televised tag team match. When Andre failed to show, WWF President Jack Tunney indefinitely suspended Andre. Later in the summer of 1986, upon Andre's return to the United States, he began wearing a mask and competing as the "Giant Machine" in a stable known as The Machines. (Big Machine and Super Machine were the other members.) The WWF's television announcers sold the Machines—a gimmick was copied from New Japan Pro Wrestling character "Super Strong Machine", played by Japanese wrestler Junji Hirata, —as "a new tag team from Japan" and claimed not to know the identities of the wrestlers, even though it was obvious to fans and the television audience that it was André competing as the Giant Machine. Heenan, Studd, and Bundy complained to Tunney, who eventually told Heenan that if it could be proven that André and the Giant Machine were the same person, André would be fired. André thwarted Heenan, Studd, and Bundy at every turn. Then, in late 1986, the Giant Machine "disappeared," and André was reinstated. Foreshadowing André's heel turn, Heenan expressed his approval of the reinstatement but did not explain why.
Feud with Hulk Hogan and WWF Champion (1987–1988).
André agreed to turn heel in early 1987 to be the counter to the biggest "babyface" in professional wrestling at that time, Hulk Hogan. On an edition of "Piper's Pit" in January 1987, Hogan was presented a trophy for being the WWF Champion for three years; André came out to congratulate him. On the following week's "Piper's Pit", André was presented a slightly smaller trophy for being "the only undefeated wrestler in wrestling history." Although André had suffered a handful of countout and disqualification losses in WWF, he had never been pinned or forced to submit in a WWF ring. Hogan came out to congratulate André and ended up being the focal point of the interview. A visibly "annoyed" André walked out in the midst of Hogan's speech. A "discussion" between André and Hogan was scheduled, and on a "Piper's Pit" that aired February 7, 1987, the two met. Hogan was introduced first, followed by André. André was led by longtime rival Bobby Heenan. Speaking on behalf of his new protégé, Heenan accused Hogan of only being André's friend so he wouldn't have to defend his title against him. Hogan tried to reason with André but his pleas were ignored as he challenged Hogan to a match for the WWF Championship at WrestleMania III. Hogan still couldn't believe what André was doing, prompting Heenan to say "You can't believe it, maybe you'll believe this Hogan" before André ripped the t-shirt and crucifix from Hogan.
Following Hogan's acceptance of André's challenge on a later edition of Piper's Pit, the two were part of a 20-man over the top rope Battle Royal on the March 14 edition of Saturday Night's Main Event at the Joe Louis Arena in Detroit. Although the Battle Royal was won by Hercules, André gained a psychological advantage over Hogan when he threw the WWF Champion over the top rope. The match, which was actually taped on February 21, 1987, aired only 2 weeks before Wrestlemania III to make it seem like Hogan had met his match in André the Giant.
At WrestleMania III, he was billed at , and the stress of such immense weight on his bones and joints resulted in constant pain. After recent back surgery, he was also wearing a brace underneath his wrestling singlet. In front of a record crowd of 93,173 Hogan won the match after body slamming (later dubbed "the bodyslam heard around the world") André, followed by Hogan's running leg drop finisher. Years later, Hogan claimed that André was so heavy, he felt more like , and that he actually tore his latissimus dorsi muscle slamming him. Another famous story about the match is that no one, not even WWF owner Vince McMahon, knew until the day of the event if André would lose the match. In reality André had agreed to lose the match some time before, mostly for health reasons, though he almost pinned Hogan (albeit unintentionally) in the second minute of the match after Hogan tried to slam The Giant but couldn't hold his weight. Contrary to popular belief, it was not the first time that Hogan had successfully body slammed André in a WWF match. A then-heel Hogan slammed a then-face André following their match at the "Showdown at Shea" on August 9, 1980, though André was much lighter (around ) and more athletic at the time (Hogan also slammed André in a match in Hamburg, PA a month later). This took place in the territorial days of American wrestling three years before WWF began national expansion, so many of those who watched Wrestlemania III had never seen The Giant slammed (André had also previously allowed Kamala, Harley Race, El Canek, Masked Superstar, Stan Hansen, Antonio Inoki & Riki Choshu to slam him.) By the time WrestleMania III had rolled around, the WWF had gone national, giving more meaning to the André–Hogan match that took place then. The feud between André and Hogan simmered during the summer of 1987, even as Roussimoff's health declined. The feud began heating up again when each wrestler was named the captain of rival teams at the inaugural Survivor Series event. André's team won the main event after André pinned Bam Bam Bigelow.
In the meantime, "The Million Dollar Man" Ted DiBiase failed to persuade Hogan to sell him the WWF Championship. After failing to defeat Hogan in a subsequent series of matches, DiBiase turned to André to win it for him. Acting as his hired gun, André won the WWF title from Hogan (his first title) on February 5, 1988 in a match where it was later revealed appointed referee Dave Hebner was "detained backstage", and a replacement (whom Hogan afterwards initially accused of having been paid by DiBiase to get plastic surgery to look like Dave, but in fact was revealed to have been his 'evil' twin brother Earl Hebner), made a three count on Hogan while his shoulders were off the mat. After winning, André "sold" the title to DiBiase; the transaction was declared invalid by then-WWF President Jack Tunney and the title was vacated. This was shown on WWF's NBC program "The Main Event". At WrestleMania IV, André and Hulk Hogan fought to a double disqualification in a WWF title tournament match (with the idea in the storyline saying that André was again working on DiBiase's behalf in giving DiBiase a clearer path in the tournament). Afterward, André and Hogan's feud died down after a steel cage match held at "WrestleFest" on July 31, 1988 in Milwaukee.
At the inaugural SummerSlam pay-per-view held at Madison Square Garden, André and DiBiase (billed as "The Mega Bucks") faced Hogan and WWF Champion "Macho Man" Randy Savage (known as The Mega Powers) in the main event, with Jesse "the Body" Ventura as the special guest referee. During the match, The Mega Powers' manager Miss Elizabeth (Savage's real life wife Elizabeth Hulette) distracted The Mega Bucks and Ventura when she climbed up on the ring apron, removed her yellow skirt and walked around in a pair of red panties. This allowed Hogan and Savage time to recover and eventually win the match with Hogan pinning DiBiase.
Concurrent with the developing feud with the Mega Powers, André was placed in a feud with Jim Duggan, which began after Duggan knocked out André with a two-by-four timber during a television taping. Despite Duggan's popularity with fans, André regularly got the upper hand in the feud.
Various feuds and The Colossal Connection (1988–1990).
André's next major feud was against Jake Roberts. In this storyline, it was said André was afraid of snakes, something Roberts exposed on "Saturday Night's Main Event" when he threw his snake, Damien, on the frightened André; as a result, André suffered a kayfabe mild heart attack and vowed revenge. During the next few weeks, Roberts frequently walked to ringside during André's matches, causing him to run from the ring in fright (since he knew what was inside the bag). Throughout their feud (which culminated at WrestleMania V), Roberts constantly used Damien to gain a psychological edge over the much larger and stronger André.
In 1989, André and the returning Big John Studd briefly reprised their feud, this time with Studd as a face and André as the heel. During the late summer and fall of 1989, André engaged in a brief feud, almost entirely consisting of house shows (non-televised events), with then-Intercontinental champion The Ultimate Warrior. The younger Warrior, WWF's rising star, regularly squashed the aging André in an attempt to showcase his star quality and promote him as the "next big thing".
In late 1989, André was joined with fellow Heenan Family member Haku to form a new tag team called The Colossal Connection, in part to fill a void left by the departure of Tully Blanchard and Arn Anderson (The Brain Busters, who were also members of Heenan's stable) from the WWF, and also to continue to keep the aging André in the main event spotlight. The Colossal Connection immediately targeted WWF Tag Team Champions Demolition (who had recently won the belts from the Brain Busters). At a television taping on December 13, 1989, the Colossal Connection defeated Demolition to win the titles. André and Haku successfully defended their titles, mostly against Demolition, until WrestleMania VI on April 1, 1990, when Demolition took advantage of a mistimed move by the champions to regain the belts. After the match, a furious Heenan blamed André for the title loss and after shouting at him slapped him in the face; an angry André responded with a slap of his own that sent Heenan staggering from the ring. André also caught Haku's kick attempt, sending him reeling from the ring as well, prompting loud cheers for André for the first time in three years. André went into the match as a heel, but left as a face. Due to his ongoing health issues, André wasn't actually able to wrestle at the time of Wrestlemania VI and Haku actually wrestled the entire match against Demolition without tagging in André.
Sporadic appearances (1990–1992).
André continued to make appearances in the WWF throughout 1990 and 1991. He came to the aid of The Big Boss Man in his WrestleMania VII match against Mr. Perfect.
At one point he was advertised to enter the 1991 Royal Rumble, but backed out due to a leg injury. Andre finally returned to action on April 26, 1991 in a six man tag-team matchup when he tagged with the Rockers in a winning effort against Mr. Fuji and The Orient Express at a house show in Belfast, Northern Ireland. On May 10th he participated in a 17 man battle royal on a house show in Long Island, NY (won by Kerry Von Erich). His last major WWF storyline following WrestleMania VII had the major heel managers (Bobby Heenan, Sensational Sherri, Slick, and Mr. Fuji) trying to recruit André one-by-one, only to be turned down in various humiliating ways (i.e. Heenan had his hand crushed, Sherri received a spanking, Slick got locked in the trunk of the car he was offering to André and Mr Fuji got a pie in his face). Finally, Jimmy Hart appeared live on WWF Superstars to announce that he successfully signed André to tag-team with Earthquake. However, when asked to confirm by Gene Okerlund, André denied the claims. This cemented André's face turn. This led to Earthquake attacking André from behind (injuring his knee). Jimmy Hart would later get revenge for the humiliation by secretly signing Tugboat and forming The Natural Disasters. This led to André's final major WWF appearance at SummerSlam '91, where he seconded The Bushwhackers in their match against the Disasters. Andre was on crutches at ringside, when the Disasters won the match they set out to attack Andre, but the Legion of Doom (wearing the football pads with large spikes they wore during their ring entrances) made their way to ringside and got in between them and The Giant who was preparing to defend himself with one of his crutches. The Disasters left the ringside area as they were outnumbered by the Legion of Doom, the Bushwhackers and Andre, who struck both Earthquake and Typhoon (the former Tugboat) with the crutch as they left. His final WWF appearance came on a house show in Paris, France on October 9. He was in Davey Boy Smith's corner as the Bulldog faced Earthquake. Davey Boy hit Earthquake with Andre's crutch, allowing Smith to win.
His last U.S. television appearance was in a brief interview on World Championship Wrestling's (WCW) "" special that aired on TBS on September 2, 1992.
All Japan Pro Wrestling (1990–1992).
After WrestleMania VI, André spent the rest of his in-ring career in All Japan Pro Wrestling (AJPW). He toured with AJPW three times per year, from September 1990 to 1992, usually teaming with Giant Baba in tag team matches. He wrestled his final match in December 1992.
Acting career.
André branched out into acting again in the 1970s and 1980s, after a 1967 French boxing movie, making his USA acting debut playing a Sasquatch ("Bigfoot") on the 1970s television series "The Six Million Dollar Man". He appeared in other television shows, including "The Greatest American Hero", "B. J. and the Bear", "The Fall Guy" and 1990s "Zorro".
Towards the end of his career, André starred in several films. He had an uncredited appearance in the 1984 film "Conan the Destroyer" as Dagoth, the resurrected horned giant god who is killed by Conan (Arnold Schwarzenegger). That same year, André also made an appearance in "Micki + Maude" (billed as André Rousimmoff). He appeared most notably as Fezzik, his own favorite role, in the 1987 film "The Princess Bride". Both the film and André's performance retain a devoted following.
In his last film, he appeared in a cameo role as a circus giant in the comedy "Trading Mom", which was released a year after his death.
Personal life.
Roussimoff had one daughter, Robin Christensen Roussimoff, who was born in 1979.
André was mentioned in the 1974 Guinness Book of World Records as the highest-paid wrestler in history to that time. He had earned US$400,000 in one year during the early 1970s.
Roussimoff has been unofficially crowned "The Greatest Drunk on Earth" for once consuming 119 beers (over 41 litres) in 6 hours. On an episode of WWE's "Legends of Wrestling", Mike Graham said André once drank 156, beers in one sitting, which was confirmed by Dusty Rhodes. In her autobiography, The Fabulous Moolah writes that André drank 127 beers in a Reading, Pennsylvania, hotel bar and later passed out in the lobby. The staff could not move him and had to leave him there until the giant awoke from his slumber.
When André underwent surgery in 1987, his size made it impossible for the anesthesiologist to estimate a dosage via standard methods; consequently, his alcohol tolerance ("it usually takes two liters of vodka just to make me feel warm inside") was used as a guideline instead.
André was arrested by the Linn County, Iowa, sheriff in August 1989 and charged with assault after he allegedly roughed up a local television cameraman.
In a eulogy after his death, William Goldman, the author of the novel and the screenplay of "The Princess Bride", wrote in his nonfiction work "Which Lie Did I Tell?" that André was one of the gentlest and most generous people he ever knew. Whenever André treated someone to a meal in a restaurant he would pay, but he would also insist on paying when he was a guest. After one meal, Arnold Schwarzenegger had quietly moved to the cashier to pay before André could, but then found himself being physically lifted, carried from his table, and deposited on top of his car by André and Wilt Chamberlain.
There was a time during his childhood when he was driven to school by the playwright Samuel Beckett.
Death.
Roussimoff died in his sleep of congestive heart failure on the night of January 27, 1993, in a Paris hotel room. He was in Paris to attend the funeral of his father. Roussimoff's body was cremated according to his wishes and his ashes were scattered at his ranch in Ellerbe, North Carolina.

</doc>
<doc id="2577" url="http://en.wikipedia.org/wiki?curid=2577" title="Adrastea (moon)">
Adrastea (moon)

"Not to be confused with the asteroid called 5 Astraea"
Adrastea ( ; ), also known as , is the second by distance, and the smallest of the four inner moons of Jupiter. It was discovered in "Voyager 2" probe photographs taken in 1979, making it the first natural satellite to be discovered from images taken by an interplanetary spacecraft, rather than through a telescope. It was officially named after the mythological Adrasteia, foster mother of the Greek god Zeus—the equivalent of the Roman god Jupiter.
Adrastea is one of the few moons in the Solar System known to orbit its planet in less than the length of that planet's day. It orbits at the edge of Jupiter's Main Ring and is thought to be the main contributor of material to the Rings of Jupiter. Despite observations made in the 1990s by the Galileo spacecraft, very little is known about the moon's physical characteristics other than its size and the fact that it is tidally locked to Jupiter.
Discovery and observations.
Adrastea was discovered by David C. Jewitt and G. Edward Danielson in "Voyager 2" probe photographs taken on July 8, 1979, and received the designation . Although it appeared only as a dot, it was the first moon to be discovered by an interplanetary spacecraft. Soon after its discovery, two other of the inner moons of Jupiter (Thebe and Metis) were observed in the images taken a few weeks earlier by "Voyager 1". The "Galileo" spacecraft was able to determine the moon's shape in 1998, but the images remain poor. In 1983, Adrastea was officially named after the Greek nymph Adrastea, the daughter of Zeus and his lover Ananke.
Physical characteristics.
Adrastea has an irregular shape and measures 20×16×14 km across. A surface area estimate would be between 840 and 1,600 (~1,200) km2. This makes it the smallest of the four inner moons. The bulk, composition and mass of Adrastea are not known, but assuming that its mean density is like that of Amalthea, around 0.86 g/cm³, its mass can be estimated at about 2 kg. Amalthea's density implies that the moon is composed of water ice with a porosity of 10–15%, and Adrastea may be similar.
No surface details of Adrastea are known, due to the low resolution of available images.
Orbit.
Adrastea is the smallest and second closest member of the inner Jovian satellite family. It orbits Jupiter at a radius of about 129,000 km (1.806 Jupiter radii) at the exterior edge of the planet's Main Ring. Adrastea is one of only three moons in the Solar System known to orbit its planet in less than the length of that planet's day—the other two being Jupiter's innermost moon Metis, and Mars' moon Phobos. The orbit has very small eccentricity and inclination—around 0.0015 and 0.03°, respectively. Inclination is relative to the equator of Jupiter.
Due to tidal locking, Adrastea rotates synchronously with its orbital period, keeping one face always looking toward the planet. Its long axis is aligned towards Jupiter, this being the lowest energy configuration.
The orbit of Adrastea lies inside Jupiter's synchronous orbit radius (as does Metis’s), and as a result, tidal forces are slowly causing its orbit to decay so that it will one day impact Jupiter. If its density is similar to Amalthea's then its orbit would actually lie within the fluid Roche limit. However, since it is not breaking up, it must still lie outside its rigid Roche limit.
Relationship with Jupiter's rings.
Adrastea is the largest contributor to material in Jupiter's rings. This appears to consist primarily of material that is ejected from the surfaces of Jupiter's four small inner satellites by meteorite impacts. It is easy for the impact ejecta to be lost from these satellites into space. This is due to the satellites' low density and their surfaces lying close to the edge of their Roche spheres.
It seems that Adrastea is the most copious source of this ring material, as evidenced by the densest ring (the Main Ring) being located at and within Adrastea's orbit. More precisely, the orbit of Adrastea lies near the outer edge of Jupiter's Main Ring. The exact extent of visible ring material depends on the phase angle of the images: in forward-scattered light Adrastea is firmly outside the Main Ring, but in back-scattered light (which reveals much bigger particles) there appears to also be a narrow ringlet outside Adrastea's orbit.
References.
Cited sources

</doc>
<doc id="2578" url="http://en.wikipedia.org/wiki?curid=2578" title="Amalthea">
Amalthea

Amalthea can refer to:

</doc>
<doc id="2580" url="http://en.wikipedia.org/wiki?curid=2580" title="Ananke">
Ananke

Ananke () has several meanings:

</doc>
<doc id="2581" url="http://en.wikipedia.org/wiki?curid=2581" title="Apache HTTP Server">
Apache HTTP Server

The Apache HTTP Server, commonly referred to as Apache ( ), is a web server application notable for playing a key role in the initial growth of the World Wide Web. Originally based on the NCSA HTTPd server, development of Apache began in early 1995 after work on the NCSA code stalled. Apache quickly overtook NCSA HTTPd as the dominant HTTP server, and has remained the most popular HTTP server in use since April 1996. In 2009, it became the first web server software to serve more than 100 million websites.
Apache is developed and maintained by an open community of developers under the auspices of the Apache Software Foundation. Most commonly used on a Unix-like system, the software is available for a wide variety of operating systems, including Unix, FreeBSD, Linux, Solaris, Novell NetWare, OS X, Microsoft Windows, OS/2, TPF, OpenVMS and eComStation. Released under the Apache License, Apache is open-source software.
, Apache was estimated to serve 54.2% of all active websites and 53.3% of the top servers across all domains.
Name.
According to the FAQ in the Apache project website, the name Apache was chosen out of respect to the Native American tribe Apache and its superior skills in warfare and strategy. The name was widely believed to be a pun on A Patchy Server (since it was a set of software patches), but this is erroneous. Official documentation used to give this very explanation of the name, but in a 2000 interview, Brian Behlendorf, one of the creators of Apache, set the record straight:
Features.
Apache supports a variety of features, many implemented as compiled modules which extend the core functionality. These can range from server-side programming language support to authentication schemes. Some common language interfaces support Perl, Python, Tcl, and PHP. Popular authentication modules include mod_access, mod_auth, mod_digest, and mod_auth_digest, the successor to mod_digest. A sample of other features include Secure Sockets Layer and Transport Layer Security support (mod_ssl), a proxy module (mod_proxy), a URL rewriter (mod_rewrite), custom log files (mod_log_config), and filtering support (mod_include and mod_ext_filter).
Popular compression methods on Apache include the external extension module, mod_gzip, implemented to help with reduction of the size (weight) of web pages served over HTTP. ModSecurity is an open source intrusion detection and prevention engine for web applications. Apache logs can be analyzed through a web browser using free scripts such as AWStats/W3Perl or Visitors.
Virtual hosting allows one Apache installation to serve many different websites. For example, one machine with one Apache installation could simultaneously serve www.example.com, www.example.org, test47.test-server.example.edu, etc.
Apache features configurable error messages, DBMS-based authentication databases, and content negotiation. It is also supported by several graphical user interfaces (GUIs).
It supports password authentication and digital certificate authentication. Because the source code is freely available, anyone can adapt the server for specific needs, and there is a large public library of Apache add-ons.
Performance.
Instead of implementing a single architecture, Apache provides a variety of MultiProcessing Modules (MPMs) which allow Apache to run in a process-based, hybrid (process and thread) or event-hybrid mode, to better match the demands of each particular infrastructure. This implies that the choice of correct MPM and the correct configuration is important. Where compromises in performance need to be made, the design of Apache is to reduce latency and increase throughput, relative to simply handling more requests, thus ensuring consistent and reliable processing of requests within reasonable time-frames.
For static pages delivery, Apache 2.2 series was considered significantly slower than nginx. To address this issue, the Apache version considered by the Apache Foundation as providing high-performance is the multi-threaded version which mixes the use of several processes and several threads per process. This architecture, and the way it was implemented in the Apache 2.4 series, provides for performance equivalent or slightly better than event-based webservers, as it claimed by President of the Apache Foundation, Jim Jagielski. However, some independent benchmarks show that it still twice slower than nginx.
Licensing.
The Apache HTTP Server codebase was relicensed to the Apache 2.0 License (from the previous 1.1 license) in January 2004, and Apache HTTP Server 1.3.31 and 2.0.49 were the first releases using the new license.
The OpenBSD project did not like the change and continued the use of pre-2.0 Apache versions, effectively forking Apache 1.3.x for its purposes. Later it switched to nginx.
Development.
The Apache HTTP Server Project is a collaborative software development effort aimed at creating a robust, commercial-grade, feature-rich and freely-available source code implementation of an HTTP (Web) server. The project is jointly managed by a group of volunteers located around the world, using the Internet and the Web to communicate, plan, and develop the server and its related documentation. This project is part of the Apache Software Foundation. In addition, hundreds of users have contributed ideas, code, and documentation to the project.

</doc>
<doc id="2582" url="http://en.wikipedia.org/wiki?curid=2582" title="Alph">
Alph

Alph may refer to:

</doc>
<doc id="2583" url="http://en.wikipedia.org/wiki?curid=2583" title="Arbroath Abbey">
Arbroath Abbey

Arbroath Abbey, in the Scottish town of Arbroath, was founded in 1178 by King William the Lion for a group of Tironensian Benedictine monks from Kelso Abbey. It was consecrated in 1197 with a dedication to the deceased Saint Thomas Becket, whom the king had met at the English court. It was William's only personal foundation — he was buried before the high altar of the church in 1214.
The last Abbot was Cardinal David Beaton, who in 1522 succeeded his uncle James to become Archbishop of St Andrews. The Abbey is cared for by Historic Scotland and is open to the public throughout the year (entrance charge). The distinctive red sandstone ruins stand at the top of the High Street in Arbroath.
History.
King William gave the Abbey independence from its mother church and endowed it generously, including income from 24 parishes, land in every royal burgh and more. The Abbey's monks were allowed to run a market and build a harbour. King John of England gave the Abbey permission to buy and sell goods anywhere in England (except London) toll-free.
The Abbey, which was the richest in Scotland, is most famous for its association with the 1320 Declaration of Arbroath, believed to have been drafted by Abbot Bernard, who was the Chancellor of Scotland under King Robert I.
Since 1947, a major historical re-enactment commemorating the Declaration's signing has been held within the roofless remains of the Abbey church. The celebration and many other events are now run by the Arbroath Abbey Timethemes a local charity, and tells the story of the events which led up to the signing. This is not an annual event; the most recent performances have been in August 2000 and 2005 but more are planned. However, a special event to mark the signing is held every year on the 6th of April and involves a street procession and short piece of street theatre. 
The Abbey fell into ruin after the Reformation. From 1590 onward, its stones were raided for buildings in the town of Arbroath. This continued until 1815 when steps were taken to preserve the remaining ruins. 
On Christmas Day 1950, the Stone of Destiny was stolen from Westminster Abbey. On April 11, 1951, the missing stone was found lying on the site of the Abbey's altar.
In 2005 The Arbroath Abbey campaign was launched. The campaign seeks to gain World Heritage Status for the iconic Angus landmark that was the birthplace of one of Scotland's most significant documents, the Declaration of Arbroath. Campaigners believe that the Abbey’s historical pronouncement makes it a prime candidate to achieve World Heritage Status. MSP Alex Johnstone wrote "Clearly, the Declaration of Arbroath is a literary work of outstanding universal significance by any stretch of the imagination" In 2008, the Campaign Group Chairman, Councillor Jim Millar launched a public petition to reinforce the bid explaining "We're simply asking people to, local people especially, to sign up to the campaign to have the Declaration of Arbroath and Arbroath Abbey recognised by the United Nations. Essentially we need local people to sign up to this campaign simply because the United Nations demand it."
Architectural Description.
The Abbey was built over some sixty years using local red sandstone, but gives the impression of a single coherent, mainly 'Early English' architectural design, though the round-arched processional doorway in the western front looks back to late Norman or transitional work. The triforium (open arcade) above the door is unique in Scottish medieval architecture. It is flanked by twin towers decorated with blind arcading. The cruciform church measured long by wide. What remains of it today are the sacristy, added by Abbot Paniter in the 15th century, the southern transept, which features Scotland's largest lancet windows, part of the choir and presbytery, the southern half of the nave, parts of the western towers and the western doorway.
The church originally had a central tower and (probably) a spire. These would once have been visible for many miles over the surrounding countryside, and no doubt once acted as a sea-mark for ships. The soft sandstone of the walls was originally protected by plaster internally and render externally. These coatings are long gone and much of the architectural detail is sadly eroded, though detached fragments found in the ruins during consolidation give an impression of the original refined, rather austere, architectural effect.
The distinctive round window high in the south transept was originally lit up at night as a beacon for mariners. It is known locally as the 'Round O', and from this tradition inhabitants of Arbroath are colloquially known as 'Reid Lichties' (Scots reid = red).
Little remains of the claustral buildings of the Abbey except for the impressive gatehouse, which stretches between the south-west corner of the church and a defensive tower on the High Street, and the still complete Abbot's House, a building of the 13th, 15th and 16th centuries, which is the best-preserved of its type in Scotland.
In the summer of 2001 a new visitors' centre was opened to the public beside the Abbey's west front. This red sandstone-clad building, with its distinctive 'wave-shaped' organic roof, planted with sedum, houses displays on the history of the Abbey and some of the best surviving stonework and other relics. The upper storey features a scale model of the Abbey complex, a computer-generated 'fly-through' reconstruction of the church as it was when complete, and a viewing gallery with excellent views of the ruins. The centre won the 2002 Angus Design Award. An archaeological investigation of the site of the visitors' centre before building started revealed the foundations of the medieval precinct wall, with a gateway, and stonework discarded during manufacture, showing that the area was the site of the masons' yard while the Abbey was being built.
In Fiction.
Arbroath Abbey was the basis for the description of the ruined monastery of St Ruth in Sir Walter Scott's The Antiquary.

</doc>
<doc id="2593" url="http://en.wikipedia.org/wiki?curid=2593" title="Accounting">
Accounting

Accounting, or accountancy, is the measurement, processing and communication of financial information about economic entities. Accounting, which has been called the "language of business", measures the results of an organization's economic activities and conveys this information to a variety of users including investors, creditors, management, and regulators. Practitioners of accounting are known as accountants.
Accounting can be divided into several fields including financial accounting, management accounting, auditing, and tax accounting. Financial accounting focuses on the reporting of an organization's financial information, including the preparation of financial statements, to external users of the information, such as investors, regulators and suppliers; and management accounting focuses on the measurement, analysis and reporting of information for internal use by management. The recording of financial transactions, so that summaries of the financials may be presented in financial reports, is known as bookkeeping, of which double-entry bookkeeping is the most common system.
Accounting is facilitated by accounting organizations such as standard-setters, accounting firms and professional bodies. Financial statements are usually audited by accounting firms, and are prepared in accordance with generally accepted accounting principles (GAAP). GAAP is set by various standard-setting organizations such as the Financial Accounting Standards Board (FASB) in the United States and the Financial Reporting Council in the United Kingdom. As of 2012, "all major economies" have plans to converge towards or adopt the International Financial Reporting Standards (IFRS).
Etymology.
Both the words accounting and accountancy were in use in Great Britain by the mid-1800s, and are derived from the words "accompting" and "accountantship" used in the 18th century. In Middle English (used roughly between the 12th and the late 15th century) the verb "to account" had the form "accounten", which was derived from the Old French word "aconter", which is in turn related to the Vulgar Latin word "computare", meaning "to reckon". The base of "computare" is "putare", which "variously meant to prune, to purify, to correct an account, hence, to count or calculate, as well as to think."
The word "accountant" is derived from the French word , which is also derived from the Latin word . The word was formerly written in English as "accomptant", but in process of time the word, which was always pronounced by dropping the "p", became gradually changed both in pronunciation and in orthography to its present form.
Accounting and accountancy.
Accounting has variously been defined as the keeping or preparation of the financial records of an entity, the analysis, verification and reporting of such records and "the principles and procedures of accounting"; it also refers to the job of being an accountant.
Accountancy refers to the occupation or profession of an accountant, particularly in British English.
History.
The history of accounting is thousands of years old and can be traced to ancient civilizations. The early development of accounting dates back to ancient Mesopotamia, and is closely related to developments in writing, counting and money; there is also evidence for early forms of bookkeeping in ancient Iran, and early auditing systems by the ancient Egyptians and Babylonians. By the time of the Emperor Augustus, the Roman government had access to detailed financial information.
Double-entry bookkeeping developed in medieval Europe, and accounting split into financial accounting and management accounting with the development of joint-stock companies. Accounting began to transition into an organized profession in the nineteenth century, with local professional bodies in England merging to form the Institute of Chartered Accountants in England and Wales in 1880.
Topics.
Accounting has several subfields or subject areas, including financial accounting, management accounting, auditing, taxation and accounting information systems.
Financial accounting.
Financial accounting focuses on the reporting of an organization's financial information to external users of the information, such as investors, regulators and suppliers. It measures and records business transactions and prepares financial statements for the external users in accordance with generally accepted accounting principles (GAAP). GAAP, in turn, arises from the wide agreement between accounting theory and practice, and change over time to meet the needs of decision-makers.
Financial accounting produces past-oriented reports—for example the financial statements prepared in 2006 reports on performance in 2005—on an annual or quarterly basis, generally about the organization as a whole.
Management accounting.
Management accounting focuses on the measurement, analysis and reporting of information that can help managers in making decisions to fulfil the goals of an organization. In management accounting, internal measures and reports are based on cost-benefit analysis, and are not required to follow GAAP.
Management accounting produces future-oriented reports—for example the budget for 2006 is prepared in 2005—and the time span of reports varies widely. Such reports may include both financial and nonfinancial information, and may, for example, focus on specific products and departments.
Auditing.
Auditing is the verification of assertions made by others regarding a payoff, and in the context of accounting it is the "unbiased examination and evaluation of the financial statements of an organization".
An audit of financial statements aims to express or disclaim an opinion on the financial statements. The auditor expresses an opinion on the fairness with which the financial statements presents the financial position, results of operations, and cash flows of an entity, in accordance with GAAP and "in all material respects". An auditor is also required to identify circumstances in which GAAP has not been consistently observed.
Accounting information systems.
An accounting information system is a part of an organisation's information system that focuses almost exclusively on processing quantitative data.
Organizations.
Professional bodies.
Professional accounting bodies include the American Institute of Certified Public Accountants (AICPA) and the other 179 members of the International Federation of Accountants (IFAC), including CPA Australia, and Institute of Chartered Accountants in England and Wales (ICAEW). Professional bodies for subfields of the accounting professions also exist, for example the Chartered Institute of Management Accountants (CIMA). Many of these professional bodies offer education and training including qualification and administration for various accounting designations, such as certified public accountant and chartered accountant.
Accounting firms.
Depending on its size, a company may be legally required to have their financial statements audited by a qualified auditor, and audits are usually carried out by accounting firms.
Accounting firms grew in the United States and Europe in the late nineteenth and early twentieth century, and through several mergers there were large international accounting firms by the mid-twentieth century. Further large mergers in the late twentieth century led to the dominance by the auditing market by the Big Five accounting firms: Arthur Andersen, Deloitte, Ernst & Young, KPMG and PricewaterhouseCoopers. The demise of Arthur Andersen following the Enron scandal reduced the Big Five to the Big Four.
Standard-setters.
Generally accepted accounting principles (GAAP) are accounting standards issued by national regulatory bodies. In addition, the International Accounting Standards Board (IASB) issues the International Financial Reporting Standards (IFRS). While standards for international audit and assurance, ethics, education, and public sector accounting are all set by independent standard settings boards supported by IFAC. The International Auditing and Assurance Standards Board sets international standards for auditing, assurance, and quality control; the International Ethics Standards Board for Accountants (IESBA) sets the internationally appropriate principles- based "Code of Ethics for Professional Accounts" the International Accounting Education Standards Board (IAESB) sets professional accounting education standards; International Public Sector Accounting Standards Board (IPSASB) sets accrual-based international public sector accounting standards 
Organizations in individual countries may issue accounting standards unique to the countries. For example, in the United States the Financial Accounting Standards Board (FASB) issues the Statements of Financial Accounting Standards, which form the basis of US GAAP, and in the United Kingdom the Financial Reporting Council (FRC) sets accounting standards. However,as of 2012 "all major economies" have plans to converge towards or adopt the IFRS.
Education and qualifications.
Accounting degrees.
At least a bachelor's degree in accounting or a related field is required for most accountant and auditor job positions, and some employers prefer applicants with a master's degree. A degree in accounting may also be required for, or may be used to fulfil the requirements for, membership to professional accounting bodies. For example, the education during an accounting degree can be used to fulfil the American Institute of CPA's (AICPA) 150 semester hour requirement, and associate membership with the Certified Public Accountants Association of the UK is available after gaining a degree in finance or accounting.
A doctorate is required in order to pursue a career in accounting academia, for example to work as a university professor. The Doctor of Philosophy (PhD) and the Doctor of Business Administration (DBA) are the most popular degrees. The PhD is the most common degree for those wishing to pursue a career in academia, while DBA programs generally focus on equipping business executives for business or public careers requiring research skills and qualifications.
Professional qualifications.
Professional accounting qualifications include the Chartered Accountant designations and other qualifications including certificates and diplomas. In the United Kingdom, chartered accountants of the ICAEW undergo annual training, and are bound by the ICAEW's code of ethics and subject to its disciplinary procedures. In the United States, the requirements for joining the AICPA as a Certified Public Accountant are set by the Board of Accountancy of each state, and members agree to abide by the AICPA's Code of Professional Conduct and Bylaws.
Accounting research.
Accounting research is research on the effects of economic events on the process of accounting, and the effects of reported information on economic events. It encompasses a broad range of research areas including financial accounting, management accounting, auditing and taxation.
Accounting research is carried out both by academic researchers and practicing accountants. Academic accounting research "addresses all aspects of the accounting profession" using the scientific method, while research by practicing accountants focuses on solving problems for a client or group of clients. Academic accounting research can make significant contribution to accounting practice, although changes in accounting education and the accounting academia in recent decades has led to a divide between academia and practice in accounting.
Methodologies in academic accounting research can be classified into archival research, which examines "objective data collected from repositories"; experimental research, which examines data "the researcher gathered by administering treatments to subjects"; and analytical research, which is "based on the act of formally modeling theories or substantiating ideas in mathematical terms". This classification is not exhaustive; other possible methodologies include the use of case studies, computer simulations and field research.
Accounting and computer software.
Many laborious practices have been simplified with the help of computer software. Enterprise resource planning (ERP) software provides a comprehensive, centralized, integrated source of information that companies can use to manage all major business processes, from purchasing to manufacturing to human resources. This software can replace up to 200 individual software programs that were previously used. Computer integrated manufacturing allows products to be made and completely untouched by human hands and can increase production by having less errors in the manufacturing process.
Computers have reduced the cost of accumulating, storing, and reporting managerial accounting information and have made it possible to produce a more detailed account of all data that is entered into any given system. They have also changed business to business interaction through e-commerce. Rather than dealing with multiple companies to purchase products, a business can purchase a product at a less expensive price and take out the third party and vastly reduces expenses companies once accrued.
Additionally, Inter-organizational information system enable suppliers and businesses to be connected at all times. When a company is low on a product the supplier will be notified and fulfill an order immediately which eliminates the need for someone to do inventory, fill out the proper documents, send them out and wait for their products.
Accounting scandals.
The year 2001 witnessed a series of financial information frauds involving Enron, auditing firm Arthur Andersen, the telecommunications company WorldCom, Qwest and Sunbeam, among other well-known corporations. These problems highlighted the need to review the effectiveness of accounting standards, auditing regulations and corporate governance principles. In some cases, management manipulated the figures shown in financial reports to indicate a better economic performance. In others, tax and regulatory incentives encouraged over-leveraging of companies and decisions to bear extraordinary and unjustified risk.
The Enron scandal deeply influenced the development of new regulations to improve the reliability of financial reporting, and increased public awareness about the importance of having accounting standards that show the financial reality of companies and the objectivity and independence of auditing firms.
In addition to being the largest bankruptcy reorganization in American history, the Enron scandal undoubtedly is the biggest audit failure. It involved a financial scandal of Enron Corporation and their auditors Arthur Andersen, which was revealed in late 2001. The scandal caused the dissolution of Arthur Andersen, which at the time was one of the five largest accounting firms in the world. After a series of revelations involving irregular accounting procedures conducted throughout the 1990s, Enron filed for Chapter 11 bankruptcy protection in December 2001.
One consequence of these events was the passage of Sarbanes–Oxley Act in 2002, as a result of the first admissions of fraudulent behavior made by Enron. The act significantly raises criminal penalties for securities fraud, for destroying, altering or fabricating records in federal investigations or any scheme or attempt to defraud shareholders.

</doc>
<doc id="2594" url="http://en.wikipedia.org/wiki?curid=2594" title="Ant">
Ant

Ants are social insects of the family Formicidae and, along with the related wasps and bees, belong to the order Hymenoptera. Ants evolved from wasp-like ancestors in the mid-Cretaceous period between 110 and 130 million years ago and diversified after the rise of flowering plants. More than 12,500 of an estimated total of 22,000 species have been classified. They are easily identified by their elbowed antennae and the distinctive node-like structure that forms their slender waists.
Ants form colonies that range in size from a few dozen predatory individuals living in small natural cavities to highly organised colonies that may occupy large territories and consist of millions of individuals. Larger colonies consist mostly of sterile, wingless females forming castes of "workers", "soldiers", or other specialised groups. Nearly all ant colonies also have some fertile males called "drones" and one or more fertile females called "queens". The colonies sometimes are described as superorganisms because the ants appear to operate as a unified entity, collectively working together to support the colony.
Ants have colonised almost every landmass on Earth. The only places lacking indigenous ants are Antarctica and a few remote or inhospitable islands. Ants thrive in most ecosystems and may form 15–25% of the terrestrial animal biomass. Their success in so many environments has been attributed to their social organisation and their ability to modify habitats, tap resources, and defend themselves. Their long co-evolution with other species has led to mimetic, commensal, parasitic, and mutualistic relationships.
Ant societies have division of labour, communication between individuals, and an ability to solve complex problems. These parallels with human societies have long been an inspiration and subject of study. Many human cultures make use of ants in cuisine, medication, and rituals. Some species are valued in their role as biological pest control agents. Their ability to exploit resources may bring ants into conflict with humans, however, as they can damage crops and invade buildings. Some species, such as the red imported fire ant ("Solenopsis invicta"), are regarded as invasive species, establishing themselves in areas where they have been introduced accidentally.
Etymology.
The word "ant" is derived from ', ' of Middle English which are derived from ' of Old English, and is related to the dialectal Dutch ' and the Old High German ', hence the modern German '. All of these words come from West Germanic "*", and the original meaning of the word was "the biter" (from Proto-Germanic "*", "off, away" + "*" "cut"). The family name Formicidae is derived from the Latin ' ("ant") from which the words in other Romance languages, such as the Portuguese ', Italian ', Spanish ', Romanian ', and French ' are derived. It has been hypothesised that a Proto-Indo-European word *morwi- was used, cf. Sanskrit vamrah, Latin formīca, Greek μύρμηξ "mýrmēx", Old Church Slavonic "mraviji", Old Irish "moirb", Old Norse "maurr", Dutch "mier".
Taxonomy and evolution.
The family Formicidae belongs to the order Hymenoptera, which also includes sawflies, bees, and wasps. Ants evolved from a lineage within the aculeate wasps, and a 2013 study suggests they are a sister group of the Apoidea. In 1966, E. O. Wilson and his colleagues identified the fossil remains of an ant ("Sphecomyrma") that lived in the Cretaceous period. The specimen, trapped in amber dating back to around 92 million years ago, has features found in some wasps, but not found in modern ants. "Sphecomyrma" possibly was a ground forager, while "Haidomyrmex" and "Haidomyrmodes", related genera in subfamily Sphecomyrminae, are reconstructed as active arboreal predators. After the rise of flowering plants about 100 million years ago they diversified and assumed ecological dominance around 60 million years ago. Some groups such as the Leptanillinae and Martialinae are suggested to have diversified from early primitive ants which were likely to have been predators underneath the surface of the soil.
During the Cretaceous period, a few species of primitive ants ranged widely on the Laurasian supercontinent (the Northern Hemisphere). They were scarce in comparison to the populations of other insects, representing only about 1% of the entire insect population. Ants became dominant after adaptive radiation at the beginning of the Paleogene period. By the Oligocene and Miocene, ants had come to represent 20–40% of all insects found in major fossil deposits. Of the species that lived in the Eocene epoch, around one in 10 genera survive to the present. Genera surviving today comprise 56% of the genera in Baltic amber fossils (early Oligocene), and 92% of the genera in Dominican amber fossils (apparently early Miocene).
Termites, although sometimes called 'white ants', are not ants. They belong to the order Isoptera. Termites are more closely related to cockroaches and mantids. Termites are eusocial, but differ greatly in the genetics of reproduction. The similarity of their social structure to that of ants is attributed to convergent evolution. Velvet ants look like large ants, but are wingless female wasps.
Distribution and diversity.
Ants are found on all continents except Antarctica, and only a few large islands such as Greenland, Iceland, parts of Polynesia and the Hawaiian Islands lack native ant species. Ants occupy a wide range of ecological niches, and are able to exploit a wide range of food resources either as direct or indirect herbivores, predators, and scavengers. Most species are omnivorous generalists, but a few are specialist feeders. Their ecological dominance may be measured by their biomass and estimates in different environments suggest that they contribute 15–20% (on average and nearly 25% in the tropics) of the total terrestrial animal biomass, which exceeds that of the vertebrates.
Ants range in size from , the largest species being the fossil "Titanomyrma giganteum", the queen of which was long with a wingspan of . Ants vary in colour; most ants are red or black, but a few species are green and some tropical species have a metallic lustre. More than 12,000 species are currently known (with upper estimates of the potential existence of about 22,000) (see the article List of ant genera), with the greatest diversity in the tropics. Taxonomic studies continue to resolve the classification and systematics of ants. Online databases of ant species, including AntBase and the Hymenoptera Name Server, help to keep track of the known and newly described species. The relative ease with which ants may be sampled and studied in ecosystems has made them useful as indicator species in biodiversity studies.
Morphology.
Ants are distinct in their morphology from other insects in having elbowed antennae, metapleural glands, and a strong constriction of their second abdominal segment into a node-like petiole. The head, mesosoma, and metasoma are the three distinct body segments. The petiole forms a narrow waist between their mesosoma (thorax plus the first abdominal segment, which is fused to it) and gaster (abdomen less the abdominal segments in the petiole). The petiole may be formed by one or two nodes (the second alone, or the second and third abdominal segments).
Like other insects, ants have an exoskeleton, an external covering that provides a protective casing around the body and a point of attachment for muscles, in contrast to the internal skeletons of humans and other vertebrates. Insects do not have lungs; oxygen and other gases such as carbon dioxide pass through their exoskeleton via tiny valves called spiracles. Insects also lack closed blood vessels; instead, they have a long, thin, perforated tube along the top of the body (called the "dorsal aorta") that functions like a heart, and pumps haemolymph toward the head, thus driving the circulation of the internal fluids. The nervous system consists of a ventral nerve cord that runs the length of the body, with several ganglia and branches along the way reaching into the extremities of the appendages.
Head.
An ant's head contains many sensory organs. Like most insects, ants have compound eyes made from numerous tiny lenses attached together. Ant eyes are good for acute movement detection, but do not offer a high resolution image. They also have three small ocelli (simple eyes) on the top of the head that detect light levels and polarization. Compared to vertebrates, most ants have poor-to-mediocre eyesight and a few subterranean species are completely blind. Some ants such as Australia's bulldog ant, however, have excellent vision and are capable of discriminating the distance and size of objects moving nearly a metre away.
Two antennae ("feelers") are attached to the head; these organs detect chemicals, air currents, and vibrations; they also are used to transmit and receive signals through touch. The head has two strong jaws, the mandibles, used to carry food, manipulate objects, construct nests, and for defence. In some species a small pocket (infrabuccal chamber) inside the mouth stores food, so it may be passed to other ants or their larvae.
Legs.
All six legs are attached to the mesosoma ("thorax"). A hooked claw at the end of each leg helps ants to climb and to hang onto surfaces.
Wings.
Only reproductive ants, queens and males, have wings. Queens shed the wings after the nuptial flight, leaving visible stubs, a distinguishing feature of queens. Wingless queens (ergatoids) and males occur in a few species, however.
Metasoma.
The metasoma (the "abdomen") of the ant houses important internal organs, including those of the reproductive, respiratory (tracheae), and excretory systems. Workers of many species have their egg-laying structures modified into stings that are used for subduing prey and defending their nests.
Polymorphism.
In the colonies of a few ant species, there are physical castes—workers in distinct size-classes, called minor, median, and major workers. Often the larger ants have disproportionately larger heads, and correspondingly stronger mandibles. Such individuals sometimes are called "soldier" ants because their stronger mandibles make them more effective in fighting, although they still are workers and their "duties" typically do not vary greatly from the minor or median workers. In a few species the median workers are absent, creating a sharp divide between the minors and majors. Weaver ants, for example, have a distinct bimodal size distribution. Some other species show continuous variation in the size of workers. The smallest and largest workers in "Pheidologeton diversus" show nearly a 500-fold difference in their dry-weights. Workers cannot mate; however, because of the haplodiploid sex-determination system in ants, workers of a number of species can lay unfertilised eggs that become fully fertile, haploid males. The role of workers may change with their age and in some species, such as honeypot ants, young workers are fed until their gasters are distended, and act as living food storage vessels. These food storage workers are called "repletes". For instance, these replete workers develop in the North American honeypot ant "Myrmecocystus mexicanus". Rissing found that usually the largest workers in the colony develop into repletes, and if repletes are removed from the colony other workers become repletes, demonstrating the flexibility of this particular polymorphism. This polymorphism in morphology and behaviour of workers initially was thought to be determined by environmental factors such as nutrition and hormones that led to different developmental paths; however, genetic differences between worker castes have been noted in "Acromyrmex" sp. These polymorphisms are caused by relatively small genetic changes; differences in a single gene of "Solenopsis invicta" can decide whether the colony will have single or multiple queens. The Australian jack jumper ant ("Myrmecia pilosula") has only a single pair of chromosomes (with the males having just one chromosome as they are haploid), the lowest number known for any animal, making it an interesting subject for studies in the genetics and developmental biology of social insects.
Development and reproduction.
The life of an ant starts from an egg. If the egg is fertilised, the progeny will be female (diploid); if not, it will be male (haploid). Ants develop by complete metamorphosis with the larva stages passing through a pupal stage before emerging as an adult. The larva is largely immobile and is fed and cared for by workers.
Food is given to the larvae by trophallaxis, a process in which an ant regurgitates liquid food held in its crop. This is also how adults share food, stored in the "social stomach". Larvae may also be provided with solid food such as trophic eggs, pieces of prey, and seeds brought back by foraging workers and the larvae may even be transported directly to captured prey in some species. 
The larvae grow through a series of moults and enter the pupal stage. The pupa has the appendages free and not fused to the body as in a butterfly pupa. The differentiation into queens and workers (which are both female), and different castes of workers (when they exist), is influenced in some species by the nutrition the larvae obtain. Genetic influences and the control of gene expression by the developmental environment are complex and the determination of caste continues to be a subject of research. Larvae and pupae need to be kept at fairly constant temperatures to ensure proper development, and so often, are moved around among the various brood chambers within the colony.
A new worker spends the first few days of its adult life caring for the queen and young. She then graduates to digging and other nest work, and later to defending the nest and foraging. These changes are sometimes fairly sudden, and define what are called temporal castes. An explanation for the sequence is suggested by the high casualties involved in foraging, making it an acceptable risk only for ants who are older and are likely to die soon of natural causes.
Most ant species have a system in which only the queen and breeding females have the ability to mate. Contrary to popular belief, some ant nests have multiple queens while others may exist without queens. Workers with the ability to reproduce are called "gamergates" and colonies that lack queens are then called gamergate colonies; colonies with queens are said to be queen-right. The winged male ants, called drones, emerge from pupae along with the breeding females (although some species, such as army ants, have wingless queens), and do nothing in life except eat and mate.
Most ants are univoltine, producing a new generation each year. During the species-specific breeding period, new reproductives, females and winged males leave the colony in what is called a nuptial flight. Typically, the males take flight before the females. Males then use visual cues to find a common mating ground, for example, a landmark such as a pine tree to which other males in the area converge. Males secrete a mating pheromone that females follow. Females of some species mate with just one male, but in some others they may mate with as many as ten or more different males.
Mated females then seek a suitable place to begin a colony. There, they break off their wings and begin to lay and care for eggs.
The females store the sperm they obtain during their nuptial flight to selectively fertilise future eggs.
The first workers to hatch are weak and smaller than later workers, but they begin to serve the colony immediately. They enlarge the nest, forage for food, and care for the other eggs. This is how new colonies start in most ant species. Species that have multiple queens may have a queen leaving the nest along with some workers to found a colony at a new site, a process akin to swarming in honeybees.
A wide range of reproductive strategies have been noted in ant species. Females of many species are known to be capable of reproducing asexually through thelytokous parthenogenesis and one species, "Mycocepurus smithii", is known to be all-female.
Ant colonies can be long-lived. The queens can live for up to 30 years, and workers live from 1 to 3 years. Males, however, are more transitory, being quite short-lived and surviving for only a few weeks. Ant queens are estimated to live 100 times longer than solitary insects of a similar size.
Ants are active all year long in the tropics, but, in cooler regions, they survive the winter in a state of dormancy or inactivity. The forms of inactivity are varied and some temperate species have larvae going into the inactive state, (diapause), while in others, the adults alone pass the winter in a state of reduced activity.
Behaviour and ecology.
Communication.
Ants communicate with each other using pheromones, sounds, and touch. The use of pheromones as chemical signals is more developed in ants, such as the red harvester ant, than in other hymenopteran groups. Like other insects, ants perceive smells with their long, thin, and mobile antennae. The paired antennae provide information about the direction and intensity of scents. Since most ants live on the ground, they use the soil surface to leave pheromone trails that may be followed by other ants. In species that forage in groups, a forager that finds food marks a trail on the way back to the colony; this trail is followed by other ants, these ants then reinforce the trail when they head back with food to the colony. When the food source is exhausted, no new trails are marked by returning ants and the scent slowly dissipates. This behaviour helps ants deal with changes in their environment. For instance, when an established path to a food source is blocked by an obstacle, the foragers leave the path to explore new routes. If an ant is successful, it leaves a new trail marking the shortest route on its return. Successful trails are followed by more ants, reinforcing better routes and gradually identifying the best path.
Ants use pheromones for more than just making trails. A crushed ant emits an alarm pheromone that sends nearby ants into an attack frenzy and attracts more ants from farther away. Several ant species even use "propaganda pheromones" to confuse enemy ants and make them fight among themselves. Pheromones are produced by a wide range of structures including Dufour's glands, poison glands and glands on the hindgut, pygidium, rectum, sternum, and hind tibia. Pheromones also are exchanged, mixed with food, and passed by trophallaxis, transferring information within the colony. This allows other ants to detect what task group ("e.g.", foraging or nest maintenance) other colony members belong to. In ant species with queen castes, when the dominant queen stops producing a specific pheromone, workers begin to raise new queens in the colony.
Some ants produce sounds by stridulation, using the gaster segments and their mandibles. Sounds may be used to communicate with colony members or with other species.
Defence.
Ants attack and defend themselves by biting and, in many species, by stinging, often injecting or spraying chemicals such as formic acid in the case of formicine ants, alkaloids and piperidines in fire ants, and a variety of protein components in other ants. Bullet ants ("Paraponera"), located in Central and South America, are considered to have the most painful sting of any insect, although it is usually not fatal to humans. This sting is given the highest rating on the Schmidt Sting Pain Index.
The sting of jack jumper ants can be fatal, and an antivenom has been developed for it.
Fire ants, "Solenopsis" spp., are unique in having a poison sac containing piperidine alkaloids. Their stings are painful and can be dangerous to hypersensitive people.
Trap-jaw ants of the genus "Odontomachus" are equipped with mandibles called trap-jaws, which snap shut faster than any other predatory appendages within the animal kingdom. One study of "Odontomachus bauri" recorded peak speeds of between 126 and 230 km/h (78 – 143 mph), with the jaws closing within 130 microseconds on average.
The ants were also observed to use their jaws as a catapult to eject intruders or fling themselves backward to escape a threat. Before striking, the ant opens its mandibles extremely widely and locks them in this position by an internal mechanism. Energy is stored in a thick band of muscle and explosively released when triggered by the stimulation of sensory organs resembling hairs on the inside of the mandibles. The mandibles also permit slow and fine movements for other tasks. Trap-jaws also are seen in the following genera: "Anochetus", "Orectognathus", and "Strumigenys", plus some members of the Dacetini tribe, which are viewed as examples of convergent evolution.
A Malaysian species of ant in the "Camponotus" "cylindricus" group has enlarged mandibular glands that extend into their gaster. When disturbed, workers rupture the membrane of the gaster, causing a burst of secretions containing acetophenones and other chemicals that immobilise small insect attackers. The worker subsequently dies.
Suicidal defences by workers are also noted in a Brazilian ant, "Forelius pusillus", where a small group of ants leaves the security of the nest after sealing the entrance from the outside each evening.
In addition to defence against predators, ants need to protect their colonies from pathogens. Some worker ants maintain the hygiene of the colony and their activities include undertaking or "necrophory", the disposal of dead nest-mates. Oleic acid has been identified as the compound released from dead ants that triggers necrophoric behaviour in "Atta mexicana" while workers of "Linepithema humile" react to the absence of characteristic chemicals (dolichodial and iridomyrmecin) present on the cuticle of their living nestmates to trigger similar behaviour.
Nests may be protected from physical threats such as flooding and overheating by elaborate nest architecture. Workers of "Cataulacus muticus", an arboreal species that lives in plant hollows, respond to flooding by drinking water inside the nest, and excreting it outside. "Camponotus anderseni", which nests in the cavities of wood in mangrove habitats, deals with submergence under water by switching to anaerobic respiration.
Learning.
Many animals can learn behaviours by imitation, but ants may be the only group apart from mammals where interactive teaching has been observed. A knowledgeable forager of "Temnothorax albipennis" will lead a naive nest-mate to newly discovered food by the process of tandem running. The follower obtains knowledge through its leading tutor. The leader is acutely sensitive to the progress of the follower and slows down when the follower lags and speeds up when the follower gets too close.
Controlled experiments with colonies of "Cerapachys biroi" suggest that an individual may choose nest roles based on her previous experience. An entire generation of identical workers was divided into two groups whose outcome in food foraging was controlled. One group was continually rewarded with prey, while it was made certain that the other failed. As a result, members of the successful group intensified their foraging attempts while the unsuccessful group ventured out fewer and fewer times. A month later, the successful foragers continued in their role while the others had moved to specialise in brood care.
Nest construction.
Complex nests are built by many ant species, but other species are nomadic and do not build permanent structures. Ants may form subterranean nests or build them on trees. These nests may be found in the ground, under stones or logs, inside logs, hollow stems, or even acorns. The materials used for construction include soil and plant matter, and ants carefully select their nest sites; "Temnothorax albipennis" will avoid sites with dead ants, as these may indicate the presence of pests or disease. They are quick to abandon established nests at the first sign of threats.
The army ants of South America, such as the "Eciton burchellii" species, and the driver ants of Africa do not build permanent nests, but instead, alternate between nomadism and stages where the workers form a temporary nest (bivouac) from their own bodies, by holding each other together.
Weaver ant ("Oecophylla" spp.) workers build nests in trees by attaching leaves together, first pulling them together with bridges of workers and then inducing their larvae to produce silk as they are moved along the leaf edges. Similar forms of nest construction are seen in some species of "Polyrhachis".
"Formica polyctena", among other ant species, constructs nests that maintain a relatively constant interior temperature that aids in the development of larvae. The ants maintain the nest temperature by choosing the location, nest materials, controlling ventilation and maintaining the heat from solar radiation, worker activity and metabolism, and in some moist nests, microbial activity in the nest materials.
Some ant species such as those that use natural cavities can be opportunistic and make use of the controlled micro-climate provided inside human dwellings and other artificial structures to house their colonies and nest structures.
Cultivation of food.
Most ants are generalist predators, scavengers, and indirect herbivores, but a few have evolved specialised ways of obtaining nutrition. Leafcutter ants ("Atta" and "Acromyrmex") feed exclusively on a fungus that grows only within their colonies. They continually collect leaves which are taken to the colony, cut into tiny pieces and placed in fungal gardens. Workers specialise in related tasks according to their sizes. The largest ants cut stalks, smaller workers chew the leaves and the smallest tend the fungus. Leafcutter ants are sensitive enough to recognise the reaction of the fungus to different plant material, apparently detecting chemical signals from the fungus. If a particular type of leaf is found to be toxic to the fungus, the colony will no longer collect it. The ants feed on structures produced by the fungi called "gongylidia". Symbiotic bacteria on the exterior surface of the ants produce antibiotics that kill bacteria introduced into the nest that may harm the fungi.
Navigation.
Foraging ants travel distances of up to from their nest and scent trails allow them to find their way back even in the dark. In hot and arid regions, day-foraging ants face death by desiccation, so the ability to find the shortest route back to the nest reduces that risk. Diurnal desert ants of the genus "Cataglyphis" such as the Sahara desert ant navigate by keeping track of direction as well as distance travelled. Distances travelled are measured using an internal pedometer that keeps count of the steps taken and also by evaluating the movement of objects in their visual field (optical flow). Directions are measured using the position of the sun.
They integrate this information to find the shortest route back to their nest.
Like all ants, they can also make use of visual landmarks when available as well as olfactory and tactile cues to navigate. Some species of ant are able to use the Earth's magnetic field for navigation. The compound eyes of ants have specialised cells that detect polarised light from the Sun, which is used to determine direction.
These polarization detectors are sensitive in the ultraviolet region of the light spectrum. In some army ant species, a group of foragers who become separated from the main column sometimes may turn back on themselves and form a circular ant mill. The workers may then run around continuously until they die of exhaustion.
Locomotion.
The female worker ants do not have wings and reproductive females lose their wings after their mating flights in order to begin their colonies. Therefore, unlike their wasp ancestors, most ants travel by walking. Some species are capable of leaping. For example, Jerdon's jumping ant ("Harpegnathos saltator") is able to jump by synchronising the action of its mid and hind pairs of legs. There are several species of gliding ant including "Cephalotes atratus"; this may be a common trait among most arboreal ants. Ants with this ability are able to control the direction of their descent while falling.
Other species of ants can form chains to bridge gaps over water, underground, or through spaces in vegetation. Some species also form floating rafts that help them survive floods. These rafts may also have a role in allowing ants to colonise islands. "Polyrhachis sokolova", a species of ant found in Australian mangrove swamps, can swim and live in underwater nests. Since they lack gills, they go to trapped pockets of air in the submerged nests to breathe.
Cooperation and competition.
Not all ants have the same kind of societies. The Australian bulldog ants are among the biggest and most basal of ants. Like virtually all ants, they are eusocial, but their social behaviour is poorly developed compared to other species. Each individual hunts alone, using her large eyes instead of chemical senses to find prey.
Some species (such as "Tetramorium caespitum") attack and take over neighbouring ant colonies. Others are less expansionist, but just as aggressive; they invade colonies to steal eggs or larvae, which they either eat or raise as workers or slaves. Extreme specialists among these slave-raiding ants, such as the Amazon ants, are incapable of feeding themselves and need captured workers to survive. Captured workers of the enslaved species "Temnothorax" have evolved a counter strategy, destroying just the female pupae of the slave-making "Protomognathus americanus", but sparing the males (who don't take part in slave-raiding as adults).
Ants identify kin and nestmates through their scent, which comes from hydrocarbon-laced secretions that coat their exoskeletons. If an ant is separated from its original colony, it will eventually lose the colony scent. Any ant that enters a colony without a matching scent will be attacked. Also, the reason why two separate colonies of ants will attack each other even if they are of the same species is because the genes responsible for pheromone production are different between them. The Argentine ant, however, does not have this characteristic, due to lack of genetic diversity, and has become a global pest because of it.
Parasitic ant species enter the colonies of host ants and establish themselves as social parasites; species such as "Strumigenys xenos" are entirely parasitic and do not have workers, but instead, rely on the food gathered by their "Strumigenys perplexa" hosts. This form of parasitism is seen across many ant genera, but the parasitic ant is usually a species that is closely related to its host. A variety of methods are employed to enter the nest of the host ant. A parasitic queen may enter the host nest before the first brood has hatched, establishing herself prior to development of a colony scent. Other species use pheromones to confuse the host ants or to trick them into carrying the parasitic queen into the nest. Some simply fight their way into the nest.
A conflict between the sexes of a species is seen in some species of ants with these reproducers apparently competing to produce offspring that are as closely related to them as possible. The most extreme form involves the production of clonal offspring. An extreme of sexual conflict is seen in "Wasmannia auropunctata", where the queens produce diploid daughters by thelytokous parthenogenesis and males produce clones by a process whereby a diploid egg loses its maternal contribution to produce haploid males who are clones of the father.
Relationships with other organisms.
Ants form symbiotic associations with a range of species, including other ant species, other insects, plants, and fungi. They also are preyed on by many animals and even certain fungi. Some arthropod species spend part of their lives within ant nests, either preying on ants, their larvae, and eggs, consuming the food stores of the ants, or avoiding predators. These inquilines may bear a close resemblance to ants. The nature of this ant mimicry (myrmecomorphy) varies, with some cases involving Batesian mimicry, where the mimic reduces the risk of predation. Others show Wasmannian mimicry, a form of mimicry seen only in inquilines.
Aphids and other hemipteran insects secrete a sweet liquid called honeydew, when they feed on plant sap. The sugars in honeydew are a high-energy food source, which many ant species collect. In some cases, the aphids secrete the honeydew in response to ants tapping them with their antennae. The ants in turn keep predators away from the aphids and will move them from one feeding location to another. When migrating to a new area, many colonies will take the aphids with them, to ensure a continued supply of honeydew. Ants also tend mealybugs to harvest their honeydew. Mealybugs may become a serious pest of pineapples if ants are present to protect mealybugs from their natural enemies.
Myrmecophilous (ant-loving) caterpillars of the butterfly family Lycaenidae (e.g., blues, coppers, or hairstreaks) are herded by the ants, led to feeding areas in the daytime, and brought inside the ants' nest at night. The caterpillars have a gland which secretes honeydew when the ants massage them. Some caterpillars produce vibrations and sounds that are perceived by the ants. Other caterpillars have evolved from ant-loving to ant-eating: these myrmecophagous caterpillars secrete a pheromone that makes the ants act as if the caterpillar is one of their own larvae. The caterpillar is then taken into the ant nest where it feeds on the ant larvae.
Fungus-growing ants that make up the tribe Attini, including leafcutter ants, cultivate certain species of fungus in the "Leucoagaricus" or "Leucocoprinus" genera of the Agaricaceae family. In this ant-fungus mutualism, both species depend on each other for survival. The ant "Allomerus decemarticulatus" has evolved a three-way association with the host plant, "Hirtella physophora" (Chrysobalanaceae), and a sticky fungus which is used to trap their insect prey.
Lemon ants make devil's gardens by killing surrounding plants with their stings and leaving a pure patch of lemon ant trees, ("Duroia hirsuta"). This modification of the forest provides the ants with more nesting sites inside the stems of the "Duroia" trees. Although some ants obtain nectar from flowers, pollination by ants is somewhat rare. Some plants have special nectar exuding structures, extrafloral nectaries that provide food for ants, which in turn protect the plant from more damaging herbivorous insects. Species such as the bullhorn acacia ("Acacia cornigera") in Central America have hollow thorns that house colonies of stinging ants ("Pseudomyrmex ferruginea") who defend the tree against insects, browsing mammals, and epiphytic vines. Isotopic labelling studies suggest that plants also obtain nitrogen from the ants. In return, the ants obtain food from protein- and lipid-rich Beltian bodies. Another example of this type of ectosymbiosis comes from the "Macaranga" tree, which has stems adapted to house colonies of "Crematogaster" ants.
Many tropical tree species have seeds that are dispersed by ants. Seed dispersal by ants or myrmecochory is widespread and new estimates suggest that nearly 9% of all plant species may have such ant associations. Some plants in fire-prone grassland systems are particularly dependent on ants for their survival and dispersal as the seeds are transported to safety below the ground. Many ant-dispersed seeds have special external structures, elaiosomes, that are sought after by ants as food.
A convergence, possibly a form of mimicry, is seen in the eggs of stick insects. They have an edible elaiosome-like structure and are taken into the ant nest where the young hatch.
Most ants are predatory and some prey on and obtain food from other social insects including other ants. Some species specialise in preying on termites ("Megaponera" and "Termitopone") while a few Cerapachyinae prey on other ants. Some termites, including "Nasutitermes corniger", form associations with certain ant species to keep away predatory ant species. The tropical wasp "Mischocyttarus drewseni" coats the pedicel of its nest with an ant-repellent chemical. It is suggested that many tropical wasps may build their nests in trees and cover them to protect themselves from ants. Stingless bees ("Trigona" and "Melipona") use chemical defences against ants.
Flies in the Old World genus "Bengalia" (Calliphoridae) prey on ants and are kleptoparasites, snatching prey or brood from the mandibles of adult ants. Wingless and legless females of the Malaysian phorid fly ("Vestigipoda myrmolarvoidea") live in the nests of ants of the genus "Aenictus" and are cared for by the ants.
Fungi in the genera "Cordyceps" and "Ophiocordyceps" infect ants. Ants react to their infection by climbing up plants and sinking their mandibles into plant tissue. The fungus kills the ants, grows on their remains, and produces a fruiting body. It appears that the fungus alters the behaviour of the ant to help disperse its spores in a microhabitat that best suits the fungus. Strepsipteran parasites also manipulate their ant host to climb grass stems, to help the parasite find mates.
A nematode ("Myrmeconema neotropicum") that infects canopy ants ("Cephalotes atratus") causes the black-coloured gasters of workers to turn red. The parasite also alters the behaviour of the ant, causing them to carry their gasters high. The conspicuous red gasters are mistaken by birds for ripe fruits such as "Hyeronima alchorneoides" and eaten. The droppings of the bird are collected by other ants and fed to their young, leading to further spread of the nematode.
South American poison dart frogs in the genus "Dendrobates" feed mainly on ants, and the toxins in their skin may come from the ants.
Army ants forage in a wide roving column, attacking any animals in that path that are unable to escape. In Central and South America, "Eciton burchellii" is the swarming ant most commonly attended by "ant-following" birds such as antbirds and woodcreepers. This behaviour was once considered mutualistic, but later studies found the birds to be parasitic. Although direct kleptoparasitism (birds stealing food from the ants' grasp) is rare, the birds eat many prey insects that the ants would otherwise eat and thus decrease their foraging success. Birds indulge in a peculiar behaviour called anting that, as yet, is not fully understood. Here birds rest on ant nests, or pick and drop ants onto their wings and feathers; this may be a means to remove ectoparasites from the birds.
Anteaters, aardvarks, pangolins, echidnas and numbats have special adaptations for living on a diet of ants. These adaptations include long, sticky tongues to capture ants and strong claws to break into ant nests. Brown bears ("Ursus arctos") have been found to feed on ants. About 12%, 16%, and 4% of their faecal volume in spring, summer, and autumn, respectively, is composed of ants.
Relationship with humans.
Ants perform many ecological roles that are beneficial to humans, including the suppression of pest populations and aeration of the soil. The use of weaver ants in citrus cultivation in southern China is considered one of the oldest known applications of biological control. On the other hand, ants may become nuisances when they invade buildings, or cause economic losses.
In some parts of the world (mainly Africa and South America), large ants, especially army ants, are used as surgical sutures. The wound is pressed together and ants are applied along it. The ant seizes the edges of the wound in its mandibles and locks in place. The body is then cut off and the head and mandibles remain in place to close the wound.
Some ants have toxic venom and are of medical importance. The species include "Paraponera clavata" (tocandira) and "Dinoponera" spp. (false tocandiras) of South America and the "Myrmecia" ants of Australia.
In South Africa, ants are used to help harvest rooibos ("Aspalathus linearis"), which are small seeds used to make a herbal tea. The plant disperses its seeds widely, making manual collection difficult. Black ants collect and store these and other seeds in their nest, where humans can gather them "en masse". Up to half a pound (200 g) of seeds may be collected from one ant-heap.
Although most ants survive attempts by humans to eradicate them, a few are highly endangered. These tend to be island species that have evolved specialized traits and risk being displaced by introduced ant species. Examples include the critically endangered Sri Lankan relict ant ("Aneuretus simoni") and "Adetomyrma venatrix" of Madagascar.
It has been estimated by E.O. Wilson that the total number of individual ants alive in the world at any one time is between one and ten quadrillion (short scale) (i.e. between 1015 and 1016). According to this estimate, the total biomass of all the ants in the world is approximately equal to the total biomass of the entire human race. Also, according to this estimate, there are approximately 1 million ants for every human on Earth.
As food.
Ants and their larvae are eaten in different parts of the world. The eggs of two species of ants are used in Mexican "escamoles". They are considered a form of insect caviar and can sell for as much as USD 40 per pound (USD 90/kg) because they are seasonal and hard to find. In the Colombian department of Santander, "hormigas culonas" (roughly interpreted as "large-bottomed ants") "Atta laevigata" are toasted alive and eaten.
In areas of India, and throughout Burma and Thailand, a paste of the green weaver ant ("Oecophylla smaragdina") is served as a condiment with curry. Weaver ant eggs and larvae, as well as the ants, may be used in a Thai salad, "yam" (), in a dish called "yam khai mot daeng" () or red ant egg salad, a dish that comes from the Issan or north-eastern region of Thailand. Saville-Kent, in the "Naturalist in Australia" wrote "Beauty, in the case of the green ant, is more than skin-deep. Their attractive, almost sweetmeat-like translucency possibly invited the first essays at their consumption by the human species". Mashed up in water, after the manner of lemon squash, "these ants form a pleasant acid drink which is held in high favor by the natives of North Queensland, and is even appreciated by many European palates".
In his "First Summer in the Sierra", John Muir notes that the Digger Indians of California ate the tickling, acid gasters of the large jet-black carpenter ants. The Mexican Indians eat the replete workers, or living honey-pots, of the honey ant ("Myrmecocystus").
As pests.
Some ant species are considered as pests. The presence of ants can be undesirable in places meant to be sterile. They can also come in the way of humans by their habit of raiding stored food, damaging indoor structures, causing damage to agricultural crops either directly or by aiding sucking pests or because of their stings and bites. The adaptive nature of ant colonies make it nearly impossible to eliminate entire colonies and most pest management practices aim to control local populations and tend to be temporary solutions.
Some of the ants classified as pests include the pavement ant, yellow crazy ant, sugar ants, the Pharaoh ant, carpenter ants, Argentine ant, odorous house ants, red imported fire ant, and European fire ant. Ant populations are managed by a combination of approaches that make use of chemical, biological and physical methods. Chemical methods include the use of insecticidal bait which is gathered by ants as food and brought back to the nest where the poison is inadvertently spread to other colony members through trophallaxis. Management is based on the species and techniques can vary according to the location and circumstance.
In science and technology.
Observed by humans since the dawn of history, the behaviour of ants has been documented and the subject of early writings and fables passed from one century to another. Those using scientific methods, myrmecologists, study ants in the laboratory and in their natural conditions. Their complex and variable social structures have made ants ideal model organisms. Ultraviolet vision was first discovered in ants by Sir John Lubbock in 1881. Studies on ants have tested hypotheses in ecology and sociobiology, and have been particularly important in examining the predictions of theories of kin selection and evolutionarily stable strategies. Ant colonies may be studied by rearing or temporarily maintaining them in "formicaria", specially constructed glass framed enclosures. Individuals may be tracked for study by marking them with dots of colours.
The successful techniques used by ant colonies have been studied in computer science and robotics to produce distributed and fault-tolerant systems for solving problems, for example Ant colony optimization and Ant robotics. This area of biomimetics has led to studies of ant locomotion, search engines that make use of "foraging trails", fault-tolerant storage, and networking algorithms.
In culture.
Anthropomorphised ants have often been used in fables and children's stories to represent industriousness and cooperative effort. They also are mentioned in religious texts. In the Book of Proverbs in the Bible, ants are held up as a good example for humans for their hard work and cooperation. Aesop did the same in his fable The Ant and the Grasshopper. In the Quran, Sulayman is said to have heard and understood an ant warning other ants to return home to avoid being accidentally crushed by Sulayman and his marching army. In parts of Africa, ants are considered to be the messengers of the deities. Some Native American mythology, such as the Hopi mythology, considers ants as the very first animals. Ant bites are often said to have curative properties. The sting of some species of "Pseudomyrmex" is claimed to give fever relief. Ant bites are used in the initiation ceremonies of some Amazon Indian cultures as a test of endurance.
Ant society has always fascinated humans and has been written about both humorously and seriously. Mark Twain wrote about ants in his 1880 book "A Tramp Abroad". Some modern authors have used the example of the ants to comment on the relationship between society and the individual. Examples are Robert Frost in his poem "Departmental" and T. H. White in his fantasy novel "The Once and Future King". The plot in French entomologist and writer Bernard Werber's "Les Fourmis" science-fiction trilogy is divided between the worlds of ants and humans; ants and their behaviour is described using contemporary scientific knowledge. H.G. Wells wrote about intelligent ants destroying human settlements in Brazil and threatening human civilization in his 1905 science-fiction short story, "The Empire of the Ants." In more recent times, animated cartoons and 3-D animated films featuring ants have been produced including "Antz", "A Bug's Life", "The Ant Bully", "The Ant and the Aardvark", and "Atom Ant." Renowned myrmecologist E. O. Wilson wrote a short story, "Trailhead" in 2010 for "The New Yorker" magazine, which describes the life and death of an ant-queen and the rise and fall of her colony, from an ants' point of view.
From the late 1950s through the late 1970s, ant farms were popular educational children's toys in the United States. Later versions use transparent gel instead of soil, allowing greater visibility. In the early 1990s, the video game SimAnt, which simulated an ant colony, won the 1992 Codie award for "Best Simulation Program".
Ants also are quite popular inspiration for many science-fiction insectoids, such as the Formics of "Ender's Game", the Bugs of "Starship Troopers", the giant ants in the films "Them!" and "Empire of the Ants," Marvel Comics' super hero Ant-Man, and ants mutated into super-intelligence in "Phase IV". In computer strategy games, ant-based species often benefit from increased production rates due to their single-minded focus, such as the Klackons in the "Master of Orion" series of games or the ChCht in "Deadlock II". These characters are often credited with a hive mind, a common misconception about ant colonies.

</doc>
